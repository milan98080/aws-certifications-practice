{
  "exam_id": 24,
  "total_pages_attempted": 73,
  "successful_pages": 73,
  "failed_pages": 0,
  "total_questions": 363,
  "extraction_date": "2025-12-24T14:21:21.855Z",
  "extraction_method": "console_scraper_api_v1",
  "questions": [
    {
      "question_id": "Ewx6l0aGI0hnvrfKFCi2",
      "question_number": 1,
      "page": 1,
      "question_text": "A company has a mobile application that makes HTTP API calls to an Application Load Balancer (ALB). The ALB routes requests to an AWS Lambda function. Many different versions of the application are in use at any given time, including versions that are in testing by a subset of users. The version of the application is defined in the user-agent header that is sent with all requests to the API.\nAfter a series of recent changes to the API, the company has observed issues with the application. The company needs to gather a metric for each API operation by response code for each version of the application that is in use. A DevOps engineer has modified the Lambda function to extract the API operation name, version information from the user-agent header and response code.\nWhich additional set of actions should the DevOps engineer take to gather the required metrics?",
      "choices": {
        "B": "Modify the Lambda function to write the API operation name, response code, and version number as a log line to an Amazon CloudWatch Logs log group. Configure a CloudWatch Logs Insights query to populate CloudWatch metrics from the log lines. Specify response code and application version as dimensions for the metric.",
        "A": "Modify the Lambda function to write the API operation name, response code, and version number as a log line to an Amazon CloudWatch Logs log group. Configure a CloudWatch Logs metric filter that increments a metric for each API operation name. Specify response code and application version as dimensions for the metric.",
        "C": "Configure the ALB access logs to write to an Amazon CloudWatch Logs log group. Modify the Lambda function to respond to the ALB with the API operation name, response code, and version number as response metadata. Configure a CloudWatch Logs metric filter that increments a metric for each API operation name. Specify response code and application version as dimensions for the metric.",
        "D": "Configure AWS X-Ray integration on the Lambda function. Modify the Lambda function to create an X-Ray subsegment with the API operation name, response code, and version number. Configure X-Ray insights to extract an aggregated metric for each API operation name and to publish the metric to Amazon CloudWatch. Specify response code and application version as dimensions for the metric."
      },
      "correct_answer": "A",
      "answer_ET": "A",
      "answers_community": [
        "A (88%)",
        "7%"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/105229-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-04-05 01:22:00",
      "unix_timestamp": 1680650520,
      "discussion_count": 36,
      "discussion": [
        {
          "poster": "jake99",
          "timestamp": "1752224100.0",
          "upvote_count": "7",
          "content": "Selected Answer: A\nA is the Correct Option\nI’ve tried a few mock test platforms, but SkillCertExams stood out. Their content is top-notch and very similar to what you see on the actual exam.",
          "comment_id": "1585504"
        },
        {
          "comments": [
            {
              "upvote_count": "1",
              "comment_id": "1577089",
              "poster": "vovahem310",
              "content": "valid information is here -> https://bitly.cx/sHea",
              "timestamp": "1749792540.0"
            }
          ],
          "content": "Selected Answer: A\nThe other options are either incomplete or involve unnecessary complexity:\n\nOption B requires using CloudWatch Logs Insights queries, which may introduce additional complexity and potential performance overhead.\nOption C involves modifying the ALB access logs, which may not provide the required level of granularity or flexibility for capturing the application version information.\nOption D requires integrating AWS X-Ray, which is primarily designed for distributed tracing and may not be necessary for this specific use case of gathering metrics by response code and application version.\n\nTherefore, option A is the most appropriate and straightforward solution for the given requirements.",
          "timestamp": "1727009940.0",
          "poster": "c3518fc",
          "upvote_count": "7",
          "comment_id": "1209003"
        },
        {
          "upvote_count": "1",
          "poster": "kamoke5287",
          "content": "Selected Answer: A\nA is valid answer and that's is valid discussion thanks https://bitly.cx/STaJ",
          "comment_id": "1576266",
          "timestamp": "1749553560.0"
        },
        {
          "content": "Selected Answer: A\nWhy Option A is Correct:\nIt directly addresses the requirement to gather metrics for each API operation by response code and application version.\nCloudWatch Logs and metric filters are designed for this purpose and are easy to implement.\nIt provides a scalable and cost-effective solution.",
          "poster": "festusbetd",
          "timestamp": "1740081480.0",
          "comment_id": "1359443",
          "upvote_count": "1"
        },
        {
          "poster": "maikerusukofyirudo",
          "upvote_count": "1",
          "content": "Selected Answer: A\nThat’s right",
          "comment_id": "1319068",
          "timestamp": "1732778880.0"
        },
        {
          "upvote_count": "1",
          "timestamp": "1732502460.0",
          "poster": "chan123",
          "comment_id": "1317297",
          "content": "Selected Answer: D\ndsfdsf"
        },
        {
          "comment_id": "1159035",
          "upvote_count": "3",
          "timestamp": "1708887420.0",
          "poster": "nothinmuch",
          "content": "Selected Answer: B\nThe answer is b based on the scenario. \n\nWhen to Choose Which\n\nChoose A (Metric Filters) if you need basic metrics with straightforward patterns (e.g., counting occurrences of specific API operations and response codes).\n\nChoose B (Insights Queries) if you require more complex metric calculations, such as:\nAggregations (averages, sums, etc.) over time\nFiltering metrics based on conditions within the logs\nCreating custom metrics not directly defined in log lines"
        },
        {
          "poster": "Gowtham5798",
          "upvote_count": "1",
          "timestamp": "1706698440.0",
          "content": "The correct answer is A.",
          "comment_id": "1136696"
        },
        {
          "comment_id": "1133780",
          "poster": "thanhnv142",
          "timestamp": "1706410200.0",
          "upvote_count": "1",
          "content": "A is correct: Because only need to gather metric but not parse log or advanced analyzing the log."
        },
        {
          "upvote_count": "1",
          "content": "A definitely",
          "timestamp": "1706327640.0",
          "comment_id": "1133047",
          "poster": "thanhnv142"
        },
        {
          "comment_id": "1110948",
          "timestamp": "1704077640.0",
          "content": "Cloudwatch log metrics filter can apply on pattern and populate metrics, CW insight significant to get it. B should be correct one",
          "poster": "hoakhanh281",
          "upvote_count": "1"
        },
        {
          "poster": "d262e67",
          "upvote_count": "3",
          "timestamp": "1703808660.0",
          "content": "Selected Answer: A\nCloudWatch Logs Insights is a powerful tool for analyzing log data, but it's more suited for ad-hoc exploration and troubleshooting rather than continuous metric collection and monitoring.",
          "comment_id": "1108202"
        },
        {
          "upvote_count": "1",
          "comment_id": "1094605",
          "timestamp": "1702390680.0",
          "poster": "DucSiu",
          "content": "The company needs to gather a metric for each API operation by response code for each version of the application that is in use\n=> A"
        },
        {
          "content": "Option A seems to be the most straightforward and effective method. By writing the required information as a log line to CloudWatch Logs and configuring a metric filter to increment metrics based on this data, the company can efficiently gather the metrics it needs with minimal complexity. This approach leverages existing AWS services in a simple and direct manner, aligning well with the company's requirements.",
          "upvote_count": "3",
          "poster": "wem",
          "timestamp": "1700329440.0",
          "comment_id": "1074186"
        },
        {
          "timestamp": "1698934680.0",
          "content": "Answer A looks pretty good except one thing \n\"Configure a CloudWatch Logs metric filter that increments a metric for each API operation name.\"\nCan the metric filter \"increment\" the metric or just send metric value?\nAnswer B looks weird to me - I don't know the way to populate Logs Insights query result as the CW metric",
          "comment_id": "1060612",
          "upvote_count": "2",
          "poster": "DevopsCircus"
        },
        {
          "timestamp": "1698336180.0",
          "poster": "zain1258",
          "content": "Selected Answer: A\nThe right option is A. In the question, the requirement is to get a metric. In option B, we are not creating any metric.",
          "comment_id": "1054736",
          "upvote_count": "1"
        },
        {
          "timestamp": "1697118060.0",
          "poster": "sivre",
          "upvote_count": "1",
          "content": "For me its B. From the question \"The company needs to gather a metric for each API operation by response code for each version of the application that is in use\". Only CloudWatch Logs Insight is querying for response code and application version. CloudWatch Logs metric filter is using API operation name to create an aggregate metric",
          "comment_id": "1041772"
        },
        {
          "upvote_count": "2",
          "poster": "rbm2023",
          "timestamp": "1696866720.0",
          "comment_id": "1038797",
          "content": "Selected Answer: A\nIt should be between A and B but the thing is the difference use cases between CW Logs Metric and CW Logs Insights. Metrics will provide Aggregated data while insights give you direct access to raw log event data. Hence I would go for A"
        },
        {
          "poster": "Aestebance",
          "content": "Selected Answer: A\nMore efficient",
          "timestamp": "1693981140.0",
          "upvote_count": "1",
          "comment_id": "1000244"
        },
        {
          "poster": "addapmr",
          "content": "real response : A",
          "comment_id": "997615",
          "timestamp": "1693745040.0",
          "upvote_count": "1"
        },
        {
          "timestamp": "1693518540.0",
          "content": "Selected Answer: A\nOption A: This option is the most efficient way to gather the required metrics. It does not require any additional infrastructure and can be easily implemented.\nOption B: This option is more complex than Option A and requires configuring a CloudWatch Logs Insights query. This can be more time-consuming to set up and can be less efficient if the query is not optimized.\nOption C: This option requires configuring the ALB access logs to write to CloudWatch Logs. This can add additional latency to the requests.\nOption D: This option requires configuring AWS X-Ray integration. This is a more complex solution that is not necessary in this case.",
          "poster": "BaburTurk",
          "comment_id": "995505",
          "upvote_count": "5"
        },
        {
          "poster": "habros",
          "timestamp": "1688468760.0",
          "content": "Selected Answer: A\nA is correct. For event-based tracking, metric filters in CW are more automatic. Insights query need some intervention still. Devops = AUTOMATION. \n\nX-Ray is used extensively for development, especially backend APIs. By production it should already had resolved any API health issues.",
          "upvote_count": "2",
          "comment_id": "942655"
        },
        {
          "timestamp": "1687577040.0",
          "upvote_count": "1",
          "poster": "mywogunleye",
          "comment_id": "932130",
          "content": "Option A is the correct Answer, \n\"Note that the metric filter is different from a log insights query, where the experience is interactive and provides immediate search results for the user to investigate. No automatic action can be invoked from an insights query. Metric filters, on the other hand, will generate metric data in the form of a time series. This lets you create alarms that integrate into your ITSM processes, execute AWS Lambda functions, or even create anomaly detection models.\"\nhttps://aws.amazon.com/blogs/mt/quantify-custom-application-metrics-with-amazon-cloudwatch-logs-and-metric-filters/"
        },
        {
          "upvote_count": "1",
          "comment_id": "929089",
          "timestamp": "1687325220.0",
          "poster": "King01",
          "content": "Option A is correct because it uses the metric filter."
        },
        {
          "timestamp": "1686527820.0",
          "comment_id": "921038",
          "upvote_count": "1",
          "poster": "SanChan",
          "content": "B while it is possible to use CloudWatch Logs Insights to generate metrics from log data, it is not the most efficient or straightforward approach for this particular use case. Therefore, option A, which suggests using CloudWatch metric filters to generate metrics directly from the log data, is a better and more appropriate solution."
        },
        {
          "timestamp": "1686527700.0",
          "poster": "SanChan",
          "content": "I think correct answer is option A: Modify the Lambda function to write the API operation name, response code, and version number as a log line to an Amazon CloudWatch Logs log group. Configure a CloudWatch Logs metric filter that increments a metric for each API operation name. Specify response code and application version as dimensions for the metric.\n\nOption A provides a straightforward way to collect the necessary metrics by using CloudWatch Logs to capture the relevant information and then using a metric filter to aggregate the data into the required format. Option B suggests using CloudWatch Logs Insights query to populate CloudWatch metrics, which is a more complex approach than necessary. Option C suggests using ALB access logs to write to CloudWatch Logs and returning metadata from the Lambda function, which may be more complicated to implement. Option D suggests using X-Ray integration, which is a viable option for tracing and performance analysis, but it is not the best fit for this use case.",
          "comment_id": "921036",
          "upvote_count": "2"
        },
        {
          "timestamp": "1686150480.0",
          "poster": "Manny20",
          "content": "Option A is the most suitable choice for gathering the required metrics. By modifying the Lambda function to write the necessary information to CloudWatch Logs, you can track the API operation name, response code, and version number. To generate metrics from the log lines, you can configure a CloudWatch Logs metric filter that increments a metric for each API operation name. The response code and application version can be specified as dimensions for the metric, allowing you to group and analyze the data based on these attributes.",
          "comment_id": "917331",
          "upvote_count": "1"
        },
        {
          "content": "CloudWatch Logs Insights query can't populate CloudWatch metrics.\nThe correct answer is A",
          "comment_id": "915947",
          "upvote_count": "1",
          "timestamp": "1686031920.0",
          "poster": "kacsabacsi78"
        },
        {
          "content": "Selected Answer: D\nD. AWS X-Ray allows you to analyze and debug your applications. It provides insights into the behavior of your applications. Moreover, X-Ray supports adding annotations and metadata to traces. Annotations are indexed for use with filter expressions and can be viewed in the AWS X-Ray console and API, which could be used to extract metrics. CloudWatch can ingest metrics from AWS X-Ray, allowing you to create and monitor custom metrics.",
          "comment_id": "913924",
          "upvote_count": "1",
          "timestamp": "1685828220.0",
          "poster": "jjkcoins"
        },
        {
          "poster": "madperro",
          "content": "Selected Answer: A\nOption A - use metric filter and API version, operation, return code as dimensions.\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/MonitoringPolicyExamples.html",
          "upvote_count": "2",
          "comment_id": "909918",
          "timestamp": "1685424600.0"
        },
        {
          "comment_id": "870437",
          "content": "Option A is the correct answer, as the other options do not provide an effective way to gather the required metrics. Launching instances in a public subnet with Elastic IP addresses attached or setting up a NAT gateway in a private subnet do not address the need to gather the required metrics. Publishing application artifacts to an S3 bucket and creating a VPC endpoint for S3 would not be relevant for gathering the required metrics either. Creating a security group for the application instances and allowing only outbound traffic to the artifact repository also does not address the need to gather the required metrics.",
          "timestamp": "1681501320.0",
          "upvote_count": "1",
          "poster": "alce2020"
        },
        {
          "upvote_count": "1",
          "poster": "jqso234",
          "comment_id": "870349",
          "content": "A is correct",
          "timestamp": "1681492140.0"
        },
        {
          "comment_id": "863661",
          "upvote_count": "1",
          "timestamp": "1680856920.0",
          "poster": "ele",
          "content": "Selected Answer: A\nagree, it;s A"
        },
        {
          "comment_id": "862431",
          "upvote_count": "1",
          "timestamp": "1680723600.0",
          "poster": "Dimidrol",
          "content": "Selected Answer: A\nA for me"
        },
        {
          "timestamp": "1680656280.0",
          "upvote_count": "3",
          "comment_id": "861650",
          "content": "Selected Answer: A\nOption A is the correct choice to gather the required metrics. The DevOps engineer can modify the Lambda function to write the API operation name, response code, and version number as a log line to an Amazon CloudWatch Logs log group. Then, a CloudWatch Logs metric filter can be configured to increment a metric for each API operation name, with response code and application version specified as dimensions for the metric.",
          "poster": "5aga"
        },
        {
          "comment_id": "861593",
          "content": "B or D",
          "timestamp": "1680650520.0",
          "poster": "lqpO_Oqpl",
          "upvote_count": "2"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:08:42.772Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "W96YqJMwx9o9XW6TbBGN",
      "question_number": 2,
      "page": 1,
      "question_text": "A security review has identified that an AWS CodeBuild project is downloading a database population script from an Amazon S3 bucket using an unauthenticated request. The security team does not allow unauthenticated requests to S3 buckets for this project.\nHow can this issue be corrected in the MOST secure manner?",
      "choices": {
        "D": "Remove unauthenticated access from the S3 bucket with a bucket policy. Use the AWS CLI to download the database population script using an IAM access key and a secret access key.",
        "C": "Remove unauthenticated access from the S3 bucket with a bucket policy. Modify the service role for the CodeBuild project to include Amazon S3 access. Use the AWS CLI to download the database population script.",
        "B": "Modify the S3 bucket settings to enable HTTPS basic authentication and specify a token. Update the build spec to use cURL to pass the token and download the database population script.",
        "A": "Add the bucket name to the AllowedBuckets section of the CodeBuild project settings. Update the build spec to use the AWS CLI to download the database population script."
      },
      "correct_answer": "C",
      "answer_ET": "C",
      "answers_community": [
        "C (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/105505-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-04-07 14:24:00",
      "unix_timestamp": 1680870240,
      "discussion_count": 10,
      "discussion": [
        {
          "content": "C is correct:\n+ Remove unauthenticated access from the S3 bucket with a bucket policy\n+ Modify the service role for the CodeBuild project to include Amazon S3 access.",
          "timestamp": "1706369760.0",
          "poster": "thanhnv142",
          "upvote_count": "6",
          "comment_id": "1133452"
        },
        {
          "timestamp": "1722793800.0",
          "poster": "namtp",
          "upvote_count": "1",
          "content": "Selected Answer: C\nC is a correct answer.\nInside AWS, using of service roles is the best option.",
          "comment_id": "1260754"
        },
        {
          "timestamp": "1702961280.0",
          "comment_id": "1100296",
          "upvote_count": "3",
          "poster": "z_inderjot",
          "content": "Selected Answer: C\nall these questions seem fairly to be part of aws devops exam"
        },
        {
          "upvote_count": "1",
          "timestamp": "1698338640.0",
          "poster": "zain1258",
          "comment_id": "1054768",
          "content": "Selected Answer: C\nC is correct"
        },
        {
          "upvote_count": "2",
          "content": "Selected Answer: C\nInvolves using a service role also, which make it the most secure manner",
          "timestamp": "1698234840.0",
          "comment_id": "1053689",
          "poster": "Cervus18"
        },
        {
          "content": "Selected Answer: C\nC is the correct answer because it involves removing unauthenticated access from the S3 bucket with a bucket policy, which ensures that only authorized users or services can access the bucket.",
          "poster": "SanChan",
          "upvote_count": "4",
          "timestamp": "1686545700.0",
          "comment_id": "921127"
        },
        {
          "upvote_count": "1",
          "timestamp": "1686218760.0",
          "content": "Selected Answer: C\nC is the best answer.",
          "comment_id": "918100",
          "poster": "madperro"
        },
        {
          "poster": "alce2020",
          "upvote_count": "2",
          "content": "c is the answer",
          "timestamp": "1681502880.0",
          "comment_id": "870449"
        },
        {
          "timestamp": "1681149600.0",
          "content": "c is the answer.",
          "upvote_count": "1",
          "comment_id": "866505",
          "poster": "ataince"
        },
        {
          "upvote_count": "1",
          "comment_id": "863856",
          "content": "Selected Answer: C\nC most secure",
          "poster": "ele",
          "timestamp": "1680870240.0"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:08:42.772Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "dRTDrGk8mrPqte2p5xW9",
      "question_number": 3,
      "page": 1,
      "question_text": "A DevOps engineer manages a large commercial website that runs on Amazon EC2. The website uses Amazon Kinesis Data Streams to collect and process web logs. The DevOps engineer manages the Kinesis consumer application, which also runs on Amazon EC2.\n\nSudden increases of data cause the Kinesis consumer application to fall behind, and the Kinesis data streams drop records before the records can be processed. The DevOps engineer must implement a solution to improve stream handling.\n\nWhich solution meets these requirements with the MOST operational efficiency?",
      "choices": {
        "C": "Convert the Kinesis consumer application to run as an AWS Lambda function. Configure the Kinesis data streams as the event source for the Lambda function to process the data streams.",
        "A": "Modify the Kinesis consumer application to store the logs durably in Amazon S3. Use Amazon EMR to process the data directly on Amazon S3 to derive customer insights. Store the results in Amazon S3.",
        "B": "Horizontally scale the Kinesis consumer application by adding more EC2 instances based on the Amazon CloudWatch GetRecords.IteratorAgeMilliseconds metric. Increase the retention period of the Kinesis data streams.",
        "D": "Increase the number of shards in the Kinesis data streams to increase the overall throughput so that the consumer application processes the data faster."
      },
      "correct_answer": "B",
      "answer_ET": "B",
      "answers_community": [
        "B (67%)",
        "C (30%)",
        "3%"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/108805-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-05-09 14:19:00",
      "unix_timestamp": 1683634740,
      "discussion_count": 35,
      "discussion": [
        {
          "upvote_count": "41",
          "timestamp": "1689587820.0",
          "poster": "emupsx1",
          "comments": [
            {
              "poster": "youonebe",
              "comment_id": "1329734",
              "upvote_count": "3",
              "content": "Remember, only 50 of the questions are graded. \nFirst, there is no way what you said is true.\nSecond, your answer might not be graded. \nCorrect answer is a firm C.",
              "timestamp": "1734734340.0"
            },
            {
              "timestamp": "1695202800.0",
              "comment_id": "1012165",
              "poster": "[Removed]",
              "content": "Were there any other questions from here in the exam?",
              "upvote_count": "2"
            },
            {
              "timestamp": "1700898960.0",
              "content": "First of all Congratulations.\nNow how do you know that this question was not from those 10 questions that do not count towards your score. and your answer to this question was Wrong but not counted towards your score. Just Saying!!\nPeace :)",
              "comment_id": "1079818",
              "comments": [
                {
                  "poster": "Jaguaroooo",
                  "timestamp": "1704797280.0",
                  "comment_id": "1117412",
                  "upvote_count": "5",
                  "content": "B is the Answer, let him show off, it is ok."
                }
              ],
              "poster": "yorkicurke",
              "upvote_count": "1"
            }
          ],
          "comment_id": "954054",
          "content": "The answer is B because:\nA few hours ago, I just finished the DOP-C02 exam.\nMy score is 1000 points.\nThis question has come up, I choose B."
        },
        {
          "upvote_count": "7",
          "poster": "yorkicurke",
          "timestamp": "1700899380.0",
          "comment_id": "1079822",
          "content": "Selected Answer: B\nwhy not C?\nbecause we just replace ONE ec2 with ONE lambda here. And no mention of aws lambda reserved concurrency or provisioned concurrency.\nIn the question were are asked for 'MOST operational efficiency'. that's my two cents.\nCiao"
        },
        {
          "content": "Selected Answer: C\nB. Scaling EC2 instances based on metrics is valid but requires manual infrastructure management, which is less efficient than serverless.\nC is correct:\nAWS Lambda integrates natively with Kinesis Data Streams and automatically scales with the number of shards.\nIt removes the need to manage EC2 instances, reducing operational overhead.\nLambda handles parallel processing of stream records and automatically polls the stream.\nYou can configure batch size and parallelization factor to optimize throughput.\nBuilt-in checkpointing ensures records are not lost or reprocessed unnecessarily.\nThis approach provides high scalability, low maintenance, and cost efficiency, which aligns with the requirement for operational efficiency.",
          "upvote_count": "1",
          "poster": "nickp84",
          "timestamp": "1747307880.0",
          "comment_id": "1569027"
        },
        {
          "timestamp": "1742212680.0",
          "content": "Selected Answer: B\nThe GetRecords.IteratorAgeMilliseconds metric in Amazon Kinesis Data Streams measures the age of the last record in all GetRecords calls made against a Kinesis stream. This age is calculated as the difference between the current time and when the last record of the GetRecords call was written to the stream.\n\nThis metric is crucial for monitoring the latency of your Kinesis consumer applications. If the IteratorAgeMilliseconds value is increasing, it could indicate issues such as slow record processing, read throttles, or connection timeouts. Monitoring this metric helps ensure that your consumer applications are processing records in a timely manner and not falling behind.",
          "poster": "DKM",
          "upvote_count": "1",
          "comment_id": "1399625"
        },
        {
          "content": "Selected Answer: B\nOption B because:\n1. It uses GetRecords.IteratorAgeMilliseconds metric\n - This metric indicates how far behind the consumer is processing\n - Perfect indicator for when scaling is needed\n - Automatically detects processing lag\n2. Horizontal scaling of EC2 consumers\n - Adds processing capacity when needed\n - Can scale down when demand decreases\n - Maintains existing application architecture\n 3. Increasing retention period\n - Provides buffer against temporary processing delays\n - Prevents data loss during scaling events\n - Default is 24 hours, can be increased up to 365 days",
          "timestamp": "1738795020.0",
          "poster": "ce0df07",
          "upvote_count": "1",
          "comment_id": "1352111"
        },
        {
          "poster": "2d943d1",
          "comment_id": "1344137",
          "content": "Selected Answer: C\nI think most people are missing the part where the question focuses on MOST operational efficiency. Lambda fits this purpose, as ww are not managing instances and Lambda can scale to meet throughput demands.",
          "upvote_count": "1",
          "timestamp": "1737462000.0"
        },
        {
          "timestamp": "1735009680.0",
          "upvote_count": "1",
          "poster": "ZinggieG87",
          "comment_id": "1331009",
          "content": "Selected Answer: D\nA, sounds a lot effort towards a different architecture solution. \nB, I don't think the insufficient kinesis throughput has can be resolved.\nC, adding lambda is a big change, no mention about the concurrency limit isn't clear mentioned in the question."
        },
        {
          "poster": "youonebe",
          "content": "Selected Answer: C\nAnswer is C.\n\nB - This solution improves the consumer application’s ability to handle increased throughput, but it still requires manual scaling of EC2 instances, which can involve some complexity and operational overhead.\nC - Using Lambda is obviously more Operational Efficient.",
          "upvote_count": "1",
          "comment_id": "1329731",
          "timestamp": "1734734220.0"
        },
        {
          "comment_id": "1232080",
          "content": "Selected Answer: B\nI think B makes more sence, because \"the Kinesis data streams drop records before the records can be processed\". It's not an intake throughput issue that needs more shards, but an outtake throughput issue.\n\"Consumer Record Processing Falling Behind\"\n\"After you identify how far behind your consumers are reading, look at the most common reasons why consumers fall behind. Start with the GetRecords.IteratorAgeMilliseconds metric, which tracks the read position across all shards and consumers in the stream. Note that if an iterator's age passes 50% of the retention period (by default, 24 hours...), there is risk for data loss due to record expiration.\"\n\"A quick stopgap solution is to increase the retention period.\"\n[...]\n\"An alternative approach is to increase your parallelism by increasing the number of shards.\"\n\"Finally, confirm you have an adequate amount of physical resources (memory, CPU utilization, etc.) on the underlying processing nodes during peak demand.\"\nhttps://docs.aws.amazon.com/streams/latest/dev/troubleshooting-consumers.html",
          "timestamp": "1718652060.0",
          "poster": "Gomer",
          "upvote_count": "3"
        },
        {
          "content": "B is a good choice\nThe GetRecords.IteratorAgeMilliseconds metric in Amazon CloudWatch for Amazon Kinesis Data Streams measures the age of the last record returned by the GetRecords operation. Specifically, it represents the time difference between the current time and the approximate arrival timestamp of the last record processed by a consumer in milliseconds.\n\nPurpose: Measures the delay in processing data records from the time they are added to the stream to the time they are processed by a consumer.\n\nScaling: If you observe high iterator age values, consider increasing the number of shards or enhancing the processing capacity of your consumers (e.g., adding more instances or increasing the processing power of existing instances).",
          "upvote_count": "1",
          "comment_id": "1226284",
          "poster": "zijo",
          "timestamp": "1717779540.0"
        },
        {
          "content": "Selected Answer: B\nI choose B",
          "comment_id": "1205450",
          "timestamp": "1714646400.0",
          "poster": "seetpt",
          "upvote_count": "1"
        },
        {
          "content": "Selected Answer: B\nConsumer Record Processing Falling Behind\n\nFor most use cases, consumer applications are reading the latest data from the stream. In certain circumstances, consumer reads may fall behind, which may not be desired. After you identify how far behind your consumers are reading, look at the most common reasons why consumers fall behind.\n\nStart with the GetRecords.IteratorAgeMilliseconds metric, which tracks the read position across all shards and consumers in the stream. Note that if an iterator's age passes 50% of the retention period (by default, 24 hours, configurable up to 365 days), there is risk for data loss due to record expiration. A quick stopgap solution is to increase the retention period. This stops the loss of important data while you troubleshoot the issue further. https://docs.aws.amazon.com/streams/latest/dev/troubleshooting-consumers.html#record-processing-falls-behind",
          "comment_id": "1198036",
          "poster": "c3518fc",
          "timestamp": "1713458640.0",
          "upvote_count": "4"
        },
        {
          "content": "B\n GetRecords.IteratorAgeMilliseconds metric : its for track the progress of Kinese consumer, this cloud watch matric is use for the measuring the difference between current time and when the last record of GetRecords calls written to the stream. IteratorAgeMilliseconds metric is 0 means is processing fast enough and if its >0 its slow in processing. Therefore, Horizontally scale the Kinesis consumer application by adding more EC2 instances based on the Amazon CloudWatch GetRecords.IteratorAgeMilliseconds metric. Increase the retention period of the Kinesis data streams.",
          "poster": "Shasha1",
          "upvote_count": "2",
          "timestamp": "1709549880.0",
          "comment_id": "1165517"
        },
        {
          "upvote_count": "2",
          "content": "Selected Answer: C\noption C (Converting the Kinesis consumer application to run as an AWS Lambda function) is the most suitable solution. This approach automatically scales with the amount of incoming data, reduces the operational burden of managing EC2 instances, and leverages the serverless model to only incur costs for the actual compute time used for processing the data. This option provides a scalable, efficient, and cost-effective solution to the problem without the need for extensive infrastructure management.",
          "timestamp": "1708087620.0",
          "comment_id": "1151997",
          "poster": "kyuhuck"
        },
        {
          "upvote_count": "1",
          "timestamp": "1706978760.0",
          "poster": "thanhnv142",
          "comment_id": "1139470",
          "content": "B is correct: <consumer application to fall behind> means we need to increase the power of the consumer. <Kinesis data streams drop records> means we should extend timeout. Only B match these requirements\nA: irrelevant\nC: Lambda is used only for short-lived tasks because its maximum execution time is 15 min. In this case, we need to process web logs. This is a time-consuming task, which is not suitable for Lambda\nD: No mention of increasing Consumer power"
        },
        {
          "timestamp": "1701648480.0",
          "comment_id": "1087217",
          "poster": "svjl",
          "upvote_count": "3",
          "content": "B & D are correct. However, the question is looking for a solution for two issues\n\"application to fall behind, and the Kinesis data streams drop records before the records can be processed\"\nThen, B is the most appropriate solution"
        },
        {
          "timestamp": "1699191660.0",
          "poster": "2pk",
          "upvote_count": "1",
          "content": "B is the answer: The data fall beind due to lack of pysical resources at consumer side. Icrease of more nodes will address this issue. https://docs.aws.amazon.com/streams/latest/dev/troubleshooting-consumers.html#record-processing-falls-behind",
          "comment_id": "1062929"
        },
        {
          "poster": "buenos",
          "content": "Selected Answer: B\nI would say B",
          "timestamp": "1696074240.0",
          "comment_id": "1021423",
          "upvote_count": "2"
        },
        {
          "comment_id": "1014425",
          "poster": "Dushank",
          "upvote_count": "1",
          "timestamp": "1695410040.0",
          "content": "Selected Answer: D\nD. Increase the number of shards in the Kinesis data streams to increase the overall throughput so that the consumer application processes the data faster.\n\nHere's the rationale for choosing this option:\n\nIncreasing Shards for Throughput:\nBy increasing the number of shards in the Kinesis data streams, you increase the overall throughput and the capacity to handle sudden increases in data. This directly addresses the issue of the consumer application falling behind during data spikes.\n\nOperational Efficiency:\nScaling the shards provides a more straightforward and efficient solution in terms of operation compared to modifying the consumer application, horizontally scaling instances, or converting the application to run as an AWS Lambda function."
        },
        {
          "comment_id": "994938",
          "upvote_count": "1",
          "poster": "Seoyong",
          "timestamp": "1693471020.0",
          "content": "Selected Answer: C\nB is not correct. it manually scale EC2 instances.\nC is operationally efficiency ."
        },
        {
          "content": "Selected Answer: B\nThe answer is B",
          "comment_id": "991345",
          "poster": "Radeeka",
          "upvote_count": "2",
          "timestamp": "1693128780.0"
        },
        {
          "content": "Selected Answer: B\nB, Scaling based on GetRecords.IteratorAgeMilliseconds is right. GetRecords.IteratorAgeMilliseconds - The age of the last record in all GetRecords calls made against a Kinesis stream, measured over the specified time period. Age is the difference between the current time and when the last record of the GetRecords call was written to the stream. The Minimum and Maximum statistics can be used to track the progress of Kinesis consumer applications. A value of zero indicates that the records being read are completely caught up with the stream.",
          "comment_id": "990492",
          "timestamp": "1693020840.0",
          "upvote_count": "2",
          "poster": "ProfXsamson"
        },
        {
          "content": "Selected Answer: B\nLambda has some limitations like the execution time(15min), cocurrency limit.",
          "poster": "gigi_devops",
          "upvote_count": "4",
          "comment_id": "969593",
          "timestamp": "1690945320.0"
        },
        {
          "timestamp": "1690389780.0",
          "poster": "haazybanj",
          "content": "Selected Answer: C\nC. Converting the Kinesis consumer application to run as an AWS Lambda function is a highly efficient solution. AWS Lambda automatically scales based on the number of incoming events, so it can easily handle sudden increases in data without manual intervention. Additionally, AWS Lambda manages the underlying infrastructure, eliminating the need for the DevOps engineer to manage EC2 instances and their scaling.\n\nB. Horizontally scaling the Kinesis consumer application by adding more EC2 instances based on the GetRecords.IteratorAgeMilliseconds metric and increasing the retention period of the Kinesis data streams might improve the consumer application's ability to catch up with sudden data increases. However, it might not be the most efficient solution, as it requires manual intervention and additional resource management.",
          "upvote_count": "4",
          "comment_id": "963988"
        },
        {
          "comment_id": "950034",
          "poster": "Just_Ninja",
          "content": "Selected Answer: B\nOption A: Storing the logs durably in Amazon S3 and using Amazon EMR to process the data adds complexity and potential cost. It also may not address the core issue of the Kinesis consumer application not keeping up with incoming data.\n\nOption C: AWS Lambda functions have some limitations (e.g., execution time, payload size, concurrency limits) which can create bottlenecks in processing large data streams.\n\nOption D: Increasing the number of shards in the Kinesis data stream may increase throughput, but it doesn't necessarily address the problem if the consumer application isn't able to process the data quickly enough. Moreover, this approach might incur higher costs.",
          "upvote_count": "2",
          "timestamp": "1689181980.0"
        },
        {
          "content": "Selected Answer: B\nI will go with B",
          "timestamp": "1688554140.0",
          "comment_id": "943596",
          "upvote_count": "2",
          "poster": "Blueee"
        },
        {
          "timestamp": "1687693080.0",
          "poster": "FunkyFresco",
          "upvote_count": "1",
          "content": "Selected Answer: C\nI will choose option C.",
          "comment_id": "933526"
        },
        {
          "timestamp": "1687205820.0",
          "poster": "allen_devops",
          "upvote_count": "2",
          "comment_id": "927889",
          "content": "I think B is correct. Operational Efficiency doesn't mean the best solution. In this case, they have develop the app using EC2 and lack autoscaling. The option B will be the simplest change to fulfil the requirement."
        },
        {
          "comment_id": "925842",
          "content": "Selected Answer: B\nI'll go with option B",
          "upvote_count": "3",
          "poster": "ducluanxutrieu",
          "timestamp": "1686987840.0"
        },
        {
          "content": "Selected Answer: B\nI think the current flow is lack scaling.\nSo adding scaling will solve this problem.\nBy using GetRecords.IteratorAgeMilliseconds metric, you can track the progress of Kinesis Consumers and scale them.\nSo B.",
          "upvote_count": "3",
          "poster": "rhinozD",
          "comment_id": "924951",
          "timestamp": "1686900120.0"
        },
        {
          "content": "Selected Answer: C\nC, with the MOST operational efficiency.",
          "comment_id": "902372",
          "poster": "PhuocT",
          "upvote_count": "4",
          "timestamp": "1684561680.0"
        },
        {
          "upvote_count": "2",
          "comment_id": "897900",
          "poster": "2pk",
          "content": "Selected Answer: B\nYes B, \nbecause it directly addresses the issue of the Kinesis consumer application falling behind due to sudden increases of data. By horizontally scaling the application with more EC2 instances, the application can handle more data and keep up with the stream. Increasing the retention period of the data streams ensures that the consumer application has more time to process the data.\nthe reason why not D, because increasing the number of shards in the Kinesis data streams will not necessarily solve the problem of the Kinesis consumer application falling behind.",
          "timestamp": "1684101780.0"
        },
        {
          "timestamp": "1684055160.0",
          "upvote_count": "1",
          "poster": "devnv",
          "content": "B is the right answer",
          "comment_id": "897423"
        },
        {
          "content": "Selected Answer: B\nB is correct",
          "comment_id": "894890",
          "upvote_count": "2",
          "timestamp": "1683797220.0",
          "poster": "ParagSanyashiv"
        },
        {
          "comment_id": "893078",
          "poster": "Jeanphi72",
          "content": "Selected Answer: C\nI think it is C",
          "timestamp": "1683634740.0",
          "upvote_count": "3"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:08:42.772Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "8Vacj5gdbGsjzLcSfpfH",
      "question_number": 4,
      "page": 1,
      "question_text": "A company recently created a new AWS Control Tower landing zone in a new organization in AWS Organizations. The landing zone must be able to demonstrate compliance with the Center for Internet Security (CIS) Benchmarks for AWS Foundations.\n\nThe company’s security team wants to use AWS Security Hub to view compliance across all accounts. Only the security team can be allowed to view aggregated Security Hub findings. In addition, specific users must be able to view findings from their own accounts within the organization. All accounts must be enrolled in Security Hub after the accounts are created.\n\nWhich combination of steps will meet these requirements in the MOST automated way? (Choose three.)",
      "choices": {
        "F": "In the organization’s management account, create an Amazon EventBridge rule that reacts to the CreateManagedAccount event. Create an AWS Lambda function that uses the Security Hub CreateMembers API operation to add new accounts to Security Hub. Configure the EventBridge rule to invoke the Lambda function.",
        "B": "Turn on trusted access for Security Hub in the organization’s management account. From the management account, provide Security Hub with the CIS Benchmarks for AWS Foundations standards.",
        "C": "Create an AWS IAM Identity Center (AWS Single Sign-On) permission set that includes the required permissions. Use the CreateAccountAssignment API operation to associate the security team users with the permission set and with the delegated security account.",
        "E": "In Security Hub, turn on automatic enablement.",
        "A": "Turn on trusted access for Security Hub in the organization’s management account. Create a new security account by using AWS Control Tower. Configure the new security account as the delegated administrator account for Security Hub. In the new security account, provide Security Hub with the CIS Benchmarks for AWS Foundations standards.",
        "D": "Create an SCP that explicitly denies any user who is not on the security team from accessing Security Hub."
      },
      "correct_answer": "ACE",
      "answer_ET": "ACE",
      "answers_community": [
        "ACE (74%)",
        "ADE (19%)",
        "7%"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/109130-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-05-13 09:36:00",
      "unix_timestamp": 1683963360,
      "discussion_count": 18,
      "discussion": [
        {
          "upvote_count": "17",
          "timestamp": "1689587880.0",
          "comment_id": "954057",
          "content": "The answer is ACE because:\nA few hours ago, I just finished the DOP-C02 exam.\nMy score is 1000 points.\nThis question has come up, I choose ACE.",
          "poster": "emupsx1",
          "comments": [
            {
              "content": "bot account, Pics or it didn't happen,",
              "comments": [
                {
                  "timestamp": "1718653860.0",
                  "comment_id": "1232092",
                  "poster": "Gomer",
                  "upvote_count": "3",
                  "content": "Either a bot or a bot for brains. Same useless comments made on multiple questions."
                }
              ],
              "comment_id": "1002996",
              "poster": "BaburTurk",
              "timestamp": "1694242500.0",
              "upvote_count": "7"
            }
          ]
        },
        {
          "poster": "nickp84",
          "timestamp": "1747308240.0",
          "upvote_count": "1",
          "content": "Selected Answer: ACE\nD. An SCP that denies access to Security Hub is too broad and could interfere with legitimate access by account owners who need to see their own findings.",
          "comment_id": "1569028"
        },
        {
          "timestamp": "1733121120.0",
          "upvote_count": "1",
          "poster": "eugene2owl",
          "comment_id": "1320809",
          "content": "Selected Answer: ADE\nI prefer \"D\" over \"C\", because no-one asks to enable SSO (which is very complex to organise and maintain)"
        },
        {
          "poster": "auxwww",
          "upvote_count": "4",
          "timestamp": "1721865600.0",
          "comment_id": "1254628",
          "content": "Selected Answer: ACE\nA - Only security team needs access to findings org wide - hence delegated account\nC - Allow security team members access to delegated account for Security hub using Identity center of control tower\nE - Each new account needs security hub for it's own users to access and also for aggregation across org"
        },
        {
          "upvote_count": "3",
          "content": "Automatic enablement in AWS Security Hub refers to the feature that allows AWS Security Hub to be automatically enabled for new and existing AWS accounts that are part of an organization in AWS Organizations. This feature simplifies the process of onboarding multiple accounts into Security Hub, ensuring consistent security posture and compliance across the organization.",
          "poster": "zijo",
          "comment_id": "1227982",
          "timestamp": "1718035500.0"
        },
        {
          "content": "Selected Answer: ACE\nACE is correct",
          "upvote_count": "3",
          "timestamp": "1714646460.0",
          "comment_id": "1205451",
          "poster": "seetpt"
        },
        {
          "timestamp": "1713256260.0",
          "poster": "didek1986",
          "upvote_count": "2",
          "comments": [],
          "comment_id": "1196470",
          "content": "Selected Answer: ACF\nACF\nE - ensures that all new accounts are automatically enrolled in Security Hub (same as F) but it does not address the requirement for specific users to view findings from their own accounts"
        },
        {
          "comment_id": "1196469",
          "content": "ACF\nE - ensures that all new accounts are automatically enrolled in Security Hub (same as F) but it does not address the requirement for specific users to view findings from their own accounts",
          "timestamp": "1713256200.0",
          "poster": "didek1986",
          "upvote_count": "1"
        },
        {
          "timestamp": "1713070260.0",
          "poster": "dkp",
          "content": "Selected Answer: ACE\nace are correct answer",
          "comment_id": "1195270",
          "upvote_count": "2"
        },
        {
          "comment_id": "1139756",
          "content": "ACE are correct: <Only the security team can be allowed to view aggregated Security Hub findings> means we need a delegated admin. <All accounts must be enrolled in Security Hub after the accounts are created> and <in the MOST automated way> means we need enable automatic enablement\nB: no mention of delegated admin\nD: This options denied access of the security team, which is irrelevant\nF: This option's result is the same as in option E, but more complicated",
          "poster": "thanhnv142",
          "timestamp": "1707018660.0",
          "upvote_count": "3"
        },
        {
          "upvote_count": "1",
          "content": "Selected Answer: ADE\nAccording to this article .. The Delegated account users have access in ANY account while the users under own account can view their own findings. So, there is no need to setup IAM policies for Security account users. https://docs.aws.amazon.com/securityhub/latest/userguide/securityhub-accounts-allowed-actions.html",
          "timestamp": "1699190820.0",
          "comment_id": "1062916",
          "poster": "2pk"
        },
        {
          "comment_id": "1060938",
          "timestamp": "1698961980.0",
          "poster": "YR4591",
          "upvote_count": "3",
          "content": "Selected Answer: ACE\nA - Create delegate account for the security hub\nC - Give access to users to security using permissions sets\nE - Use auto enable so every new account will be monitored by security hub"
        },
        {
          "timestamp": "1689448200.0",
          "content": "Selected Answer: ACE\nACE are the correct answers.",
          "comment_id": "952624",
          "poster": "sb333",
          "upvote_count": "2"
        },
        {
          "timestamp": "1689249840.0",
          "content": "Selected Answer: ACE\nACE. Reason being, it is a landing zone and AWS SSO (IAM IC) is already part of the Control Tower product! Add security dept users as a SSO group and attach the security permission set to access security hub",
          "upvote_count": "2",
          "comment_id": "950644",
          "poster": "habros"
        },
        {
          "content": "Selected Answer: ACE\nwith Control Tower comes the Identity Center implementation with default Identity Center directory.",
          "comment_id": "934725",
          "poster": "Wardove",
          "timestamp": "1687805640.0",
          "upvote_count": "3"
        },
        {
          "poster": "robotgeek",
          "comment_id": "906758",
          "comments": [
            {
              "timestamp": "1688667360.0",
              "content": "Identity Center is not exclusively related to Active Directory\nAn SCP can only prevent access but doesnt enable any access, so D is not sufficient\nACE for me \nhttps://docs.aws.amazon.com/securityhub/latest/userguide/accounts-orgs-auto-enable.html",
              "comment_id": "944918",
              "poster": "jnv007",
              "upvote_count": "1"
            }
          ],
          "upvote_count": "2",
          "content": "Selected Answer: ADE\nB is not the typical way AWS separates responsabilities in multi account (management, sec, audit)\nC is related with Active Directory \nE is more automated than F",
          "timestamp": "1685028720.0"
        },
        {
          "comment_id": "905225",
          "content": "ACF IS MORE EFFICIENT",
          "poster": "Kodoma",
          "upvote_count": "4",
          "timestamp": "1684875240.0"
        },
        {
          "upvote_count": "1",
          "comment_id": "896465",
          "timestamp": "1683963360.0",
          "poster": "Dimidrol",
          "content": "Selected Answer: ADE\nAde for me"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:08:42.772Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "l3wk5YXE2BeXh2EmKYX9",
      "question_number": 5,
      "page": 1,
      "question_text": "A company runs applications in AWS accounts that are in an organization in AWS Organizations. The applications use Amazon EC2 instances and Amazon S3.\n\nThe company wants to detect potentially compromised EC2 instances, suspicious network activity, and unusual API activity in its existing AWS accounts and in any AWS accounts that the company creates in the future. When the company detects one of these events, the company wants to use an existing Amazon Simple Notification Service (Amazon SNS) topic to send a notification to its operational support team for investigation and remediation.\n\nWhich solution will meet these requirements in accordance with AWS best practices?",
      "choices": {
        "C": "In the organization’s management account, create an AWS CloudTrail organization trail. Activate the organization trail in all AWS accounts in the organization. Create an SCP that enables VPC Flow Logs in each account in the organization. Configure AWS Security Hub for the organization. Create an Amazon EventBridge rule with an event pattern to match Security Hub events and to forward matching events to the SNS topic.",
        "A": "In the organization’s management account, configure an AWS account as the Amazon GuardDuty administrator account. In the GuardDuty administrator account, add the company’s existing AWS accounts to GuardDuty as members. In the GuardDuty administrator account, create an Amazon EventBridge rule with an event pattern to match GuardDuty events and to forward matching events to the SNS topic.",
        "B": "In the organization’s management account, configure Amazon GuardDuty to add newly created AWS accounts by invitation and to send invitations to the existing AWS accounts. Create an AWS CloudFormation stack set that accepts the GuardDuty invitation and creates an Amazon EventBridge rule. Configure the rule with an event pattern to match GuardDuty events and to forward matching events to the SNS topic. Configure the CloudFormation stack set to deploy into all AWS accounts in the organization.",
        "D": "In the organization’s management account, configure an AWS account as the AWS CloudTrail administrator account. In the CloudTrail administrator account, create a CloudTrail organization trail. Add the company’s existing AWS accounts to the organization trail. Create an SCP that enables VPC Flow Logs in each account in the organization. Configure AWS Security Hub for the organization. Create an Amazon EventBridge rule with an event pattern to match Security Hub events and to forward matching events to the SNS topic."
      },
      "correct_answer": "A",
      "answer_ET": "A",
      "answers_community": [
        "A (87%)",
        "13%"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/108803-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-05-09 13:21:00",
      "unix_timestamp": 1683631260,
      "discussion_count": 27,
      "discussion": [
        {
          "poster": "Just_Ninja",
          "upvote_count": "13",
          "timestamp": "1689182640.0",
          "content": "Selected Answer: A\nDear Admin, Please Fix the Wrong response here! \nIt´s A:\nThis solution meets all the requirements:\n\nDetect potentially compromised EC2 instances, suspicious network activity, and unusual API activity: Amazon GuardDuty is a threat detection service that continuously monitors for malicious or unauthorized behavior. It analyzes events from AWS CloudTrail, Amazon VPC Flow Logs, and DNS logs to detect such activities.\n\nSend a notification to the operational support team: Creating an Amazon EventBridge rule that matches GuardDuty findings and then forwarding these to an SNS topic allows for the generation of notifications whenever suspicious activity is detected.\n\nCover future AWS accounts: By designating a GuardDuty administrator account in AWS Organizations, you can manage GuardDuty across all of your existing and future AWS accounts. This ensures that any new account created under the organization is automatically covered by GuardDuty.",
          "comment_id": "950044"
        },
        {
          "upvote_count": "1",
          "comment_id": "1338507",
          "poster": "Mrflip",
          "content": "Selected Answer: B\nB is the right answer",
          "timestamp": "1736455740.0"
        },
        {
          "content": "Selected Answer: A\nkeywords: compromised EC2 instances, suspicious network activity, and unusual API activity\n= GuardDuty",
          "comment_id": "1261086",
          "poster": "jamesf",
          "upvote_count": "2",
          "timestamp": "1722867720.0"
        },
        {
          "comment_id": "1227991",
          "poster": "zijo",
          "upvote_count": "2",
          "content": "Selected Answer: A\nWhen you use GuardDuty with an AWS organization, the management account of that organization can designate any account within the organization as the delegated GuardDuty administrator account. For this administrator account, GuardDuty gets enabled automatically only in the designated AWS Region. This account also has the permission to enable and manage GuardDuty for all of the accounts in the organization within that Region. The administrator account can view the members of and add members to this AWS organization.\n\nAWS GuardDuty can detect unusual API activity within existing AWS accounts in an AWS Organization. It monitors AWS CloudTrail event logs, which include records of all API calls made within your AWS environment. GuardDuty analyzes these logs to identify unusual or suspicious API activity that might indicate a potential security threat.",
          "timestamp": "1718036880.0"
        },
        {
          "upvote_count": "1",
          "timestamp": "1718036580.0",
          "poster": "zijo",
          "content": "A looks like a better choice.\nWhen you use GuardDuty with an AWS organization, the management account of that organization can designate any account within the organization as the delegated GuardDuty administrator account. For this administrator account, GuardDuty gets enabled automatically only in the designated AWS Region. This account also has the permission to enable and manage GuardDuty for all of the accounts in the organization within that Region. The administrator account can view the members of and add members to this AWS organization.",
          "comment_id": "1227989"
        },
        {
          "timestamp": "1713070980.0",
          "comment_id": "1195276",
          "content": "Selected Answer: A\nanswer A",
          "poster": "dkp",
          "upvote_count": "2"
        },
        {
          "upvote_count": "1",
          "content": "If GuardDuty is indeed set up at the organization level (which is supported and encouraged by AWS for simplicity and coverage), then Option A becomes a very strong choice. It provides centralized management and automatic, seamless inclusion of all organization accounts in security monitoring without requiring manual intervention for each new account.",
          "timestamp": "1712997840.0",
          "poster": "Mordans",
          "comment_id": "1194808"
        },
        {
          "comment_id": "1183142",
          "poster": "stoy123",
          "content": "Selected Answer: B\nDefinitely B",
          "timestamp": "1711442760.0",
          "upvote_count": "1"
        },
        {
          "upvote_count": "3",
          "content": "Selected Answer: A\nA is correct: <detect potentially compromised EC2 instances, suspicious network activity, and unusual API activity> means AWS GuardDuty\nB: dont have to invite other accounts because all accounts are in an org in AWS org.\nC and D: no mention of GuardDuty",
          "comment_id": "1139758",
          "timestamp": "1707019140.0",
          "poster": "thanhnv142"
        },
        {
          "timestamp": "1705067160.0",
          "content": "Selected Answer: A\ninvitation is used to handle users OUTSIDE the organization.",
          "poster": "a54b16f",
          "comment_id": "1120811",
          "upvote_count": "3"
        },
        {
          "content": "Selected Answer: A\nGo For A. \nhttps://docs.aws.amazon.com/guardduty/latest/ug/guardduty_organizations.html",
          "upvote_count": "2",
          "timestamp": "1704743040.0",
          "poster": "davdan99",
          "comment_id": "1116955"
        },
        {
          "poster": "saysamsuf",
          "upvote_count": "2",
          "content": "Selected Answer: B\nMember accounts must accept invite from the designated guard duty account before its effect. I use AWS organisation at work and quite familiar with the workings. I lean towards B",
          "timestamp": "1699783920.0",
          "comment_id": "1068415"
        },
        {
          "content": "It true it's missing auto enabled on. but Invitation is organization is not needed as Organization get precedence with account management when you have deligated Guardduty account. \"If you have already set up a GuardDuty administrator account with associated member accounts by invitation and the member accounts are part of the same organization, their Type changes from By Invitation to Via Organizations\" https://docs.aws.amazon.com/guardduty/latest/ug/guardduty_organizations.html",
          "timestamp": "1699189140.0",
          "comment_id": "1062898",
          "poster": "2pk",
          "upvote_count": "1"
        },
        {
          "upvote_count": "2",
          "timestamp": "1692105660.0",
          "comment_id": "981701",
          "poster": "lakescix",
          "comments": [
            {
              "poster": "lunt",
              "comment_id": "984519",
              "upvote_count": "2",
              "timestamp": "1692364860.0",
              "content": "A= does handle automatic enablement. If the GD delegated account is setup properly with automatic enablement check box ticked. As soon as the AWS account is created, GD auto enablement kicks into gear. B = How does CFN accept the GD invite? New AWS Account. CFN runs on new account > accept GD invite...but when was this invite sent? I have to login to AWS Console > GD > create invite vs A = no invite = directly enabled for GB in new account. https://docs.aws.amazon.com/guardduty/latest/ug/guardduty_organizations.html"
            }
          ],
          "content": "B is the correct answer.\nA could better if not for the fact that it doesn't handle automatic enablement on new AWS account.\nB handles this case with CloudFormation stacksets : https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-guardduty-master.html"
        },
        {
          "comment_id": "980144",
          "poster": "lunt",
          "content": "Selected Answer: A\nB is wrong. Newly created AWS accounts = you don't need to do this if the GD Orgz is configured properly, you can accept from delegated admin account. The point remains, you can add the accounts using option A. The misdirect here is that A does not state anything about new accounts vs B which does. Bearing in mind A + B still have to do something in GD, A is actually the better option.\nA is right.",
          "timestamp": "1691944680.0",
          "upvote_count": "3"
        },
        {
          "timestamp": "1691432160.0",
          "poster": "jason7",
          "comment_id": "974896",
          "content": "Selected Answer: B\nOption A is not the best choice because although it correctly configures GuardDuty as the administrator, it does not handle the automatic addition of new AWS accounts to GuardDuty and the forwarding of events to the SNS topic",
          "upvote_count": "1"
        },
        {
          "comment_id": "974895",
          "content": "Option B is the most suitable solution as it combines GuardDuty, AWS CloudFormation StackSets, and Amazon EventBridge to automatically monitor all existing and future AWS accounts and send notifications to the specified SNS topic when security events are detected.",
          "upvote_count": "1",
          "timestamp": "1691432100.0",
          "poster": "jason7"
        },
        {
          "timestamp": "1690982040.0",
          "upvote_count": "2",
          "content": "Selected Answer: A\nCannot be A, it does not deal with future accounts at all.",
          "poster": "gigi_devops",
          "comment_id": "970205"
        },
        {
          "content": "Selected Answer: A\nA for me. \n\nB technically sounds about right (to have an invitation accepter), but it does not address allocating a delegated admin for GD, which is required for account grouped under an Organization. https://docs.aws.amazon.com/guardduty/latest/ug/guardduty_organizations.html",
          "poster": "habros",
          "timestamp": "1688905320.0",
          "comment_id": "947210",
          "upvote_count": "2"
        },
        {
          "timestamp": "1688364720.0",
          "comments": [
            {
              "content": "have you taken the exam yet? is this dump valid?",
              "upvote_count": "1",
              "poster": "johnslayer123",
              "comment_id": "947032",
              "timestamp": "1688892960.0"
            }
          ],
          "upvote_count": "2",
          "poster": "pepecastr0",
          "comment_id": "941502",
          "content": "Selected Answer: A\nA - Once you are administrator you can assign other accounts as members, no need to send invitation"
        },
        {
          "timestamp": "1686919500.0",
          "comment_id": "925167",
          "upvote_count": "3",
          "poster": "rhinozD",
          "content": "A is correct.\n\"If the account that you want to specify as the GuardDuty administrator account is part of an organization in AWS Organizations, then you can specify that account as the organization's delegated administrator for GuardDuty. The account that is registered as the delegated administrator automatically becomes the GuardDuty administrator account.\n\nYou can use this administrator account to enable and manage GuardDuty for any account in the organization when you add that account as a member account.\"\nhttps://docs.aws.amazon.com/guardduty/latest/ug/guardduty_accounts.html",
          "comments": [
            {
              "content": "I agree",
              "comment_id": "941501",
              "upvote_count": "1",
              "poster": "pepecastr0",
              "timestamp": "1688364660.0"
            }
          ]
        },
        {
          "comment_id": "899236",
          "timestamp": "1684244040.0",
          "content": "I got A as my answer",
          "poster": "OrganizedChaos25",
          "upvote_count": "1"
        },
        {
          "poster": "Mail1964",
          "comment_id": "899184",
          "upvote_count": "2",
          "timestamp": "1684240680.0",
          "content": "Selected Answer: A\nFor me A makes the most sense. The q states they are in an AWS organisation, invitations are when you are not using AWS organisations. https://docs.aws.amazon.com/guardduty/latest/ug/guardduty_accounts.html"
        },
        {
          "poster": "ParagSanyashiv",
          "upvote_count": "1",
          "comments": [
            {
              "comment_id": "925166",
              "content": "Actually, None of your answers makes sense if you don't give any explanation.",
              "upvote_count": "5",
              "poster": "rhinozD",
              "timestamp": "1686919440.0"
            }
          ],
          "content": "Selected Answer: B\nB makes more sense. Ignore the previous response.",
          "timestamp": "1684119120.0",
          "comment_id": "898014"
        },
        {
          "content": "A is the right answer",
          "timestamp": "1684057740.0",
          "poster": "devnv",
          "upvote_count": "1",
          "comment_id": "897447"
        },
        {
          "content": "Selected Answer: A\nA seems more suitable.",
          "upvote_count": "1",
          "comments": [],
          "timestamp": "1683798060.0",
          "comment_id": "894908",
          "poster": "ParagSanyashiv"
        },
        {
          "content": "Selected Answer: A\n\" In the GuardDuty administrator account, create an Amazon EventBridge rule with an event pattern to match GuardDuty events and to forward matching events to the SNS topic.\" seems correct, B is really weird.",
          "upvote_count": "1",
          "timestamp": "1683631260.0",
          "poster": "Jeanphi72",
          "comment_id": "893043"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:08:42.772Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "LuUnbd5Hfv5kg3a1ct9l",
      "question_number": 6,
      "page": 2,
      "question_text": "A company’s DevOps engineer is working in a multi-account environment. The company uses AWS Transit Gateway to route all outbound traffic through a network operations account. In the network operations account, all account traffic passes through a firewall appliance for inspection before the traffic goes to an internet gateway.\n\nThe firewall appliance sends logs to Amazon CloudWatch Logs and includes event severities of CRITICAL, HIGH, MEDIUM, LOW, and INFO. The security team wants to receive an alert if any CRITICAL events occur.\n\nWhat should the DevOps engineer do to meet these requirements?",
      "choices": {
        "D": "Use AWS Firewall Manager to apply consistent policies across all accounts. Create an Amazon EventBridge event rule that is invoked by Firewall Manager events that are CRITICAL. Define an Amazon Simple Notification Service (Amazon SNS) topic as a target. Subscribe the security team’s email address to the topic.",
        "C": "Enable Amazon GuardDuty in the network operations account. Configure GuardDuty to monitor flow logs. Create an Amazon EventBridge event rule that is invoked by GuardDuty events that are CRITICAL. Define an Amazon Simple Notification Service (Amazon SNS) topic as a target. Subscribe the security team’s email address to the topic.",
        "A": "Create an Amazon CloudWatch Synthetics canary to monitor the firewall state. If the firewall reaches a CRITICAL state or logs a CRITICAL event, use a CloudWatch alarm to publish a notification to an Amazon Simple Notification Service (Amazon SNS) topic. Subscribe the security team’s email address to the topic.",
        "B": "Create an Amazon CloudWatch metric filter by using a search for CRITICAL events. Publish a custom metric for the finding. Use a CloudWatch alarm based on the custom metric to publish a notification to an Amazon Simple Notification Service (Amazon SNS) topic. Subscribe the security team’s email address to the topic."
      },
      "correct_answer": "B",
      "answer_ET": "B",
      "answers_community": [
        "B (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/109224-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-05-14 11:52:00",
      "unix_timestamp": 1684057920,
      "discussion_count": 7,
      "discussion": [
        {
          "content": "Selected Answer: B\nThe logs from the firewall appliance are already being sent to Amazon CloudWatch Logs. So , The best approach to meet the given requirements is to create an Amazon CloudWatch metric filter by using a search for CRITICAL events",
          "poster": "2pk",
          "timestamp": "1699980600.0",
          "comment_id": "897620",
          "upvote_count": "7"
        },
        {
          "poster": "habros",
          "comment_id": "947215",
          "upvote_count": "5",
          "timestamp": "1704810420.0",
          "content": "B. As the appliance pipes to CW Logs for consolidation. Define an alarm listening to the metric and should be okay.\n\nD is ONLY CORRECT IF YOU ARE USING AWS FIREWALL MANAGER."
        },
        {
          "timestamp": "1730551320.0",
          "comment_id": "1205452",
          "content": "Selected Answer: B\nI think B",
          "poster": "seetpt",
          "upvote_count": "1"
        },
        {
          "upvote_count": "2",
          "content": "Selected Answer: B\nanswer B",
          "comment_id": "1195278",
          "poster": "dkp",
          "timestamp": "1728882300.0"
        },
        {
          "upvote_count": "4",
          "timestamp": "1722736920.0",
          "poster": "thanhnv142",
          "content": "B is correct: <firewall appliance sends logs to Amazon CloudWatch Logs> means we already have the log in CW logs, only need to create alarm on these log files and send to sends\nA: No need to monitor the state of the firewall\nC and D: no mention of CloudWatch Logs",
          "comment_id": "1139760"
        },
        {
          "comment_id": "899239",
          "content": "B is the correct answer",
          "upvote_count": "1",
          "poster": "OrganizedChaos25",
          "timestamp": "1700148960.0"
        },
        {
          "content": "B is correct",
          "upvote_count": "1",
          "poster": "devnv",
          "comment_id": "897448",
          "timestamp": "1699962720.0"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:08:53.358Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "A70SNvjSFSb0t6346B1p",
      "question_number": 7,
      "page": 2,
      "question_text": "A company is divided into teams. Each team has an AWS account, and all the accounts are in an organization in AWS Organizations. Each team must retain full administrative rights to its AWS account. Each team also must be allowed to access only AWS services that the company approves for use. AWS services must gain approval through a request and approval process.\n\nHow should a DevOps engineer configure the accounts to meet these requirements?",
      "choices": {
        "C": "Place all the accounts under a new top-level OU within the organization. Create an SCP that denies access to restricted AWS services. Attach the SCP to the OU.",
        "B": "Use AWS Control Tower to provision the accounts into OUs within the organization. Configure AWS Control Tower to enable AWS IAM Identity Center (AWS Single Sign-On). Configure IAM Identity Center to provide administrative access. Include deny policies on user roles for restricted AWS services.",
        "D": "Create an SCP that allows access to only approved AWS services. Attach the SCP to the root OU of the organization. Remove the FullAWSAccess SCP from the root OU of the organization.",
        "A": "Use AWS CloudFormation StackSets to provision IAM policies in each account to deny access to restricted AWS services. In each account, configure AWS Config rules that ensure that the policies are attached to IAM principals in the account."
      },
      "correct_answer": "D",
      "answer_ET": "D",
      "answers_community": [
        "D (54%)",
        "C (44%)",
        "2%"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/109237-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-05-14 17:03:00",
      "unix_timestamp": 1684076580,
      "discussion_count": 31,
      "discussion": [
        {
          "timestamp": "1685117940.0",
          "comment_id": "907493",
          "upvote_count": "22",
          "content": "Selected Answer: D\nA=local account admin can change this.\nB=local admin has admin permissions. Complicated.\nC=implicit permit on everything else = breaks requirements.\nD= As they want to approve each service, its got to be white-list based SCP setup.\nAnswer is D.",
          "poster": "lunt"
        },
        {
          "upvote_count": "6",
          "poster": "dkp",
          "timestamp": "1713072360.0",
          "content": "Selected Answer: D\nAns D:\nIt is easier to allow approved services than deny all the other services, considering the vast amount of AWS services. it's easier to whitelist than blacklisting all the remaining services.",
          "comment_id": "1195286"
        },
        {
          "poster": "nickp84",
          "timestamp": "1747310100.0",
          "upvote_count": "1",
          "content": "Selected Answer: C\nD. SCP that allows only approved services at the root OU: This is risky because it affects all accounts in the organization, including potentially critical shared services or management accounts. It's better to scope restrictions to a specific OU.",
          "comment_id": "1569032"
        },
        {
          "timestamp": "1738797960.0",
          "content": "Selected Answer: C\nNot Option D, Using allow-list SCP, since:\n - Too restrictive\n - More difficult to maintain\n - Might block essential services\n - Could break account functionality\n - Requires constant updates",
          "poster": "ce0df07",
          "comment_id": "1352120",
          "upvote_count": "2"
        },
        {
          "poster": "teo2157",
          "content": "Selected Answer: C\nGoing for C as removing the FullAWSAccess SCP from the root OU requires impacts directly in the Administrative Access and restrict necessary administrative actions required for account management and operations.",
          "comment_id": "1345184",
          "upvote_count": "3",
          "timestamp": "1737619140.0"
        },
        {
          "upvote_count": "2",
          "timestamp": "1728828300.0",
          "comment_id": "1296942",
          "content": "Selected Answer: D\nD is more straight forward",
          "poster": "auxwww"
        },
        {
          "timestamp": "1724596680.0",
          "poster": "hzaki",
          "content": "Selected Answer: D\nThe answer is (D). The following SCP example from the AWS DOCUMENT allows accounts to create resource shares that share prefix lists\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps_examples_ram.html",
          "upvote_count": "2",
          "comment_id": "1272180"
        },
        {
          "comment_id": "1259688",
          "timestamp": "1722576480.0",
          "poster": "jamesf",
          "content": "Selected Answer: C\nI prefer C than D.\nAs SCP more in Deny but not Allow\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html",
          "upvote_count": "2"
        },
        {
          "content": "Selected Answer: C\nSCP - only deny not allow - So answer is C",
          "poster": "auxwww",
          "comment_id": "1254632",
          "upvote_count": "3",
          "timestamp": "1721866560.0"
        },
        {
          "upvote_count": "1",
          "timestamp": "1718843760.0",
          "comment_id": "1233386",
          "content": "The question is looking to use the Allow List Strategy using SCP. So the answer that best fits is D.",
          "poster": "zsoni"
        },
        {
          "timestamp": "1718046780.0",
          "upvote_count": "4",
          "comment_id": "1228066",
          "content": "Selected Answer: C\nSCPs primary function is not grant permissions by themselves but restrict the permissions that IAM policies and other access control mechanisms can grant.",
          "poster": "zijo"
        },
        {
          "comment_id": "1205457",
          "timestamp": "1714646940.0",
          "content": "Selected Answer: D\nD seems better.",
          "upvote_count": "2",
          "poster": "seetpt"
        },
        {
          "upvote_count": "2",
          "timestamp": "1711287300.0",
          "poster": "fdoxxx",
          "comments": [
            {
              "content": "I think Option C is wrong because the question says 'Each team also must be allowed to access only AWS services that the company approves for use'\nWhen you deny specific services they can still access services that have not been approved.",
              "comment_id": "1201445",
              "timestamp": "1713968520.0",
              "upvote_count": "2",
              "poster": "MalonJay"
            }
          ],
          "comment_id": "1181613",
          "content": "Selected Answer: C\nOption C:\nPlace all the accounts under a new top-level OU within the organization: This allows for centralized management of the accounts.\nCreate an SCP that denies access to restricted AWS services: This ensures that only approved services are accessible. SCPs (Service Control Policies) are the best way to control permissions at the organizational level.\nAttach the SCP to the OU: By attaching the SCP to the OU, all accounts within the OU will inherit the restrictions set by the SCP.\nD is wrong: This option allows access only to approved AWS services by creating an SCP that allows access to only approved services and attaching it to the root OU of the organization. However, this would restrict all accounts, including those of other departments or teams within the organization. It doesn't meet the requirement of allowing each team to retain full administrative rights to its AWS account."
        },
        {
          "content": "Selected Answer: C\nConclusion: Option C is the best solution to meet the requirements with operational efficiency and scalability. It allows teams to retain administrative rights while enforcing company-wide controls on service access through SCPs. This approach is straightforward to manage at scale, as adding or removing services from the SCP can adjust access permissions across all accounts within the OU. It directly aligns with the goal of allowing access only to approved AWS services and supports a governance model that can evolve with the organization's needs.",
          "poster": "kyuhuck",
          "timestamp": "1708154280.0",
          "comment_id": "1152410",
          "upvote_count": "4"
        },
        {
          "poster": "vortegon",
          "content": "Selected Answer: C\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html",
          "upvote_count": "2",
          "timestamp": "1707325860.0",
          "comment_id": "1143560"
        },
        {
          "timestamp": "1707053160.0",
          "upvote_count": "4",
          "poster": "thanhnv142",
          "content": "Selected Answer: C\nC is correct: <all the accounts are in an organization in AWS Organizations> means we need scps\nA and B: no mention of scps\nD: SCP only denies access, not allow. Additionally, should not attack SCP to the root OU because this may inadvertently denies users' access to AWS services",
          "comments": [
            {
              "timestamp": "1707235440.0",
              "upvote_count": "2",
              "content": "correction: D: SCP has allow statement. D perfectly fits this question",
              "poster": "thanhnv142",
              "comment_id": "1142348"
            }
          ],
          "comment_id": "1140144"
        },
        {
          "comment_id": "1131278",
          "timestamp": "1706151360.0",
          "poster": "sksegha",
          "upvote_count": "2",
          "content": "Selected Answer: C\nC is correct; apart from SCP's only denying ... why would u want to add SCPs to the root org."
        },
        {
          "comment_id": "1118514",
          "content": "D is wrong SCP can only deny, not approve. my answer is C",
          "timestamp": "1704886380.0",
          "upvote_count": "2",
          "poster": "yuliaqwerty"
        },
        {
          "comment_id": "1098042",
          "upvote_count": "2",
          "poster": "vikasnm123",
          "timestamp": "1702720380.0",
          "content": "Option C is Correct \nOption D is wrong because AWS strongly recommends that you don't attach SCPs to the root of your organization without thoroughly testing the impact that the policy has on accounts. Instead, create an OU that you can move your accounts into one at a time, or at least in small numbers, to ensure that you don't inadvertently lock users out of key services.\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html"
        },
        {
          "content": "Sill C must correct answer. \nThe use case is just to restrict access to not allowed services. Everything else should stay as the current configuration.\n\" Each team must RETAIN full administrative rights to its AWS account. Each team also must be allowed to ACCESS ONLY AWS services that the company approves for use",
          "upvote_count": "2",
          "timestamp": "1701785700.0",
          "comment_id": "1088556",
          "poster": "svjl"
        },
        {
          "upvote_count": "2",
          "comment_id": "1066448",
          "poster": "hzhang",
          "content": "Selected Answer: C\nSCPs alone are not sufficient in granting permissions to the accounts in your organization. No permissions are granted by an SCP. An SCP defines a guardrail, or sets limits, on the actions that the account's administrator can delegate to the IAM users and roles in the affected accounts. \nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html",
          "timestamp": "1699537980.0"
        },
        {
          "comment_id": "1062891",
          "comments": [
            {
              "upvote_count": "1",
              "timestamp": "1718682540.0",
              "content": "Not true. Ok to remove if you replace it with something. References:\n\"SCPs don't affect users or roles in the management account. They affect only the member accounts in your organization.\"\n\"You should not remove the FullAWSAccess policy unless you modify or replace it with a separate policy with allowed actions, otherwise all AWS actions from member accounts will fail.\"\nThe second URL below gives and example of a custom \"Deny\" SCP to replace default FullAWSAccess policy.\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps_evaluation.html#strategy_using_scps",
              "comment_id": "1232228",
              "poster": "Gomer"
            }
          ],
          "content": "Selected Answer: C\nD wrong because: Attach the SCP to the root OU of the organization. Remove the FullAWSAccess SCP from the root OU of the organization. This makes locking your own root account with inability with full root access. You need keep root OU full access for better management for other accounts. Better move all to separate OU with the restricted access.",
          "upvote_count": "2",
          "timestamp": "1699188240.0",
          "poster": "2pk"
        },
        {
          "content": "Selected Answer: D\nThis ensures only the approved services can be used in all accounts. \nC can work , however everytime AWS introduces a new service that will be accessible and need to be included in SCP deny to disable it",
          "poster": "RVivek",
          "comment_id": "1013414",
          "timestamp": "1695333900.0",
          "upvote_count": "4"
        },
        {
          "upvote_count": "1",
          "timestamp": "1693022340.0",
          "poster": "beanxyz",
          "content": "Selected Answer: C\nI prefer C to D. Say users have full admin access with permission set in their own account and what we need is to use SCP to deny certain services and actions. What is the point of granting explicit access to them again from scp when they have already?",
          "comment_id": "990496"
        },
        {
          "poster": "totopopo",
          "timestamp": "1689526440.0",
          "content": "Selected Answer: D\nIt’s D, because it’s the only one to propose a white list and not a black list.\nWhite list is important because AWS regularly opens new services as GA.",
          "upvote_count": "3",
          "comment_id": "953492"
        },
        {
          "poster": "ogwu2000",
          "timestamp": "1689501240.0",
          "comment_id": "953220",
          "content": "C seems better if you can create top level OU. Retain the FullAccess to prevent unforeseen issues and restrict access only to selected resources.",
          "upvote_count": "2"
        },
        {
          "content": "I’ll insist on D. FullAWSAccess is the whitelist for ALL services. To deny every service, you will have a very good time writing deny policies as long as FullAWSAccess is around. Hence, remove it to have service whitelisting via SCP.",
          "timestamp": "1688906160.0",
          "poster": "habros",
          "upvote_count": "1",
          "comment_id": "947222"
        },
        {
          "timestamp": "1687547220.0",
          "comment_id": "931888",
          "poster": "Wardove",
          "comments": [
            {
              "upvote_count": "2",
              "timestamp": "1688365080.0",
              "content": "D states that \"Create an SCP that allows access to only approved AWS services\"",
              "comment_id": "941510",
              "poster": "pepecastr0"
            }
          ],
          "upvote_count": "1",
          "content": "Selected Answer: C\nBy default, an OU in AWS Organizations comes with a FullAWSAccess service control policy (SCP) attached, which allows all services and actions in the AWS accounts under the OU.\n\nIf you remove the FullAWSAccess SCP without having another SCP in place that allows necessary permissions, you could inadvertently lock out users, including administrators, from the resources and services in the AWS accounts. \nSo answer has to be C"
        },
        {
          "comment_id": "925209",
          "poster": "rhinozD",
          "content": "Selected Answer: D\nI see no reason to create a new OU for this case.\nAnd by using the explicit deny strategy, D is more secure.",
          "upvote_count": "4",
          "timestamp": "1686922020.0"
        },
        {
          "timestamp": "1686296400.0",
          "content": "Selected Answer: B\nB also seems as a possible solution. any objection?",
          "poster": "BasselBuzz",
          "comment_id": "919054",
          "upvote_count": "2"
        },
        {
          "timestamp": "1684076580.0",
          "poster": "2pk",
          "content": "Selected Answer: C\nC is correct answer, D is wrong because it does not meet the specific requirement of allowing each team to retain full administrative rights to its own AWS account.",
          "comments": [
            {
              "timestamp": "1685118120.0",
              "content": "having full admin rights = does not mean having full execution rights. They can still be admins with IAM admin permissions > SCP permissions will still stop them from using a service without approval. Answer is D.",
              "comment_id": "907496",
              "poster": "lunt",
              "upvote_count": "7"
            }
          ],
          "upvote_count": "2",
          "comment_id": "897630"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:08:53.358Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "jFzrGRx0sHX9gFp6ksq5",
      "question_number": 8,
      "page": 2,
      "question_text": "A DevOps engineer used an AWS CloudFormation custom resource to set up AD Connector. The AWS Lambda function ran and created AD Connector, but CloudFormation is not transitioning from CREATE_IN_PROGRESS to CREATE_COMPLETE.\n\nWhich action should the engineer take to resolve this issue?",
      "choices": {
        "B": "Ensure the Lambda function code returns a response to the pre-signed URL.",
        "D": "Ensure the Lambda function IAM role has ds:ConnectDirectory permissions for the AWS account.",
        "A": "Ensure the Lambda function code has exited successfully.",
        "C": "Ensure the Lambda function IAM role has cloudformation:UpdateStack permissions for the stack ARN."
      },
      "correct_answer": "B",
      "answer_ET": "B",
      "answers_community": [
        "B (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/109225-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-05-14 12:13:00",
      "unix_timestamp": 1684059180,
      "discussion_count": 8,
      "discussion": [
        {
          "poster": "haazybanj",
          "content": "Selected Answer: B\nB. Ensure the Lambda function code returns a response to the pre-signed URL.\nExplanation:\nWhen using a custom resource in CloudFormation, the AWS Lambda function responsible for handling the resource creation should send a response to the pre-signed URL provided by CloudFormation. This response signals the completion status of the custom resource creation process to CloudFormation.\n\nIn this case, since the Lambda function successfully created the AD Connector, the engineer should ensure that the Lambda function code includes the logic to send a response to the pre-signed URL. This response should indicate the success status and any relevant data, such as the ARN or other details of the created AD Connector.",
          "upvote_count": "14",
          "comment_id": "938307",
          "timestamp": "1703870700.0"
        },
        {
          "poster": "dkp",
          "comment_id": "1195289",
          "timestamp": "1728883980.0",
          "upvote_count": "1",
          "content": "Selected Answer: B\nits B"
        },
        {
          "comment_id": "1140148",
          "content": "B is correct: <but CloudFormation is not transitioning from CREATE_IN_PROGRESS to CREATE_COMPLETE> means ACF hasnot received a response code from Lambda\nA, C and D: no mention of response code",
          "timestamp": "1722771240.0",
          "poster": "thanhnv142",
          "upvote_count": "2"
        },
        {
          "upvote_count": "1",
          "timestamp": "1714212780.0",
          "content": "Selected Answer: B\nLambda should send a cfnresponsse to presign url",
          "poster": "YR4591",
          "comment_id": "1055427"
        },
        {
          "poster": "BaburTurk",
          "content": "B is correct \nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-custom-resources.html",
          "timestamp": "1712909700.0",
          "upvote_count": "3",
          "comment_id": "1041541"
        },
        {
          "content": "Selected Answer: B\nIt's B.",
          "comment_id": "928822",
          "upvote_count": "2",
          "timestamp": "1703115300.0",
          "poster": "MarDog"
        },
        {
          "timestamp": "1700228580.0",
          "poster": "OrganizedChaos25",
          "content": "B is correct",
          "upvote_count": "2",
          "comment_id": "900071"
        },
        {
          "poster": "devnv",
          "upvote_count": "2",
          "comment_id": "897463",
          "timestamp": "1699963980.0",
          "content": "B is the right answer"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:08:53.358Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "hDt3k6dc2HdLltwZpdVz",
      "question_number": 9,
      "page": 2,
      "question_text": "A company uses AWS CodeCommit for source code control. Developers apply their changes to various feature branches and create pull requests to move those changes to the main branch when the changes are ready for production.\n\nThe developers should not be able to push changes directly to the main branch. The company applied the AWSCodeCommitPowerUser managed policy to the developers’ IAM role, and now these developers can push changes to the main branch directly on every repository in the AWS account.\n\nWhat should the company do to restrict the developers’ ability to push changes to the main branch directly?",
      "choices": {
        "C": "Modify the IAM policy. Include a Deny rule for the GitPush and PutFile actions for the specific repositories in the policy statement with a condition that references the main branch.",
        "A": "Create an additional policy to include a Deny rule for the GitPush and PutFile actions. Include a restriction for the specific repositories in the policy statement with a condition that references the main branch.",
        "D": "Create an additional policy to include an Allow rule for the GitPush and PutFile actions. Include a restriction for the specific repositories in the policy statement with a condition that references the feature branches.",
        "B": "Remove the IAM policy, and add an AWSCodeCommitReadOnly managed policy. Add an Allow rule for the GitPush and PutFile actions for the specific repositories in the policy statement with a condition that references the main branch."
      },
      "correct_answer": "A",
      "answer_ET": "A",
      "answers_community": [
        "A (94%)",
        "6%"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/109226-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-05-14 12:16:00",
      "unix_timestamp": 1684059360,
      "discussion_count": 12,
      "discussion": [
        {
          "upvote_count": "15",
          "comment_id": "950053",
          "timestamp": "1689183600.0",
          "poster": "Just_Ninja",
          "content": "Selected Answer: A\nA is possible!\nIf you think C is correct, then you should know that a policy managed by AWS cannot be modified."
        },
        {
          "poster": "jamesf",
          "content": "Selected Answer: A\nNot C as AWS managed policy cannot be modified",
          "comment_id": "1261089",
          "upvote_count": "1",
          "timestamp": "1722868260.0"
        },
        {
          "comment_id": "1228624",
          "poster": "zijo",
          "content": "Selected Answer: A\nAWS Managed Policies are read-only, meaning you cannot modify their contents. If you need a similar policy with slight modifications, you can copy the managed policy and create a customer-managed policy.",
          "timestamp": "1718132160.0",
          "upvote_count": "2"
        },
        {
          "upvote_count": "1",
          "comment_id": "1195291",
          "timestamp": "1713073020.0",
          "content": "Selected Answer: A\nit s A.",
          "poster": "dkp"
        },
        {
          "timestamp": "1707054180.0",
          "content": "Selected Answer: A\nA is correct: <The developers should not be able to push changes directly to the main branch> means we should deny these permissions in IAM policy. <managed polic> means we should add another policy, not modify this one. \nB: <Remove the IAM policy>: this is an managed policy, cannot remove it\nC: Cannot modify a managed policy. We can only create another policy\nD: This option would deny commiting code to every sub-branches, which is not correct",
          "upvote_count": "3",
          "poster": "thanhnv142",
          "comment_id": "1140159"
        },
        {
          "comment_id": "1106015",
          "poster": "giovanna_mag",
          "upvote_count": "3",
          "timestamp": "1703594880.0",
          "content": "Selected Answer: A\nA, AWS managed policy cannot be modified, additional policy must be attached with a DENY"
        },
        {
          "comment_id": "943616",
          "timestamp": "1688555220.0",
          "poster": "Blueee",
          "content": "Selected Answer: A\nA is correct",
          "upvote_count": "1"
        },
        {
          "content": "Selected Answer: A\nAWSCodeCommitPowerUser is an AWS-managed policy.\nSo you need to add an additional policy to deny push to the main branch directly.",
          "comment_id": "925221",
          "upvote_count": "3",
          "poster": "rhinozD",
          "timestamp": "1686922920.0"
        },
        {
          "comment_id": "905228",
          "upvote_count": "1",
          "poster": "Kodoma",
          "content": "A is correct",
          "timestamp": "1684875420.0"
        },
        {
          "timestamp": "1684735320.0",
          "content": "Selected Answer: A\nIt`s A",
          "poster": "Ryan1002",
          "comment_id": "903755",
          "upvote_count": "2"
        },
        {
          "timestamp": "1684562700.0",
          "comments": [
            {
              "poster": "EricZhang",
              "timestamp": "1684918260.0",
              "upvote_count": "5",
              "content": "You can never modify a managed policy",
              "comment_id": "905664"
            }
          ],
          "comment_id": "902379",
          "poster": "PhuocT",
          "upvote_count": "2",
          "content": "Selected Answer: C\nC, why we need to create an additional policy?"
        },
        {
          "comment_id": "897464",
          "upvote_count": "3",
          "content": "A is correct",
          "timestamp": "1684059360.0",
          "poster": "devnv"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:08:53.358Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "MFR6FZE0r81S1rPx4doP",
      "question_number": 10,
      "page": 2,
      "question_text": "A company manages a web application that runs on Amazon EC2 instances behind an Application Load Balancer (ALB). The EC2 instances run in an Auto Scaling group across multiple Availability Zones. The application uses an Amazon RDS for MySQL DB instance to store the data. The company has configured Amazon Route 53 with an alias record that points to the ALB.\n\nA new company guideline requires a geographically isolated disaster recovery (DR) site with an RTO of 4 hours and an RPO of 15 minutes.\n\nWhich DR strategy will meet these requirements with the LEAST change to the application stack?",
      "choices": {
        "C": "Launch a replica environment of everything except Amazon RDS in a different AWS Region. In the event of an outage, copy and restore the latest RDS snapshot from the primary Region to the DR Region. Adjust the Route 53 record set to point to the ALB in the DR Region.",
        "A": "Launch a replica environment of everything except Amazon RDS in a different Availability Zone. Create an RDS read replica in the new Availability Zone, and configure the new stack to point to the local RDS DB instance. Add the new stack to the Route 53 record set by using a health check to configure a failover routing policy.",
        "B": "Launch a replica environment of everything except Amazon RDS in a different AWS Region. Create an RDS read replica in the new Region, and configure the new stack to point to the local RDS DB instance. Add the new stack to the Route 53 record set by using a health check to configure a latency routing policy.",
        "D": "Launch a replica environment of everything except Amazon RDS in a different AWS Region. Create an RDS read replica in the new Region, and configure the new environment to point to the local RDS DB instance. Add the new stack to the Route 53 record set by using a health check to configure a failover routing policy. In the event of an outage, promote the read replica to primary."
      },
      "correct_answer": "D",
      "answer_ET": "D",
      "answers_community": [
        "D (90%)",
        "10%"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/109227-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-05-14 12:20:00",
      "unix_timestamp": 1684059600,
      "discussion_count": 12,
      "discussion": [
        {
          "poster": "YR4591",
          "comment_id": "1055436",
          "content": "Selected Answer: D\nD is correct. Failover policy will route traffic to the ALB in the backup region.",
          "timestamp": "1714213080.0",
          "upvote_count": "5"
        },
        {
          "timestamp": "1734790380.0",
          "poster": "youonebe",
          "content": "Selected Answer: D\nD is correct. My answer was C, but GPT told me this:\nThis process does not guarantee a quick recovery within the 4-hour RTO. Restoring from a snapshot would take significant time, and it doesn't provide continuous replication of data to minimize RPO. This option does not meet the RTO requirement because the recovery process (snapshot restore) will take too long. It also doesn’t address continuous data replication, leading to a higher potential RPO.",
          "comment_id": "1330060",
          "upvote_count": "1"
        },
        {
          "content": "Selected Answer: D\ngeographically isolated location applies to option D",
          "poster": "dkp",
          "timestamp": "1728884940.0",
          "upvote_count": "2",
          "comment_id": "1195297"
        },
        {
          "comment_id": "1186686",
          "poster": "WhyIronMan",
          "upvote_count": "3",
          "timestamp": "1727685960.0",
          "content": "Selected Answer: D\nAnswer is D.\n\nA. It did not cover the whole scenario. there is a need to promote the read replica otherwise the application in the region will not be able to write in that rds. Also, another Region means geographically isolated, while other Az will not solve the problem.\n\nDetails are everything during an investigation..."
        },
        {
          "upvote_count": "1",
          "content": "Answer is D.\n\nA. It did not cover the whole scenario. there is a need to promote the read replica otherwise the application in the region will not be able to write in that rds. Also, another Region means geographically isolated, while other Az will not solve the problem.\n\nDetails are everything during an investigation...",
          "timestamp": "1727685900.0",
          "poster": "WhyIronMan",
          "comment_id": "1186685"
        },
        {
          "content": "Selected Answer: A\nthey mentioned geographically not regional , the cost will be more if we make it regional so we can go with the AZ DR",
          "comment_id": "1153859",
          "comments": [
            {
              "upvote_count": "1",
              "timestamp": "1727685840.0",
              "comment_id": "1186684",
              "content": "A did not cover the whole scenario. there is a need to promote the read replica otherwise the application in the region will not be able to write in that rds. Also, another Region means geographically isolated.\nDetails are everything during an investigation...",
              "poster": "WhyIronMan"
            }
          ],
          "timestamp": "1724058480.0",
          "upvote_count": "2",
          "poster": "jojom19980"
        },
        {
          "poster": "thanhnv142",
          "comment_id": "1140174",
          "timestamp": "1722772260.0",
          "upvote_count": "3",
          "content": "Selected Answer: D\nD is correct: < configured Amazon Route 53> and <requires a geographically isolated disaster recovery (DR) site> means fail-over routing and the DR site should be in another region\nA: <Amazon RDS in a different Availability Zone>: We need to setup the DB in a different region, not in a different AZ, which is still in the same region\nB and C: no mention of fail-over"
        },
        {
          "poster": "sarlos",
          "comment_id": "1112380",
          "upvote_count": "2",
          "timestamp": "1719959280.0",
          "content": "D is the answer"
        },
        {
          "comment_id": "1083603",
          "poster": "HugoFM",
          "content": "What about C? I have an RTO of 4 hours I mean we got time we dont need a read replica we could take time to restore from a snapshot",
          "timestamp": "1716988620.0",
          "comments": [
            {
              "poster": "davdan99",
              "comment_id": "1117345",
              "timestamp": "1720510500.0",
              "content": "Restore Time vs. RTO: While 4 hours might seem like a sufficient window for restoring a snapshot, the actual restore time can vary depending on several factors:\nSnapshot size: Larger snapshots take longer to restore.\nRDS instance type: High-performance instance types can handle restorations faster.\nNetwork bandwidth: Sufficient bandwidth is crucial for speedy data transfer during restoration.\nRDS engine version: Newer versions might have optimized restore processes.\nIn practice, restoring a large RDS snapshot, especially across Regions, could easily take more than 15 minutes, potentially exceeding the RPO and resulting in data loss.",
              "upvote_count": "2"
            }
          ],
          "upvote_count": "3"
        },
        {
          "timestamp": "1703872140.0",
          "content": "Selected Answer: D\nD is right",
          "poster": "haazybanj",
          "upvote_count": "4",
          "comment_id": "938332"
        },
        {
          "poster": "Kodoma",
          "content": "D is true",
          "comment_id": "905229",
          "timestamp": "1700780220.0",
          "upvote_count": "2"
        },
        {
          "content": "D is correct",
          "timestamp": "1699964400.0",
          "poster": "devnv",
          "comment_id": "897466",
          "upvote_count": "1"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:08:53.358Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "trsK82R4oiSkxttsYBwg",
      "question_number": 11,
      "page": 3,
      "question_text": "A large enterprise is deploying a web application on AWS. The application runs on Amazon EC2 instances behind an Application Load Balancer. The instances run in an Auto Scaling group across multiple Availability Zones. The application stores data in an Amazon RDS for Oracle DB instance and Amazon DynamoDB. There are separate environments for development, testing, and production.\n\nWhat is the MOST secure and flexible way to obtain password credentials during deployment?",
      "choices": {
        "A": "Retrieve an access key from an AWS Systems Manager SecureString parameter to access AWS services. Retrieve the database credentials from a Systems Manager SecureString parameter.",
        "B": "Launch the EC2 instances with an EC2 IAM role to access AWS services. Retrieve the database credentials from AWS Secrets Manager.",
        "D": "Launch the EC2 instances with an EC2 IAM role to access AWS services. Store the database passwords in an encrypted config file with the application artifacts.",
        "C": "Retrieve an access key from an AWS Systems Manager plaintext parameter to access AWS services. Retrieve the database credentials from a Systems Manager SecureString parameter."
      },
      "correct_answer": "B",
      "answer_ET": "B",
      "answers_community": [
        "B (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/109228-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-05-14 12:22:00",
      "unix_timestamp": 1684059720,
      "discussion_count": 12,
      "discussion": [
        {
          "comment_id": "1257259",
          "upvote_count": "2",
          "content": "Selected Answer: B\nKeywords: MOST secure",
          "timestamp": "1722227940.0",
          "poster": "jamesf"
        },
        {
          "poster": "zijo",
          "content": "Selected Answer: B\nThis step is important for applications running on EC2 instances to retrieve passwords from AWS Secrets Manager.\nCreate an IAM role with the necessary permissions to access AWS Secrets Manager.\nAttach this IAM role to your EC2 instance.",
          "upvote_count": "2",
          "comment_id": "1229433",
          "timestamp": "1718218860.0"
        },
        {
          "content": "Selected Answer: B\nThe most secure and flexible way to obtain password credentials during deployment in the given scenario is to use AWS Secrets Manager. AWS Secrets Manager is a service that allows you to securely store, retrieve, and rotate credentials, such as passwords, API keys, and other sensitive data.",
          "upvote_count": "3",
          "timestamp": "1713536460.0",
          "comment_id": "1198665",
          "poster": "c3518fc"
        },
        {
          "upvote_count": "2",
          "timestamp": "1713073980.0",
          "comment_id": "1195301",
          "poster": "dkp",
          "content": "Selected Answer: B\nB seems more relevant"
        },
        {
          "content": "Selected Answer: B\nB. EC2 Role + Secrets Mananger",
          "timestamp": "1711874880.0",
          "comment_id": "1186687",
          "upvote_count": "2",
          "poster": "WhyIronMan"
        },
        {
          "upvote_count": "4",
          "timestamp": "1707054840.0",
          "content": "Selected Answer: B\nB is correct: <obtain password credentials> means we should consider AWS SSM and secret manager. However, <the MOST secure > means we should opt for secret manager, which is more costly but more secure\nA, C and D: no mention of secret manager",
          "poster": "thanhnv142",
          "comment_id": "1140182"
        },
        {
          "comments": [
            {
              "content": "<obtain password credentials> means we should consider AWS SSM and secret manager. However, <the MOST secure > means we should opt for secret manager, which is more costly but more secure",
              "upvote_count": "3",
              "poster": "thanhnv142",
              "timestamp": "1707054900.0",
              "comment_id": "1140184"
            },
            {
              "upvote_count": "1",
              "comment_id": "1117348",
              "content": "We are not storing access keys for EC2 instances, instead we are using instance profile for that it is the best practice, and for database credentials it is correct to use Secret manager, it is more integrated with RDS, and other database services within AWS.",
              "poster": "davdan99",
              "timestamp": "1704793200.0"
            }
          ],
          "upvote_count": "1",
          "timestamp": "1704245160.0",
          "comment_id": "1112400",
          "content": "why not A?",
          "poster": "sarlos"
        },
        {
          "comment_id": "1106017",
          "poster": "giovanna_mag",
          "content": "Selected Answer: B\nI vote B",
          "upvote_count": "2",
          "timestamp": "1703595180.0"
        },
        {
          "upvote_count": "3",
          "content": "Selected Answer: B\nNo Brainer",
          "poster": "Snape",
          "timestamp": "1689545160.0",
          "comment_id": "953672"
        },
        {
          "poster": "haazybanj",
          "timestamp": "1688054160.0",
          "comment_id": "938339",
          "upvote_count": "4",
          "content": "Selected Answer: B\nMost secure is B"
        },
        {
          "timestamp": "1687767420.0",
          "poster": "FunkyFresco",
          "content": "Selected Answer: B\nOption B is the right answer.",
          "upvote_count": "2",
          "comment_id": "934201"
        },
        {
          "timestamp": "1684059720.0",
          "poster": "devnv",
          "comment_id": "897467",
          "content": "B sounds the right answer",
          "upvote_count": "1"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:09:04.452Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "2vep9YgB0wGNWyhIMacz",
      "question_number": 12,
      "page": 3,
      "question_text": "The security team depends on AWS CloudTrail to detect sensitive security issues in the company’s AWS account. The DevOps engineer needs a solution to auto-remediate CloudTrail being turned off in an AWS account.\n\nWhat solution ensures the LEAST amount of downtime for the CloudTrail log deliveries?",
      "choices": {
        "D": "Launch a t2.nano instance with a script running every 5 minutes that uses the AWS SDK to query CloudTrail in the current account. If the CloudTrail trail is disabled, have the script re-enable the trail.",
        "B": "Deploy the AWS-managed CloudTrail-enabled AWS Config rule, set with a periodic interval of 1 hour. Create an Amazon EventBridge rule for AWS Config rules compliance change. Create an AWS Lambda function that uses the AWS SDK to call StartLogging on the ARN of the resource in which StopLogging was called. Add the Lambda function ARN as a target to the EventBridge rule.",
        "C": "Create an Amazon EventBridge rule for a scheduled event every 5 minutes. Create an AWS Lambda function that uses the AWS SDK to call StartLogging on a CloudTrail trail in the AWS account. Add the Lambda function ARN as a target to the EventBridge rule.",
        "A": "Create an Amazon EventBridge rule for the CloudTrail StopLogging event. Create an AWS Lambda function that uses the AWS SDK to call StartLogging on the ARN of the resource in which StopLogging was called. Add the Lambda function ARN as a target to the EventBridge rule."
      },
      "correct_answer": "A",
      "answer_ET": "A",
      "answers_community": [
        "A (93%)",
        "7%"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/108802-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-05-09 12:36:00",
      "unix_timestamp": 1683628560,
      "discussion_count": 15,
      "discussion": [
        {
          "poster": "rhinozD",
          "timestamp": "1702742040.0",
          "comment_id": "925233",
          "upvote_count": "15",
          "comments": [
            {
              "poster": "daburahjail",
              "timestamp": "1710814620.0",
              "content": "Good job, buddy",
              "upvote_count": "2",
              "comment_id": "1010933"
            }
          ],
          "content": "Selected Answer: A\nA.\nold but gold link: \nhttps://aws.amazon.com/blogs/mt/monitor-changes-and-auto-enable-logging-in-aws-cloudtrail/"
        },
        {
          "upvote_count": "2",
          "content": "Selected Answer: A\nThis solution ensures the least amount of downtime for CloudTrail log deliveries when auto-remediating CloudTrail being turned off. Here's why:\n\nEvent-Driven Automation: By creating an Amazon EventBridge rule for the CloudTrail StopLogging event, the remediation process is triggered immediately when CloudTrail logging is stopped, minimizing the downtime.\nTargeted Remediation: The Lambda function uses the AWS SDK to call StartLogging on the specific CloudTrail trail ARN where the StopLogging event occurred. This targeted approach ensures that logging is re-enabled for the affected trail without impacting other trails or introducing unnecessary overhead.\nLow Latency: EventBridge rules and Lambda functions are designed to be highly responsive, ensuring that the remediation action is initiated with minimal delay after the StopLogging event occurs.",
          "poster": "c3518fc",
          "timestamp": "1729348560.0",
          "comment_id": "1198684"
        },
        {
          "comment_id": "1186690",
          "timestamp": "1727686380.0",
          "upvote_count": "1",
          "poster": "WhyIronMan",
          "content": "Selected Answer: A\nA. is correct\nDetails are everything during an investigation"
        },
        {
          "upvote_count": "2",
          "comment_id": "1140202",
          "timestamp": "1722772920.0",
          "content": "A is correct: <The DevOps engineer needs a solution to auto-remediate CloudTrail being turned off> means we should turn on it again if we detect that it is turn-off. AWS config rule or Eventbridge would be considered. < the LEAST amount of downtime> means we should choose A because this minimizes downtime\nB: this option utilizes an AWS config rule, which is good. But it sets the rule with a periodic interval of 1 hours, which would introduce a lot of downtime\nC: this option utilizes evenbridge, but the event to trigger eventbridge is undetermined \nD: Should not use a custom script to do the task",
          "poster": "thanhnv142"
        },
        {
          "comment_id": "1083609",
          "upvote_count": "2",
          "poster": "HugoFM",
          "timestamp": "1716988980.0",
          "content": "Selected Answer: A\nA its quicker and the solution is asking for the leeast amount of downtime"
        },
        {
          "content": "Selected Answer: A\n\"LEAST amount of downtime\" = A\ncloudwatch event is near real time. Al the other options are not.",
          "upvote_count": "1",
          "comment_id": "1055448",
          "poster": "YR4591",
          "timestamp": "1714213740.0"
        },
        {
          "content": "Selected Answer: A\nBoth A and B will work. However the question mentions leatst Cloutrial down time. Option A is correct beacuse the remiation is triggred immeiately .\nOption B can be delayed a it uns once in ever hour",
          "comment_id": "997468",
          "poster": "RVivek",
          "timestamp": "1709463360.0",
          "upvote_count": "1"
        },
        {
          "poster": "Seoyong",
          "upvote_count": "1",
          "comment_id": "988110",
          "content": "I don't think stoplogging is CloudTrail being turned off.\nYou can stop logging anytime - https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-turning-off-logging.html\n\nBut it doesn't means CloudTrail being turned off",
          "timestamp": "1708685160.0"
        },
        {
          "poster": "Seoyong",
          "comment_id": "982298",
          "upvote_count": "1",
          "content": "Selected Answer: B\ncloudtrail-enabled rule will check CloudTrail being turned off.",
          "timestamp": "1708076340.0",
          "comments": [
            {
              "content": "Please notice that the question says \"LEAST amount of downtime\" while B is possible it says \"set with a periodic interval of 1 hour.\" which can basically take 1h to enable CloudTrail again with causes a lot of downtime",
              "comment_id": "1186689",
              "poster": "WhyIronMan",
              "timestamp": "1727686320.0",
              "upvote_count": "2"
            }
          ]
        },
        {
          "timestamp": "1700229240.0",
          "comment_id": "900084",
          "upvote_count": "2",
          "content": "I got A as my answer",
          "poster": "OrganizedChaos25"
        },
        {
          "comment_id": "899210",
          "upvote_count": "2",
          "poster": "Mail1964",
          "timestamp": "1700147100.0",
          "content": "The requirement is - What solution ensures the LEAST amount of downtime for the CloudTrail log deliveries? For me that means reacting to AWS events. AWS config rule with 1 hr schedule does not meet the criteria in my opinion."
        },
        {
          "poster": "2pk",
          "upvote_count": "3",
          "timestamp": "1699993020.0",
          "content": "Selected Answer: A\nAnswer A, \nThis solution is the most appropriate as it listens to the StopLogging event and automatically starts logging immediately. This approach eliminates the need to wait for a scheduled interval, thereby reducing the amount of downtime and ensuring the security team can detect security issues in real-time.\n\nOption B is incorrect as it uses AWS Config rules to detect CloudTrail stoppage, which might not be an immediate solution to this issue",
          "comment_id": "897821"
        },
        {
          "upvote_count": "2",
          "timestamp": "1699964760.0",
          "poster": "devnv",
          "content": "A is the right answer",
          "comment_id": "897471"
        },
        {
          "content": "Selected Answer: A\nA is correct.",
          "timestamp": "1699775400.0",
          "poster": "ParagSanyashiv",
          "upvote_count": "2",
          "comment_id": "895675"
        },
        {
          "upvote_count": "1",
          "content": "Selected Answer: B\nB for me",
          "comment_id": "892997",
          "poster": "Jeanphi72",
          "timestamp": "1699533360.0"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:09:04.453Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "DEXxKuYXHSVQwQfCHIzF",
      "question_number": 13,
      "page": 3,
      "question_text": "An ecommerce company has chosen AWS to host its new platform. The company's DevOps team has started building an AWS Control Tower landing zone. The DevOps team has set the identity store within AWS IAM Identity Center (AWS Single Sign-On) to external identity provider (IdP) and has configured SAML 2.0.\nThe DevOps team wants a robust permission model that applies the principle of least privilege. The model must allow the team to build and manage only the team's own resources.\nWhich combination of steps will meet these requirements? (Choose three.)",
      "choices": {
        "B": "Create permission sets. Attach an inline policy that includes the required permissions and uses the aws:PrincipalTag condition key to scope the permissions.",
        "A": "Create IAM policies that include the required permissions. Include the aws:PrincipalTag condition key.",
        "C": "Create a group in the IdP. Place users in the group. Assign the group to accounts and the permission sets in IAM Identity Center.",
        "F": "Enable attributes for access control in IAM Identity Center. Map attributes from the IdP as key-value pairs.",
        "D": "Create a group in the IdP. Place users in the group. Assign the group to OUs and IAM policies.",
        "E": "Enable attributes for access control in IAM Identity Center. Apply tags to users. Map the tags as key-value pairs."
      },
      "correct_answer": "BCF",
      "answer_ET": "BCF",
      "answers_community": [
        "BCF (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/105236-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-04-05 02:42:00",
      "unix_timestamp": 1680655320,
      "discussion_count": 15,
      "discussion": [
        {
          "comment_id": "910300",
          "upvote_count": "13",
          "content": "Selected Answer: BCF\nI would go with BCF. I cannot make a large comment on why but manage an identity center setup at work and find that these are the correct ones IMHO. Your IdP has attributes, not tags, ou have to rely on the IdP's attributes for instance. And you work with permission sets almost always, so the three answers about the permission sets make the full answer. You do not use IAM directly or tags for this.",
          "poster": "bcx",
          "timestamp": "1685454960.0"
        },
        {
          "timestamp": "1680768300.0",
          "content": "Selected Answer: BCF\nThis is clearly stated here:\nhttps://aws.amazon.com/blogs/aws/new-attributes-based-access-control-with-aws-single-sign-on/\nAnswers are: BCF - permissions sets + IDP attributes mapping + groups \nFor example a user with IDP attribute of Dep/hr will be able to delete instances with this specific tag",
          "poster": "asfsdfsdf",
          "comment_id": "862759",
          "upvote_count": "6"
        },
        {
          "timestamp": "1722958920.0",
          "comment_id": "1261737",
          "upvote_count": "1",
          "poster": "namtp",
          "content": "Selected Answer: BCF\nBCF is correct anwers. \nPermission set + group created in the IdP, and map attributes is key"
        },
        {
          "poster": "Gomer",
          "upvote_count": "1",
          "content": "Selected Answer: BCF\nWhile I have no great insights or expertise in this area, I do know how to read (RTFM) and quasi-solve the puzzle in my head. This reference URL (pdf) seems to touch all the steps listed in \"B\", \"C\", \"F\" and showed some extra steps not listed. Search and see for yourself.\nhttps://d1.awsstatic.com/events/aws-reinforce-2022/IAM309_Designing-a-well-architected-identity-and-access-management-solution.pdf",
          "comment_id": "1217329",
          "timestamp": "1716541620.0",
          "comments": [
            {
              "upvote_count": "3",
              "timestamp": "1716542220.0",
              "poster": "Gomer",
              "comment_id": "1217336",
              "content": "Also, I might add, rather than just memorize the most votes answer to the question, I'd suggest actually going out to do some research and taking some long term notes you can reference later. That may take more time, but you also be more competent at work, and maybe keep your job longer. I love the fact that exam topics gives a forum to discuss and research complex questions and share findings. It's pretty lame If you come here to just memorize answers long enough to pass an exam."
            }
          ]
        },
        {
          "poster": "zijo",
          "upvote_count": "1",
          "timestamp": "1708635000.0",
          "content": "Permission sets are stored in IAM Identity Center. So you know all answers that mention about permission sets and IAM Identity Center are likely correct",
          "comment_id": "1156720"
        },
        {
          "poster": "thanhnv142",
          "timestamp": "1706413260.0",
          "content": "B, C, E seem more accurate:\nB- need to attach the policy so that it can be usable. A is not true because IAM policies is not the same as in IAM Identity Center\nC- not D because cannot assign group to IAM policies. IAM policies is attached to groups. also, need permission sets in Identity Center\nE- attributes is basically tagging.",
          "comment_id": "1133802",
          "upvote_count": "1"
        },
        {
          "content": "correct answer seen as A-B-C. but 11 people sure the correct answer is B-C-F in discussion.\nWhat is the answer? \nCan the system show the correct answer as wrong or are people mistaken?",
          "comment_id": "1083390",
          "comments": [
            {
              "comment_id": "1114695",
              "comments": [
                {
                  "timestamp": "1719821400.0",
                  "content": "Then why do people pay the fee for access, I dont understand. If it is from a discussion the people have to understand the answer (that too not very sure), why do they charge so much for the contributor access?!",
                  "comment_id": "1240036",
                  "poster": "ajeeshb",
                  "upvote_count": "2"
                }
              ],
              "upvote_count": "4",
              "poster": "davdan99",
              "content": "The examTopics answers in most cases are wrong, please read discussions, and references that users provide",
              "timestamp": "1704475380.0"
            }
          ],
          "upvote_count": "1",
          "poster": "SafranboluLokumu",
          "timestamp": "1701258420.0"
        },
        {
          "upvote_count": "2",
          "timestamp": "1688478000.0",
          "content": "https://docs.aws.amazon.com/singlesignon/latest/userguide/provision-automatically.html",
          "poster": "habros",
          "comment_id": "942822"
        },
        {
          "upvote_count": "5",
          "timestamp": "1688477940.0",
          "comments": [
            {
              "timestamp": "1691309700.0",
              "content": "An inline policy is a policy created for a single IAM identity (a user, group, or role). Inline policies maintain a strict one-to-one relationship between a policy and an identity\nA permission set is a template that you create and maintain that defines a collection of one or more IAM policies.",
              "poster": "Aja1",
              "comments": [
                {
                  "comment_id": "973646",
                  "poster": "Aja1",
                  "timestamp": "1691309880.0",
                  "upvote_count": "1",
                  "content": "IAM Identity Center helps you securely create, or connect, your workforce identities and manage their access centrally across AWS accounts and applications\nAttribute mappings are used to map attribute types that exist in IAM Identity Center with like attributes in an AWS Managed Microsoft AD directory. IAM Identity Center retrieves user attributes from your Microsoft AD directory and maps them to IAM Identity Center user attributes. These IAM Identity Center user attribute mappings are also used for generating SAML assertions for your cloud applications."
                }
              ],
              "comment_id": "973641",
              "upvote_count": "1"
            }
          ],
          "poster": "habros",
          "comment_id": "942819",
          "content": "Selected Answer: BCF\nExample if I use IdP as my group, and I add users to the group, then my users will be onboarded via the SCIM method. \nIAM roles does not apply to Control Tower landing zone. Hence B and C is secured (only permission sets for AWS SSO)\nDoes not make sense granting RBAC via tags…"
        },
        {
          "content": "Selected Answer: BCF\nBCF\nhttps://docs.aws.amazon.com/singlesignon/latest/userguide/abac.html",
          "timestamp": "1686219900.0",
          "upvote_count": "4",
          "poster": "madperro",
          "comment_id": "918121"
        },
        {
          "upvote_count": "1",
          "content": "Selected Answer: BCF\nI beleive BCF",
          "timestamp": "1685510820.0",
          "poster": "Rick365",
          "comment_id": "910833"
        },
        {
          "poster": "ParagSanyashiv",
          "upvote_count": "2",
          "timestamp": "1683523140.0",
          "comment_id": "891824",
          "content": "Selected Answer: BCF\nBCF makes more sense here."
        },
        {
          "upvote_count": "2",
          "poster": "alce2020",
          "comment_id": "871142",
          "timestamp": "1681578300.0",
          "content": "ill go with B,C,F"
        },
        {
          "poster": "ele",
          "upvote_count": "2",
          "content": "Selected Answer: BCF\nagree, BCF - permissions sets + IDP attributes mapping + groups",
          "comment_id": "863881",
          "timestamp": "1680871920.0"
        },
        {
          "poster": "lqpO_Oqpl",
          "upvote_count": "1",
          "comment_id": "861643",
          "timestamp": "1680655320.0",
          "content": "A, C, E"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:09:04.453Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "LXMRJsKWpOA08CAtdSBL",
      "question_number": 14,
      "page": 3,
      "question_text": "A company uses AWS CodeArtifact to centrally store Python packages. The CodeArtifact repository is configured with the following repository policy:\n\n//IMG//\n\n\nA development team is building a new project in an account that is in an organization in AWS Organizations. The development team wants to use a Python library that has already been stored in the CodeArtifact repository in the organization. The development team uses AWS CodePipeline and AWS CodeBuild to build the new application. The CodeBuild job that the development team uses to build the application is configured to run in a VPC. Because of compliance requirements, the VPC has no internet connectivity.\n\nThe development team creates the VPC endpoints for CodeArtifact and updates the CodeBuild buildspec.yaml file. However, the development team cannot download the Python library from the repository.\n\nWhich combination of steps should a DevOps engineer take so that the development team can use CodeArtifact? (Choose two.)",
      "choices": {
        "E": "Specify the account that hosts the repository as the delegated administrator for CodeArtifact in the organization.",
        "C": "Share the CodeArtifact repository with the organization by using AWS Resource Access Manager (AWS RAM).",
        "A": "Create an Amazon S3 gateway endpoint. Update the route tables for the subnets that are running the CodeBuild job.",
        "D": "Update the role that the CodeBuild project uses so that the role has sufficient permissions to use the CodeArtifact repository.",
        "B": "Update the repository policy’s Principal statement to include the ARN of the role that the CodeBuild project uses."
      },
      "correct_answer": "AD",
      "answer_ET": "AD",
      "answers_community": [
        "AD (52%)",
        "BD (40%)",
        "8%"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/108749-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [
        "https://img.examtopics.com/aws-certified-devops-engineer-professional-dop-c02/image7.png"
      ],
      "answer_images": [],
      "timestamp": "2023-05-08 15:43:00",
      "unix_timestamp": 1683553380,
      "discussion_count": 23,
      "discussion": [
        {
          "content": "Selected Answer: AD\nI guess the answer is AD because of this:\n\"AWS CodeArtifact operates in multiple Availability Zones and stores artifact data and metadata in Amazon S3 and Amazon DynamoDB. Your encrypted data is redundantly stored across multiple facilities and multiple devices in each facility, making it highly available and highly durable.\"\nhttps://aws.amazon.com/codeartifact/features/\nWith no internet connectivity, a gateway endpoint becomes necessary to access S3.",
          "comment_id": "908206",
          "poster": "TroyMcLure",
          "comments": [
            {
              "timestamp": "1685860860.0",
              "poster": "Arnaud92",
              "content": "https://docs.aws.amazon.com/codeartifact/latest/ug/create-s3-gateway-endpoint.html\n\nIt clearly state that you need to create a S3 endpoint to use codeartifact in a private network.",
              "upvote_count": "8",
              "comment_id": "914211",
              "comments": [
                {
                  "content": "An Amazon S3 endpoint is not needed when using Python or Swift package formats.",
                  "comments": [
                    {
                      "timestamp": "1720659300.0",
                      "upvote_count": "2",
                      "content": "When this question was created, there was no exception for Python and Swift packages. You can check this using the Wayback machine: https://web.archive.org/web/20230521063821/https://docs.aws.amazon.com/codeartifact/latest/ug/create-s3-gateway-endpoint.html\n\nConsidering that it's very common to have outdated questions in the exam, I'd say this is one those cases. So yeah, I'll also go with AD (also because B is not needed since the repository policy is already allowing the entire org).",
                      "comment_id": "1245805",
                      "poster": "syh_rapha"
                    }
                  ],
                  "upvote_count": "5",
                  "timestamp": "1707327480.0",
                  "poster": "vortegon",
                  "comment_id": "1143589"
                }
              ]
            },
            {
              "content": "A- incorrect because the question says Devops engineers careted VPC endpoints for CodeArtifact",
              "comment_id": "1007404",
              "comments": [
                {
                  "comment_id": "1011783",
                  "upvote_count": "2",
                  "timestamp": "1695166260.0",
                  "comments": [
                    {
                      "upvote_count": "1",
                      "content": "note here says \"An Amazon S3 endpoint is not needed when using Python or Swift package formats.\" \nhttps://docs.aws.amazon.com/codeartifact/latest/ug/create-s3-gateway-endpoint.html",
                      "comment_id": "1232537",
                      "timestamp": "1718729760.0",
                      "poster": "Venki_dev"
                    }
                  ],
                  "content": "AD even though Devops engineer created a CodeArtifcat still a S3 end point is required",
                  "poster": "RVivek"
                }
              ],
              "poster": "RVivek",
              "upvote_count": "2",
              "timestamp": "1694682480.0"
            }
          ],
          "upvote_count": "13",
          "timestamp": "1685227680.0"
        },
        {
          "poster": "Jowblow",
          "timestamp": "1683553380.0",
          "content": "Selected Answer: AD\nCodeartifact uses s3 gateway endpoints to store packages. The key word here are no internet access.",
          "upvote_count": "6",
          "comment_id": "892214"
        },
        {
          "timestamp": "1748273280.0",
          "poster": "92a2133",
          "comment_id": "1572485",
          "upvote_count": "1",
          "content": "Selected Answer: CD\nIts CD and when I saw the most voted I was shocked with the answers\n\nA. Even though CodeArtifact uses S3 it can also use DynamoDB, theres no mention of either so I immediately negated this answer\nB. Principal is already set to allow any\nE. Irrelevant"
        },
        {
          "upvote_count": "2",
          "content": "Selected Answer: BD\nCodeArtifact does not store packages in user-managed S3 buckets, and it does not require S3 endpoint access. This is unnecessary for CodeArtifact access.",
          "poster": "nickp84",
          "timestamp": "1746958680.0",
          "comment_id": "1568071"
        },
        {
          "content": "Selected Answer: AD\nAWS CodeArtifact stores artifact data and metadata in Amazon S3. To pull packages from CodeArtifact, you need to create an Amazon S3 gateway endpoint. You can use the aws ec2 create-vpc-endpoint AWS CLI command to create the endpoint.",
          "upvote_count": "1",
          "timestamp": "1733239500.0",
          "comment_id": "1321426",
          "poster": "spring21"
        },
        {
          "content": "Selected Answer: BD\nI vote for BD even though it's not so clear if both repo and CodeBuild are in the same org. Moreover, S3 GW endpoint auto-creates the routes with prefix-lists at your table. https://docs.aws.amazon.com/vpc/latest/privatelink/vpc-endpoints-s3.html#create-gateway-endpoint-s3\n\n Nevertheless as mentioned here in AWS documentation that other users posted S3 .\n\nI hope this question is removed from the exam.",
          "poster": "steli0",
          "upvote_count": "1",
          "timestamp": "1732554960.0",
          "comment_id": "1317659"
        },
        {
          "content": "Selected Answer: AD\nhttps://docs.aws.amazon.com/codeartifact/latest/ug/create-s3-gateway-endpoint.html",
          "upvote_count": "2",
          "timestamp": "1723016460.0",
          "poster": "iulian0585",
          "comment_id": "1261992"
        },
        {
          "content": "B - doesn't make any sense because aws:PrincipalOrgID condition key in repo policy already allows any principal within the org to access the repo",
          "upvote_count": "2",
          "poster": "auxwww",
          "comment_id": "1254634",
          "timestamp": "1721868300.0"
        },
        {
          "upvote_count": "1",
          "comment_id": "1232539",
          "poster": "Venki_dev",
          "content": "Selected Answer: BD\nBD\nnote here says \"An Amazon S3 endpoint is not needed when using Python or Swift package formats.\"\nhttps://docs.aws.amazon.com/codeartifact/latest/ug/create-s3-gateway-endpoint.html",
          "timestamp": "1718730180.0"
        },
        {
          "timestamp": "1718291580.0",
          "poster": "zijo",
          "comment_id": "1229912",
          "content": "Selected Answer: CD\nC is needed\nif the codeartifact and codebuild are in different organization accounts, AWS RAM is a service that allows you to share AWS resources with other AWS accounts within your organization. AWS RAM can be used to share CodeArtifact resources across different accounts.\n\nA is not needed\nyou do not need an S3 gateway as a VPC endpoint specifically for using AWS CodeArtifact with Python packages. AWS CodeArtifact itself manages the storage and retrieval of packages, and it uses its own service endpoints for these operations.\n\nD is needed for\nEnsure the IAM role used by CodeBuild has permissions to access CodeArtifact\n\nB is not needed \nHere it is not required because the CodeArtifact policy has Principal as *",
          "upvote_count": "3"
        },
        {
          "timestamp": "1715618280.0",
          "comments": [
            {
              "comment_id": "1217903",
              "content": "Pls Read link https://docs.aws.amazon.com/ram/latest/userguide/shareable.html",
              "upvote_count": "1",
              "poster": "vn_thanhtung",
              "timestamp": "1716596520.0"
            }
          ],
          "comment_id": "1210970",
          "poster": "that1guy",
          "content": "Selected Answer: CD\nC and D\n\nA - S3 gateway endpoint is not required for Python: https://docs.aws.amazon.com/codeartifact/latest/ug/create-s3-gateway-endpoint.html\n\nB - Principal is already \"*\".",
          "upvote_count": "1"
        },
        {
          "timestamp": "1714647660.0",
          "upvote_count": "2",
          "comment_id": "1205460",
          "poster": "seetpt",
          "content": "Selected Answer: AD\nAD because Principal is already \"*\"."
        },
        {
          "upvote_count": "2",
          "content": "Selected Answer: BD\nas for A: \"To pull packages from CodeArtifact, you must create a gateway endpoint for Amazon S3.\" but... \"Note - An Amazon S3 endpoint is not needed when using Python or Swift package formats.\"\nhttps://docs.aws.amazon.com/codeartifact/latest/ug/create-s3-gateway-endpoint.html",
          "timestamp": "1714143780.0",
          "comment_id": "1202667",
          "poster": "xdkonorek2"
        },
        {
          "poster": "c3518fc",
          "content": "Selected Answer: BD\nThe issue here is policy update as the developers have already enabled VPC endpoint (CodeArtifact uses Amazon Simple Storage Service (Amazon S3) to store package assets. To pull packages from CodeArtifact, you must create a gateway endpoint for Amazon S3. When your build or deployment process downloads packages from CodeArtifact, it must access CodeArtifact to get package metadata and Amazon S3 to download package assets (for example, Maven .jar files).\n\nNote\nAn Amazon S3 endpoint is not needed when using Python or Swift package formats.\n\nTo create the Amazon S3 gateway endpoint for CodeArtifact, use the Amazon EC2 create-vpc-endpoint AWS CLI command. When you create the endpoint, you must select the route tables for your VPC. For more information, see Gateway VPC Endpoints in the Amazon Virtual Private Cloud User Guide.)",
          "comments": [
            {
              "upvote_count": "1",
              "poster": "c3518fc",
              "timestamp": "1713539820.0",
              "comment_id": "1198725",
              "content": "https://docs.aws.amazon.com/codeartifact/latest/ug/create-s3-gateway-endpoint.html"
            }
          ],
          "timestamp": "1713539700.0",
          "comment_id": "1198724",
          "upvote_count": "4"
        },
        {
          "upvote_count": "4",
          "comment_id": "1195307",
          "timestamp": "1713074940.0",
          "poster": "dkp",
          "content": "Selected Answer: BD\nANS B&D\nCodeArtifact uses Amazon Simple Storage Service (Amazon S3) to store package assets. To pull packages from CodeArtifact, you must create a gateway endpoint for Amazon S3. When your build or deployment process downloads packages from CodeArtifact, it must access CodeArtifact to get package metadata and Amazon S3 to download package assets (for example, Maven .jar files).\nNote\nAn Amazon S3 endpoint is not needed when using Python or Swift package formats."
        },
        {
          "timestamp": "1711875420.0",
          "upvote_count": "3",
          "poster": "WhyIronMan",
          "comment_id": "1186693",
          "content": "Selected Answer: AD\nA,D are correct"
        },
        {
          "timestamp": "1708477440.0",
          "poster": "kyuhuck",
          "content": "Selected Answer: AD\n'ad' correct = 'AWS codeartiface' operates in multiple availability zones and stores artiface data and metadata in amazon s3 and amazon dynamoDB your encrypted data is redundanly stored across myltiple facilities and multiple devices in each facility, marking it highly availiable and highly durable...",
          "comment_id": "1155117",
          "upvote_count": "3"
        },
        {
          "timestamp": "1707327600.0",
          "poster": "vortegon",
          "comment_id": "1143591",
          "content": "Selected Answer: BD\nhttps://docs.aws.amazon.com/codeartifact/latest/ug/create-s3-gateway-endpoint.html\nAn Amazon S3 endpoint is not needed when using Python or Swift package formats.",
          "upvote_count": "3"
        },
        {
          "timestamp": "1707056520.0",
          "comment_id": "1140224",
          "content": "C and D:<the development team cannot download the Python library from the repository.> indicates insufficient permission or network problem\nA: irrelevant\nB: principal is already * for everyone, including the ARN of the codebuild role\nE: irrelevant",
          "poster": "thanhnv142",
          "upvote_count": "3"
        },
        {
          "content": "AD\n\nhttps://docs.aws.amazon.com/codeartifact/latest/ug/create-s3-gateway-endpoint.html",
          "upvote_count": "2",
          "poster": "abdulwahab_sysops",
          "timestamp": "1697334720.0",
          "comment_id": "1043819"
        },
        {
          "poster": "AWSdeveloper08",
          "upvote_count": "3",
          "timestamp": "1697179680.0",
          "comment_id": "1042370",
          "content": "Selected Answer: BD\nB - This could be valid. If the CodeBuild job doesn’t have permission to access the CodeArtifact repository because of the repository policy, updating the policy to include the CodeBuild role ARN in the Principal statement could solve the access issue.\nD- If the role used by AWS CodeBuild does not have the necessary IAM permissions to access CodeArtifact, updating the role to grant these permissions might resolve the issue. Ensuring that the IAM role has the codeartifact:DescribePackageVersion, codeartifact:GetPackageVersionReadme, codeartifact:GetRepositoryEndpoint, codeartifact:ListPackageVersions, and codeartifact:ReadFromRepository permissions could be essential.",
          "comments": [
            {
              "comment_id": "1062734",
              "poster": "2pk",
              "content": "B is incorrect- The policy clearly indicate that the permission is given to all the resources access within the Org-ID. So this is not needed",
              "comments": [
                {
                  "timestamp": "1713538920.0",
                  "content": "Goes the policy have an ARN permission?",
                  "comment_id": "1198715",
                  "upvote_count": "1",
                  "poster": "c3518fc"
                }
              ],
              "upvote_count": "2",
              "timestamp": "1699176180.0"
            }
          ]
        },
        {
          "comment_id": "897830",
          "poster": "2pk",
          "content": "Selected Answer: BD\nBD correct, A is incorrect because creating an Amazon S3 gateway endpoint is not required to enable connectivity to CodeArtifact. S3 endpoints are used to enable private communication to S3 buckets within a VPC, but they are not related to CodeArtifact.",
          "timestamp": "1684088820.0",
          "upvote_count": "4",
          "comments": [
            {
              "comment_id": "1003055",
              "poster": "BaburTurk",
              "upvote_count": "1",
              "content": "https://docs.aws.amazon.com/codeartifact/latest/ug/create-s3-gateway-endpoint.html CodeArtifact uses Amazon Simple Storage Service (Amazon S3) to store package assets. To pull packages from CodeArtifact, you must create a gateway endpoint for Amazon S3. When your build or deployment process downloads packages from CodeArtifact, it must access CodeArtifact to get package metadata and Amazon S3 to download package assets (for example, Maven .jar files).\n\nTo create the Amazon S3 gateway endpoint for CodeArtifact, use the Amazon EC2 create-vpc-endpoint AWS CLI command. When you create the endpoint, you must select the route tables for your VPC",
              "timestamp": "1694250120.0"
            }
          ]
        },
        {
          "timestamp": "1683628500.0",
          "upvote_count": "3",
          "content": "Selected Answer: AD\nhttps://aws.amazon.com/codeartifact/features/",
          "comment_id": "892995",
          "poster": "Jeanphi72"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:09:04.453Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "vyr5vqtipfcjGU8fLkG5",
      "question_number": 15,
      "page": 3,
      "question_text": "A company uses a series of individual Amazon CloudFormation templates to deploy its multi-Region applications. These templates must be deployed in a specific order. The company is making more changes to the templates than previously expected and wants to deploy new templates more efficiently. Additionally, the data engineering team must be notified of all changes to the templates.\n\nWhat should the company do to accomplish these goals?",
      "choices": {
        "B": "Host the CloudFormation templates in Amazon S3. Use Amazon S3 events to directly trigger CloudFormation updates and Amazon SNS notifications.",
        "A": "Create an AWS Lambda function to deploy the CloudFormation templates in the required order. Use stack policies to alert the data engineering team.",
        "D": "Leverage CloudFormation nested stacks and stack sets for deployments. Use Amazon SNS to notify the data engineering team.",
        "C": "Implement CloudFormation StackSets and use drift detection to trigger update alerts to the data engineering team."
      },
      "correct_answer": "D",
      "answer_ET": "D",
      "answers_community": [
        "D (94%)",
        "6%"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/108801-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-05-09 12:26:00",
      "unix_timestamp": 1683627960,
      "discussion_count": 16,
      "discussion": [
        {
          "timestamp": "1705492980.0",
          "upvote_count": "13",
          "comment_id": "954062",
          "poster": "emupsx1",
          "content": "The answer is D because:\nA few hours ago, I just finished the DOP-C02 exam.\nMy score is 1000 points.\nThis question has come up, I choose D.",
          "comments": [
            {
              "poster": "BaburTurk",
              "comment_id": "1003059",
              "content": "pics or it did not happen, troll bot account",
              "timestamp": "1709982480.0",
              "upvote_count": "9"
            }
          ]
        },
        {
          "timestamp": "1729351980.0",
          "upvote_count": "5",
          "poster": "c3518fc",
          "content": "Selected Answer: D\nHere's why this solution is the best approach:\n\nNested Stacks: CloudFormation nested stacks allow you to break down complex templates into smaller, more manageable templates. You can create a root stack that references and manages multiple nested stacks. This approach simplifies the management and deployment of multiple interdependent templates in the correct order.\nStackSets: CloudFormation StackSets allow you to create, update, or delete stacks across multiple AWS accounts and regions with a single operation. This addresses the requirement of deploying applications across multiple regions efficiently.\nAmazon SNS: Amazon Simple Notification Service (SNS) can be used to send notifications to the data engineering team whenever changes are made to the CloudFormation templates or stacks.",
          "comment_id": "1198733"
        },
        {
          "poster": "dkp",
          "upvote_count": "1",
          "comment_id": "1195309",
          "timestamp": "1728886500.0",
          "content": "Selected Answer: D\nshould be D"
        },
        {
          "content": "Selected Answer: D\nAnswer is D.",
          "upvote_count": "1",
          "poster": "WhyIronMan",
          "comment_id": "1186694",
          "timestamp": "1727686740.0"
        },
        {
          "comment_id": "1153888",
          "upvote_count": "2",
          "content": "Selected Answer: D\nthe nested for order :\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-nested-stacks.html",
          "poster": "jojom19980",
          "timestamp": "1724061000.0"
        },
        {
          "timestamp": "1722774360.0",
          "comment_id": "1140231",
          "content": "D is correct: <uses a series of individual Amazon CloudFormation templates to deploy its multi-Region applications> and <wants to deploy new templates more efficiently> mean stacksets, which is template for multiple regions. <the data engineering team must be notified of all changes> means SNS\nA and B: no mention of stacksets\nC: no mention of SNS",
          "upvote_count": "3",
          "poster": "thanhnv142"
        },
        {
          "timestamp": "1719963900.0",
          "poster": "sarlos",
          "upvote_count": "1",
          "comment_id": "1112409",
          "content": "D is the answer"
        },
        {
          "content": "Selected Answer: D\nIt's D.\nC is not correct since according to this link:\nhttps://aws.amazon.com/blogs/mt/implementing-an-alarm-to-automatically-detect-drift-in-aws-cloudformation-stacks/\nWe need AWS config rule to detect drifts and to send event. There is no build in solution to notify drift detection like mentioned in C,",
          "timestamp": "1714214460.0",
          "upvote_count": "1",
          "comment_id": "1055465",
          "poster": "YR4591"
        },
        {
          "comment_id": "1010939",
          "upvote_count": "1",
          "timestamp": "1710815040.0",
          "content": "Selected Answer: D\nC is for notifying changes on what has been deployed by Cloud Formation\nD is for notifying changes made on the Cloud Formation template (the recipe) itself",
          "poster": "daburahjail"
        },
        {
          "upvote_count": "1",
          "content": "Selected Answer: D\nC didn't mention how to deploy in a specific order.\nD mentioned nested stack, which you can configure the dependency ordering",
          "timestamp": "1710121140.0",
          "comment_id": "1004361",
          "poster": "beanxyz"
        },
        {
          "poster": "OrganizedChaos25",
          "timestamp": "1700229300.0",
          "upvote_count": "1",
          "comment_id": "900086",
          "content": "I got D as my answer"
        },
        {
          "timestamp": "1700015400.0",
          "comment_id": "897945",
          "poster": "devnv",
          "upvote_count": "1",
          "content": "D is correct"
        },
        {
          "content": "Selected Answer: D\nit' s D, I think.",
          "comment_id": "896387",
          "poster": "PhuocT",
          "upvote_count": "1",
          "timestamp": "1699853940.0"
        },
        {
          "content": "Selected Answer: D\nD is correct.",
          "upvote_count": "1",
          "poster": "ParagSanyashiv",
          "timestamp": "1699776240.0",
          "comment_id": "895681"
        },
        {
          "comment_id": "894251",
          "poster": "Sazeka",
          "upvote_count": "1",
          "content": "Selected Answer: D\nThe correct solution is D:\n\nThe solution works as follows:\n\nAWS Config triggers the evaluation when any resource that matches the rule’s scope (currently set to “AWS::CloudFormation::Stack”) changes in configuration and at the frequency (“MaximumExecutionFrequency” parameter) that you specify at the time of this solution deployment.\nEventBridge receives the events from AWS Config, applies the EventBridge rule to match the compliance change event, and transforms the input (customize the text) as defined in the “InputTransformer” template.\nThe chosen customer-managed KMS Key is then accessed to encrypt the notification.\nThe encrypted notification is published to the target SNS topic.\n The endpoints subscribed to this topic start receiving the published messages.",
          "timestamp": "1699646700.0"
        },
        {
          "poster": "Jeanphi72",
          "content": "Selected Answer: C\nI think C: https://aws.amazon.com/blogs/mt/implementing-an-alarm-to-automatically-detect-drift-in-aws-cloudformation-stacks/",
          "upvote_count": "1",
          "comment_id": "892989",
          "timestamp": "1699532760.0"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:09:04.453Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "6QNDHhUFEZr4JE3nB04f",
      "question_number": 16,
      "page": 4,
      "question_text": "A DevOps engineer has implemented a CI/CD pipeline to deploy an AWS CloudFormation template that provisions a web application. The web application consists of an Application Load Balancer (ALB), a target group, a launch template that uses an Amazon Linux 2 AMI, an Auto Scaling group of Amazon EC2 instances, a security group, and an Amazon RDS for MySQL database. The launch template includes user data that specifies a script to install and start the application.\n\nThe initial deployment of the application was successful. The DevOps engineer made changes to update the version of the application with the user data. The CI/CD pipeline has deployed a new version of the template. However, the health checks on the ALB are now failing. The health checks have marked all targets as unhealthy.\n\nDuring investigation, the DevOps engineer notices that the CloudFormation stack has a status of UPDATE_COMPLETE. However, when the DevOps engineer connects to one of the EC2 instances and checks /var/log/messages, the DevOps engineer notices that the Apache web server failed to start successfully because of a configuration error.\n\nHow can the DevOps engineer ensure that the CloudFormation deployment will fail if the user data fails to successfully finish running?",
      "choices": {
        "B": "Create an Amazon CloudWatch alarm for the UnhealthyHostCount metric. Include an appropriate alarm threshold for the target group. Create an Amazon Simple Notification Service (Amazon SNS) topic as the target to signal success or failure to CloudFormation.",
        "A": "Use the cfn-signal helper script to signal success or failure to CloudFormation. Use the WaitOnResourceSignals update policy within the CloudFormation template. Set an appropriate timeout for the update policy.",
        "D": "Use the Amazon CloudWatch agent to stream the cloud-init logs. Create a subscription filter that includes an AWS Lambda function with an appropriate invocation timeout. Configure the Lambda function to use the SignalResource API operation to signal success or failure to CloudFormation.",
        "C": "Create a lifecycle hook on the Auto Scaling group by using the AWS::AutoScaling::LifecycleHook resource. Create an Amazon Simple Notification Service (Amazon SNS) topic as the target to signal success or failure to CloudFormation. Set an appropriate timeout on the lifecycle hook."
      },
      "correct_answer": "A",
      "answer_ET": "A",
      "answers_community": [
        "A (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/109044-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-05-12 08:14:00",
      "unix_timestamp": 1683872040,
      "discussion_count": 9,
      "discussion": [
        {
          "content": "Selected Answer: A\nA is correct",
          "comment_id": "956674",
          "upvote_count": "6",
          "timestamp": "1689772920.0",
          "poster": "Certified101"
        },
        {
          "upvote_count": "3",
          "poster": "zijo",
          "content": "Selected Answer: A\nTo ensure that the CloudFormation deployment fails if the user data script does not successfully finish, you can use a combination of AWS CloudFormation's CreationPolicy, cfn-signal, and wait condition resources. These mechanisms can signal CloudFormation about the success or failure of the instance creation process, including the execution of user data scripts.",
          "comment_id": "1230061",
          "timestamp": "1718305740.0"
        },
        {
          "upvote_count": "2",
          "comment_id": "1195313",
          "content": "Selected Answer: A\nA is correct",
          "timestamp": "1713075660.0",
          "poster": "dkp"
        },
        {
          "timestamp": "1707057240.0",
          "upvote_count": "3",
          "comment_id": "1140246",
          "poster": "thanhnv142",
          "content": "A is correct: <ensure that the CloudFormation deployment will fail if the user data fails to successfully finish running> means we need cfn-signal\nB, C and D: no mention of cfn-signal"
        },
        {
          "content": "yes A.\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-signal.html",
          "poster": "sarlos",
          "timestamp": "1704246480.0",
          "upvote_count": "3",
          "comment_id": "1112411"
        },
        {
          "content": "The instance is running, and the logs are available. The configuration happens inside the instance by the userdata. How A is correct if the issue is beyond CF?",
          "comments": [
            {
              "upvote_count": "2",
              "timestamp": "1701872940.0",
              "content": "Ok now make sense: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-signal.html",
              "poster": "svjl",
              "comment_id": "1089430"
            }
          ],
          "timestamp": "1701872820.0",
          "poster": "svjl",
          "upvote_count": "1",
          "comment_id": "1089427"
        },
        {
          "comment_id": "900088",
          "upvote_count": "1",
          "timestamp": "1684324620.0",
          "poster": "OrganizedChaos25",
          "content": "A is correct"
        },
        {
          "timestamp": "1684110840.0",
          "comment_id": "897951",
          "content": "A is the right answer",
          "upvote_count": "1",
          "poster": "devnv"
        },
        {
          "poster": "ParagSanyashiv",
          "timestamp": "1683872040.0",
          "upvote_count": "2",
          "content": "Selected Answer: A\nAgree with A",
          "comment_id": "895686"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:09:15.304Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "hVreEfM7Or4kt5ZytOkD",
      "question_number": 17,
      "page": 4,
      "question_text": "A company has a data ingestion application that runs across multiple AWS accounts. The accounts are in an organization in AWS Organizations. The company needs to monitor the application and consolidate access to the application. Currently, the company is running the application on Amazon EC2 instances from several Auto Scaling groups. The EC2 instances have no access to the internet because the data is sensitive. Engineers have deployed the necessary VPC endpoints. The EC2 instances run a custom AMI that is built specifically for the application.\n\nTo maintain and troubleshoot the application, system administrators need the ability to log in to the EC2 instances. This access must be automated and controlled centrally. The company’s security team must receive a notification whenever the instances are accessed.\n\nWhich solution will meet these requirements?",
      "choices": {
        "C": "Use EC2 Image Builder to rebuild the custom AMI. Include the most recent version of AWS Systems Manager Agent in the image. Configure the Auto Scaling group to attach the AmazonSSMManagedInstanceCore role to all the EC2 instances. Use Systems Manager Session Manager to log in to the instances. Enable logging of session details to Amazon S3. Create an S3 event notification for new file uploads to send a message to the security team through an Amazon Simple Notification Service (Amazon SNS) topic.",
        "B": "Deploy a NAT gateway and a bastion host that has internet access. Create a security group that allows incoming traffic on all the EC2 instances from the bastion host. Install AWS Systems Manager Agent on all the EC2 instances. Use Auto Scaling group lifecycle hooks for monitoring and auditing access. Use Systems Manager Session Manager to log in to the instances. Send logs to a log group in Amazon CloudWatch Logs. Export data to Amazon S3 for auditing. Send notifications to the security team by using S3 event notifications.",
        "A": "Create an Amazon EventBridge rule to send notifications to the security team whenever a user logs in to an EC2 instance. Use EC2 Instance Connect to log in to the instances. Deploy Auto Scaling groups by using AWS CloudFormation. Use the cfn-init helper script to deploy appropriate VPC routes for external access. Rebuild the custom AMI so that the custom AMI includes AWS Systems Manager Agent.",
        "D": "Use AWS Systems Manager Automation to build Systems Manager Agent into the custom AMI. Configure AWS Config to attach an SCP to the root organization account to allow the EC2 instances to connect to Systems Manager. Use Systems Manager Session Manager to log in to the instances. Enable logging of session details to Amazon S3. Create an S3 event notification for new file uploads to send a message to the security team through an Amazon Simple Notification Service (Amazon SNS) topic."
      },
      "correct_answer": "C",
      "answer_ET": "C",
      "answers_community": [
        "C (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/109191-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-05-14 02:07:00",
      "unix_timestamp": 1684022820,
      "discussion_count": 8,
      "discussion": [
        {
          "timestamp": "1688557500.0",
          "upvote_count": "7",
          "comment_id": "943697",
          "content": "Selected Answer: C\nC and D are left over choice due to no internet access for EC2\n\nC is correct \nBy using EC2 Image Builder to rebuild the custom AMI and including the most recent version of AWS Systems Manager Agent in the image, you can configure the Auto Scaling group to attach the AmazonSSMManagedInstanceCore role to all the EC2 instances. This allows you to use Systems Manager Session Manager to log in to the instances. You can enable logging of session details to Amazon S3 and create an S3 event notification for new file uploads to send a message to the security team through an Amazon Simple Notification Service (Amazon SNS) topic2",
          "poster": "Blueee"
        },
        {
          "poster": "thanhnv142",
          "timestamp": "1707058140.0",
          "upvote_count": "6",
          "content": "C is correct: <The company needs to monitor the application and consolidate access to the application> means using SSM. We should install SSM agent on all EC2 instances. <The EC2 instances run a custom AMI that is built specifically for the application> means we should rebuild the image and integrate agent into the AMI. To rebuild, the best option is EC2 image builder. <The company’s security team must receive a notification whenever the instances are accessed.> means SNS \nA: <Rebuild the custom AMI so that the custom AMI includes AWS Systems Manager Agent.>: no mention of using EC2 image builder and SNS\nB: no mention of integrating SSM agents into the AMI and we cannot just send S3 noti to users <Send notifications to the security team by using S3 event notifications.>\nD: no me ntion of using EC2 image builder to rebuild the AMI.",
          "comment_id": "1140267"
        },
        {
          "timestamp": "1731525240.0",
          "comment_id": "1311578",
          "poster": "Saudis",
          "content": "Ans is C because The keyword is access must be automated and controlled centrally",
          "upvote_count": "1"
        },
        {
          "comment_id": "1259704",
          "timestamp": "1722579360.0",
          "upvote_count": "2",
          "content": "Selected Answer: C\n- AWS Systems Manager Agent\n- Systems Manager Session Manager for login the instances\n- enable logging of session details to s3\n- s3 event notification to SNS.",
          "poster": "jamesf"
        },
        {
          "poster": "dkp",
          "comment_id": "1195351",
          "content": "Selected Answer: C\nC is correct",
          "upvote_count": "1",
          "timestamp": "1713079020.0"
        },
        {
          "comment_id": "964262",
          "poster": "haazybanj",
          "timestamp": "1690419720.0",
          "content": "Selected Answer: C\nC\n\nOption C offers a well-architected approach to addressing the requirements, providing both centralized access and logging, and automated login to EC2 instances for system administrators. Additionally, it ensures that the security team receives notifications for auditing and monitoring purposes.",
          "upvote_count": "2"
        },
        {
          "upvote_count": "3",
          "timestamp": "1684563720.0",
          "comment_id": "902386",
          "content": "D is not a good option for the following reasons:\n\n1. AWS Systems Manager Automation is not the ideal choice for building a custom AMI. Instead, EC2 Image Builder, as stated in option C, is an AWS service designed for building, testing, and maintaining Golden Amazon Machine Images (AMIs), making it a suitable choice for both building and managing custom AMIs.\n \n2. The option D suggests attaching an SCP (Service Control Policy) to the root organization to allow EC2 instances to connect to Systems Manager. This approach is incorrect because SCPs are used to define permissions on an organizational level, rather than allowing specific access between resources like EC2 instances and Systems Manager. Attaching the AmazonSSMManagedInstanceCore role to EC2 instances as mentioned in option C is the correct method, which allows instances to communicate with Systems Manager.",
          "poster": "PhuocT"
        },
        {
          "comments": [
            {
              "timestamp": "1748274300.0",
              "poster": "92a2133",
              "comment_id": "1572492",
              "content": "because SCPs are not meant to be used to explicitly allow access to resources, its meant to deny permissions within an organization",
              "upvote_count": "1"
            },
            {
              "content": "Because I don't think AWS Config can be used to attach an SCP.",
              "upvote_count": "2",
              "poster": "MarDog",
              "comment_id": "928836",
              "timestamp": "1687298160.0"
            }
          ],
          "poster": "2pk",
          "content": "if someone know why D is not correct , pls post",
          "upvote_count": "2",
          "timestamp": "1684022820.0",
          "comment_id": "897140"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:09:15.304Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "yQZZGEJn7TK1voINxbUx",
      "question_number": 18,
      "page": 4,
      "question_text": "A company uses Amazon S3 to store proprietary information. The development team creates buckets for new projects on a daily basis. The security team wants to ensure that all existing and future buckets have encryption, logging, and versioning enabled. Additionally, no buckets should ever be publicly read or write accessible.\n\nWhat should a DevOps engineer do to meet these requirements?",
      "choices": {
        "C": "Enable AWS Trusted Advisor and configure automatic remediation using Amazon EventBridge.",
        "D": "Enable AWS Systems Manager and configure automatic remediation using Systems Manager documents.",
        "B": "Enable AWS Config rules and configure automatic remediation using AWS Systems Manager documents.",
        "A": "Enable AWS CloudTrail and configure automatic remediation using AWS Lambda."
      },
      "correct_answer": "B",
      "answer_ET": "B",
      "answers_community": [
        "B (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/109533-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-05-17 14:13:00",
      "unix_timestamp": 1684325580,
      "discussion_count": 6,
      "discussion": [
        {
          "comment_id": "1195353",
          "upvote_count": "2",
          "timestamp": "1728890340.0",
          "poster": "dkp",
          "content": "Selected Answer: B\nAWS config to remediate non-compliace"
        },
        {
          "comment_id": "1140657",
          "upvote_count": "4",
          "content": "Selected Answer: B\nB is correct: <wants to ensure that all existing and future buckets have encryption, logging, and versioning enabled> means we need aws config. \nA, C and D: no mention of AWS config",
          "poster": "thanhnv142",
          "timestamp": "1722822300.0"
        },
        {
          "content": "Answer B",
          "timestamp": "1720605540.0",
          "comment_id": "1118527",
          "upvote_count": "1",
          "poster": "yuliaqwerty"
        },
        {
          "comment_id": "1112416",
          "timestamp": "1719964560.0",
          "upvote_count": "1",
          "content": "yes B is correct",
          "poster": "sarlos"
        },
        {
          "comment_id": "914239",
          "poster": "Arnaud92",
          "upvote_count": "4",
          "timestamp": "1701682800.0",
          "content": "Selected Answer: B\nAWS Config allows you to remediate noncompliant resources that are evaluated by AWS Config Rules. AWS Config applies remediation using AWS Systems Manager Automation documents.\n\nsee https://docs.aws.amazon.com/config/latest/developerguide/remediation.html"
        },
        {
          "content": "Selected Answer: B\nB is correct",
          "comment_id": "900104",
          "timestamp": "1700230380.0",
          "upvote_count": "3",
          "poster": "OrganizedChaos25"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:09:15.304Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "E0MPffNCwopfZAuZ37ly",
      "question_number": 19,
      "page": 4,
      "question_text": "A DevOps engineer is researching the least expensive way to implement an image batch processing cluster on AWS. The application cannot run in Docker containers and must run on Amazon EC2. The batch job stores checkpoint data on an NFS volume and can tolerate interruptions. Configuring the cluster software from a generic EC2 Linux image takes 30 minutes.\n\nWhat is the MOST cost-effective solution?",
      "choices": {
        "D": "Use Amazon EFS for checkpoint data. Use EC2 Fleet to launch EC2 Spot Instances. Create a custom AMI for the cluster and use the latest AMI when creating instances.",
        "A": "Use Amazon EFS for checkpoint data. To complete the job, use an EC2 Auto Scaling group and an On-Demand pricing model to provision EC2 instances temporarily.",
        "C": "Use Amazon EFS for checkpoint data. Use EC2 Fleet to launch EC2 Spot Instances, and utilize user data to configure the EC2 Linux instance on startup.",
        "B": "Use GlusterFS on EC2 instances for checkpoint data. To run the batch job, configure EC2 instances manually. When the job completes, shut down the instances manually."
      },
      "correct_answer": "D",
      "answer_ET": "D",
      "answers_community": [
        "D (88%)",
        "8%"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/108799-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-05-09 00:00:00",
      "unix_timestamp": 1683583200,
      "discussion_count": 13,
      "discussion": [
        {
          "timestamp": "1683876780.0",
          "content": "Selected Answer: D\nD is more suitable, as it says to avoid 30min launch time.",
          "poster": "ParagSanyashiv",
          "comment_id": "895731",
          "comments": [
            {
              "timestamp": "1684244340.0",
              "content": "I assume you are saying D over C as D will make the EC2 instances operational quicker, while C would require 30 minutes to install the software before it can start to be used. resulting in it being more cost effective.",
              "comment_id": "899244",
              "upvote_count": "3",
              "poster": "Mail1964"
            }
          ],
          "upvote_count": "7"
        },
        {
          "comment_id": "1257267",
          "upvote_count": "2",
          "content": "Selected Answer: D\nkeywords: MOST cost-effective, a generic EC2 Linux image takes 30 minutes.\nMean take 30mins or longer time for EC2 booting and will cost more.",
          "poster": "jamesf",
          "timestamp": "1722229680.0"
        },
        {
          "upvote_count": "3",
          "poster": "WhyIronMan",
          "timestamp": "1711882320.0",
          "comment_id": "1186720",
          "content": "Selected Answer: D\nD is the correct answer.\nMake the calculations 30 min of bootstraping when you have multiple scale actions is a lot of time idle when you have many instances, so the money spent during a lot of spot request that wast time boostraping is larger than keeping a single AMI.\n\nDetails are everything during an investigation..."
        },
        {
          "content": "Selected Answer: C\nAnswer is C.\nC is cheaper than D",
          "upvote_count": "2",
          "comment_id": "1155871",
          "comments": [
            {
              "poster": "HayLLlHuK",
              "comment_id": "1190910",
              "upvote_count": "3",
              "content": "\"utilize user data to configure the EC2 Linux instance on startup\" - it takes an addiction time to configure an instance.\nit's better to use a custom AMI and have everything preinstalled",
              "timestamp": "1712488620.0"
            }
          ],
          "timestamp": "1708551120.0",
          "poster": "Diego1414"
        },
        {
          "poster": "thanhnv142",
          "timestamp": "1707105060.0",
          "comment_id": "1140658",
          "comments": [
            {
              "poster": "GripZA",
              "upvote_count": "1",
              "content": "The 30min bootstrapping will incur runtime costs, which can be avoided by using the latest AMI already containing the files, installtions etc done by the bootstrapping.",
              "comment_id": "1562198",
              "timestamp": "1745151180.0"
            },
            {
              "comment_id": "1175897",
              "content": "As for me, extra 30 minutes for each EC2 launch seems like an extra cost comparing to one-time built AMI.\nSo D looks cheaper than C",
              "timestamp": "1710686880.0",
              "upvote_count": "3",
              "poster": "vmahilevskyi"
            }
          ],
          "upvote_count": "3",
          "content": "C is correct: <can tolerate interruptions> means EC2 spot instances. \nA and B: no mention of spot instances\nD: Create a custom AMI for the cluster and use the latest AMI when creating instances: this incurs more cost than option C, which incurs no cost for the configuration step"
        },
        {
          "comment_id": "1072625",
          "timestamp": "1700157720.0",
          "content": "Selected Answer: D\nIt's D",
          "upvote_count": "2",
          "poster": "zain1258"
        },
        {
          "comment_id": "1050257",
          "timestamp": "1697955960.0",
          "poster": "Cloud_noob",
          "content": "Selected Answer: D\nD most suitable. why would you be willing to wait for 30 minutes for software installation?",
          "upvote_count": "2"
        },
        {
          "comment_id": "993943",
          "upvote_count": "4",
          "timestamp": "1693390500.0",
          "comments": [
            {
              "upvote_count": "2",
              "comment_id": "1140660",
              "content": "I think you may be wrong. D - creating custom AMI costs you a lot. You would need to buy services from EC2 image builder to rebuild and pay for the one who rebuild the custom AMI. Meanwhile, configuring manually (option C) costs you nothing but time. Of course, in corporate environment, time is money, but you would cost way less than option D - which suggests cost from creating and maintaining the AMI image (latest AMI image)",
              "poster": "thanhnv142",
              "timestamp": "1707105360.0"
            }
          ],
          "content": "D - Spot Instances are cheaper than Ec2 and the workload can tolerate interruptions. Also Custom AMI removes the need for 30 mins configuration which takes a resource that will need to be paid",
          "poster": "FEEREWMWKA"
        },
        {
          "timestamp": "1689508140.0",
          "upvote_count": "3",
          "comment_id": "953275",
          "content": "C is the answer. Because it specifically mentioned EC2 Linux instance as requested in the question. D only mentioned creating an AMI but is it Linux AMI?",
          "poster": "ogwu2000",
          "comments": [
            {
              "content": "Read the question properly. :D It says Linux EC2 instances..",
              "comment_id": "1062727",
              "timestamp": "1699174740.0",
              "upvote_count": "2",
              "poster": "2pk"
            }
          ]
        },
        {
          "content": "Selected Answer: D\nD is correct",
          "timestamp": "1684325640.0",
          "comment_id": "900106",
          "poster": "OrganizedChaos25",
          "upvote_count": "4"
        },
        {
          "upvote_count": "3",
          "content": "D is the right answer",
          "comment_id": "898776",
          "poster": "devnv",
          "timestamp": "1684195320.0"
        },
        {
          "timestamp": "1683727140.0",
          "poster": "meisme",
          "comment_id": "894052",
          "content": "Selected Answer: D\nD is correct",
          "upvote_count": "2"
        },
        {
          "upvote_count": "1",
          "timestamp": "1683626400.0",
          "comment_id": "892968",
          "poster": "Jeanphi72",
          "content": "Selected Answer: B\nB because: \" Configuring the cluster software from a generic EC2 Linux image takes 30 minutes.\n\""
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:09:15.304Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "H0sVLC3uT7MZDVh6UKaO",
      "question_number": 20,
      "page": 4,
      "question_text": "A company recently migrated its legacy application from on-premises to AWS. The application is hosted on Amazon EC2 instances behind an Application Load Balancer, which is behind Amazon API Gateway. The company wants to ensure users experience minimal disruptions during any deployment of a new version of the application. The company also wants to ensure it can quickly roll back updates if there is an issue.\n\nWhich solution will meet these requirements with MINIMAL changes to the application?",
      "choices": {
        "C": "Introduce changes as a separate target group behind the existing Application Load Balancer. Configure API Gateway to route user traffic to the new target group in steps.",
        "D": "Introduce changes as a separate target group behind the existing Application Load Balancer. Configure API Gateway to route all traffic to the Application Load Balancer, which then sends the traffic to the new target group.",
        "B": "Introduce changes as a separate environment parallel to the existing one. Update the application’s DNS alias records to point to the new environment.",
        "A": "Introduce changes as a separate environment parallel to the existing one. Configure API Gateway to use a canary release deployment to send a small subset of user traffic to the new environment."
      },
      "correct_answer": "A",
      "answer_ET": "A",
      "answers_community": [
        "A (84%)",
        "Other"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/108798-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-05-09 11:52:00",
      "unix_timestamp": 1683625920,
      "discussion_count": 15,
      "discussion": [
        {
          "content": "Selected Answer: A\nOption A is also a valid approach that can meet the requirements with MINIMAL changes to the application.\n\nIn Option A, the changes are introduced as a separate environment parallel to the existing one. This new environment can be used to deploy the new version of the application. By configuring API Gateway to use a canary release deployment, a small subset of user traffic is directed to the new environment, while the majority of traffic continues to be routed to the existing environment hosting the current version of the application.",
          "poster": "haazybanj",
          "timestamp": "1690420440.0",
          "upvote_count": "6",
          "comment_id": "964265"
        },
        {
          "content": "Selected Answer: A\nKeywords: minimal disruptions, MINIMAL changes",
          "timestamp": "1722229860.0",
          "comment_id": "1257268",
          "upvote_count": "1",
          "poster": "jamesf"
        },
        {
          "timestamp": "1718383560.0",
          "upvote_count": "2",
          "poster": "zijo",
          "comment_id": "1230572",
          "content": "Selected Answer: A\nI chose A because the question says MINIMAL changes to the application. Canary deployment needs minimal changes to the application as it requires only adding canary settings to the deployment stage. AWS API Gateway supports canary deployments, allowing you to route a percentage of your traffic to a new stage or version of your API."
        },
        {
          "content": "Selected Answer: C\nC:\nSeparate Target Group: By introducing a new target group behind the existing Application Load Balancer, you can direct traffic to this new target group without affecting the existing environment. This means the new version of the application can be tested in isolation.\nStep-by-Step Traffic Routing: API Gateway allows you to gradually shift user traffic from one target group (existing version) to another (new version). This means you can start with a small percentage of traffic and gradually increase it, allowing you to monitor the new version's performance and stability.\nQuick Rollback: If the new version has any issues, you can quickly revert the traffic to the original target group, ensuring minimal disruption to users.\nSeparate Environment with Canary Deployment: This introduces a completely separate environment, requiring additional infrastructure management and potentially more configuration changes depending on how the environments are set up.",
          "timestamp": "1713080580.0",
          "comment_id": "1195365",
          "upvote_count": "2",
          "poster": "dkp"
        },
        {
          "comment_id": "1186728",
          "content": "Selected Answer: A\nAgree with A. A parallel environment will allow the company to test the deployment, do smoke tests and all the basic stuff to check if the application is working fine. Then, they can start serving traffic to the users, allowing 10% of the users to go to the new environment and test. \nThe key is the word \"disruptions\" disruption can be considered a failure in the infrastructure, in the communications (networking) but NOT a Bug. Even do, switching 10% of the users to test is best than switching the entire loadbalancer to a new target group because in this scenarios is 100% of users affect against 10%\n\nDetails are everything during an investigation...",
          "poster": "WhyIronMan",
          "upvote_count": "3",
          "timestamp": "1711882980.0"
        },
        {
          "upvote_count": "1",
          "timestamp": "1709087340.0",
          "poster": "dzn",
          "comment_id": "1161200",
          "content": "Selected Answer: D\nA is not meet the \"MINIMAL changes to the application\" requirement. If application receives requests from a different ALB, the application will receive a different request value, such as HTTP headers, and may need to be modified application. Since D is the same ALB, it is unlikely that changes will be necessary."
        },
        {
          "timestamp": "1707111660.0",
          "content": "A is correct: <users experience minimal disruptions during any deployment of a new version of the application.> and <ensure it can quickly roll back updates if there is an issue> means deploy in parallel: canary release or blue/green deployment\nB, C and D: If there was a large bug with the a new version, users would experience huge service disruptions",
          "poster": "thanhnv142",
          "upvote_count": "3",
          "comment_id": "1140692"
        },
        {
          "comment_id": "1136753",
          "poster": "Ramdi1",
          "upvote_count": "3",
          "timestamp": "1706705220.0",
          "content": "Selected Answer: A\nCanary deployment is used to stop disruption hence I have voted A"
        },
        {
          "content": "Selected Answer: D\nAnswer is D. Even API supports canary deployment it is only if the API can redirect the traffic between two stages, in this case the API sends the traffic directly to the ALB, and from the ALB yo can choose to which environment redirect the traffic",
          "upvote_count": "2",
          "timestamp": "1703705640.0",
          "comment_id": "1107122",
          "poster": "zolthar_z"
        },
        {
          "upvote_count": "1",
          "comment_id": "1095728",
          "timestamp": "1702490520.0",
          "content": "Why not Option D, as the deployment is API gateway-->ALB-->target groups(EC2). and question is saying that: The company wants to ensure users experience minimal disruptions during any deployment of a new version of the application, with minimal changes to application (it is not saying that we shouldn't change deployment steps)",
          "poster": "Ffida2214"
        },
        {
          "poster": "OrganizedChaos25",
          "upvote_count": "3",
          "comment_id": "900109",
          "timestamp": "1684325700.0",
          "content": "Selected Answer: A\nA is definitely correct"
        },
        {
          "upvote_count": "1",
          "content": "A is correct",
          "timestamp": "1684195500.0",
          "comment_id": "898777",
          "poster": "devnv"
        },
        {
          "content": "Selected Answer: A\nA is correct answer",
          "poster": "ParagSanyashiv",
          "upvote_count": "2",
          "timestamp": "1683877080.0",
          "comment_id": "895732"
        },
        {
          "timestamp": "1683742620.0",
          "content": "Selected Answer: A\nThe correct answer is A\nCorrect Answer is A. API Gateway supports canary deployment on a deployment stage before you direct all traffic to that stage. A parallel environment means we will create a new ALB and a target group that will target a new set of EC2 instances on which the newer version of the app will be deployed. So the canary setting associated to the new version of the API will connect with the new ALB instance which in turn will direct the traffic to the new EC2 instances on which the newer version of the application is deployed.",
          "upvote_count": "4",
          "poster": "Sazeka",
          "comment_id": "894262"
        },
        {
          "content": "Selected Answer: A\nI disagree with C: https://docs.aws.amazon.com/apigateway/latest/developerguide/canary-release.html",
          "comment_id": "892964",
          "upvote_count": "3",
          "timestamp": "1683625920.0",
          "poster": "Jeanphi72"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:09:15.304Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "k6kgRvAvshGPu3wADXL9",
      "question_number": 21,
      "page": 5,
      "question_text": "A company is storing 100 GB of log data in .csv format in an Amazon S3 bucket. SQL developers want to query this data and generate graphs to visualize it. The SQL developers also need an efficient, automated way to store metadata from the .csv file.\n\nWhich combination of steps will meet these requirements with the LEAST amount of effort? (Choose three.)",
      "choices": {
        "E": "Use the AWS Glue Data Catalog as the persistent metadata store.",
        "C": "Query the data with Amazon Athena.",
        "A": "Filter the data through AWS X-Ray to visualize the data.",
        "B": "Filter the data through Amazon QuickSight to visualize the data.",
        "D": "Query the data with Amazon Redshift.",
        "F": "Use Amazon DynamoDB as the persistent metadata store."
      },
      "correct_answer": "BCE",
      "answer_ET": "BCE",
      "answers_community": [
        "BCE (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/109048-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-05-12 09:42:00",
      "unix_timestamp": 1683877320,
      "discussion_count": 7,
      "discussion": [
        {
          "upvote_count": "6",
          "comment_id": "947287",
          "content": "Selected Answer: BCE\nBCE. Glue Data Catalog can crawl S3 buckets to store table metadata. Then call the data catalog directly in Athena. It will show the partitions of the data. \n\nAthena does not deal with DynamoDB directly. Hence F is out.",
          "poster": "habros",
          "comments": [
            {
              "poster": "habros",
              "timestamp": "1704814500.0",
              "comment_id": "947288",
              "content": "https://docs.aws.amazon.com/glue/latest/dg/catalog-and-crawler.html",
              "upvote_count": "3"
            }
          ],
          "timestamp": "1704814440.0"
        },
        {
          "upvote_count": "5",
          "content": "BCE are correct: < query this data and generate graphs to visualize it> means athena and quicksight\nA: irrelevant\nD: too expensive\nF: Dynamodb is primarily used for storing web session data and not for this purpose",
          "timestamp": "1722829680.0",
          "poster": "thanhnv142",
          "comment_id": "1140695"
        },
        {
          "poster": "92a2133",
          "timestamp": "1748348460.0",
          "upvote_count": "1",
          "comment_id": "1572689",
          "content": "Selected Answer: BCE\nif you get this wrong you need to revoke any AWS certs you already have..."
        },
        {
          "upvote_count": "3",
          "comment_id": "900131",
          "timestamp": "1700231700.0",
          "content": "Selected Answer: BCE\nBCE are correct",
          "poster": "OrganizedChaos25"
        },
        {
          "comment_id": "898780",
          "content": "BCE are correct",
          "poster": "devnv",
          "upvote_count": "1",
          "timestamp": "1700100540.0"
        },
        {
          "upvote_count": "1",
          "content": "yep, agree with B,C and E.",
          "comment_id": "896391",
          "poster": "PhuocT",
          "timestamp": "1699855620.0"
        },
        {
          "upvote_count": "2",
          "poster": "ParagSanyashiv",
          "content": "Selected Answer: BCE\nAgree with BCE",
          "timestamp": "1699782120.0",
          "comment_id": "895734"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:09:25.769Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "UL2FV6VWZNDPh21QOwOM",
      "question_number": 22,
      "page": 5,
      "question_text": "A company deploys its corporate infrastructure on AWS across multiple AWS Regions and Availability Zones. The infrastructure is deployed on Amazon EC2 instances and connects with AWS IoT Greengrass devices. The company deploys additional resources on on-premises servers that are located in the corporate headquarters.\n\nThe company wants to reduce the overhead involved in maintaining and updating its resources. The company’s DevOps team plans to use AWS Systems Manager to implement automated management and application of patches. The DevOps team confirms that Systems Manager is available in the Regions that the resources are deployed in. Systems Manager also is available in a Region near the corporate headquarters.\n\nWhich combination of steps must the DevOps team take to implement automated patch and configuration management across the company’s EC2 instances, IoT devices, and on-premises infrastructure? (Choose three.)",
      "choices": {
        "A": "Apply tags to all the EC2 instances, AWS IoT Greengrass devices, and on-premises servers. Use Systems Manager Session Manager to push patches to all the tagged devices.",
        "B": "Use Systems Manager Run Command to schedule patching for the EC2 instances, AWS IoT Greengrass devices, and on-premises servers.",
        "D": "Configure Amazon EventBridge to monitor Systems Manager Patch Manager for updates to patch baselines. Associate Systems Manager Run Command with the event to initiate a patch action for all EC2 instances, AWS IoT Greengrass devices, and on-premises servers.",
        "C": "Use Systems Manager Patch Manager to schedule patching for the EC2 instances, AWS IoT Greengrass devices, and on-premises servers as a Systems Manager maintenance window task.",
        "E": "Create an IAM instance profile for Systems Manager. Attach the instance profile to all the EC2 instances in the AWS account. For the AWS IoT Greengrass devices and on-premises servers, create an IAM service role for Systems Manager.",
        "F": "Generate a managed-instance activation. Use the Activation Code and Activation ID to install Systems Manager Agent (SSM Agent) on each server in the on-premises environment. Update the AWS IoT Greengrass IAM token exchange role. Use the role to deploy SSM Agent on all the IoT devices."
      },
      "correct_answer": "CEF",
      "answer_ET": "CEF",
      "answers_community": [
        "CEF (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/108792-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-05-09 10:15:00",
      "unix_timestamp": 1683620100,
      "discussion_count": 10,
      "discussion": [
        {
          "content": "Selected Answer: CEF\nI also choose E instead of B.\n\nWhy E is correct: \"Previously in this post, you created and deployed the SSM Agent component which would have created an IAM service role. Suppose the AWS IoT Greengrass documentation was followed to deploy the SSM agent. In that case, the name of the IAM service role should be SSMServiceRole.\"\n\nWhy is B wrong: B is redundant given that answer C calls out Systems Manager Patch Manager which itself uses Systems Manager Run Command. Furthermore Run Command is described here to be used to run automated scripts and not to schedule patching: \"we’ll demonstrate how to use Session Manager to open remote login to an edge device, patch them using Patch Manager, and run automated scripts through Run Command\"\n\nQuotes above are from: https://aws.amazon.com/blogs/mt/how-to-centrally-manage-aws-iot-greengrass-devices-using-aws-systems-manager/?force_isolation=true",
          "timestamp": "1684847040.0",
          "poster": "4bed5ff",
          "upvote_count": "8",
          "comment_id": "904929"
        },
        {
          "poster": "thanhnv142",
          "content": "CEF: \n- < implement automated patch> means Systems Manager Patch Manager\n- < configuration management > means we need install system manager agent\n- we need to configure sufficient permissions for SSM",
          "timestamp": "1707116820.0",
          "upvote_count": "7",
          "comment_id": "1140723"
        },
        {
          "upvote_count": "1",
          "comment_id": "1257273",
          "content": "Selected Answer: CEF\nSystems Manager Patch Manager, System Manager Agent, permission",
          "poster": "jamesf",
          "timestamp": "1722230220.0"
        },
        {
          "upvote_count": "3",
          "timestamp": "1713549240.0",
          "content": "Selected Answer: CEF\nBy following the combination of steps C, E, and F, the DevOps team can effectively implement automated patch and configuration management across the company's EC2 instances, IoT Greengrass devices, and on-premises infrastructure using AWS Systems Manager's capabilities and best practices.",
          "poster": "c3518fc",
          "comment_id": "1198798"
        },
        {
          "upvote_count": "1",
          "content": "Selected Answer: CEF\nans is CEF",
          "poster": "dkp",
          "timestamp": "1713080940.0",
          "comment_id": "1195373"
        },
        {
          "poster": "DanShone",
          "content": "Selected Answer: CEF\nCEF are correct",
          "timestamp": "1710606600.0",
          "upvote_count": "1",
          "comment_id": "1175104"
        },
        {
          "content": "Selected Answer: CEF\nCEF are correct",
          "poster": "OrganizedChaos25",
          "timestamp": "1684326960.0",
          "comment_id": "900133",
          "upvote_count": "1"
        },
        {
          "timestamp": "1684024140.0",
          "poster": "2pk",
          "content": "Agreed with Parag CEF",
          "comment_id": "897154",
          "upvote_count": "1"
        },
        {
          "upvote_count": "1",
          "timestamp": "1683877560.0",
          "content": "Selected Answer: CEF\nCEF make more sense.",
          "comment_id": "895736",
          "poster": "ParagSanyashiv"
        },
        {
          "poster": "Jeanphi72",
          "timestamp": "1683620100.0",
          "content": "Selected Answer: CEF\nI disagree with the solution ... FEC for me",
          "upvote_count": "1",
          "comment_id": "892917"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:09:25.769Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "ccYhjoOqDIEBtDz7h7RY",
      "question_number": 23,
      "page": 5,
      "question_text": "A company is testing a web application that runs on Amazon EC2 instances behind an Application Load Balancer. The instances run in an Auto Scaling group across multiple Availability Zones. The company uses a blue/green deployment process with immutable instances when deploying new software.\n\nDuring testing, users are being automatically logged out of the application at random times. Testers also report that, when a new version of the application is deployed, all users are logged out. The development team needs a solution to ensure users remain logged in across scaling events and application deployments.\n\nWhat is the MOST operationally efficient way to ensure users remain logged in?",
      "choices": {
        "C": "Store user session information in an Amazon S3 bucket and modify the application to read session information from the bucket.",
        "B": "Enable session sharing on the load balancer and modify the application to read from the session store.",
        "A": "Enable smart sessions on the load balancer and modify the application to check for an existing session.",
        "D": "Modify the application to store user session information in an Amazon ElastiCache cluster."
      },
      "correct_answer": "D",
      "answer_ET": "D",
      "answers_community": [
        "D (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/109194-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-05-14 02:32:00",
      "unix_timestamp": 1684024320,
      "discussion_count": 9,
      "discussion": [
        {
          "upvote_count": "5",
          "content": "D is correct: <During testing, users are being automatically logged out of the application at random times>: the cause is there is no data storage that stores user's session. We need a session data storage to store user session",
          "comments": [
            {
              "poster": "thanhnv142",
              "upvote_count": "1",
              "timestamp": "1707120060.0",
              "comment_id": "1140748",
              "content": "A. Enable smart sessions on the load balance: there is no smart session on ALB\nB. Enable session sharing on the load balancer: load balancer does not store session data\nC. storing session data in S3 introduces latency"
            }
          ],
          "poster": "thanhnv142",
          "timestamp": "1707118440.0",
          "comment_id": "1140734"
        },
        {
          "upvote_count": "1",
          "poster": "jamesf",
          "timestamp": "1722870480.0",
          "comment_id": "1261096",
          "content": "Selected Answer: D\nkeywords: Amazon ElastiCache cluster"
        },
        {
          "comment_id": "1195376",
          "timestamp": "1713081660.0",
          "content": "Selected Answer: D\nD is correct",
          "upvote_count": "2",
          "poster": "dkp"
        },
        {
          "timestamp": "1711884240.0",
          "poster": "WhyIronMan",
          "content": "Selected Answer: D\nD is the correct one",
          "upvote_count": "2",
          "comment_id": "1186737"
        },
        {
          "poster": "a54b16f",
          "comment_id": "1120841",
          "upvote_count": "1",
          "timestamp": "1705069980.0",
          "content": "Selected Answer: D\nhttps://aws.amazon.com/blogs/developer/elasticache-as-an-asp-net-session-store/"
        },
        {
          "comments": [
            {
              "comment_id": "1162240",
              "upvote_count": "3",
              "content": "This is why many web application frameworks support Redis and Memcached session. Also S3 is expensive to read, latency.",
              "timestamp": "1709184960.0",
              "poster": "dzn"
            },
            {
              "comment_id": "908156",
              "content": "like comment by accident. S3 is an object store, its not a mounted FS as such, potential performance issues & consistency rule it out. Session data - keep it local, caching system like redis or DB. C actually requires a lot more work as who is now managing the sessions, the bucket, keeping all in sync?",
              "timestamp": "1685211780.0",
              "poster": "lunt",
              "upvote_count": "2"
            }
          ],
          "comment_id": "903724",
          "poster": "EricZhang",
          "content": "Why not C? Compared to D C is serverless thus more operationally efficient.",
          "timestamp": "1684732500.0",
          "upvote_count": "2"
        },
        {
          "comment_id": "900135",
          "poster": "OrganizedChaos25",
          "content": "Selected Answer: D\nD is correct",
          "upvote_count": "4",
          "timestamp": "1684327020.0"
        },
        {
          "content": "agree with D",
          "comment_id": "898784",
          "upvote_count": "1",
          "timestamp": "1684196280.0",
          "poster": "devnv"
        },
        {
          "content": "Yep D, session should be stored to share.",
          "comment_id": "897157",
          "upvote_count": "1",
          "poster": "2pk",
          "timestamp": "1684024320.0"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:09:25.769Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "C34R3XZIu4ACsWqjvmyp",
      "question_number": 24,
      "page": 5,
      "question_text": "An ecommerce company is receiving reports that its order history page is experiencing delays in reflecting the processing status of orders. The order processing system consists of an AWS Lambda function that uses reserved concurrency. The Lambda function processes order messages from an Amazon Simple Queue Service (Amazon SQS) queue and inserts processed orders into an Amazon DynamoDB table. The DynamoDB table has auto scaling enabled for read and write capacity.\nWhich actions should a DevOps engineer take to resolve this delay? (Choose two.)",
      "choices": {
        "C": "Check the NumberOfMessagesSent metric for the SQS queue. Increase the SQS queue visibility timeout.",
        "A": "Check the ApproximateAgeOfOldestMessage metric for the SQS queue. Increase the Lambda function concurrency limit.",
        "B": "Check the ApproximateAgeOfOldestMessage metnc for the SQS queue Configure a redrive policy on the SQS queue.",
        "E": "Check the Throttles metric for the Lambda function. Increase the Lambda function timeout.",
        "D": "Check the WriteThrottleEvents metric for the DynamoDB table. Increase the maximum write capacity units (WCUs) for the table's scaling policy."
      },
      "correct_answer": "AD",
      "answer_ET": "AD",
      "answers_community": [
        "AD (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/105508-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-04-07 14:59:00",
      "unix_timestamp": 1680872340,
      "discussion_count": 8,
      "discussion": [
        {
          "comment_id": "918126",
          "upvote_count": "10",
          "content": "Selected Answer: AD\nAD look fine.",
          "poster": "madperro",
          "timestamp": "1686220140.0"
        },
        {
          "content": "Everyone who has commented AD has not provided any reasoning. Bunch of sheeps in the comments.",
          "comments": [
            {
              "timestamp": "1718272860.0",
              "poster": "tgv",
              "upvote_count": "4",
              "content": "D: This action is important because if the WriteThrottleEvents metric is high, it indicates that DynamoDB is throttling writes due to insufficient write capacity. By increasing the maximum WCUs, you ensure that the table can handle the increased write throughput required by the Lambda function, thus reducing delays in order processing.\n even though DynamoDB has auto-scaling enabled, it’s still important to monitor the WriteThrottleEvents metric. Auto-scaling adjusts capacity based on the workload, but it may not always keep up with sudden spikes in demand or be configured optimally for this specific use case. Ensuring that the maximum write capacity units (WCUs) are set appropriately can help prevent throttling during peak times.",
              "comment_id": "1229742"
            }
          ],
          "poster": "kiwtirApp",
          "comment_id": "1209956",
          "upvote_count": "3",
          "timestamp": "1715458980.0"
        },
        {
          "comment_id": "1133808",
          "upvote_count": "3",
          "poster": "thanhnv142",
          "content": "A and D is correct:\nA: Check ApproximateAgeOfOldestMessage and increase concurrency accordingly\nD: Check throttleevent (the number of rejected requests) and increase max write accordingly.",
          "timestamp": "1706414040.0"
        },
        {
          "poster": "SPRao",
          "comments": [
            {
              "upvote_count": "4",
              "comment_id": "1092302",
              "content": "Eventhough the statement say\n\n\n\"The DynamoDB table has auto scaling enabled for read and write capacity.\"\n\nA and D still are a good answer. \n\nOption C looks at the NumberOfMessagesSent metric and suggests increasing the SQS queue visibility timeout. This action is more relevant when messages are not being processed before the visibility timeout expires, but it does not seem to be the primary issue in this scenario.",
              "timestamp": "1702189740.0",
              "poster": "harithzainudin"
            }
          ],
          "timestamp": "1700375700.0",
          "content": "Answer should be A & C. D is wrong as DynamoDB has autoscaling enabled so Writethrottle should not be the case.",
          "upvote_count": "3",
          "comment_id": "1074478"
        },
        {
          "upvote_count": "4",
          "timestamp": "1694371380.0",
          "poster": "Jonfernz",
          "content": "A: Check the ApproximateAgeOfOldestMessage metric for the SQS queue. If this age is high, increasing the Lambda function's concurrency limit would help speed up processing.\n\nD: Check the WriteThrottleEvents metric for the DynamoDB table. If write operations are being throttled, increasing the maximum WCUs for the table’s scaling policy could help.",
          "comment_id": "1004237"
        },
        {
          "timestamp": "1683523200.0",
          "content": "Selected Answer: AD\nAD are accurate for this scenario.",
          "upvote_count": "4",
          "poster": "ParagSanyashiv",
          "comment_id": "891825"
        },
        {
          "poster": "alce2020",
          "timestamp": "1681503060.0",
          "comment_id": "870450",
          "content": "a and d are correct",
          "upvote_count": "1"
        },
        {
          "timestamp": "1680872340.0",
          "poster": "ele",
          "comment_id": "863887",
          "content": "Selected Answer: AD\nupscale both",
          "upvote_count": "2"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:09:25.769Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "IWOvfftwVKFkc2HJRiBW",
      "question_number": 25,
      "page": 5,
      "question_text": "A DevOps engineer needs to configure a blue/green deployment for an existing three-tier application. The application runs on Amazon EC2 instances and uses an Amazon RDS database. The EC2 instances run behind an Application Load Balancer (ALB) and are in an Auto Scaling group.\n\nThe DevOps engineer has created a launch template and an Auto Scaling group for the blue environment. The DevOps engineer also has created a launch template and an Auto Scaling group for the green environment. Each Auto Scaling group deploys to a matching blue or green target group. The target group also specifies which software, blue or green, gets loaded on the EC2 instances. The ALB can be configured to send traffic to the blue environment’s target group or the green environment’s target group. An Amazon Route 53 record for www.example.com points to the ALB.\n\nThe deployment must move traffic all at once between the software on the blue environment’s EC2 instances to the newly deployed software on the green environment’s EC2 instances.\n\nWhat should the DevOps engineer do to meet these requirements?",
      "choices": {
        "A": "Start a rolling restart of the Auto Scaling group for the green environment to deploy the new software on the green environment’s EC2 instances. When the rolling restart is complete, use an AWS CLI command to update the ALB to send traffic to the green environment’s target group.",
        "D": "Start a rolling restart of the Auto Scaling group for the green environment to deploy the new software on the green environment’s EC2 instances. When the rolling restart is complete, update the Route 53 DNS to point to the green environment’s endpoint on the ALB.",
        "C": "Update the launch template to deploy the green environment’s software on the blue environment’s EC2 instances. Keep the target groups and Auto Scaling groups unchanged in both environments. Perform a rolling restart of the blue environment’s EC2 instances.",
        "B": "Use an AWS CLI command to update the ALB to send traffic to the green environment’s target group. Then start a rolling restart of the Auto Scaling group for the green environment to deploy the new software on the green environment’s EC2 instances."
      },
      "correct_answer": "A",
      "answer_ET": "A",
      "answers_community": [
        "A (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/109195-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-05-14 02:40:00",
      "unix_timestamp": 1684024800,
      "discussion_count": 14,
      "discussion": [
        {
          "comment_id": "902393",
          "poster": "PhuocT",
          "content": "A is correct, cannot be D, as there is only one ALB. The ALB can be configured to send traffic to the blue environment’s target group or the green environment’s target group. Traffic route to blue or green, must be configure at Load balancer, in this case, update the target group.",
          "timestamp": "1684564680.0",
          "upvote_count": "17"
        },
        {
          "poster": "jamesf",
          "upvote_count": "3",
          "content": "Selected Answer: A\nOne Application Load Balancer (ALB)\nOne Auto Scaling Group (ASG) for Blue and one Auto Scaling Group (ASG) for Green",
          "comment_id": "1257277",
          "timestamp": "1722230700.0"
        },
        {
          "comment_id": "1195379",
          "upvote_count": "2",
          "timestamp": "1713082200.0",
          "poster": "dkp",
          "content": "Selected Answer: A\nits A."
        },
        {
          "poster": "sirronido",
          "timestamp": "1712748000.0",
          "content": "B correct .......\noption A reverses the order of operations, which goes against the recommended practice of updating the load balancer first to send traffic to the new environment before deploying the new software.",
          "upvote_count": "1",
          "comment_id": "1192913"
        },
        {
          "timestamp": "1707120360.0",
          "poster": "thanhnv142",
          "comment_id": "1140750",
          "upvote_count": "4",
          "content": "Selected Answer: A\nA is correct: <The deployment must move traffic all at once between the software on the blue environment’s EC2 instances to the newly deployed software on the green environment’s EC2 instances.> and <The ALB can be configured to send traffic to the blue environment’s target group or the green environment’s target group.>means we should do the traffic migration manually by config the ALB\nB and C: no mention of migration step\nD: should not use route 53 DNS, we need to configure the ALB"
        },
        {
          "comment_id": "1059658",
          "poster": "rlf",
          "content": "Answer is A\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-updatepolicy.html#cfn-attributes-updatepolicy-rollingupdate",
          "timestamp": "1698838860.0",
          "upvote_count": "2"
        },
        {
          "content": "The client-side caches the results of DNS queries, so DNS switching lacks immediacy, and it's challenging to transition traffic all at once. Therefore, option D doesn't meet the requirement of moving traffic all at once and is not suitable.",
          "comment_id": "984613",
          "upvote_count": "1",
          "timestamp": "1692371760.0",
          "poster": "kyuhobe"
        },
        {
          "upvote_count": "1",
          "timestamp": "1692174300.0",
          "content": "What???? No one read the question carefully. There are two ALB.\nThe DevOps engineer has created a launch template and an Auto Scaling group for the blue environment. The DevOps engineer also has created a launch template and an Auto Scaling group for the green environment.",
          "poster": "ixdb",
          "comments": [
            {
              "timestamp": "1692174360.0",
              "poster": "ixdb",
              "upvote_count": "2",
              "content": "D is right.",
              "comment_id": "982328"
            },
            {
              "poster": "lluukkyy",
              "content": "No, you mixed it up. There is only one ALB and two ASGs. Option A is the answer as there is no need to touch Route53 in this scenario(it's pointing to the single ALB already).",
              "comment_id": "1083891",
              "upvote_count": "2",
              "timestamp": "1701303900.0"
            }
          ],
          "comment_id": "982325"
        },
        {
          "comment_id": "957262",
          "content": "Selected Answer: A\nCannot be D as the Route 53 record will be unchanged - points to the same ALB - the target group on the ALB need to be updated",
          "poster": "Certified101",
          "timestamp": "1689838080.0",
          "upvote_count": "2"
        },
        {
          "upvote_count": "2",
          "timestamp": "1688612460.0",
          "poster": "Blueee",
          "comment_id": "944266",
          "content": "Selected Answer: A\nA is correct"
        },
        {
          "poster": "MarDog",
          "content": "I don't think it's D, because it's a single ALB and Route 53 is already pointing at it as wwww.example.com. What needs to occur is an ASG switch within the ALB. So, A is the best bet.",
          "comment_id": "928843",
          "timestamp": "1687298940.0",
          "upvote_count": "2"
        },
        {
          "content": "A is correct. \nFor D, DNS alias record needs to be updated, and green environment's endpoint wasn't mentioned in the question.",
          "poster": "walkwolf3",
          "timestamp": "1686221100.0",
          "upvote_count": "1",
          "comment_id": "918146"
        },
        {
          "content": "Yes D sounds the best approach",
          "timestamp": "1684196640.0",
          "poster": "devnv",
          "comment_id": "898787",
          "upvote_count": "1"
        },
        {
          "timestamp": "1684024800.0",
          "comment_id": "897161",
          "poster": "2pk",
          "content": "It must be D. DNS record should be diverted to the green ALB",
          "upvote_count": "1",
          "comments": [
            {
              "poster": "sb333",
              "comment_id": "956089",
              "content": "The question states there is only one ALB with two target groups (Blue and Green). The DNS record points to that one ALB. So answer D is not correct.",
              "upvote_count": "1",
              "timestamp": "1689730200.0"
            }
          ]
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:09:25.769Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "LTvcnNElGDDtisNHpXH5",
      "question_number": 26,
      "page": 6,
      "question_text": "A company is building a new pipeline by using AWS CodePipeline and AWS CodeBuild in a build account. The pipeline consists of two stages. The first stage is a CodeBuild job to build and package an AWS Lambda function. The second stage consists of deployment actions that operate on two different AWS accounts: a development environment account and a production environment account. The deployment stages use the AWS CloudFormation action that CodePipeline invokes to deploy the infrastructure that the Lambda function requires.\n\nA DevOps engineer creates the CodePipeline pipeline and configures the pipeline to encrypt build artifacts by using the AWS Key Management Service (AWS KMS) AWS managed key for Amazon S3 (the aws/s3 key). The artifacts are stored in an S3 bucket. When the pipeline runs, the CloudFormation actions fail with an access denied error.\n\nWhich combination of actions must the DevOps engineer perform to resolve this error? (Choose two.)",
      "choices": {
        "D": "In the development account and in the production account, create an IAM role for CodePipeline. Configure the roles with permissions to perform CloudFormation operations and with permissions to retrieve and decrypt objects from the artifacts S3 bucket. In the CodePipeline account, configure the CodePipeline CloudFormation action to use the roles.",
        "B": "Create a customer managed KMS key. Configure the KMS key policy to allow the IAM roles used by the CloudFormation action to perform decrypt operations. Modify the pipeline to use the customer managed KMS key to encrypt artifacts.",
        "A": "Create an S3 bucket in each AWS account for the artifacts. Allow the pipeline to write to the S3 buckets. Create a CodePipeline S3 action to copy the artifacts to the S3 bucket in each AWS account. Update the CloudFormation actions to reference the artifacts S3 bucket in the production account.",
        "C": "Create an AWS managed KMS key. Configure the KMS key policy to allow the development account and the production account to perform decrypt operations. Modify the pipeline to use the KMS key to encrypt artifacts.",
        "E": "In the development account and in the production account, create an IAM role for CodePipeline. Configure the roles with permissions to perform CloudFormation operations and with permissions to retrieve and decrypt objects from the artifacts S3 bucket. In the CodePipeline account, modify the artifacts S3 bucket policy to allow the roles access. Configure the CodePipeline CloudFormation action to use the roles."
      },
      "correct_answer": "BE",
      "answer_ET": "BE",
      "answers_community": [
        "BE (80%)",
        "13%",
        "7%"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/108791-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-05-09 09:52:00",
      "unix_timestamp": 1683618720,
      "discussion_count": 17,
      "discussion": [
        {
          "poster": "lunt",
          "timestamp": "1685212140.0",
          "upvote_count": "14",
          "comments": [
            {
              "comments": [
                {
                  "timestamp": "1702204020.0",
                  "comments": [
                    {
                      "poster": "heff_bezos",
                      "upvote_count": "1",
                      "timestamp": "1727072460.0",
                      "content": "From your link:\n\"You can add or remove IAM users, IAM roles, and AWS accounts in the key policy, and change the actions that are allowed or denied for those principals.\"\nThe answer is BE because you don't want to grant permissions to the KMS key for an ENTIRE account, you'd want to allow access for a particular role.",
                      "comment_id": "1288009"
                    }
                  ],
                  "comment_id": "1092417",
                  "content": "From your link: https://docs.aws.amazon.com/kms/latest/developerguide/key-policy-modifying.html\n\nWhen changing a key policy, keep in mind the following rules:\n- You can view the key policy for an AWS managed key or a customer managed key, but you can only change the key policy for a customer managed key. \n- The policies of AWS managed keys are created and managed by the AWS service that created the KMS key in your account. \n- You cannot view or change the key policy for an AWS owned key.",
                  "upvote_count": "2",
                  "poster": "robertohyena"
                }
              ],
              "timestamp": "1701959280.0",
              "poster": "svjl",
              "content": "You van modify the key policies, it is a managed key. What is wrong is change it to use for different account.\nhttps://docs.aws.amazon.com/kms/latest/developerguide/key-policy-modifying.html",
              "comment_id": "1090369",
              "upvote_count": "2"
            }
          ],
          "content": "Selected Answer: BE\nC = AWS KMS fundamentals. Cannot modify AWS managed KMS key policies. No Cross account access = will not work. Not sure why there is even a discussion on this. Associate level basics.",
          "comment_id": "908158"
        },
        {
          "timestamp": "1734794040.0",
          "poster": "youonebe",
          "comment_id": "1330101",
          "upvote_count": "1",
          "content": "Selected Answer: BD\nthere is no need to modify the artifacts S3 bucket policy to allow the roles access"
        },
        {
          "content": "Selected Answer: BE\nB - Cannot modify AWS managed KMS key policies.\nE - Cross account access and we need bucket policies also to be updated, if its same account then we do not need bucket policies permissions",
          "upvote_count": "2",
          "poster": "jamesf",
          "timestamp": "1722232740.0",
          "comment_id": "1257285"
        },
        {
          "comment_id": "1232977",
          "upvote_count": "3",
          "poster": "xdkonorek2",
          "content": "Selected Answer: BD\nBD,\ntry it yourself, create account with a bucket, create role with access to s3 operations, and trust policy for another account. \nrole assumed by another account has full access to s3 resources thereby it's not needed to set up resource policy on s3 bucket",
          "timestamp": "1718815800.0"
        },
        {
          "content": "Selected Answer: BD\nAnswer is BD , \nI have recently implemented similar solution, and my S3 bucket do not have any policy configured , my IAM role has required KMS key permission and it worked. \n\nmodifying the S3 bucket policy, but this is not necessary if the IAM roles are correctly configured and used by the CodePipeline CloudFormation action",
          "timestamp": "1717909440.0",
          "comment_id": "1227073",
          "poster": "Venki_dev",
          "comments": [
            {
              "timestamp": "1718760180.0",
              "content": "I switch to BE , \n\nbecause its cross account access and we need bucket policies also to be updated, if its same account then we do not need bucket policies permissions",
              "poster": "Venki_dev",
              "upvote_count": "3",
              "comment_id": "1232675"
            }
          ],
          "upvote_count": "1"
        },
        {
          "poster": "c3518fc",
          "comment_id": "1199865",
          "timestamp": "1713743280.0",
          "content": "Selected Answer: BD\nNobody is saying why we are modifying the artifacts in S3 in Option E in the Codecommit account. Doesn't seem to make sense to me.",
          "upvote_count": "1"
        },
        {
          "content": "Selected Answer: BE\nBE. are correct",
          "upvote_count": "2",
          "timestamp": "1713082680.0",
          "poster": "dkp",
          "comment_id": "1195386"
        },
        {
          "poster": "thanhnv142",
          "timestamp": "1707126480.0",
          "upvote_count": "4",
          "comment_id": "1140897",
          "content": "B and E are correct: <fail with an access denied error.> this means there are issues with policies and permissions. \nA: no mention of policies\nC: This is what the dev team has tried but failed. Can not modify managed key policy, can only view it\nD: no mention of configuring S3 bucket policy"
        },
        {
          "upvote_count": "3",
          "content": "Selected Answer: BE\nexact steps are in this doc\nhttps://docs.aws.amazon.com/codepipeline/latest/userguide/pipelines-create-cross-account.html",
          "timestamp": "1702204560.0",
          "poster": "robertohyena",
          "comment_id": "1092426"
        },
        {
          "timestamp": "1698960360.0",
          "poster": "YR4591",
          "content": "Selected Answer: BE\nIt's BE,\nAccording to this, aws managed kms key can't be used cross account:\nhttps://repost.aws/knowledge-center/cross-account-access-denied-error-s3\n\n\"Warning: AWS managed AWS KMS key policies can't be modified because they're read-only. However, you can always view both the AWS managed KMS key policies and customer managed KMS key policies. Because AWS managed KMS key policies can't be updated, cross-account permissions also can't be granted for those key policies. Additionally, objects that are encrypted using an AWS managed KMS key can't be accessed by other AWS accounts. For customer managed KMS key policies, you can change the key policy only from the AWS account that created the policy.\"",
          "comment_id": "1060924",
          "upvote_count": "4"
        },
        {
          "timestamp": "1689233760.0",
          "poster": "Certified101",
          "upvote_count": "3",
          "comment_id": "950449",
          "content": "Selected Answer: BE\nBE, bucket policy needs to be amended also as it will assume roles in the prod and dev account"
        },
        {
          "comment_id": "947309",
          "content": "Selected Answer: BE\nBE. CMEK = you determine access (key policy) and rotation period (you define instead of 365 days for AWS managed keys). Perfect for cross account resources.",
          "timestamp": "1688911200.0",
          "poster": "habros",
          "upvote_count": "2"
        },
        {
          "upvote_count": "4",
          "comment_id": "904759",
          "content": "Selected Answer: BE\nYou can view the key policy for an AWS managed key or a customer managed key, but you can only change the key policy for a customer managed key.",
          "poster": "Mail1964",
          "timestamp": "1684832280.0"
        },
        {
          "timestamp": "1684197540.0",
          "content": "CE for me",
          "comment_id": "898794",
          "poster": "devnv",
          "upvote_count": "1"
        },
        {
          "comments": [
            {
              "poster": "2pk",
              "timestamp": "1684879920.0",
              "upvote_count": "1",
              "content": "I thought again, it should be A & E correct. \nB is worng becasue The access denied error typically occurs when the IAM roles used by the CloudFormation action lack the necessary permissions to access the required resources. Therefore, option B does not directly address the access denied error in the given scenario.",
              "comment_id": "905273"
            }
          ],
          "poster": "2pk",
          "timestamp": "1684018020.0",
          "content": "B & E , i guess too",
          "comment_id": "897087",
          "upvote_count": "2"
        },
        {
          "content": "Selected Answer: BE\nI think it is B and E",
          "comment_id": "896397",
          "timestamp": "1683951960.0",
          "upvote_count": "2",
          "poster": "PhuocT"
        },
        {
          "comments": [
            {
              "timestamp": "1691029500.0",
              "poster": "sb333",
              "upvote_count": "4",
              "content": "Answer C is incorrect because you cannot \"create\" an AWS-managed key or modify its key policy. In order to modify a key policy, you need an customer-managed key (Answer B). The question states they used an AWS-managed key, but got an error. So you have to re-evaluate how to make this work, which requires a customer-managed key.\n\nhttps://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#key-mgmt",
              "comment_id": "970690"
            }
          ],
          "comment_id": "892908",
          "content": "Selected Answer: CE\nQuestions says: \"A DevOps engineer creates the CodePipeline pipeline and configures the pipeline to encrypt build artifacts by using the AWS Key Management Service (AWS KMS) AWS managed key for Amazon S3 (the aws/s3 key).\" not CMK ...",
          "upvote_count": "3",
          "poster": "Jeanphi72",
          "timestamp": "1683618720.0"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:09:36.340Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "qgkpPsCLokMM7N2ueRwU",
      "question_number": 27,
      "page": 6,
      "question_text": "A company is using an organization in AWS Organizations to manage multiple AWS accounts. The company’s development team wants to use AWS Lambda functions to meet resiliency requirements and is rewriting all applications to work with Lambda functions that are deployed in a VPC. The development team is using Amazon Elastic File System (Amazon EFS) as shared storage in Account A in the organization.\n\nThe company wants to continue to use Amazon EFS with Lambda. Company policy requires all serverless projects to be deployed in Account B.\n\nA DevOps engineer needs to reconfigure an existing EFS file system to allow Lambda functions to access the data through an existing EFS access point.\n\nWhich combination of steps should the DevOps engineer take to meet these requirements? (Choose three.)",
      "choices": {
        "F": "Configure the Lambda functions in Account B to assume an existing IAM role in Account A.",
        "A": "Update the EFS file system policy to provide Account B with access to mount and write to the EFS file system in Account A.",
        "E": "Create a VPC peering connection to connect Account A to Account B.",
        "D": "Update the Lambda execution roles with permission to access the VPC and the EFS file system.",
        "B": "Create SCPs to set permission guardrails with fine-grained control for Amazon EFS.",
        "C": "Create a new EFS file system in Account B. Use AWS Database Migration Service (AWS DMS) to keep data from Account A and Account B synchronized."
      },
      "correct_answer": "ADE",
      "answer_ET": "ADE",
      "answers_community": [
        "ADE (75%)",
        "AEF (19%)",
        "6%"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/108790-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-05-09 09:41:00",
      "unix_timestamp": 1683618060,
      "discussion_count": 21,
      "discussion": [
        {
          "comment_id": "900150",
          "poster": "OrganizedChaos25",
          "timestamp": "1684327920.0",
          "upvote_count": "13",
          "content": "Selected Answer: ADE\nI got ADE"
        },
        {
          "timestamp": "1700182440.0",
          "content": "Selected Answer: ADE\nInitially, I thought of A,E,F. But after reading the docs I came to conclusion A,D,E is correct answer.\nE: https://docs.aws.amazon.com/lambda/latest/dg/configuration-filesystem.html#configuration-filesystem-cross-account\nA,D: https://docs.aws.amazon.com/lambda/latest/dg/configuration-filesystem.html#configuration-filesystem-permissions",
          "poster": "learnwithaniket",
          "upvote_count": "7",
          "comment_id": "1072934"
        },
        {
          "comment_id": "1257287",
          "timestamp": "1722233100.0",
          "content": "Selected Answer: ADE\nShould be ADE\nVPC peering required.",
          "upvote_count": "2",
          "poster": "jamesf"
        },
        {
          "content": "Selected Answer: ADE\nA,D,E is correct",
          "comment_id": "1195389",
          "timestamp": "1713083160.0",
          "upvote_count": "3",
          "poster": "dkp"
        },
        {
          "poster": "DanShone",
          "upvote_count": "3",
          "comment_id": "1175092",
          "timestamp": "1710606180.0",
          "content": "A,D,E is correct"
        },
        {
          "timestamp": "1708478460.0",
          "comment_id": "1155127",
          "poster": "kyuhuck",
          "upvote_count": "1",
          "content": "Selected Answer: AEF\n1.need to update the file system plocy on efs to allow mounting the file system into account b\n2.need vpc peering between account account a and account b as the pre-requisite\n3.need to assume cross-account iam role to descibe the mounts so that a specific mount can be chosen"
        },
        {
          "comment_id": "1140911",
          "timestamp": "1707127140.0",
          "content": "Selected Answer: ADE\nADE are correct: <The company wants to continue to use Amazon EFS with Lambda. Company policy requires all serverless projects to be deployed in Account B.> means we need assign relevant IAM policies to lambda in account b\nB: no mention of policy \nC: no mention of policy\nF: <assume an existing IAM role in Account A>: What role?",
          "poster": "thanhnv142",
          "upvote_count": "5"
        },
        {
          "timestamp": "1705344720.0",
          "poster": "a54b16f",
          "upvote_count": "5",
          "content": "Selected Answer: ADE\nNOT F: account B will mount EFS and would read/write as a local folder. There is no way/no need to assume role. Option D would assign permission that allow account B to read/write the EFS.",
          "comment_id": "1123571"
        },
        {
          "upvote_count": "3",
          "content": "Selected Answer: ADE\nIt's ADE.",
          "comment_id": "1071865",
          "timestamp": "1700080440.0",
          "poster": "zain1258"
        },
        {
          "timestamp": "1699618920.0",
          "comment_id": "1067224",
          "poster": "hzhang",
          "upvote_count": "2",
          "content": "Selected Answer: AEF\nD only works if both lamda function and EFS are in the same account.",
          "comments": [
            {
              "comment_id": "1071867",
              "timestamp": "1700080500.0",
              "content": "When peering enabled between two VPCs, this is possible even if the function and EFS are in different account.",
              "poster": "zain1258",
              "upvote_count": "1"
            }
          ]
        },
        {
          "timestamp": "1698485520.0",
          "poster": "YR4591",
          "upvote_count": "3",
          "content": "Selected Answer: ADE\n1) Lambda in account a can get access directly to EFS using cross account policy on the efs.\n2) Access to the efs is via network, thats why vpc peering is needed.\n\nhttps://aws.amazon.com/blogs/storage/mount-amazon-efs-file-systems-cross-account-from-amazon-eks/",
          "comment_id": "1056088"
        },
        {
          "upvote_count": "4",
          "comment_id": "998868",
          "poster": "RVivek",
          "content": "Selected Answer: AEF\nA & E are obvious answers.\nD is wrong Lamda execuation role is in account B. You cannot directly assign permission to that role . Instead you add AWS STS AssumeRole API call to your Lambda function's code in account B",
          "timestamp": "1693866060.0"
        },
        {
          "comment_id": "956130",
          "content": "Selected Answer: ADE\nhttps://docs.aws.amazon.com/efs/latest/ug/create-file-system-policy.html (Answer A)\nhttps://aws.amazon.com/blogs/compute/using-amazon-efs-for-aws-lambda-in-your-serverless-applications/ (Answer D)\nhttps://docs.aws.amazon.com/lambda/latest/dg/services-efs.html (Answer E)",
          "upvote_count": "4",
          "poster": "sb333",
          "timestamp": "1689734340.0"
        },
        {
          "timestamp": "1689629460.0",
          "content": "Selected Answer: AEF\nAEF Makes more sense",
          "comment_id": "954635",
          "poster": "unknownuser123",
          "upvote_count": "3"
        },
        {
          "content": "The answer is AEF because:\nA few hours ago, I just finished the DOP-C02 exam.\nMy score is 1000 points.\nThis question has come up, I choose AEF.",
          "timestamp": "1689588480.0",
          "comment_id": "954069",
          "comments": [
            {
              "upvote_count": "3",
              "timestamp": "1690326180.0",
              "content": "I am sure you didn't get 1000 if you got this answer wrong",
              "comment_id": "963178",
              "poster": "CirusD"
            },
            {
              "content": "Please provide supporting links, since the documentation points to ADE.\nhttps://docs.aws.amazon.com/efs/latest/ug/create-file-system-policy.html (Answer A)\nhttps://aws.amazon.com/blogs/compute/using-amazon-efs-for-aws-lambda-in-your-serverless-applications/ (Answer D)\nhttps://docs.aws.amazon.com/lambda/latest/dg/services-efs.html (Answer E)",
              "timestamp": "1689734280.0",
              "upvote_count": "3",
              "comment_id": "956129",
              "poster": "sb333",
              "comments": [
                {
                  "upvote_count": "2",
                  "content": "Another support for D and not F.\nhttps://repost.aws/knowledge-center/access-efs-across-accounts\n\nThis talks about assigning IAM permissions on the account B side, with EFS located in account A. For Lambda, those IAM permissions are part of the execution role. There is nothing indicating the need for using roles from account A. Only an EFS file system policy in account A. And of course peering is needed between the two accounts.\n\nIf you did get 1000 points, and you selected AEF, this could have been one of those questions that did not count against your raw score. AWS will have some questions that are not included in your score, but are questions that may be new and are being evaluated.",
                  "poster": "sb333",
                  "comment_id": "956145",
                  "timestamp": "1689735600.0"
                }
              ]
            },
            {
              "content": "In exam there are a few questions that does not have any impact on your score. No matter you mark them right or wrong.",
              "poster": "zain1258",
              "comment_id": "1071869",
              "timestamp": "1700080620.0",
              "upvote_count": "1"
            }
          ],
          "upvote_count": "4",
          "poster": "emupsx1"
        },
        {
          "upvote_count": "1",
          "comment_id": "953456",
          "content": "ADF for me",
          "poster": "ogwu2000",
          "timestamp": "1689523440.0",
          "comments": [
            {
              "timestamp": "1689523620.0",
              "content": "E is wrong . All accounts in same VPC so, you cant do VPC peering.",
              "upvote_count": "1",
              "poster": "ogwu2000",
              "comment_id": "953458"
            }
          ]
        },
        {
          "comment_id": "898798",
          "upvote_count": "2",
          "timestamp": "1684198020.0",
          "content": "AEF are correct",
          "poster": "devnv"
        },
        {
          "content": "A,D & E in my point of view\nOption A is required to allow Account B to mount and write to the EFS file system in Account A. This can be done by updating the EFS file system policy to allow access from the security group associated with the VPC in Account B.\n\nOption D is required to allow the Lambda function to access the VPC and the EFS file system. This can be done by updating the Lambda execution role with the required permissions. The Lambda execution role should be updated to include permissions to access the VPC in Account B and the EFS file system in Account A.\n\nOption E is required to allow traffic to flow between Account A and Account B. This can be done by creating a VPC peering connection between the VPC in Account B and the VPC in Account A that is associated with the EFS file system.",
          "comments": [
            {
              "upvote_count": "1",
              "timestamp": "1689487320.0",
              "content": "what does access to VPC means in the context of Lamnda's IAM role? I think it should be AEF.",
              "poster": "punj",
              "comments": [
                {
                  "timestamp": "1689733800.0",
                  "content": "Explanation for answer D. https://aws.amazon.com/blogs/compute/using-amazon-efs-for-aws-lambda-in-your-serverless-applications/\n\"The execution role for Lambda function must provide access to the VPC and EFS.\"",
                  "poster": "sb333",
                  "comment_id": "956124",
                  "upvote_count": "1"
                }
              ],
              "comment_id": "953062"
            }
          ],
          "comment_id": "897090",
          "timestamp": "1684018620.0",
          "upvote_count": "1",
          "poster": "2pk"
        },
        {
          "content": "Selected Answer: AEF\nAEF makes more sense",
          "upvote_count": "2",
          "comment_id": "895765",
          "poster": "ParagSanyashiv",
          "timestamp": "1683880680.0"
        },
        {
          "upvote_count": "4",
          "content": "Selected Answer: ADF\n1. Need to update the file system policy on EFS to allow mounting the file system into Account B.\n## File System Policy\n$ cat file-system-policy.json\n{\n\"Statement\": [\n{\n\"Effect\": \"Allow\",\n\"Action\": [\n\"elasticfilesystem:ClientMount\",\n\"elasticfilesystem:ClientWrite\"\n],\n\"Principal\": {\n\"AWS\": \"arn:aws:iam::<aws-account-id-A>:root\" # Replace with AWS account ID of EKS cluster\n}\n}\n]\n}\n2. Need VPC peering between Account A and Account B as the pre-requisite\n3. Need to assume cross-account IAM role to describe the mounts so that a specific mount can be chosen.",
          "timestamp": "1683744600.0",
          "poster": "Sazeka",
          "comment_id": "894287"
        },
        {
          "timestamp": "1683618060.0",
          "poster": "Jeanphi72",
          "comment_id": "892901",
          "content": "Selected Answer: ADE\nTo my knowledge: You can connect to Amazon EFS file systems from \nEC2 instances in other AWS regions using an inter-region VPC peering connection, and \nfrom on-premises servers using an AWS VPN connection.",
          "upvote_count": "3"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:09:36.340Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "ymcwoL1J0kMIRPJpfjzu",
      "question_number": 28,
      "page": 6,
      "question_text": "A media company has several thousand Amazon EC2 instances in an AWS account. The company is using Slack and a shared email inbox for team communications and important updates. A DevOps engineer needs to send all AWS-scheduled EC2 maintenance notifications to the Slack channel and the shared inbox. The solution must include the instances’ Name and Owner tags.\n\nWhich solution will meet these requirements?",
      "choices": {
        "C": "Create an AWS Lambda function that sends EC2 maintenance notifications to the Slack channel and the shared inbox. Monitor EC2 health events by using Amazon CloudWatch metrics. Configure a CloudWatch alarm that invokes the Lambda function when a maintenance notification is received.",
        "D": "Configure AWS Support integration with AWS CloudTrail. Create a CloudTrail lookup event to invoke an AWS Lambda function to pass EC2 maintenance notifications to Amazon Simple Notification Service (Amazon SNS). Configure Amazon SNS to target the Slack channel and the shared inbox.",
        "B": "Use Amazon EventBridge to monitor for AWS Health events. Configure the maintenance events to target an Amazon Simple Notification Service (Amazon SNS) topic. Subscribe an AWS Lambda function to the SNS topic to send notifications to the Slack channel and the shared inbox.",
        "A": "Integrate AWS Trusted Advisor with AWS Config. Configure a custom AWS Config rule to invoke an AWS Lambda function to publish notifications to an Amazon Simple Notification Service (Amazon SNS) topic. Subscribe a Slack channel endpoint and the shared inbox to the topic."
      },
      "correct_answer": "B",
      "answer_ET": "B",
      "answers_community": [
        "B (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/109364-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-05-16 02:51:00",
      "unix_timestamp": 1684198260,
      "discussion_count": 8,
      "discussion": [
        {
          "timestamp": "1728894540.0",
          "poster": "dkp",
          "upvote_count": "2",
          "content": "Selected Answer: B\nB is the answer",
          "comment_id": "1195391"
        },
        {
          "poster": "thanhnv142",
          "comment_id": "1140922",
          "timestamp": "1722845400.0",
          "content": "Selected Answer: B\nB is correct: <eeds to send all AWS-scheduled EC2 maintenance notifications to the Slack channel and the shared inbox> means SNS\nC: no mention of SNS\nA: AWS trusted advisor has nothing to do here\nD: AWS Support is support plan. It has nothing to do here. so as AWS cloud trail",
          "upvote_count": "2"
        },
        {
          "content": "Answer is B",
          "comment_id": "1118971",
          "upvote_count": "1",
          "poster": "yuliaqwerty",
          "timestamp": "1720631940.0"
        },
        {
          "timestamp": "1720049400.0",
          "poster": "sarlos",
          "content": "Yes it's B",
          "upvote_count": "1",
          "comment_id": "1113285"
        },
        {
          "poster": "n_d1",
          "upvote_count": "4",
          "comment_id": "958347",
          "content": "Selected Answer: B\nhttps://docs.aws.amazon.com/health/latest/ug/cloudwatch-events-health.html",
          "timestamp": "1705841280.0"
        },
        {
          "upvote_count": "2",
          "poster": "Certified101",
          "timestamp": "1705755600.0",
          "comment_id": "957423",
          "content": "Selected Answer: B\nB is correct"
        },
        {
          "comment_id": "900152",
          "poster": "OrganizedChaos25",
          "content": "Selected Answer: B\nB is the answer I got",
          "upvote_count": "2",
          "timestamp": "1700232780.0"
        },
        {
          "timestamp": "1700103060.0",
          "comment_id": "898802",
          "content": "B is correct",
          "poster": "devnv",
          "upvote_count": "1"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:09:36.340Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "PQLItmhMiURSfJA9RHkz",
      "question_number": 29,
      "page": 6,
      "question_text": "An AWS CodePipeline pipeline has implemented a code release process. The pipeline is integrated with AWS CodeDeploy to deploy versions of an application to multiple Amazon EC2 instances for each CodePipeline stage.\n\nDuring a recent deployment, the pipeline failed due to a CodeDeploy issue. The DevOps team wants to improve monitoring and notifications during deployment to decrease resolution times.\n\nWhat should the DevOps engineer do to create notifications when issues are discovered?",
      "choices": {
        "D": "Implement Amazon EventBridge for CodePipeline and CodeDeploy, create an Amazon Inspector assessment target to evaluate code deployment issues, and create an Amazon Simple Notification Service (Amazon SNS) topic to notify stakeholders of deployment issues.",
        "B": "Implement Amazon EventBridge for CodePipeline and CodeDeploy, create an AWS Lambda function to evaluate code deployment issues, and create an Amazon Simple Notification Service (Amazon SNS) topic to notify stakeholders of deployment issues.",
        "A": "Implement Amazon CloudWatch Logs for CodePipeline and CodeDeploy, create an AWS Config rule to evaluate code deployment issues, and create an Amazon Simple Notification Service (Amazon SNS) topic to notify stakeholders of deployment issues.",
        "C": "Implement AWS CloudTrail to record CodePipeline and CodeDeploy API call information, create an AWS Lambda function to evaluate code deployment issues, and create an Amazon Simple Notification Service (Amazon SNS) topic to notify stakeholders of deployment issues."
      },
      "correct_answer": "B",
      "answer_ET": "B",
      "answers_community": [
        "B (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/109365-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-05-16 02:58:00",
      "unix_timestamp": 1684198680,
      "discussion_count": 6,
      "discussion": [
        {
          "content": "Selected Answer: B\nB is correct: <monitoring and notifications during deployment> means eventbridge and SNS\nA: cloudwatchlog has nothing to do here. This is use for continuous monitoring of AWS services\nC: cloudtrail is for account activities monitoring\nD: Inspector is for threat detection",
          "poster": "thanhnv142",
          "timestamp": "1722845820.0",
          "comment_id": "1140938",
          "upvote_count": "5"
        },
        {
          "content": "Selected Answer: B\nB is correc",
          "poster": "dkp",
          "upvote_count": "2",
          "timestamp": "1728894720.0",
          "comment_id": "1195395"
        },
        {
          "comment_id": "1056098",
          "content": "Selected Answer: B\nIts B, They want to monitor issued DURING deployment, means near real time, so cloudwatch event will do the work.\n\nC is wrong for to reasons, first, cloudtrail alone can't trigger lambda without an event. Second, cloud trail logs are update in 5 minutes intervals, which means monitoring for the code deploy will not be during deployment.",
          "timestamp": "1714297560.0",
          "poster": "YR4591",
          "upvote_count": "2"
        },
        {
          "upvote_count": "3",
          "timestamp": "1703902920.0",
          "comment_id": "938658",
          "content": "Selected Answer: B\nB. Implement Amazon EventBridge for CodePipeline and CodeDeploy, create an AWS Lambda function to evaluate code deployment issues, and create an Amazon Simple Notification Service (Amazon SNS) topic to notify stakeholders of deployment issues.\n\nExplanation:\nAmazon EventBridge provides a serverless event bus that integrates with various AWS services. By implementing EventBridge for CodePipeline and CodeDeploy, the engineer can capture deployment events and trigger actions based on those events. Creating an AWS Lambda function allows for evaluating code deployment issues and performing custom actions. Additionally, creating an Amazon SNS topic provides a means to notify stakeholders of any deployment issues detected.",
          "poster": "haazybanj"
        },
        {
          "upvote_count": "3",
          "timestamp": "1700232900.0",
          "comment_id": "900157",
          "poster": "OrganizedChaos25",
          "content": "Selected Answer: B\nB is correct"
        },
        {
          "poster": "devnv",
          "content": "Yes it's B",
          "timestamp": "1700103480.0",
          "comment_id": "898805",
          "upvote_count": "1"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:09:36.340Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "7GUVPw0PfWrbsiwfSRIC",
      "question_number": 30,
      "page": 6,
      "question_text": "A global company manages multiple AWS accounts by using AWS Control Tower. The company hosts internal applications and public applications.\n\nEach application team in the company has its own AWS account for application hosting. The accounts are consolidated in an organization in AWS Organizations. One of the AWS Control Tower member accounts serves as a centralized DevOps account with CI/CD pipelines that application teams use to deploy applications to their respective target AWS accounts. An IAM role for deployment exists in the centralized DevOps account.\n\nAn application team is attempting to deploy its application to an Amazon Elastic Kubernetes Service (Amazon EKS) cluster in an application AWS account. An IAM role for deployment exists in the application AWS account. The deployment is through an AWS CodeBuild project that is set up in the centralized DevOps account. The CodeBuild project uses an IAM service role for CodeBuild. The deployment is failing with an Unauthorized error during attempts to connect to the cross-account EKS cluster from CodeBuild.\n\nWhich solution will resolve this error?",
      "choices": {
        "C": "Configure the centralized DevOps account’s deployment IAM role to have a trust relationship with the application account. Configure the trust relationship to allow the sts:AssumeRoleWithSAML action. Configure the centralized DevOps account’s deployment IAM role to allow the required access to CodeBuild.",
        "D": "Configure the application account’s deployment IAM role to have a trust relationship with the AWS Control Tower management account. Configure the trust relationship to allow the sts:AssumeRole action. Configure the application account’s deployment IAM role to have the required access to the EKS cluster. Configure the EKS cluster aws-auth ConfigMap to map the role to the appropriate system permissions.",
        "B": "Configure the centralized DevOps account’s deployment IAM role to have a trust relationship with the application account. Configure the trust relationship to allow the sts:AssumeRole action. Configure the centralized DevOps account’s deployment IAM role to allow the required access to CodeBuild.",
        "A": "Configure the application account’s deployment IAM role to have a trust relationship with the centralized DevOps account. Configure the trust relationship to allow the sts:AssumeRole action. Configure the application account’s deployment IAM role to have the required access to the EKS cluster. Configure the EKS cluster aws-auth ConfigMap to map the role to the appropriate system permissions."
      },
      "correct_answer": "A",
      "answer_ET": "A",
      "answers_community": [
        "A (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/109182-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-05-13 23:33:00",
      "unix_timestamp": 1684013580,
      "discussion_count": 9,
      "discussion": [
        {
          "timestamp": "1689234840.0",
          "poster": "Certified101",
          "content": "Selected Answer: A\nA. Configure the application account’s deployment IAM role to have a trust relationship with the centralized DevOps account. Configure the trust relationship to allow the sts:AssumeRole action. Configure the application account’s deployment IAM role to have the required access to the EKS cluster. Configure the EKS cluster aws-auth ConfigMap to map the role to the appropriate system permissions.\n\nOptions B, C, and D are not correct because the centralized DevOps account’s deployment IAM role doesn't need to trust the application account, it's the other way around. The sts:AssumeRoleWithSAML action in option C is used for federation from a SAML 2.0 compliant identity provider and is not necessary in this scenario. Lastly, there's no need to have a trust relationship with the AWS Control Tower management account as in option D, as the interaction is directly between the DevOps account and the application account.",
          "upvote_count": "10",
          "comment_id": "950458"
        },
        {
          "comment_id": "1141080",
          "timestamp": "1707136500.0",
          "content": "Selected Answer: A\nA is correct: <Unauthorized error during attempts to connect> means we need to setup relevant permissions and policies\n- A is correct because < AWS CodeBuild project that is set up in the centralized DevOps account>, so we should setup trust relationship on the account that has resources, which is the application account and allow codebuild from centralized account assume it\nB and C are wrong: we need to setup trust from the app account, not the centralized account.\nD: this option mentions control Tower, which is irrelevant",
          "poster": "thanhnv142",
          "upvote_count": "6"
        },
        {
          "timestamp": "1722582060.0",
          "comment_id": "1259737",
          "poster": "jamesf",
          "upvote_count": "2",
          "content": "Selected Answer: A\nA. Configure the application account’s deployment IAM role to have a trust relationship with the centralized DevOps account. \n- setup trust relationship on the account that has resources, which is the application account\n\nConfigure the trust relationship to allow the sts:AssumeRole action. \n- allow CodeBuild from centralized account assume it\n- CodeBuild is configured in Centralized DevOps account but not in application account.\n\nConfigure the application account’s deployment IAM role to have the required access to the EKS cluster. Configure the EKS cluster aws-auth ConfigMap to map the role to the appropriate system permissions.\n- the application account has access to the resources"
        },
        {
          "poster": "tartarus23",
          "comment_id": "928755",
          "timestamp": "1687290300.0",
          "content": "Selected Answer: A\n(A) This solution addresses the Unauthorized error by allowing the DevOps account to assume the IAM role in the application account that has the necessary permissions to access the EKS cluster. The other options don't provide the necessary cross-account permissions or correctly configure the roles for accessing EKS.",
          "upvote_count": "3"
        },
        {
          "content": "B is correct.\nUnauthorized error happened from CodeBuild in Dev account to EKS cluster in application account, instead of reverse direction.",
          "upvote_count": "2",
          "comment_id": "918253",
          "poster": "walkwolf3",
          "timestamp": "1686227820.0",
          "comments": [
            {
              "timestamp": "1700079240.0",
              "poster": "zain1258",
              "content": "CodeBuild is configured in Centralized DevOps account not in application account.",
              "upvote_count": "2",
              "comment_id": "1071848"
            }
          ]
        },
        {
          "upvote_count": "3",
          "timestamp": "1684876560.0",
          "comment_id": "905240",
          "content": "I'd like to add more, don't get the source and destination mixed up. Because the Application team must deploy resources in the Dev account. So, the source should be the Application team and the destination should be the Dev team.",
          "poster": "2pk"
        },
        {
          "content": "Selected Answer: A\nA is correct",
          "poster": "PhuocT",
          "comment_id": "904930",
          "upvote_count": "1",
          "timestamp": "1684847100.0"
        },
        {
          "upvote_count": "1",
          "poster": "ParagSanyashiv",
          "comment_id": "897413",
          "timestamp": "1684054620.0",
          "content": "A is correct Answer"
        },
        {
          "content": "Answer is A.\nIn the source AWS account, the IAM role used by the CI/CD pipeline should have permissions to access the source code repository, build artifacts, and any other resources required for the build process.\nIn the destination AWS accounts, the IAM role used for deployment should have permissions to access the AWS resources required for deploying the application, such as EC2 instances, RDS databases, S3 buckets, etc. The exact permissions required will depend on the specific resources being used by the application.\nthe IAM role used for deployment in the destination accounts should also have permissions to assume the IAM role for deployment in the centralized DevOps account. This is typically done using an IAM role trust policy that allows the destination account to assume the DevOps account role.",
          "poster": "2pk",
          "comment_id": "897065",
          "upvote_count": "4",
          "timestamp": "1684013580.0"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:09:36.340Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "7mMz6z1asHdonTDdKnw8",
      "question_number": 31,
      "page": 7,
      "question_text": "A highly regulated company has a policy that DevOps engineers should not log in to their Amazon EC2 instances except in emergencies. If a DevOps engineer does log in, the security team must be notified within 15 minutes of the occurrence.\n\nWhich solution will meet these requirements?",
      "choices": {
        "A": "Install the Amazon Inspector agent on each EC2 instance. Subscribe to Amazon EventBridge notifications. Invoke an AWS Lambda function to check if a message is about user logins. If it is, send a notification to the security team using Amazon SNS.",
        "D": "Set up a script on each Amazon EC2 instance to push all logs to Amazon S3. Set up an S3 event to invoke an AWS Lambda function, which invokes an Amazon Athena query to run. The Athena query checks for logins and sends the output to the security team using Amazon SNS.",
        "B": "Install the Amazon CloudWatch agent on each EC2 instance. Configure the agent to push all logs to Amazon CloudWatch Logs and set up a CloudWatch metric filter that searches for user logins. If a login is found, send a notification to the security team using Amazon SNS.",
        "C": "Set up AWS CloudTrail with Amazon CloudWatch Logs. Subscribe CloudWatch Logs to Amazon Kinesis. Attach AWS Lambda to Kinesis to parse and determine if a log contains a user login. If it does, send a notification to the security team using Amazon SNS."
      },
      "correct_answer": "B",
      "answer_ET": "B",
      "answers_community": [
        "B (95%)",
        "5%"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/109183-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-05-13 23:58:00",
      "unix_timestamp": 1684015080,
      "discussion_count": 9,
      "discussion": [
        {
          "timestamp": "1707136740.0",
          "upvote_count": "6",
          "comment_id": "1141085",
          "poster": "thanhnv142",
          "content": "Selected Answer: B\nB is correct: <the security team must be notified > means SNS\nA: irrelevant, inspector is for vulnerability scanning\nC: cloud trail is for monitoring account activities\nD: This options uses manual script, which is irrelevant"
        },
        {
          "poster": "zijo",
          "timestamp": "1719508440.0",
          "content": "Selected Answer: B\nB is the cheapest and correct solution\nCloudTrail does not capture:\nSSH logins to Linux instances.\nRDP logins to Windows instances.\nCommands executed on the instances.\nLocal file access or modifications within the instances.",
          "comment_id": "1238292",
          "upvote_count": "1"
        },
        {
          "timestamp": "1690018980.0",
          "comments": [
            {
              "content": "Cloud Trail will track calls to AWS API, but not the OS login in an EC2. That can be checked only using Cloud watch logs",
              "upvote_count": "10",
              "comment_id": "1007739",
              "timestamp": "1694701020.0",
              "poster": "RVivek"
            }
          ],
          "poster": "haazybanj",
          "content": "Selected Answer: C\nWhile Option B can provide valuable insights into user logins and send notifications to the security team, it might not guarantee that the security team is notified within 15 minutes of a login occurrence. The time it takes for the CloudWatch metric filter to process and detect the login event, along with the potential delays in the SNS notification, could result in notifications being sent beyond the required 15-minute timeframe.\n\nOn the other hand, Option C, which uses AWS CloudTrail with Amazon CloudWatch Logs and Amazon Kinesis, allows real-time processing and immediate notifications when a user login event is detected. This makes Option C more suitable for meeting the specific requirement of notifying the security team within 15 minutes of a login occurrence.",
          "upvote_count": "1",
          "comment_id": "959394"
        },
        {
          "upvote_count": "3",
          "comment_id": "958508",
          "content": "Selected Answer: B\nhttps://aws.amazon.com/blogs/security/how-to-monitor-and-visualize-failed-ssh-access-attempts-to-amazon-ec2-linux-instances/",
          "poster": "n_d1",
          "timestamp": "1689944580.0"
        },
        {
          "comment_id": "948588",
          "upvote_count": "4",
          "timestamp": "1689040020.0",
          "poster": "ProfXsamson",
          "content": "Selected Answer: B\nB, \nEventhough it's not stated in some questions, the cheapest solution to a problem is always AWS favorite."
        },
        {
          "poster": "gdtypk",
          "upvote_count": "2",
          "content": "Selected Answer: B\nIsn't it possible to get login events with CloudTrail?",
          "comment_id": "910599",
          "timestamp": "1685481420.0"
        },
        {
          "timestamp": "1684833120.0",
          "poster": "Mail1964",
          "upvote_count": "3",
          "content": "Selected Answer: B\nSubtle difference Cloudtrail is \"near\" realtime - You can use subscriptions to get access to a real-time feed of log events from CloudWatch Logs and have it delivered to other services such as an Amazon Kinesis stream, an Amazon Kinesis Data Firehose stream, or AWS Lambda for custom processing, analysis, or loading to other systems.",
          "comment_id": "904775"
        },
        {
          "upvote_count": "1",
          "timestamp": "1684199220.0",
          "poster": "devnv",
          "comment_id": "898807",
          "content": "B is the right answer"
        },
        {
          "timestamp": "1684015080.0",
          "poster": "2pk",
          "upvote_count": "1",
          "comments": [
            {
              "content": "AWS CloudTrail captures API calls made on your account and sends log files to CloudWatch Logs. The provided solution monitors for login-related API calls. While this may detect some login activity (like a RunInstances API call), it will not catch SSH logins to an EC2 instance. Therefore, this solution isn't comprehensive enough.\n=> Correct answer is B.",
              "upvote_count": "3",
              "poster": "buiquangbk90",
              "comment_id": "980743",
              "timestamp": "1692013560.0"
            }
          ],
          "content": "i think its C, Both B&C solutions are valid and can meet the requirement of notifying the security team within 15 minutes of a DevOps engineer logging into an EC2 instance.\n\nHowever, there are some differences in how quickly each solution can detect and notify the security team of a login event.\n\nThe CloudTrail-based solution can detect a login event more quickly than the CloudWatch-based solution because CloudTrail captures API events in near-real-time, while CloudWatch logs may have a delay of a few minutes before they appear in the log group. Therefore, the CloudTrail-based solution is more likely to meet the 15-minute notification requirement.",
          "comment_id": "897073"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:09:46.850Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "lXZ4GNsiPBFUB8kJ7mG6",
      "question_number": 32,
      "page": 7,
      "question_text": "A company updated the AWS CloudFormation template for a critical business application. The stack update process failed due to an error in the updated template, and AWS CloudFormation automatically began the stack rollback process. Later, a DevOps engineer discovered that the application was still unavailable and that the stack was in the UPDATE_ROLLBACK_FAILED state.\n\nWhich combination of actions should the DevOps engineer perform so that the stack rollback can complete successfully? (Choose two.)",
      "choices": {
        "A": "Attach the AWSCloudFormationFullAccess IAM policy to the AWS CloudFormation role.",
        "B": "Automatically recover the stack resources by using AWS CloudFormation drift detection.",
        "D": "Manually adjust the resources to match the expectations of the stack.",
        "E": "Update the existing AWS CloudFormation stack by using the original template.",
        "C": "Issue a ContinueUpdateRollback command from the AWS CloudFormation console or the AWS CLI."
      },
      "correct_answer": "CD",
      "answer_ET": "CD",
      "answers_community": [
        "CD (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/109184-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-05-14 00:08:00",
      "unix_timestamp": 1684015680,
      "discussion_count": 8,
      "discussion": [
        {
          "comment_id": "897077",
          "timestamp": "1684015680.0",
          "content": "yes C & D\nC. Issue a ContinueUpdateRollback command from the AWS CloudFormation console or the AWS CLI: This command allows CloudFormation to continue the rollback process from the point where it was paused. By using this command, CloudFormation will attempt to restore the resources to their previous state and delete any resources that were created during the update.\n\nD. Manually adjust the resources to match the expectations of the stack: This involves identifying and correcting the root cause of the update failure, which could involve changing the resource configuration or resolving any dependencies or inconsistencies in the stack.",
          "poster": "2pk",
          "upvote_count": "10"
        },
        {
          "comment_id": "1288028",
          "content": "Selected Answer: CD\nD.\n\"In most cases, you must fix the error that causes the update rollback to fail before you can continue to roll back your stack. In other cases, you can continue to roll back the update without any changes, for example when a stack operation times out.\"\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-updating-stacks-continueupdaterollback.html",
          "timestamp": "1727077800.0",
          "poster": "heff_bezos",
          "upvote_count": "2"
        },
        {
          "content": "C and D are correct: < UPDATE_ROLLBACK_FAILED state> means we are left with ContinueUpdateRollback command\nA: irrelevant, AWSCloudFormationFullAccess IAM policy is used to access ACF, not to fix this\nB: AWS CloudFormation drift detection is to check if the stack has been updated unexpectedly, not to fix irrelevant\nE: The original template didnt work, so update the stack using it wont work",
          "upvote_count": "3",
          "timestamp": "1707137040.0",
          "comment_id": "1141089",
          "poster": "thanhnv142"
        },
        {
          "poster": "yuliaqwerty",
          "content": "Selected Answer: CD\nAgree with C and D",
          "upvote_count": "2",
          "timestamp": "1704915060.0",
          "comment_id": "1118983"
        },
        {
          "poster": "sarlos",
          "content": "C and D are right",
          "timestamp": "1704332580.0",
          "comment_id": "1113287",
          "upvote_count": "1"
        },
        {
          "content": "Selected Answer: CD\nYes it's C & D",
          "timestamp": "1689853020.0",
          "comment_id": "957488",
          "upvote_count": "3",
          "poster": "Certified101"
        },
        {
          "poster": "tartarus23",
          "upvote_count": "4",
          "content": "Selected Answer: CD\n(C) This command will try to roll back the stack to the previously working state after you have resolved the issues that caused the rollback failure.\n\n(D) Sometimes a stack update can fail because the current state of a resource differs from the state expected by AWS CloudFormation (e.g., a resource that AWS CloudFormation is trying to modify or delete is locked by another process). Manually resolving these issues, by stopping the conflicting process or by modifying the resource to match the expected state, will allow the stack update or rollback to proceed.",
          "comment_id": "928750",
          "timestamp": "1687290120.0"
        },
        {
          "upvote_count": "2",
          "content": "Yes it's C & D",
          "comment_id": "898809",
          "poster": "devnv",
          "timestamp": "1684199520.0"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:09:46.850Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "NMVDM5BKWBM0r1U2BR3i",
      "question_number": 33,
      "page": 7,
      "question_text": "A development team manually builds an artifact locally and then places it in an Amazon S3 bucket. The application has a local cache that must be cleared when a deployment occurs. The team runs a command to do this, downloads the artifact from Amazon S3, and unzips the artifact to complete the deployment.\n\nA DevOps team wants to migrate to a CI/CD process and build in checks to stop and roll back the deployment when a failure occurs. This requires the team to track the progression of the deployment.\n\nWhich combination of actions will accomplish this? (Choose three.)",
      "choices": {
        "A": "Allow developers to check the code into a code repository. Using Amazon EventBridge, on every pull into the main branch, invoke an AWS Lambda function to build the artifact and store it in Amazon S3.",
        "E": "Use AWS CodeBuild to build the artifact and place it in Amazon S3. Use AWS CodeDeploy to deploy the artifact to Amazon EC2 instances.",
        "D": "Set up AWS CodePipeline to deploy the application. Allow developers to check the code into a code repository as a source for the pipeline.",
        "B": "Create a custom script to clear the cache. Specify the script in the BeforeInstall lifecycle hook in the AppSpec file.",
        "C": "Create user data for each Amazon EC2 instance that contains the clear cache script. Once deployed, test the application. If it is not successful, deploy it again.",
        "F": "Use AWS Systems Manager to fetch the artifact from Amazon S3 and deploy it to all the instances."
      },
      "correct_answer": "BDE",
      "answer_ET": "BDE",
      "answers_community": [
        "BDE (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/109366-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-05-16 03:16:00",
      "unix_timestamp": 1684199760,
      "discussion_count": 5,
      "discussion": [
        {
          "upvote_count": "8",
          "poster": "tartarus23",
          "timestamp": "1703108460.0",
          "content": "Selected Answer: BDE\n(B) This would help ensure that the local cache is cleared before the new version of the application is installed. AppSpec (Application Specification) file is a unique file to AWS CodeDeploy. It defines the deployment actions you want AWS CodeDeploy to execute.\n\n(D) This would allow you to automate the build and deployment processes. AWS CodePipeline is a fully managed continuous delivery service that helps you automate your release pipelines for fast and reliable application and infrastructure updates.\n\n(E) This would allow you to automate the build process and ensure that the application is built in a consistent environment. AWS CodeBuild is a fully managed build service that compiles source code, runs tests, and produces software packages that are ready to deploy. AWS CodeDeploy automates software deployments to a variety of compute services including Amazon EC2, AWS Fargate, AWS Lambda, and your on-premises servers.",
          "comment_id": "928749"
        },
        {
          "content": "Selected Answer: BDE\nBDE combination will do all requirments",
          "upvote_count": "5",
          "comment_id": "950463",
          "timestamp": "1705140540.0",
          "poster": "Certified101"
        },
        {
          "comment_id": "1141988",
          "timestamp": "1722928800.0",
          "content": "Selected Answer: BDE\nBDE are correct: < migrate to a CI/CD process > means codepipeline, code build and code deploy\nA,C and F: no mention of the above three",
          "upvote_count": "3",
          "poster": "thanhnv142"
        },
        {
          "upvote_count": "1",
          "poster": "sarlos",
          "timestamp": "1720050420.0",
          "comment_id": "1113288",
          "content": "BDE is rigtht"
        },
        {
          "timestamp": "1700104560.0",
          "upvote_count": "2",
          "comment_id": "898810",
          "poster": "devnv",
          "content": "Yes it's BD&E"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:09:46.850Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "ZXahlm3pFdscm3FI7Y3a",
      "question_number": 34,
      "page": 7,
      "question_text": "A DevOps engineer is working on a project that is hosted on Amazon Linux and has failed a security review. The DevOps manager has been asked to review the company buildspec.yaml file for an AWS CodeBuild project and provide recommendations. The buildspec.yaml file is configured as follows:\n\n//IMG//\n\n\nWhat changes should be recommended to comply with AWS security best practices? (Choose three.)",
      "choices": {
        "C": "Store the DB_PASSWORD as a SecureString value in AWS Systems Manager Parameter Store and then remove the DB_PASSWORD from the environment variables.",
        "B": "Update the CodeBuild project role with the necessary permissions and then remove the AWS credentials from the environment variable.",
        "A": "Add a post-build command to remove the temporary files from the container before termination to ensure they cannot be seen by other CodeBuild users.",
        "E": "Use AWS Systems Manager run command versus scp and ssh commands directly to the instance.",
        "F": "Scramble the environment variables using XOR followed by Base64, add a section to install, and then run XOR and Base64 to the build phase.",
        "D": "Move the environment variables to the ‘db-deploy-bucket’ Amazon S3 bucket, add a prebuild stage to download, then export the variables."
      },
      "correct_answer": "BCE",
      "answer_ET": "BCE",
      "answers_community": [
        "BCE (82%)",
        "ABC (18%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/109181-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [
        "https://img.examtopics.com/aws-certified-devops-engineer-professional-dop-c02/image8.png"
      ],
      "answer_images": [],
      "timestamp": "2023-05-13 23:20:00",
      "unix_timestamp": 1684012800,
      "discussion_count": 19,
      "discussion": [
        {
          "content": "Selected Answer: BCE\nBCE is correct\nA is WRONG. CodeBuild does not keep files for next builds in that way, once the build is done, the files will be deleted.",
          "poster": "WhyIronMan",
          "upvote_count": "11",
          "timestamp": "1711893720.0",
          "comment_id": "1186840"
        },
        {
          "upvote_count": "5",
          "timestamp": "1690141320.0",
          "poster": "sb333",
          "content": "Selected Answer: BCE\nBCE are the correct answers.",
          "comment_id": "960784"
        },
        {
          "comment_id": "1288029",
          "content": "Selected Answer: BCE\nCode Build is a managed service. There's no way for other users to see what's in the container.",
          "upvote_count": "2",
          "timestamp": "1727078580.0",
          "poster": "heff_bezos"
        },
        {
          "upvote_count": "1",
          "poster": "jamesf",
          "content": "Selected Answer: BCE\nPrefer BCE\n\nOption A incorrect as \n- CodeBuild does not keep files for next builds in that way, once the build is done, the files will be deleted.\n- and don't think have such \"CodeBuild users\"",
          "comment_id": "1259753",
          "timestamp": "1722583080.0"
        },
        {
          "timestamp": "1721638200.0",
          "poster": "ericphl",
          "comment_id": "1252946",
          "upvote_count": "1",
          "content": "Selected Answer: ABC\nABC seems right."
        },
        {
          "poster": "ajeeshb",
          "content": "Selected Answer: ABC\nA - Cleans up temp files that stores the my.cnf and the instance key files\nB - Removes hardcoded AWS credentials\nC - Securely stores DB password",
          "timestamp": "1720296780.0",
          "upvote_count": "1",
          "comment_id": "1243538"
        },
        {
          "poster": "Diego1414",
          "content": "Selected Answer: ABC\nABC seems appropriate, since the emphasis is on security.",
          "timestamp": "1708634040.0",
          "comment_id": "1156698",
          "upvote_count": "2"
        },
        {
          "comment_id": "1142295",
          "content": "Selected Answer: ABC\nABC are correct: security best practices are related to removing credentials and sensitive data\n- A remove temporary files is important because they might contain sensitive data\n- B: <remove the AWS credentials> is removing the access key\n- C: <remove the DB_PASSWORD> means removing hardcoded DB_PASSWORD\nAll other options dont relate to senstive data or password",
          "timestamp": "1707232140.0",
          "poster": "thanhnv142",
          "upvote_count": "2"
        },
        {
          "timestamp": "1704333120.0",
          "comment_id": "1113293",
          "poster": "sarlos",
          "upvote_count": "1",
          "content": "its BCE\nhttps://stackoverflow.com/questions/76854227/i-want-to-copy-files-to-aws-ec2-using-buildspec-yml-file-the-22-port-is-open-fo"
        },
        {
          "upvote_count": "4",
          "content": "Selected Answer: BCE\nIt's BCE.\nA is wrong. I don't think there is any concept of `CodeBuild users`.",
          "timestamp": "1700078520.0",
          "comment_id": "1071839",
          "poster": "zain1258"
        },
        {
          "upvote_count": "1",
          "content": "BCE\nhttps://www.examtopics.com/discussions/amazon/view/46729-exam-aws-devops-engineer-professional-topic-1-question-17/",
          "poster": "buiquangbk90",
          "timestamp": "1692014640.0",
          "comment_id": "980754"
        },
        {
          "timestamp": "1690087920.0",
          "poster": "einn",
          "upvote_count": "1",
          "comment_id": "960071",
          "content": "Selected Answer: ABC\nA: remove sensitive data that could left behind in container\nB: remove crendentials and use role \nC: Use SecureString AWS Systems Manager Parameter Store"
        },
        {
          "comment_id": "957505",
          "poster": "Certified101",
          "upvote_count": "3",
          "timestamp": "1689853740.0",
          "content": "Selected Answer: BCE\nBCE are the correct ones."
        },
        {
          "content": "Selected Answer: BCE\nBCE are the correct ones.",
          "upvote_count": "3",
          "comment_id": "934221",
          "timestamp": "1687769760.0",
          "poster": "FunkyFresco"
        },
        {
          "content": "BCE is correct answer",
          "poster": "Kodoma",
          "upvote_count": "1",
          "timestamp": "1684876380.0",
          "comment_id": "905237"
        },
        {
          "timestamp": "1684200660.0",
          "upvote_count": "3",
          "comment_id": "898812",
          "poster": "devnv",
          "content": "Sorry, I've read again and it's AB & C."
        },
        {
          "timestamp": "1684200420.0",
          "comment_id": "898811",
          "content": "Yes BCE are correct",
          "poster": "devnv",
          "upvote_count": "1"
        },
        {
          "content": "Selected Answer: BCE\nBCE is correct",
          "timestamp": "1684054980.0",
          "comment_id": "897418",
          "poster": "ParagSanyashiv",
          "upvote_count": "2"
        },
        {
          "upvote_count": "3",
          "comment_id": "897062",
          "poster": "2pk",
          "timestamp": "1684012800.0",
          "content": "I think its A, B & C"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:09:46.850Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "5Ke5BwCXvDIc1aGRQ3Zz",
      "question_number": 35,
      "page": 7,
      "question_text": "A company has a single AWS account that runs hundreds of Amazon EC2 instances in a single AWS Region. New EC2 instances are launched and terminated each hour in the account. The account also includes existing EC2 instances that have been running for longer than a week.\nThe company's security policy requires all running EC2 instances to use an EC2 instance profile. If an EC2 instance does not have an instance profile attached, the EC2 instance must use a default instance profile that has no IAM permissions assigned.\nA DevOps engineer reviews the account and discovers EC2 instances that are running without an instance profile. During the review, the DevOps engineer also observes that new EC2 instances are being launched without an instance profile.\nWhich solution will ensure that an instance profile is attached to all existing and future EC2 instances in the Region?",
      "choices": {
        "B": "Configure the ec2-instance-profile-attached AWS Config managed rule with a trigger type of configuration changes. Configure an automatic remediation action that invokes an AWS Systems Manager Automation runbook to attach the default instance profile to the EC2 instances.",
        "A": "Configure an Amazon EventBridge rule that reacts to EC2 RunInstances API calls. Configure the rule to invoke an AWS Lambda function to attach the default instance profile to the EC2 instances.",
        "C": "Configure an Amazon EventBridge rule that reacts to EC2 StartInstances API calls. Configure the rule to invoke an AWS Systems Manager Automation runbook to attach the default instance profile to the EC2 instances",
        "D": "Configure the iam-role-managed-policy-check AWS Config managed rule with a trigger type of configuration changes. Configure an automatic remediation action that invokes an AWS Lambda function to attach the default instance profile to the EC2 instances."
      },
      "correct_answer": "B",
      "answer_ET": "B",
      "answers_community": [
        "B (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/105512-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-04-07 15:11:00",
      "unix_timestamp": 1680873060,
      "discussion_count": 10,
      "discussion": [
        {
          "comment_id": "1075407",
          "timestamp": "1716203100.0",
          "poster": "koenigParas2324",
          "content": "Selected Answer: B\nWS Config, specifically utilizing the \"ec2-instance-profile-attached\" managed rule with the configuration change trigger type. This rule helps monitor the attachment of instance profiles to EC2 instances. An automatic remediation action can be configured within AWS Config to respond when instances are found without an instance profile attached. The remediation action would execute an AWS Systems Manager Automation runbook to attach the default instance profile to those instances.",
          "upvote_count": "9"
        },
        {
          "content": "Selected Answer: B\nHow is it possible to remember the list of all the config rules?",
          "poster": "AWSPICHI",
          "comment_id": "1346417",
          "upvote_count": "5",
          "timestamp": "1737803040.0"
        },
        {
          "upvote_count": "1",
          "comment_id": "1217826",
          "timestamp": "1732490040.0",
          "poster": "Gomer",
          "content": "Selected Answer: B\nI concur the best answer seems to be \"B\". However, I have not been able to trigger exactly what kind of \"configuration change\" triggers the config rule (e.g. \"starting\" or \"running\" an instance isn't a configuration change, but a state change. The real world answer (IMHO) would be to just kick off the AWS Config rule manually or on a schedule. I'd also take steps to ensure that all EC2 launch templates specify an instance profile so I'm not running around trying to fix things that shouldn't have been left broken from the start."
        },
        {
          "content": "Selected Answer: B\nThe rules AWS Config",
          "comment_id": "1192343",
          "upvote_count": "1",
          "timestamp": "1728485460.0",
          "poster": "Gillar"
        },
        {
          "upvote_count": "4",
          "timestamp": "1722132300.0",
          "content": "B is correct: AWS config + runbook is the right way for remediation",
          "poster": "thanhnv142",
          "comments": [
            {
              "timestamp": "1722734040.0",
              "upvote_count": "5",
              "content": "B is correct: AWS Config run in combination with SSM Automation run book is the recommended way\nA: this option only remediate new instances\nC: this option only remedidate instances that have been stopped. \nD: automatic remediation action should invoke Automation run book, not lambda",
              "poster": "thanhnv142",
              "comment_id": "1139743"
            }
          ],
          "comment_id": "1133811"
        },
        {
          "comment_id": "1095561",
          "poster": "DucSiu",
          "upvote_count": "3",
          "timestamp": "1718283000.0",
          "content": "B\nConfig: ec2-instance-profile-attached\nSSM Automation: AttachedIAMtoinstances"
        },
        {
          "comment_id": "918163",
          "timestamp": "1702040520.0",
          "content": "Selected Answer: B\nB is correct.\nhttps://docs.aws.amazon.com/config/latest/developerguide/ec2-instance-profile-attached.html",
          "upvote_count": "2",
          "poster": "madperro"
        },
        {
          "content": "Selected Answer: B\nB is correct",
          "comment_id": "891828",
          "poster": "ParagSanyashiv",
          "timestamp": "1699428060.0",
          "upvote_count": "1"
        },
        {
          "content": "correct answer is B",
          "upvote_count": "1",
          "timestamp": "1697314380.0",
          "poster": "alce2020",
          "comment_id": "870451"
        },
        {
          "upvote_count": "1",
          "timestamp": "1696684260.0",
          "content": "Selected Answer: B\nB , no brainer",
          "poster": "ele",
          "comment_id": "863902"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:09:46.850Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "usy4XIFKRHCDJ3d2fTpZ",
      "question_number": 36,
      "page": 8,
      "question_text": "A company has a legacy application. A DevOps engineer needs to automate the process of building the deployable artifact for the legacy application. The solution must store the deployable artifact in an existing Amazon S3 bucket for future deployments to reference.\n\nWhich solution will meet these requirements in the MOST operationally efficient way?",
      "choices": {
        "A": "Create a custom Docker image that contains all the dependencies for the legacy application. Store the custom Docker image in a new Amazon Elastic Container Registry (Amazon ECR) repository. Configure a new AWS CodeBuild project to use the custom Docker image to build the deployable artifact and to save the artifact to the S3 bucket.",
        "D": "Create an Amazon Elastic Kubernetes Service (Amazon EKS) cluster with an AWS Fargate profile that runs in multiple Availability Zones. Create a custom Docker image that contains all the dependencies for the legacy application. Store the custom Docker image in a new Amazon Elastic Container Registry (Amazon ECR) repository. Use the custom Docker image inside the EKS cluster to build the deployable artifact and to save the artifact to the S3 bucket.",
        "B": "Launch a new Amazon EC2 instance. Install all the dependencies for the legacy application on the EC2 instance. Use the EC2 instance to build the deployable artifact and to save the artifact to the S3 bucket.",
        "C": "Create a custom EC2 Image Builder image. Install all the dependencies for the legacy application on the image. Launch a new Amazon EC2 instance from the image. Use the new EC2 instance to build the deployable artifact and to save the artifact to the S3 bucket."
      },
      "correct_answer": "A",
      "answer_ET": "A",
      "answers_community": [
        "A (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/112704-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-06-20 21:12:00",
      "unix_timestamp": 1687288320,
      "discussion_count": 6,
      "discussion": [
        {
          "comment_id": "964907",
          "timestamp": "1690476240.0",
          "poster": "haazybanj",
          "content": "Selected Answer: A\nThe most operationally efficient solution for automating the process of building the deployable artifact for the legacy application and storing it in an existing Amazon S3 bucket is:\n\nA. This solution leverages containerization with Docker, which allows for consistent and isolated builds, making it easier to manage application dependencies. The use of AWS CodeBuild allows for scalable and automated builds using the custom Docker image, making the process efficient and reliable. The deployable artifact can then be saved to the existing S3 bucket for future reference and deployments.",
          "upvote_count": "8"
        },
        {
          "poster": "thanhnv142",
          "upvote_count": "6",
          "comment_id": "1142303",
          "timestamp": "1707232380.0",
          "content": "Selected Answer: A\nA is correct: <needs to automate the process of building the deployable artifact for the legacy application> means codebuild\nBCD dont mention codebuild, only A mentions"
        },
        {
          "timestamp": "1722583260.0",
          "poster": "jamesf",
          "upvote_count": "1",
          "comment_id": "1259755",
          "content": "Selected Answer: A\nkeywords: CodeBuild for Reusable artifacts"
        },
        {
          "poster": "habros",
          "upvote_count": "3",
          "content": "Selected Answer: A\nReusable artifacts = A.",
          "comment_id": "947348",
          "timestamp": "1688914320.0"
        },
        {
          "upvote_count": "2",
          "content": "Selected Answer: A\nOption A makes more sense to me.",
          "comment_id": "934325",
          "poster": "FunkyFresco",
          "timestamp": "1687777260.0"
        },
        {
          "upvote_count": "3",
          "content": "(A) This approach is the most operationally efficient because it leverages the benefits of containerization, such as isolation and reproducibility, as well as AWS managed services. AWS CodeBuild is a fully managed build service that can compile your source code, run tests, and produce deployable software packages. By using a custom Docker image that includes all dependencies, you can ensure that the environment in which your code is built is consistent. Using Amazon ECR to store Docker images lets you easily deploy the images to any environment. Also, you can directly upload the build artifacts to Amazon S3 from AWS CodeBuild, which is beneficial for version control and archival purposes.",
          "timestamp": "1687288320.0",
          "poster": "tartarus23",
          "comment_id": "928726"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:09:57.494Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "oc0a5CkSqsY5289y5C6w",
      "question_number": 37,
      "page": 8,
      "question_text": "A company builds a container image in an AWS CodeBuild project by running Docker commands. After the container image is built, the CodeBuild project uploads the container image to an Amazon S3 bucket. The CodeBuild project has an IAM service role that has permissions to access the S3 bucket.\n\nA DevOps engineer needs to replace the S3 bucket with an Amazon Elastic Container Registry (Amazon ECR) repository to store the container images. The DevOps engineer creates an ECR private image repository in the same AWS Region of the CodeBuild project. The DevOps engineer adjusts the IAM service role with the permissions that are necessary to work with the new ECR repository. The DevOps engineer also places new repository information into the docker build command and the docker push command that are used in the buildspec.yml file.\n\nWhen the CodeBuild project runs a build job, the job fails when the job tries to access the ECR repository.\n\nWhich solution will resolve the issue of failed access to the ECR repository?",
      "choices": {
        "D": "Update the buildspec.yml file to use the AWS CLI to assume the IAM service role for ECR operations. Add an ECR repository policy that allows the IAM service role to have access.",
        "A": "Update the buildspec.yml file to log in to the ECR repository by using the aws ecr get-login-password AWS CLI command to obtain an authentication token. Update the docker login command to use the authentication token to access the ECR repository.",
        "B": "Add an environment variable of type SECRETS_MANAGER to the CodeBuild project. In the environment variable, include the ARN of the CodeBuild project's IAM service role. Update the buildspec.yml file to use the new environment variable to log in with the docker login command to access the ECR repository.",
        "C": "Update the ECR repository to be a public image repository. Add an ECR repository policy that allows the IAM service role to have access."
      },
      "correct_answer": "A",
      "answer_ET": "A",
      "answers_community": [
        "A (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/112703-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-06-20 21:05:00",
      "unix_timestamp": 1687287900,
      "discussion_count": 5,
      "discussion": [
        {
          "poster": "tartarus23",
          "comment_id": "928721",
          "timestamp": "1703106300.0",
          "upvote_count": "8",
          "content": "Selected Answer: A\n(A) When Docker communicates with an Amazon Elastic Container Registry (ECR) repository, it requires authentication. You can authenticate your Docker client to the Amazon ECR registry with the help of the AWS CLI (Command Line Interface). Specifically, you can use the \"aws ecr get-login-password\" command to get an authorization token and then use Docker's \"docker login\" command with that token to authenticate to the registry. You would need to perform these steps in your buildspec.yml file before attempting to push or pull images from/to the ECR repository."
        },
        {
          "poster": "haazybanj",
          "upvote_count": "5",
          "content": "Selected Answer: A\nA:\nWhen using Amazon ECR, you need to authenticate Docker to the ECR registry before pushing or pulling container images. The authentication token can be obtained using the aws ecr get-login-password AWS CLI command. The obtained token needs to be used with the docker login command to authenticate Docker to the ECR repository.\n\nBy following this approach, the CodeBuild project will have the necessary credentials to access the ECR repository, and the build job will be able to push the container image to the ECR repository successfully.",
          "timestamp": "1706381160.0",
          "comment_id": "964910"
        },
        {
          "upvote_count": "4",
          "comment_id": "1142322",
          "poster": "thanhnv142",
          "content": "Selected Answer: A\nA is correct: <the job fails when the job tries to access the ECR repository.> This means there is problem when accessing the repo. <adjusts the IAM service role with the permissions that are necessary to work with the new ECR repository> means have got sufficient permission. Need token to access with aws ecr get-login-password command\nBCD no mention of ecr get-login-password",
          "timestamp": "1722950640.0"
        },
        {
          "comment_id": "983165",
          "timestamp": "1708140540.0",
          "upvote_count": "3",
          "content": "Selected Answer: A\nA is right.",
          "poster": "ixdb"
        },
        {
          "comment_id": "964172",
          "upvote_count": "3",
          "timestamp": "1706310960.0",
          "poster": "CirusD",
          "content": "A..version: 0.2\n\nphases:\n pre_build:\n commands:\n - $(aws ecr get-login --no-include-email --region region-name)\n build:\n commands:\n - docker build -t repository-name .\n - docker tag repository-name:latest repository-uri:latest\n post_build:\n commands:\n - docker push repository-uri:latest"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:09:57.494Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "NRYxPcapUsVk4ksChcw7",
      "question_number": 38,
      "page": 8,
      "question_text": "A company manually provisions IAM access for its employees. The company wants to replace the manual process with an automated process. The company has an existing Active Directory system configured with an external SAML 2.0 identity provider (IdP).\n\nThe company wants employees to use their existing corporate credentials to access AWS. The groups from the existing Active Directory system must be available for permission management in AWS Identity and Access Management (IAM). A DevOps engineer has completed the initial configuration of AWS IAM Identity Center (AWS Single Sign-On) in the company’s AWS account.\n\nWhat should the DevOps engineer do next to meet the requirements?",
      "choices": {
        "B": "Configure AWS Directory Service as an identity source. Configure automatic provisioning of users and groups by using the SAML protocol.",
        "C": "Configure an AD Connector as an identity source. Configure automatic provisioning of users and groups by using the SCIM protocol.",
        "D": "Configure an external IdP as an identity source Configure automatic provisioning of users and groups by using the SAML protocol.",
        "A": "Configure an external IdP as an identity source. Configure automatic provisioning of users and groups by using the SCIM protocol."
      },
      "correct_answer": "A",
      "answer_ET": "A",
      "answers_community": [
        "A (93%)",
        "7%"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/112702-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-06-20 21:00:00",
      "unix_timestamp": 1687287600,
      "discussion_count": 12,
      "discussion": [
        {
          "timestamp": "1687287600.0",
          "content": "Selected Answer: A\n(A) AWS SSO (Single Sign-On) integrates with external identity providers using SAML 2.0, and it can automatically synchronize users and groups from a connected directory using the SCIM (System for Cross-domain Identity Management) protocol. Thus, the DevOps engineer should configure the external IdP as an identity source and then configure automatic provisioning of users and groups by using the SCIM protocol. This will ensure the groups from the existing Active Directory system are available for permission management in AWS Identity and Access Management (IAM) and that employees can use their existing corporate credentials to access AWS.",
          "comment_id": "928715",
          "upvote_count": "8",
          "poster": "tartarus23"
        },
        {
          "upvote_count": "5",
          "content": "Selected Answer: A\nFor Note: SAML (Security Assertion Markup Language) is primarily used for authentication and authorization \nwhile SCIM (System for Cross-domain Identity Management) is a protocol used for automating user provisioning and deprovisioning across different systems and domains",
          "comment_id": "1259777",
          "timestamp": "1722584580.0",
          "poster": "jamesf"
        },
        {
          "poster": "thanhnv142",
          "content": "Selected Answer: A\nA is correct: <The company wants employees to use their existing corporate credentials to access AWS> means we need to assign the existing IdP as an identity source\nB: <Configure AWS Directory Service as an identity source> is irrelevant\nC: < Configure an AD Connector as an identity source>: AD connector is use for connecting AWS active directory with that of on-prem. This question requires AWS identity Center\nD: <provisioning of users and groups by using the SAML protocol.>: SAML is an authenticate protocol. SCIM is the protocol for Idp connection",
          "comment_id": "1142325",
          "timestamp": "1707233580.0",
          "upvote_count": "4"
        },
        {
          "upvote_count": "2",
          "content": "Selected Answer: A\nA: Explanation: What is the difference between SCIM and SSO? SSO (single-sign on) is a way to authenticate (sign in), and SCIM is a way to provision (create an account).",
          "poster": "zolthar_z",
          "comment_id": "1084417",
          "timestamp": "1701352320.0"
        },
        {
          "comment_id": "981964",
          "upvote_count": "1",
          "timestamp": "1692130500.0",
          "content": "This is quoted from aws documentationThe SAML protocol however does not provide a way to query the IdP to learn about users and groups. Therefore, you must make IAM Identity Center aware of those users and groups by provisioning them into IAM Identity Center.\n\nhttps://docs.aws.amazon.com/singlesignon/latest/userguide/scim-profile-saml.html",
          "poster": "XP_2600"
        },
        {
          "timestamp": "1690406340.0",
          "upvote_count": "1",
          "content": "Answer is A : AWS Single Sign-On (AWS SSO) can be integrated with an external SAML 2.0 identity provider (IdP). AWS SSO also supports automatic provisioning (auto-provisioning) of user and group information using the System for Cross-domain Identity Management (SCIM) protocol.",
          "comment_id": "964175",
          "poster": "CirusD"
        },
        {
          "poster": "sb333",
          "upvote_count": "2",
          "timestamp": "1690214400.0",
          "content": "Selected Answer: A\nAnswer A is correct. It is SCIM that can provision users and groups in AWS. Of course the IdP needs to support SCIM (AWS has a list of IdPs that use SCIM). Answer D is not correct as SAML is an authentication protocol (cannot be used to provision users in AWS).\n\nhttps://docs.aws.amazon.com/singlesignon/latest/userguide/scim-profile-saml.html\nhttps://docs.aws.amazon.com/singlesignon/latest/userguide/supported-idps.html",
          "comment_id": "961781"
        },
        {
          "content": "Selected Answer: A\nThe AWS IAM Identity Center (AWS Single Sign-On) has been configured initially. Now, to automate the provisioning of users and groups from the external IdP into AWS IAM, the engineer should choose the SCIM protocol. SCIM is specifically designed for automatic user provisioning, making it the appropriate choice for this scenario.\n\nOption D (Configure an external IdP as an identity source and use the SAML protocol) could work, but it does not address the requirement for automatic provisioning of users and groups. The use of SCIM (Option A) is preferred for automated user and group provisioning, as it is designed for this purpose.",
          "upvote_count": "1",
          "poster": "haazybanj",
          "timestamp": "1690019940.0",
          "comment_id": "959401"
        },
        {
          "poster": "Snape",
          "timestamp": "1689560220.0",
          "upvote_count": "1",
          "comment_id": "953780",
          "content": "Selected Answer: D\nThe company already has an external SAML 2.0 IdP, so the DevOps engineer should configure this IdP as an identity source in AWS Single Sign-On. Vs in option A would require to configure new identity source"
        },
        {
          "content": "Selected Answer: A\nA. SCIM is the automated way to provision users. You do it in AAD/AD and it propagates automatically into AWS SSO.",
          "comment_id": "947359",
          "poster": "habros",
          "upvote_count": "1",
          "timestamp": "1688915040.0"
        },
        {
          "poster": "Blueee",
          "comment_id": "944276",
          "content": "Selected Answer: A\nSCIM protocol is to sync the user and groups from the external identity source",
          "upvote_count": "2",
          "timestamp": "1688613420.0"
        },
        {
          "poster": "Toptip",
          "upvote_count": "1",
          "timestamp": "1688547600.0",
          "comment_id": "943504",
          "content": "Selected Answer: D\nD is correct"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:09:57.494Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "ni2AhK7XxRsiM8emXpvA",
      "question_number": 39,
      "page": 8,
      "question_text": "A company is using AWS to run digital workloads. Each application team in the company has its own AWS account for application hosting. The accounts are consolidated in an organization in AWS Organizations.\n\nThe company wants to enforce security standards across the entire organization. To avoid noncompliance because of security misconfiguration, the company has enforced the use of AWS CloudFormation. A production support team can modify resources in the production environment by using the AWS Management Console to troubleshoot and resolve application-related issues.\n\nA DevOps engineer must implement a solution to identify in near real time any AWS service misconfiguration that results in noncompliance. The solution must automatically remediate the issue within 15 minutes of identification. The solution also must track noncompliant resources and events in a centralized dashboard with accurate timestamps.\n\nWhich solution will meet these requirements with the LEAST development overhead?",
      "choices": {
        "D": "Turn on AWS CloudTrail in the AWS accounts. Analyze CloudTrail logs by using Amazon CloudWatch Logs to identify noncompliant resources. Use CloudWatch Logs filters for drift detection. Use Amazon EventBridge to invoke the Lambda function for remediation. Stream filtered CloudWatch logs to Amazon OpenSearch Service. Set up a dashboard on OpenSearch Service for tracking.",
        "A": "Use CloudFormation drift detection to identify noncompliant resources. Use drift detection events from CloudFormation to invoke an AWS Lambda function for remediation. Configure the Lambda function to publish logs to an Amazon CloudWatch Logs log group. Configure an Amazon CloudWatch dashboard to use the log group for tracking.",
        "B": "Turn on AWS CloudTrail in the AWS accounts. Analyze CloudTrail logs by using Amazon Athena to identify noncompliant resources. Use AWS Step Functions to track query results on Athena for drift detection and to invoke an AWS Lambda function for remediation. For tracking, set up an Amazon QuickSight dashboard that uses Athena as the data source.",
        "C": "Turn on the configuration recorder in AWS Config in all the AWS accounts to identify noncompliant resources. Enable AWS Security Hub with the --no-enable-default-standards option in all the AWS accounts. Set up AWS Config managed rules and custom rules. Set up automatic remediation by using AWS Config conformance packs. For tracking, set up a dashboard on Security Hub in a designated Security Hub administrator account."
      },
      "correct_answer": "C",
      "answer_ET": "C",
      "answers_community": [
        "C (89%)",
        "11%"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/112701-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-06-20 20:53:00",
      "unix_timestamp": 1687287180,
      "discussion_count": 11,
      "discussion": [
        {
          "content": "Selected Answer: C\n\"complaince\"\n\nautomatically default to a AWS config answer",
          "comments": [
            {
              "timestamp": "1748533140.0",
              "comment_id": "1573337",
              "upvote_count": "1",
              "content": "compliance* sorry just waking up",
              "poster": "92a2133"
            }
          ],
          "upvote_count": "1",
          "poster": "92a2133",
          "comment_id": "1573335",
          "timestamp": "1748533080.0"
        },
        {
          "timestamp": "1727114760.0",
          "comments": [
            {
              "poster": "heff_bezos",
              "content": "Lambda functions also have a execution limit of 15 minutes. If a remediation task were to take longer than that, it would fail.",
              "timestamp": "1727114880.0",
              "comment_id": "1288269",
              "upvote_count": "1"
            }
          ],
          "upvote_count": "1",
          "poster": "heff_bezos",
          "comment_id": "1288268",
          "content": "Selected Answer: C\nI don't think it is A because the question is asking the LEAST development overhead. Configuring Lambdas to remediate and send logs is development. It is much easier to use the built in features of AWS Config and SecurityHub"
        },
        {
          "upvote_count": "3",
          "comment_id": "1240866",
          "timestamp": "1719933720.0",
          "poster": "zijo",
          "content": "Selected Answer: C\nC is the better solution. AWS CloudFormation drift detection helps identify whether the actual configuration of your AWS resources matches their expected configuration as defined in the CloudFormation stack template. While it is a powerful tool for maintaining compliance and consistency, it alone cannot fully prevent noncompliance due to security misconfigurations. Thats where you need AWS config to continuously monitor service configurations and even use aggregator to collect all aws config data from all member accounts in aws organization to Security Hub to provide a centralized dashboard."
        },
        {
          "content": "Leaning towards \"A\" unless someone can convince me otherwise. Why?:\nI have a problem with this step in \"C\": \"Turn on the configuration recorder in AWS Config in all the AWS accounts to identify noncompliant resources.\" \nThe fact is your not going to detect any \"drift\" by turning on the recorder AFTER the accounts are noncompliant.\nAWS Config rules (canned or custom) and Conformance Packs can do a lot, but it's definitely duplicating settings any security settings allready defined CloudFormation stacks. \nI lean towards \"A\" because \"To avoid noncompliance because of security misconfiguration, the company has enforced the use of AWS CloudFormation\".\nTherefore CloudFormation stacks are is where the security settings are defined, and thereby CloudFormation is implied to be part of the detection and remediation process.\nCloudFormation drift detection can be automated, and one can just \"automatically remediate the issue within 15 minutes of identification\" by just doing a stack refresh. Easy peasy.",
          "comments": [
            {
              "comment_id": "1243552",
              "content": "Correct, A is the answer.",
              "timestamp": "1720298640.0",
              "poster": "ajeeshb",
              "upvote_count": "1"
            }
          ],
          "comment_id": "1235033",
          "poster": "Gomer",
          "timestamp": "1719011040.0",
          "upvote_count": "2"
        },
        {
          "poster": "dkp",
          "content": "Selected Answer: C\nanswer is C with minimal overhead",
          "timestamp": "1713086460.0",
          "comment_id": "1195414",
          "upvote_count": "1"
        },
        {
          "comment_id": "1176670",
          "content": "Selected Answer: A\nBoth Option A and C work. However, considering Option C involves a lot 'all the AWS accounts,' it undoubtedly increases development overhead",
          "poster": "tristan_07",
          "timestamp": "1710783420.0",
          "upvote_count": "1"
        },
        {
          "comment_id": "1142333",
          "upvote_count": "1",
          "comments": [
            {
              "content": "A not correct because not mention how to remediate",
              "poster": "vn_thanhtung",
              "upvote_count": "1",
              "timestamp": "1716256800.0",
              "comment_id": "1214652"
            }
          ],
          "poster": "thanhnv142",
          "timestamp": "1707234540.0",
          "content": "Selected Answer: A\nA is correct: drift detection is the best for this scenario, which utilizes AWS cloudformation\nB and D: using cloudtraid is for monitoring account activities\nC: AWS Config conformance packs cannot make remediation actions. It needs to trigger AWS SSM automation document"
        },
        {
          "upvote_count": "2",
          "content": "C is right \nhttps://aws.amazon.com/blogs/security/optimize-aws-config-for-aws-security-hub-to-effectively-manage-your-cloud-security-posture/",
          "poster": "AzureDP900",
          "comment_id": "1010087",
          "timestamp": "1694987280.0"
        },
        {
          "comment_id": "953784",
          "content": "Selected Answer: C\nCompliance usually indicates towards config",
          "upvote_count": "4",
          "timestamp": "1689560460.0",
          "poster": "Snape"
        },
        {
          "content": "Selected Answer: C\ncompliance means aws config,automatic remedy aws config,central dashboard security hub",
          "upvote_count": "2",
          "timestamp": "1687313040.0",
          "poster": "ds50421",
          "comment_id": "928961"
        },
        {
          "upvote_count": "4",
          "content": "Selected Answer: C\n(C) This solution meets all of the requirements. AWS Config can monitor resource configurations for compliance with defined rules. The use of AWS Security Hub allows for centralized management of security alerts and compliance checks across all accounts. AWS Config conformance packs allow for automated remediation of non-compliant resources. AWS Security Hub provides a comprehensive view of high-priority security alerts and compliance status across AWS accounts. This solution is also the one with the least development overhead as it uses built-in AWS services specifically designed for configuration management and compliance tracking.",
          "poster": "tartarus23",
          "comment_id": "928709",
          "timestamp": "1687287180.0"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:09:57.494Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "yRX8dVhYnU7kL8YP1LKC",
      "question_number": 40,
      "page": 8,
      "question_text": "A company uses AWS Organizations to manage its AWS accounts. The organization root has an OU that is named Environments. The Environments OU has two child OUs that are named Development and Production, respectively.\n\nThe Environments OU and the child OUs have the default FullAWSAccess policy in place. A DevOps engineer plans to remove the FullAWSAccess policy from the Development OU and replace the policy with a policy that allows all actions on Amazon EC2 resources.\n\nWhat will be the outcome of this policy replacement?",
      "choices": {
        "A": "All users in the Development OU will be allowed all API actions on all resources.",
        "B": "All users in the Development OU will be allowed all API actions on EC2 resources. All other API actions will be denied.",
        "D": "All users in the Development OU will be denied all API actions on EC2 resources. All other API actions will be allowed.",
        "C": "All users in the Development OU will be denied all API actions on all resources."
      },
      "correct_answer": "B",
      "answer_ET": "B",
      "answers_community": [
        "B (81%)",
        "A (19%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/112699-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-06-20 20:48:00",
      "unix_timestamp": 1687286880,
      "discussion_count": 19,
      "discussion": [
        {
          "upvote_count": "14",
          "poster": "d262e67",
          "content": "Selected Answer: B\nThe key point is that \"SCP inheritance works differently for Allow and Deny policies\". Allowed policies are only inherited if the children don't have any Allow policy. Once they have an allow policy, only actions defined in that policy will be allowed and no \"Allow\" policy will be inherited from the parent(s) OUs. What inherits is the implicit Deny policy which is a hidden policy sitting above all.\n\nCheck the tables in this link:\nhttps://aws.amazon.com/blogs/security/get-more-out-of-service-control-policies-in-a-multi-account-environment/",
          "comment_id": "1110259",
          "comments": [
            {
              "poster": "MalonJay",
              "timestamp": "1715116560.0",
              "comment_id": "1208033",
              "content": "Very good link about SCPs.",
              "upvote_count": "1"
            }
          ],
          "timestamp": "1703999100.0"
        },
        {
          "poster": "devakram",
          "timestamp": "1712829360.0",
          "upvote_count": "7",
          "comment_id": "1193666",
          "content": "Selected Answer: B\nI've just tested in my AWS account with the same scenario. I removed the SCP from the dev env and kept the EC2 policy, which by that I was denied access to all other operations except EC2."
        },
        {
          "upvote_count": "1",
          "comment_id": "1297536",
          "poster": "auxwww",
          "content": "Selected Answer: B\nBest explanation I found in this forum\n\nFrom: learnwithaniket\n\"For a permission to be allowed for a specific account, there must be an explicit Allow statement at every level from the root through each OU in the direct path to the account (including the target account itself). This is why when you enable SCPs, AWS Organizations attaches an AWS managed SCP policy named FullAWSAccess which allows all services and actions. If this policy is removed and not replaced at any level of the organization, all OUs and accounts under that level would be blocked from taking any actions.\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps_evaluation.html\"",
          "timestamp": "1728908820.0"
        },
        {
          "content": "Selected Answer: B\nNote: Adding an SCP with full AWS access doesn’t give all the principals in an account access to everything. SCPs don’t grant permissions; they are used to filter permissions. Principals still need a policy within the account that grants them access.",
          "comment_id": "1191541",
          "timestamp": "1712574420.0",
          "upvote_count": "2",
          "poster": "HayLLlHuK"
        },
        {
          "timestamp": "1710605400.0",
          "upvote_count": "1",
          "content": "Selected Answer: A\nA - Inherited SCPs cannot be removed so FullAWSAccess will still apply",
          "poster": "DanShone",
          "comments": [
            {
              "upvote_count": "2",
              "content": "no, I've just tested it in my account now, and B is the true answer. Although there were inherited SCPs coming from root and env which still showed in the SCP page for that OU, after detaching the allow all SCP, I was denied access on any other API except EC2.",
              "timestamp": "1712829300.0",
              "comment_id": "1193664",
              "poster": "devakram"
            }
          ],
          "comment_id": "1175084"
        },
        {
          "poster": "thanhnv142",
          "comment_id": "1142343",
          "content": "B is correct: SCP have allow statement and this matchs",
          "upvote_count": "2",
          "timestamp": "1707235140.0"
        },
        {
          "content": "a is the answer",
          "comment_id": "1130161",
          "upvote_count": "1",
          "poster": "sarlos",
          "timestamp": "1706065200.0"
        },
        {
          "poster": "1123lluu",
          "timestamp": "1701488940.0",
          "upvote_count": "1",
          "comment_id": "1085686",
          "content": "should be B, see example in here: https://aws.amazon.com/blogs/security/get-more-out-of-service-control-policies-in-a-multi-account-environment/"
        },
        {
          "content": "Selected Answer: A\nAnswer is A: You can't remove heritage policy from child OU",
          "poster": "zolthar_z",
          "upvote_count": "2",
          "timestamp": "1701354000.0",
          "comment_id": "1084443"
        },
        {
          "poster": "learnwithaniket",
          "content": "Selected Answer: B\nB is the right answer.\nFor a permission to be allowed for a specific account, there must be an explicit Allow statement at every level from the root through each OU in the direct path to the account (including the target account itself). This is why when you enable SCPs, AWS Organizations attaches an AWS managed SCP policy named FullAWSAccess which allows all services and actions. If this policy is removed and not replaced at any level of the organization, all OUs and accounts under that level would be blocked from taking any actions.\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps_evaluation.html",
          "timestamp": "1700193660.0",
          "upvote_count": "3",
          "comment_id": "1073021"
        },
        {
          "timestamp": "1697763060.0",
          "upvote_count": "4",
          "content": "Selected Answer: B\n\"SCP evaluation follows a deny-by-default model, meaning that any permissions not explicitly allowed in the SCPs are denied. If an allow statement is not present in the SCPs at any of the levels such as Root, Production OU or Account B, the access is denied.\"\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps_evaluation.html#:~:text=SCP%20evaluation%20follows%20a%20deny%2Dby%2Ddefault%20model%2C%20meaning%20that%20any%20permissions%20not%20explicitly%20allowed%20in%20the%20SCPs%20are%20denied.%20If%20an%20allow%20statement%20is%20not%20present%20in%20the%20SCPs%20at%20any%20of%20the%20levels%20such%20as%20Root%2C%20Production%20OU%20or%20Account%20B%2C%20the%20access%20is%20denied.",
          "poster": "tatarai1964",
          "comment_id": "1048274"
        },
        {
          "comments": [
            {
              "upvote_count": "1",
              "content": "This URL does not explain the SCP.",
              "timestamp": "1700077680.0",
              "comment_id": "1071828",
              "poster": "zain1258"
            }
          ],
          "poster": "jdx000",
          "comment_id": "1041616",
          "timestamp": "1697102940.0",
          "upvote_count": "1",
          "content": "Selected Answer: A\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_inheritance_mgmt.html"
        },
        {
          "comment_id": "987472",
          "timestamp": "1692711420.0",
          "upvote_count": "3",
          "poster": "Radeeka",
          "content": "Selected Answer: A\nEven the default policy is removed, Child OU will inherit the SCP from the Environment OU, which is AWSFullAccess. So the Child OU will still have full access."
        },
        {
          "content": "Its A, the new policy is an allow policy not deny, thus all permissions are gratned to Dev OU.",
          "poster": "Gathix444",
          "comment_id": "985479",
          "upvote_count": "2",
          "timestamp": "1692489420.0"
        },
        {
          "timestamp": "1692236700.0",
          "poster": "ixdb",
          "content": "Selected Answer: B\nSCP can define An allow list – actions are prohibited by default, and you specify what services and actions are allowed.",
          "comment_id": "983173",
          "upvote_count": "3"
        },
        {
          "poster": "vherman",
          "content": "Selected Answer: A\nA is correct. \nDevelopment OU will inherit FullAccess from the Environments OU\nno explicit DENY in the new AllowAllEc2 Policy",
          "upvote_count": "4",
          "timestamp": "1691069580.0",
          "comments": [
            {
              "poster": "Aja1",
              "content": "The answer is B.\n\nWhen a policy is removed from an OU, the default policy for the parent OU is inherited. In this case, the default policy for the Environments OU is FullAWSAccess, which allows all API actions on all resources.\n\nWhen the DevOps engineer replaces the FullAWSAccess policy with a policy that allows all actions on Amazon EC2 resources, the new policy will take precedence over the default policy. This means that all users in the Development OU will be allowed all API actions on EC2 resources. All other API actions will be denied.",
              "comment_id": "978767",
              "timestamp": "1691768220.0",
              "upvote_count": "3",
              "comments": []
            },
            {
              "poster": "yorkicurke",
              "content": "because SCPs define the maximum permissions for an organization or organizational unit (OU) in AWS Organizations. \nIf an SCP doesn’t explicitly grant permissions for an action, then that action is implicitly denied.",
              "comment_id": "1080077",
              "timestamp": "1700923560.0",
              "upvote_count": "1",
              "comments": [
                {
                  "upvote_count": "1",
                  "poster": "yorkicurke",
                  "content": "link;\nhttps://repost.aws/questions/QUSHz1PpiJTOqWRuguGn_Trw/resource-and-iam-policy-with-scp",
                  "timestamp": "1700923680.0",
                  "comment_id": "1080079"
                }
              ]
            }
          ],
          "comment_id": "971144"
        },
        {
          "timestamp": "1687770300.0",
          "upvote_count": "4",
          "poster": "FunkyFresco",
          "content": "Selected Answer: B\nB is the correct option.",
          "comment_id": "934227"
        },
        {
          "comment_id": "928959",
          "upvote_count": "4",
          "timestamp": "1687312980.0",
          "content": "Selected Answer: B\nAll users in the Development OU will be allowed all API actions on EC2 resources. All other API actions will be denied.",
          "poster": "ds50421"
        },
        {
          "poster": "tartarus23",
          "upvote_count": "4",
          "comment_id": "928703",
          "content": "Selected Answer: B\nAWS Organizations uses Service Control Policies (SCPs) to manage permissions across accounts within an organization. By removing the FullAWSAccess policy and replacing it with a policy that allows all actions on Amazon EC2 resources, the effect would be that users in the Development OU can perform all actions on EC2 resources, but will be denied all other AWS actions. This is because an SCP doesn't grant permissions, but instead acts as a guardrail that defines the maximum permissions users and roles can have.",
          "timestamp": "1687286880.0"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:09:57.494Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "ZTfC9SvMGYkf2XRn0BFh",
      "question_number": 41,
      "page": 9,
      "question_text": "A company is examining its disaster recovery capability and wants the ability to switch over its daily operations to a secondary AWS Region. The company uses AWS CodeCommit as a source control tool in the primary Region.\n\nA DevOps engineer must provide the capability for the company to develop code in the secondary Region. If the company needs to use the secondary Region, developers can add an additional remote URL to their local Git configuration.\n\nWhich solution will meet these requirements?",
      "choices": {
        "A": "Create a CodeCommit repository in the secondary Region. Create an AWS CodeBuild project to perform a Git mirror operation of the primary Region's CodeCommit repository to the secondary Region's CodeCommit repository. Create an AWS Lambda function that invokes the CodeBuild project. Create an Amazon EventBridge rule that reacts to merge events in the primary Region's CodeCommit repository. Configure the EventBridge rule to invoke the Lambda function.",
        "C": "Create an AWS CodeArtifact repository in the secondary Region. Create an AWS CodePipeline pipeline that uses the primary Region’s CodeCommit repository for the source action. Create a cross-Region stage in the pipeline that packages the CodeCommit repository contents and stores the contents in the CodeArtifact repository when a pull request is merged into the CodeCommit repository.",
        "B": "Create an Amazon S3 bucket in the secondary Region. Create an AWS Fargate task to perform a Git mirror operation of the primary Region's CodeCommit repository and copy the result to the S3 bucket. Create an AWS Lambda function that initiates the Fargate task. Create an Amazon EventBridge rule that reacts to merge events in the CodeCommit repository. Configure the EventBridge rule to invoke the Lambda function.",
        "D": "Create an AWS Cloud9 environment and a CodeCommit repository in the secondary Region. Configure the primary Region's CodeCommit repository as a remote repository in the AWS Cloud9 environment. Connect the secondary Region's CodeCommit repository to the AWS Cloud9 environment."
      },
      "correct_answer": "A",
      "answer_ET": "A",
      "answers_community": [
        "A (87%)",
        "13%"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/119654-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-09-01 15:07:00",
      "unix_timestamp": 1693573620,
      "discussion_count": 10,
      "discussion": [
        {
          "comment_id": "1562237",
          "content": "Selected Answer: A\nthe goal is to enable code development in a secondary Region in case of a failover.\n\n[A] provides:\n- a mirrored CodeCommit repo in the secondary Region.\n- automated sync via Git mirroring (a Git feature that fully syncs repositories).\n- automation through CodeBuild, Lambda, and EventBridge to keep the secondary repo up to date.\n\nDevelopers can simply add the secondary repo as a remote (git remote add backup URL) and push/pull as needed\n\nproblem with D:\n- cloud9 is a development env (IDE), not a repository mirroring solution\n- having both repos in the Cloud9 env doesn’t keep them synchronized automatically.\n\nThis doesn’t scale or allow other devs to use the secondary repo unless they also set up custom workflows",
          "upvote_count": "1",
          "timestamp": "1745159820.0",
          "poster": "GripZA"
        },
        {
          "content": "Why is not D a solution? If the developers in the secondary region can configure primary region's codecommit repository as a remote repository in the AWS Cloud9 environment they can do development and do all git functions remote.",
          "comment_id": "1240965",
          "upvote_count": "1",
          "timestamp": "1719944820.0",
          "poster": "zijo"
        },
        {
          "content": "A: (NO) \"Create an AWS CodeBuild project to perform a Git mirror operation of the primary Region's CodeCommit repository to the secondary Region's CodeCommit repository.\"\nCodeBuild doesn't have the ability to do a \"git mirror\" operation itself. All online examples have CodeCommit actions calling Lambda (directly or through EventWatch) which calls fargate (or EC2) which does the actual git mirror\n\nA: (NO) \"Create an AWS Lambda function that invokes the CodeBuild project.\nThe is exactly the reverse from online examples\n\nB: (NO) \"Create an AWS Fargate task to perform a Git mirror operation of the primary Region's CodeCommit repository and copy the result to the S3 bucket.\"\nDoes it really make sense to use a \"git\" mirror operations copy from a CodeCommit repo to an S3 bucket? All online examples using \"git\" \"mirror\" have CodeCommit repo as remote target.",
          "upvote_count": "1",
          "timestamp": "1719031920.0",
          "comments": [
            {
              "comments": [
                {
                  "upvote_count": "2",
                  "comments": [
                    {
                      "comment_id": "1235127",
                      "upvote_count": "1",
                      "content": "Also want to add that \"D\" would would work fine if you presume that the \"git\" \"mirror\" is also being done (though additional undefined step in the solution). Nothing says \"D\" is the complete solution. The ONLY requirement here is to provide developers a remote environment to develop in.",
                      "timestamp": "1719033840.0",
                      "poster": "Gomer"
                    }
                  ],
                  "poster": "Gomer",
                  "content": "Actually, I meant to say I lean towards \"D\" (using Cloud9 as remote development environment)",
                  "timestamp": "1719032460.0",
                  "comment_id": "1235111"
                }
              ],
              "poster": "Gomer",
              "content": "The specific requirement here isn't \"disaster recovery capability\" and \"ability to switch over its daily operations to a secondary AWS Region.\" That is just being investigated.\nThe specific requirement is to \"provide the capability for the company to develop code in the secondary Region.\"\n\"If the company needs to use the secondary Region, developers can add an additional remote URL to their local Git configuration.\"\n\nTo me this sounds to like the specific requirement here is only to provide developers with a complete remote development environment (not to provide a DR solution)\nIf that is true, then using Cloud9 web development environment (includes git, etc.) with same local CodeCommit repo is acceptiable\nI'm not a developer, but the specific criteria wording and logic make me lean towards \"C\"",
              "upvote_count": "1",
              "timestamp": "1719032040.0",
              "comment_id": "1235106"
            },
            {
              "upvote_count": "2",
              "timestamp": "1719032100.0",
              "comment_id": "1235107",
              "content": "Flow:\n\nAWS Example: CodeCommit(action) ----------------> Lambda -> Fargate task (\"git clone --mirror\" local repo, \"git remote set-url --push origin\" destination repo) -> CodeCommit(remote repo)\n\nSolution \"B\": CodeCommit(action) -> EventBridge -> Lambda -> Fargate task (\"git clone --mirror\" local repo, \"git remote set-url --push origin\" destination repo) -> S3(remote bucket)\n\nReferences:\nhttps://aws.amazon.com/blogs/devops/replicate-aws-codecommit-repository-between-regions-using-aws-fargate/\nhttps://aws.amazon.com/cloud9/",
              "poster": "Gomer"
            }
          ],
          "poster": "Gomer",
          "comment_id": "1235105"
        },
        {
          "poster": "thanhnv142",
          "content": "A is correct: <A DevOps engineer must provide the capability for the company to develop code in the secondary Region> means code commit",
          "comments": [
            {
              "timestamp": "1707485040.0",
              "upvote_count": "2",
              "poster": "thanhnv142",
              "comment_id": "1145497",
              "content": "A is correct: < develop code in the secondary Region>: code commit cannot automatically clone cross-region.Must use a tool to do this duplication task\nB: Using S3 as a secondary repo is incorrect\nC and D: no mention of using codecommit as the secondary repo"
            }
          ],
          "upvote_count": "3",
          "comment_id": "1142915",
          "timestamp": "1707275460.0"
        },
        {
          "timestamp": "1704916140.0",
          "content": "Selected Answer: A\nAgree answer is A",
          "comment_id": "1118996",
          "poster": "yuliaqwerty",
          "upvote_count": "2"
        },
        {
          "timestamp": "1702045140.0",
          "comments": [
            {
              "poster": "bnagaraja9099",
              "upvote_count": "1",
              "timestamp": "1702836060.0",
              "content": "This part of B is incorrect. - It should use Code commit instead of S3. \"Create an AWS Fargate task to perform a Git mirror operation of the primary Region's CodeCommit repository and copy the result to the S3 bucket. \"",
              "comment_id": "1099120"
            }
          ],
          "comment_id": "1091139",
          "poster": "svjl",
          "upvote_count": "2",
          "content": "B- It does the replication out of the box and meets the requirements\nhttps://aws.amazon.com/blogs/devops/replicate-aws-codecommit-repository-between-regions-using-aws-fargate/"
        },
        {
          "poster": "vandergun",
          "content": "Selected Answer: A\nA is at least operation and cost",
          "comment_id": "1074444",
          "timestamp": "1700368860.0",
          "upvote_count": "2"
        },
        {
          "content": "Selected Answer: A\nThis solution meets all of the company's requirements:\n\nIt allows developers to add an additional remote URL to their local Git configuration to develop code in the secondary Region.\nIt is automated: the EventBridge rule will automatically invoke the Lambda function whenever a merge event occurs in the primary Region's CodeCommit repository.\nIt is reliable: the CodeBuild project will use Git to ensure that a perfect copy of the primary Region's CodeCommit repository is created in the secondary Region.",
          "upvote_count": "4",
          "comment_id": "1014728",
          "timestamp": "1695453060.0",
          "poster": "Dushank"
        },
        {
          "timestamp": "1695212640.0",
          "poster": "RVivek",
          "content": "Selected Answer: A\nhttps://dev.to/apatil88/replicate-aws-codecommit-repositories-between-regions-using-codebuild-and-codepipeline-5fh1",
          "upvote_count": "4",
          "comment_id": "1012299"
        },
        {
          "poster": "ixdb",
          "comment_id": "996073",
          "comments": [
            {
              "upvote_count": "3",
              "poster": "zendevloper",
              "content": "B is wrong because it uses S3. Developers need a valid git remote URL.\nCorrect answer is A",
              "comment_id": "1014750",
              "timestamp": "1695456360.0"
            }
          ],
          "timestamp": "1693573620.0",
          "content": "Selected Answer: B\nB is right. https://aws.amazon.com/cn/blogs/devops/replicate-aws-codecommit-repository-between-regions-using-aws-fargate/",
          "upvote_count": "2"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:10:08.045Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "ZohDSqkuoezY3m3Rc5PH",
      "question_number": 42,
      "page": 9,
      "question_text": "A DevOps team is merging code revisions for an application that uses an Amazon RDS Multi-AZ DB cluster for its production database. The DevOps team uses continuous integration to periodically verify that the application works. The DevOps team needs to test the changes before the changes are deployed to the production database.\n\nWhich solution will meet these requirements?",
      "choices": {
        "B": "Deploy the application to production. Configure an audit log of data control language (DCL) operations to capture database activities to perform if verification fails.",
        "D": "Ensure that the DB cluster is a Multi-AZ deployment. Deploy the application with the updates. Fail over to the standby instance if verification fails.",
        "A": "Use a buildspec file in AWS CodeBuild to restore the DB cluster from a snapshot of the production database, run integration tests, and drop the restored database after verification.",
        "C": "Create a snapshot of the DB cluster before deploying the application. Use the Update requires:Replacement property on the DB instance in AWS CloudFormation to deploy the application and apply the changes."
      },
      "correct_answer": "A",
      "answer_ET": "A",
      "answers_community": [
        "A (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/119637-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-09-01 11:00:00",
      "unix_timestamp": 1693558800,
      "discussion_count": 7,
      "discussion": [
        {
          "content": "Selected Answer: A\nA is the solution which will allow testing without any such consequence",
          "upvote_count": "5",
          "poster": "Ramdi1",
          "timestamp": "1707578040.0",
          "comment_id": "1146396"
        },
        {
          "upvote_count": "2",
          "timestamp": "1722236760.0",
          "poster": "jamesf",
          "comment_id": "1257303",
          "content": "Selected Answer: A\nA correct as allow testing before real deployment. \nhttps://aws.amazon.com/blogs/devops/enhancing-automated-database-continuous-integration-with-aws-codebuild-and-amazon-rds-database-snapshot/"
        },
        {
          "poster": "thanhnv142",
          "upvote_count": "4",
          "timestamp": "1707486180.0",
          "comment_id": "1145513",
          "content": "Selected Answer: A\nA is correct: This option allow testing before real deployment\nB: < Deploy the application to production> : this would not allow testing before changes are made\nC: <Create a snapshot of the DB cluster before deploying the application>: This means the same as B - would not allow testing before changes are made\nD: <Ensure that the DB cluster is a Multi-AZ deployment. Deploy the application with the updates> - This deploy the app before testing, so it is incoorect"
        },
        {
          "poster": "thanhnv142",
          "upvote_count": "1",
          "content": "A is correct: <The DevOps team uses continuous integration to periodically verify that the application works> and <The DevOps team needs to test the changes before the changes are deployed to the production database> means codebuild",
          "comment_id": "1142916",
          "timestamp": "1707275580.0"
        },
        {
          "content": "This solution meets all of the company's requirements:\n\nIt allows the DevOps team to test the changes before they are deployed to the production database.\nIt is automated: the CodeBuild buildspec file will automatically restore the DB cluster from a snapshot, run the integration tests, and drop the restored database after verification.\nIt is reliable: the CodeBuild buildspec file will ensure that the integration tests are run against a copy of the production database.",
          "poster": "Dushank",
          "timestamp": "1695454200.0",
          "upvote_count": "1",
          "comment_id": "1014735"
        },
        {
          "poster": "ixdb",
          "timestamp": "1693573680.0",
          "content": "Selected Answer: A\nA is right. All others will change the prod db.",
          "comment_id": "996075",
          "upvote_count": "4"
        },
        {
          "timestamp": "1693558800.0",
          "content": "I think it is A\n\nhttps://aws.amazon.com/blogs/devops/enhancing-automated-database-continuous-integration-with-aws-codebuild-and-amazon-rds-database-snapshot/",
          "upvote_count": "2",
          "comment_id": "995869",
          "poster": "traveller37"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:10:08.045Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "9GNCdhKinLY7DtI5HWUa",
      "question_number": 43,
      "page": 9,
      "question_text": "A company manages a multi-tenant environment in its VPC and has configured Amazon GuardDuty for the corresponding AWS account. The company sends all GuardDuty findings to AWS Security Hub.\n\nTraffic from suspicious sources is generating a large number of findings. A DevOps engineer needs to implement a solution to automatically deny traffic across the entire VPC when GuardDuty discovers a new suspicious source.\n\nWhich solution will meet these requirements?",
      "choices": {
        "D": "Create an AWS Lambda function that will create a GuardDuty suppression rule. Configure the Lambda function to run in response to new Security Hub findings that come from GuardDuty.",
        "A": "Create a GuardDuty threat list. Configure GuardDuty to reference the list. Create an AWS Lambda function that will update the threat list. Configure the Lambda function to run in response to new Security Hub findings that come from GuardDuty.",
        "C": "Configure a firewall in AWS Network Firewall. Create an AWS Lambda function that will create a Drop action rule in the firewall policy. Configure the Lambda function to run in response to new Security Hub findings that come from GuardDuty.",
        "B": "Configure an AWS WAF web ACL that includes a custom rule group. Create an AWS Lambda function that will create a block rule in the custom rule group. Configure the Lambda function to run in response to new Security Hub findings that come from GuardDuty."
      },
      "correct_answer": "C",
      "answer_ET": "C",
      "answers_community": [
        "C (92%)",
        "4%"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/119638-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-09-01 11:02:00",
      "unix_timestamp": 1693558920,
      "discussion_count": 9,
      "discussion": [
        {
          "comments": [
            {
              "comment_id": "996920",
              "timestamp": "1693662060.0",
              "poster": "traveller37",
              "upvote_count": "1",
              "content": "Sorry i means B",
              "comments": [
                {
                  "poster": "denccc",
                  "content": "You mean C?",
                  "comment_id": "1062927",
                  "upvote_count": "1",
                  "timestamp": "1699191420.0"
                }
              ]
            }
          ],
          "timestamp": "1693558920.0",
          "content": "I think C:\nhttps://aws.amazon.com/blogs/security/automatically-block-suspicious-traffic-with-aws-network-firewall-and-amazon-guardduty/",
          "comment_id": "995872",
          "upvote_count": "14",
          "poster": "traveller37"
        },
        {
          "content": "Selected Answer: C\nC is correct . Only Network Firewall can block traffic at VPC level. \nA only updates the list , no blocking action\nB- WAF and Web ACL can block only HTTPS traffic for a API/VPC endpoint/ Cloudfron distribution not for enire VPC",
          "upvote_count": "11",
          "poster": "RVivek",
          "timestamp": "1695419280.0",
          "comment_id": "1014529"
        },
        {
          "poster": "jamesf",
          "upvote_count": "2",
          "timestamp": "1722237000.0",
          "comment_id": "1257304",
          "content": "Selected Answer: C\nC, AWS Network Firewall can block traffic at VPC level.\nhttps://aws.amazon.com/blogs/security/automatically-block-suspicious-traffic-with-aws-network-firewall-and-amazon-guardduty/"
        },
        {
          "poster": "zijo",
          "comment_id": "1241035",
          "timestamp": "1719952620.0",
          "content": "Selected Answer: C\nB blocks traffic at the http/https web traffic layer not for VPC layer",
          "upvote_count": "1"
        },
        {
          "timestamp": "1707275760.0",
          "comment_id": "1142918",
          "content": "Selected Answer: C\nC is correct: <a solution to automatically deny traffic> means network FW. \nA: irrelevant\nB: We need network fw, not WAF\nD: irrelevant",
          "upvote_count": "3",
          "poster": "thanhnv142"
        },
        {
          "poster": "yorkicurke",
          "content": "hmmm\nis this the last question as of now(25th Nov 23)",
          "comment_id": "1080310",
          "timestamp": "1700943360.0",
          "upvote_count": "1"
        },
        {
          "content": "Selected Answer: C\nHere's the rationale for choosing this option:\n\nAWS Network Firewall:\nAWS Network Firewall is designed to provide centralized network traffic inspection and filtering. It's a suitable choice for implementing network-level controls.\n\nLambda Function for Automation:\nCreating a Lambda function to trigger the creation of a Drop action rule in the firewall policy allows for automated response based on Security Hub findings. This enables you to take immediate action when suspicious sources are detected.\n\nSpecific Action (Drop):\nThe Drop action rule is effective for denying traffic from suspicious sources, effectively controlling access and preventing unwanted traffic.\n\nThis approach aligns well with the requirement to automatically deny traffic when GuardDuty identifies a new suspicious source, enhancing security in the multi-tenant VPC environment.",
          "comment_id": "1014738",
          "timestamp": "1695454680.0",
          "poster": "Dushank",
          "upvote_count": "7"
        },
        {
          "poster": "RVivek",
          "comment_id": "1012316",
          "upvote_count": "1",
          "timestamp": "1695213300.0",
          "content": "Selected Answer: B\nA only will upadte threat list. the requirement is to block the taffic.\nB is corerect. Also it is event driven immditae action"
        },
        {
          "poster": "vladik820",
          "timestamp": "1695030780.0",
          "upvote_count": "1",
          "comment_id": "1010447",
          "content": "Selected Answer: A\nA is right"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:10:08.045Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "xmnUqFeyav2oub67o5UC",
      "question_number": 44,
      "page": 9,
      "question_text": "A company uses AWS Secrets Manager to store a set of sensitive API keys that an AWS Lambda function uses. When the Lambda function is invoked the Lambda function retrieves the API keys and makes an API call to an external service. The Secrets Manager secret is encrypted with the default AWS Key Management Service (AWS KMS) key.\n\nA DevOps engineer needs to update the infrastructure to ensure that only the Lambda function’s execution role can access the values in Secrets Manager. The solution must apply the principle of least privilege.\n\nWhich combination of steps will meet these requirements? (Choose two.)",
      "choices": {
        "B": "Create a KMS customer managed key that trusts Secrets Manager and allows the Lambda function's execution role to decrypt. Update Secrets Manager to use the new customer managed key",
        "D": "Ensure that the Lambda function’s execution role has the KMS permissions scoped on the resource level. Configure the permissions so that the KMS key can encrypt the Secrets Manager secret",
        "C": "Create a KMS customer managed key that trusts Secrets Manager and allows the account's root principal to decrypt. Update Secrets Manager to use the new customer managed key",
        "A": "Update the default KMS key for Secrets Manager to allow only the Lambda function’s execution role to decrypt",
        "E": "Remove all KMS permissions from the Lambda function’s execution role"
      },
      "correct_answer": "BD",
      "answer_ET": "BD",
      "answers_community": [
        "BD (95%)",
        "5%"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/126917-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-11-22 14:15:00",
      "unix_timestamp": 1700658900,
      "discussion_count": 8,
      "discussion": [
        {
          "poster": "thanhnv142",
          "upvote_count": "5",
          "content": "Selected Answer: BD\nB and D are correct: <update the infrastructure to ensure that only the Lambda function’s execution role> means we need to ensure that lambda's IAM role has sufficient permissions and KMS policy allows Lambda's IAM role\nA: cannot update default key\nC: <allows the account's root principal to decrypt> this against the principal of least privilege\nE: irrelevant",
          "timestamp": "1707276060.0",
          "comment_id": "1142920"
        },
        {
          "poster": "heff_bezos",
          "upvote_count": "1",
          "timestamp": "1727116980.0",
          "content": "Selected Answer: BD\nIf default keys are the same as the AWS managed keys, then the answer is B. You cannot modify the \"default\" key's policy to allow access only from the Lambda execution role.",
          "comment_id": "1288282"
        },
        {
          "comment_id": "1257309",
          "poster": "jamesf",
          "timestamp": "1722237420.0",
          "content": "Selected Answer: BD\nI go for BD",
          "upvote_count": "2"
        },
        {
          "comment_id": "1169684",
          "content": "Selected Answer: BD\nThe requirement is to update the infrastructure to ensure that only the Lambda function’s execution\nrole can access the values in Secrets Manager. The solution must apply the principle of least\nprivilege, which means granting the minimum permissions necessary to perform a task.",
          "timestamp": "1710005820.0",
          "poster": "4555894",
          "upvote_count": "2"
        },
        {
          "timestamp": "1707040140.0",
          "upvote_count": "1",
          "content": "Selected Answer: AD\n{\n\"Version\": \"2012-10-17\",\n\"Id\": \"key-consolepolicy-2\",\n\"Statement\": [\n{\n\"Sid\": \"Allow use of the key\",\n\"Effect\": \"Allow\",\n\"Principal\": {\"AWS\": [\n\"arn:aws:iam::111122223333:role/KeyCreatorRole\"\n]},\n\"Action\": [\n\"kms:Encrypt\",\n\"kms:Decrypt\",\n\"kms:ReEncrypt*\",\n\"kms:GenerateDataKey*\",\n\"kms:DescribeKey\"\n],\n\"Resource\": here arn of secret manager\n}\n]\n}\n\nI think A is correct answer , why to create CMK as customer is using default KMS",
          "poster": "hotblooded",
          "comment_id": "1139961"
        },
        {
          "comment_id": "1084451",
          "poster": "zolthar_z",
          "timestamp": "1701354600.0",
          "content": "Selected Answer: BD\nI think B:D",
          "upvote_count": "4"
        },
        {
          "timestamp": "1700731500.0",
          "comment_id": "1078286",
          "poster": "radev",
          "upvote_count": "4",
          "comments": [
            {
              "poster": "hotblooded",
              "content": "{\n \"Version\": \"2012-10-17\",\n \"Id\": \"key-consolepolicy-2\",\n \"Statement\": [\n {\n \"Sid\": \"Allow use of the key\",\n \"Effect\": \"Allow\",\n \"Principal\": {\"AWS\": [\n \"arn:aws:iam::111122223333:role/KeyCreatorRole\"\n ]},\n \"Action\": [\n \"kms:Encrypt\",\n \"kms:Decrypt\",\n \"kms:ReEncrypt*\",\n \"kms:GenerateDataKey*\",\n \"kms:DescribeKey\"\n ],\n \"Resource\": here arn of secret manager\n }\n ]\n}\n\nI think A is correct answer , why to create CMK as customer is using default KMS",
              "comment_id": "1139956",
              "comments": [
                {
                  "timestamp": "1707039960.0",
                  "comment_id": "1139959",
                  "content": "Or we can ad below condition also \n\n\"Condition\": {\n \"StringEquals\": {\n \"kms:CallerAccount\": \"111122223333\",\n \"kms:ViaService\": \"secretsmanager.us-west-2.amazonaws.com\"\n }\n }",
                  "upvote_count": "1",
                  "poster": "hotblooded"
                }
              ],
              "timestamp": "1707039720.0",
              "upvote_count": "1"
            }
          ],
          "content": "Selected Answer: BD\nB, D\nA is incorrect because updating the default KMS key for Secrets Manager to allow only the Lambda function's execution role to decrypt would grant access to all other resources using the default key, which violates the principle of least privilege.\n\nC is incorrect because allowing the account's root principal to decrypt the secret would grant unnecessary access to the secret, which violates the principle of least privilege.\n\nE is incorrect because removing all KMS permissions from the Lambda function's execution role would prevent the Lambda function from decrypting the secret, which is required for it to function properly."
        },
        {
          "timestamp": "1700658900.0",
          "content": "Selected Answer: BD\nI vote B,D",
          "comment_id": "1077401",
          "upvote_count": "2",
          "poster": "vandergun"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:10:08.045Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "95Fl4M7IL8YdeDgdHCgl",
      "question_number": 45,
      "page": 9,
      "question_text": "A company's DevOps engineer is creating an AWS Lambda function to process notifications from an Amazon Simple Notification Service (Amazon SNS) topic. The Lambda function will process the notification messages and will write the contents of the notification messages to an Amazon RDS Multi-AZ DB instance.\n\nDuring testing, a database administrator accidentally shut down the DB instance. While the database was down the company lost several of the SNS notification messages that were delivered during that time.\n\nThe DevOps engineer needs to prevent the loss of notification messages in the future.\n\nWhich solutions will meet this requirement? (Choose two.)",
      "choices": {
        "C": "Configure an Amazon Simple Queue Service (Amazon SQS) dead-letter queue for the SNS topic.",
        "A": "Replace the RDS Multi-AZ DB instance with an Amazon DynamoDB table.",
        "B": "Configure an Amazon Simple Queue Service (Amazon SQS) queue as a destination of the Lambda function.",
        "E": "Replace the SNS topic with an Amazon EventBridge event bus. Configure an EventBridge rule on the new event bus to invoke the Lambda function for each event.",
        "D": "Subscribe an Amazon Simple Queue Service (Amazon SQS) queue to the SNS topic. Configure the Lambda function to process messages from the SQS queue."
      },
      "correct_answer": "CD",
      "answer_ET": "CD",
      "answers_community": [
        "CD (87%)",
        "13%"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/127016-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-11-23 13:29:00",
      "unix_timestamp": 1700742540,
      "discussion_count": 8,
      "discussion": [
        {
          "upvote_count": "11",
          "content": "Selected Answer: CD\nThe two solutions that will meet the requirement of preventing the loss of notification messages in the future are:\n\nD. Subscribe an Amazon Simple Queue Service (Amazon SQS) queue to the SNS topic. Configure the Lambda function to process messages from the SQS queue.\n\nThis solution will ensure that notification messages are delivered to the SQS queue even if the Lambda function is unavailable or the RDS DB instance is down. The Lambda function can then process the messages from the SQS queue at its own pace.\n\nC. Configure an Amazon Simple Queue Service (Amazon SQS) dead-letter queue for the SNS topic.\n\nThis solution will ensure that notification messages that cannot be delivered to the RDS DB instance are not lost. Instead, they will be moved to a dead-letter queue. The DevOps engineer can then manually process the messages from the dead-letter queue.",
          "poster": "vandergun",
          "timestamp": "1700742540.0",
          "comment_id": "1078424"
        },
        {
          "content": "Selected Answer: CD\nC:D , D is a best practice for this scenario, C because you can send failed SNS o SQS Dead letter queue, https://docs.aws.amazon.com/sns/latest/dg/sns-dead-letter-queues.html",
          "poster": "zolthar_z",
          "comment_id": "1084455",
          "upvote_count": "5",
          "timestamp": "1701354840.0"
        },
        {
          "content": "Selected Answer: BD\nC does not make sense. Adding a DLQ to an SNS topic will capture messages that failed to be sent to the consumer, due to an issue with SNS. We are looking to capture messages that couldn't be processed due to a issue at the consumer side (RDS database to be exact, but the Lambda function will be aware of this). \nHence there are two solutions:\n1. configure a SQS between the SNS topic and the Lambda function \n2. configure a DLQ as a destination for failure for the Lambda function (raise an exceuption in the Lambda code)",
          "poster": "ce0df07",
          "upvote_count": "1",
          "comment_id": "1353164",
          "timestamp": "1738972680.0"
        },
        {
          "poster": "CHRIS12722222",
          "timestamp": "1734249060.0",
          "comment_id": "1326745",
          "content": "Selected Answer: BD\nCorrect answer\n\nhttps://www.youtube.com/watch?v=rYFAdRCibyc",
          "upvote_count": "1"
        },
        {
          "content": "BD\nC. Dead-letter queues can only be added to SNS subscriptions, not to topics.",
          "upvote_count": "1",
          "timestamp": "1728542040.0",
          "comment_id": "1295440",
          "poster": "weixing"
        },
        {
          "comment_id": "1253092",
          "poster": "h432ng",
          "content": "AD.\n\nC is wrong, \"Configuring an Amazon SNS dead-letter queue for a subscription\" not for SNS topic\nA is correct, with Dynamodb, admin can no longer \"accidentally shut down the DB instance.\"\n\nA fixes the root cause. With D an SQS is there, no need for DLQ for SNS. If lambda process data from SQS, what is SNS DLQ help here?",
          "timestamp": "1721653380.0",
          "upvote_count": "2"
        },
        {
          "poster": "thanhnv142",
          "comment_id": "1142922",
          "content": "Selected Answer: CD\nC and D are correct: <. While the database was down the company lost several of the SNS notification messages that were delivered during that time> means dead-letter queue in SQS and output SNS to SQS to store dead-letter queue",
          "upvote_count": "4",
          "timestamp": "1707276420.0"
        },
        {
          "upvote_count": "1",
          "poster": "zain1258",
          "timestamp": "1701183120.0",
          "comment_id": "1082667",
          "content": "Selected Answer: BD\nB & D are correct",
          "comments": [
            {
              "upvote_count": "2",
              "content": "Here's what I get when you break it down graphically between CD and BC:\n\nCD: SNS > SQS|DLQ) > Lambda > RDS\nBD: SNS > SQS > Lambda > SQS > RDS\nThe DLQ is just there to handle any SNS messages that have errors and can't be processed. There is no way you want/need two SQS queues in series (on either side of the Lambda). The ONLY thing you need to add for the requirements is queue to hold stuff while DB is down. The DLQ just makes sure even an messed up message data is captured for later review. Only C&D make any sense here.",
              "timestamp": "1719258900.0",
              "comment_id": "1236588",
              "poster": "Gomer"
            }
          ]
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:10:08.045Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "1VBTPQ8sJS1iSjHq9HAQ",
      "question_number": 46,
      "page": 10,
      "question_text": "A DevOps engineer is building a continuous deployment pipeline for a serverless application that uses AWS Lambda functions. The company wants to reduce the customer impact of an unsuccessful deployment. The company also wants to monitor for issues.\nWhich deploy stage configuration will meet these requirements?",
      "choices": {
        "C": "Use AWS CloudFormation to publish a new version on every stack update, and include Amazon CloudWatch alarms on all resources. Use the RoutingConfig property of the AWS::Lambda::Alias resource to update the traffic routing during the stack update.",
        "B": "Use AWS CloudFormation to publish a new stack update, and include Amazon CloudWatch alarms on all resources. Set up an AWS CodePipeline approval action for a developer to verify and approve the AWS CloudFormation change set.",
        "A": "Use an AWS Serverless Application Model (AWS SAM) template to define the serverless application. Use AWS CodeDeploy to deploy the Lambda functions with the Canary10Percent15Minutes Deployment Preference Type. Use Amazon CloudWatch alarms to monitor the health of the functions.",
        "D": "Use AWS CodeBuild to add sample event payloads for testing to the Lambda functions. Publish a new version of the functions, and include Amazon CloudWatch alarms. Update the production alias to point to the new version. Configure rollbacks to occur when an alarm is in the ALARM state."
      },
      "correct_answer": "A",
      "answer_ET": "A",
      "answers_community": [
        "A (85%)",
        "D (15%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/105513-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-04-07 15:16:00",
      "unix_timestamp": 1680873360,
      "discussion_count": 15,
      "discussion": [
        {
          "content": "Certification TIP: 99% of questions regarding lambda and cloudformation the answer is the one that involves SAM",
          "comments": [
            {
              "poster": "harithzainudin",
              "content": "i couldnt agree more with this",
              "upvote_count": "3",
              "timestamp": "1702190280.0",
              "comment_id": "1092306"
            }
          ],
          "upvote_count": "23",
          "comment_id": "1073535",
          "timestamp": "1700245860.0",
          "poster": "zolthar_z"
        },
        {
          "upvote_count": "7",
          "timestamp": "1694371980.0",
          "comment_id": "1004250",
          "poster": "Jonfernz",
          "content": "Selected Answer: A\nA\n\nReducing Customer Impact: AWS CodeDeploy with Canary deployments (Canary10Percent15Minutes) will incrementally roll out the new version. Initially, 10% of the traffic will be directed to the new version, and if everything goes well, the rest of the traffic will be shifted over the span of 15 minutes. This cautious rollout minimizes the risk and impact on customers.\n\nMonitoring: Amazon CloudWatch alarms can be configured to track function errors, latency, and other important metrics. If anything goes awry, you can act promptly."
        },
        {
          "comment_id": "1254753",
          "timestamp": "1721887860.0",
          "poster": "jamesf",
          "upvote_count": "4",
          "content": "Selected Answer: A\nA\nKeywords: Serverless Application related with AWS SAM"
        },
        {
          "comment_id": "1159967",
          "upvote_count": "1",
          "timestamp": "1708969500.0",
          "poster": "zijo",
          "content": "Canary10Percent15Minutes refers to a specific type of deployment strategy used in the context of serverless applications, particularly with tools like AWS SAM (Serverless Application Model)."
        },
        {
          "upvote_count": "4",
          "comment_id": "1139744",
          "poster": "thanhnv142",
          "timestamp": "1707016680.0",
          "content": "A is correct: <a continuous deployment pipeline for a serverless application> means AWS SAM. \nB, C and D: no mention of SAM"
        },
        {
          "comment_id": "1133812",
          "upvote_count": "1",
          "poster": "thanhnv142",
          "timestamp": "1706414880.0",
          "content": "A: use serverless code deployment is the right way"
        },
        {
          "comment_id": "1101202",
          "poster": "z_inderjot",
          "content": "Selected Answer: A\nA - SAM for Lambda deployment\nReduce Custome Impact - Canary got it covered",
          "timestamp": "1703045820.0",
          "upvote_count": "1"
        },
        {
          "poster": "RVivek",
          "timestamp": "1695001800.0",
          "content": "Selected Answer: A\nA CodeDepoly is for canary deployment , cloudwatch alarm for monitoring, if aram is raised then codedeploy automatically rolls back\nD- Using Codebuild for controlled deployment is not good. Codebuild is for build and testing",
          "comment_id": "1010175",
          "upvote_count": "1"
        },
        {
          "timestamp": "1693964760.0",
          "upvote_count": "2",
          "comment_id": "1000072",
          "content": "A is the answer: https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/automating-updates-to-serverless-apps.html",
          "poster": "DaddyDee"
        },
        {
          "comment_id": "956470",
          "content": "Selected Answer: A Canary Deployment",
          "timestamp": "1689758880.0",
          "poster": "Kojhani",
          "upvote_count": "3"
        },
        {
          "upvote_count": "3",
          "comment_id": "921151",
          "poster": "SanChan",
          "timestamp": "1686548400.0",
          "comments": [
            {
              "timestamp": "1694317920.0",
              "poster": "RVivek",
              "content": "Alarms: These are CloudWatch alarms that are triggered by any errors raised by the deployment. When encountered, they automatically roll back your deployment. For example, if the updated code you're deploying causes errors within the application. Another example is if any AWS Lambda or custom CloudWatch metrics that you specified have breached the alarm threshold.\n\nhttps://docs.aws.amazon.com/serverless-application-model/latest/developerguide/automating-updates-to-serverless-apps.html",
              "upvote_count": "1",
              "comment_id": "1003664"
            },
            {
              "timestamp": "1687770720.0",
              "poster": "franklinfocus",
              "content": "codebuild is not use for continous deployment",
              "comment_id": "934232",
              "upvote_count": "2"
            },
            {
              "upvote_count": "6",
              "poster": "SanChan",
              "comment_id": "921171",
              "content": "checked, Answer D does not provide a gradual deployment strategy that reduces the customer impact of a new deployment, which was one of the requirements given in the question.\n\nSo the final answer should be A",
              "timestamp": "1686550680.0"
            }
          ],
          "content": "Selected Answer: D\nMy Answer is D which can help reduce the customer impact of an unsuccessful deployment, also got monitor and rollbacks to occur when an alarm is in the ALARM state.\n\nA, this option does not provide a rollback plan in case of failures, which could further increase the customer impact of a failed deployment.\nThis strategy can help detect any issues early on, it does not guarantee that the impact on customers will be reduced since some customers might still be affected by the issues.\n\nSomeone can tell me more?"
        },
        {
          "upvote_count": "1",
          "comment_id": "918165",
          "poster": "madperro",
          "timestamp": "1686222420.0",
          "content": "Selected Answer: A\nA looks fine."
        },
        {
          "timestamp": "1682985780.0",
          "content": "Selected Answer: A\nA. Use an AWS Serverless Application Model (AWS SAM) template to define the serverless application. Use AWS CodeDeploy to deploy the Lambda functions with the Canary10Percent15Minutes Deployment Preference Type. Use Amazon CloudWatch alarms to monitor the health of the functions would be the best deploy stage configuration for meeting the requirements of reducing customer impact of an unsuccessful deployment and monitoring for issues.\n\nOption A uses AWS CodeDeploy to deploy the Lambda functions with the Canary10Percent15Minutes Deployment Preference Type, which gradually deploys the new version of the function to a small subset of users before deploying it to the entire fleet. This approach reduces customer impact of an unsuccessful deployment.\n\nAdditionally, Amazon CloudWatch alarms are used to monitor the health of the functions, which can provide real-time feedback on any issues that arise. This meets the requirement of monitoring for issues.",
          "upvote_count": "2",
          "comment_id": "886839",
          "poster": "haazybanj"
        },
        {
          "comment_id": "870452",
          "upvote_count": "1",
          "content": "A is the correct answer",
          "timestamp": "1681503300.0",
          "poster": "alce2020"
        },
        {
          "poster": "ele",
          "timestamp": "1680873360.0",
          "content": "Selected Answer: A\nA looks realistic",
          "comment_id": "863907",
          "upvote_count": "1"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:10:18.593Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "qq3pcxVH9L8rwNJ2jYyg",
      "question_number": 47,
      "page": 10,
      "question_text": "A company has an application that runs on Amazon EC2 instances. The company uses an AWS CodePipeline pipeline to deploy the application into multiple AWS Regions. The pipeline is configured with a stage for each Region. Each stage contains an AWS CloudFormation action for each Region.\n\nWhen the pipeline deploys the application to a Region, the company wants to confirm that the application is in a healthy state before the pipeline moves on to the next Region. Amazon Route 53 record sets are configured for the application in each Region. A DevOps engineer creates a Route 53 health check that is based on an Amazon CloudWatch alarm for each Region where the application is deployed.\n\nWhat should the DevOps engineer do next to meet the requirements?",
      "choices": {
        "C": "Create a new pipeline stage for each Region where the application is deployed. Configure a CloudWatch alarm action for the new stage to check the state of the CloudWatch alarm and to exit with an error if the alarm is in the ALARM state",
        "B": "Configure an AWS CodeDeploy application to deploy a CloudFormation template with automatic rollback. Configure the CloudWatch alarm as the instance health check for the CodeDeploy application. Remove the CloudFormation actions from the pipeline. Create a CodeDeploy action in the pipeline stage for each Region.",
        "D": "Configure the CloudWatch agent on the EC2 instances to report the application status to the Route 53 health check. Create a new pipeline stage for each Region where the application is deployed. Configure a CloudWatch alarm action to exit with an error if the CloudWatch alarm is in the ALARM state.",
        "A": "Create an AWS Step Functions workflow to check the state of the CloudWatch alarm. Configure the Step Functions workflow to exit with an error if the alarm is in the ALARM state. Create a new stage in the pipeline between each Region deployment stage. In each new stage, include an action to invoke the Step Functions workflow."
      },
      "correct_answer": "A",
      "answer_ET": "A",
      "answers_community": [
        "A (94%)",
        "6%"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/127270-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-11-26 19:14:00",
      "unix_timestamp": 1701022440,
      "discussion_count": 8,
      "discussion": [
        {
          "content": "Selected Answer: A\nThere are no such things as cloudwatch alarm actions. The only things alarms can do is send notifications to an SNS topic. You can perform actions by using EventBridge (CloudWatch Log events) or Step Functions.",
          "poster": "heff_bezos",
          "timestamp": "1727122500.0",
          "comment_id": "1288303",
          "upvote_count": "2"
        },
        {
          "poster": "zijo",
          "comment_id": "1245133",
          "content": "Selected Answer: D\nD seems to be simple solution for me\nThe CloudWatch agent on EC2 instances can be configured to report the application status, and this information can then be used by Route 53 health checks.\nCreate Route 53 health checks that are based on the CloudWatch alarms. When you create a health check in Route 53, you can specify that the health check should be based on the state of a CloudWatch alarm. Route 53 health checks can be configured to treat the CloudWatch alarm state as Healthy or Unhealthy.",
          "upvote_count": "1",
          "timestamp": "1720557780.0"
        },
        {
          "timestamp": "1707481620.0",
          "poster": "govindrk",
          "content": "D is correct - https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/monitoring-cloudwatch.html",
          "comment_id": "1145446",
          "upvote_count": "1"
        },
        {
          "poster": "thanhnv142",
          "content": "A is correct: <confirm that the application is in a healthy state before the pipeline moves on to the next Region.> means we need a new stage\nB and C: no mention of creating a new stage\nD: irrelevant",
          "upvote_count": "3",
          "timestamp": "1707277740.0",
          "comment_id": "1142929"
        },
        {
          "poster": "a54b16f",
          "upvote_count": "4",
          "timestamp": "1705346940.0",
          "comment_id": "1123593",
          "content": "Selected Answer: A\nExact scenario for Step usage: different routing options based on choices"
        },
        {
          "content": "Selected Answer: A\nA: https://dev.to/aws-builders/dynamic-build-orchestration-using-codepipeline-codebuild-and-step-functions-2kpa",
          "upvote_count": "4",
          "comment_id": "1084458",
          "timestamp": "1701355080.0",
          "poster": "zolthar_z"
        },
        {
          "poster": "zain1258",
          "upvote_count": "3",
          "timestamp": "1701183660.0",
          "comment_id": "1082671",
          "content": "Selected Answer: A\nA is correct answer"
        },
        {
          "content": "Selected Answer: A\nA - 'If the state machine execution reaches a terminal status of FAILED, TIMED_OUT, or ABORTED, the action execution fails.' https://docs.aws.amazon.com/codepipeline/latest/userguide/action-reference-StepFunctions.html\n\nCan't be D because you can't update a Route53 healhcheck via the Cloudwatch agent",
          "comment_id": "1080940",
          "timestamp": "1701022440.0",
          "upvote_count": "3",
          "poster": "TheAWSRhino"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:10:18.593Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "evMxyR0KhOgdDCrxodNQ",
      "question_number": 48,
      "page": 10,
      "question_text": "A company plans to use Amazon CloudWatch to monitor its Amazon EC2 instances. The company needs to stop EC2 instances when the average of the NetworkPacketsIn metric is less than 5 for at least 3 hours in a 12-hour time window. The company must evaluate the metric every hour. The EC2 instances must continue to run if there is missing data for the NetworkPacketsIn metric during the evaluation period.\n\nA DevOps engineer creates a CloudWatch alarm for the NetworkPacketsIn metric. The DevOps engineer configures a threshold value of 5 and an evaluation period of 1 hour.\n\nWhich set of additional actions should the DevOps engineer take to meet these requirements?",
      "choices": {
        "B": "Configure the Datapoints to Alarm value to be 3 out of 12. Configure the alarm to treat missing data as not breaching the threshold. Add an EC2 action to stop the instance when the alarm enters the ALARM state.",
        "D": "Configure the Datapoints to Alarm value to be 9 out of 12. Configure the alarm to treat missing data as not breaching the threshold. Add an AWS Systems Manager action to stop the instance when the alarm enters the ALARM state.",
        "C": "Configure the Datapoints to Alarm value to be 9 out of 12. Configure the alarm to treat missing data as breaching the threshold. Add an EC2 action to stop the instance when the alarm enters the ALARM state.",
        "A": "Configure the Datapoints to Alarm value to be 3 out of 12. Configure the alarm to treat missing data as breaching the threshold. Add an AWS Systems Manager action to stop the instance when the alarm enters the ALARM state."
      },
      "correct_answer": "B",
      "answer_ET": "B",
      "answers_community": [
        "B (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/126987-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-11-23 09:25:00",
      "unix_timestamp": 1700727900,
      "discussion_count": 6,
      "discussion": [
        {
          "content": "Selected Answer: B\nB: This is the reason https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/UsingAlarmActions.html#AddingStopActions",
          "poster": "zolthar_z",
          "comment_id": "1084464",
          "upvote_count": "7",
          "timestamp": "1701355380.0"
        },
        {
          "upvote_count": "1",
          "poster": "zijo",
          "content": "In CloudWatch alarms, datapoints are the individual metric values collected during each period. Here the period is 1 hour and 1 datapoint every hour collected. So 3 datapoints out of 12 is the alrm state because the network threshold has to be less than 5 for atleast 3 hours to Alarm state. The DevOps Engineer sets evaluation for every hour to look for the threshold value of 5. So if 1 hour has no data it is not breaching threshold. If there The alarm evaluates these datapoints against the conditions you set to determine whether it should trigger an action which is stopping an EC2 instance. My explanation for B",
          "comment_id": "1270250",
          "timestamp": "1724256480.0"
        },
        {
          "content": "Selected Answer: B\nB is correct",
          "comment_id": "1109384",
          "timestamp": "1703908080.0",
          "upvote_count": "4",
          "poster": "PrasannaBalaji"
        },
        {
          "comment_id": "1079492",
          "upvote_count": "3",
          "timestamp": "1700850960.0",
          "content": "Selected Answer: B\nI think B",
          "poster": "tom_cat"
        },
        {
          "comment_id": "1078470",
          "timestamp": "1700748360.0",
          "poster": "vandergun",
          "upvote_count": "4",
          "content": "Selected Answer: B\nB should be corrected"
        },
        {
          "poster": "vandergun",
          "upvote_count": "3",
          "timestamp": "1700727900.0",
          "content": "B should be corrected",
          "comment_id": "1078227"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:10:18.593Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "9JL7L082OBxLWUhRJElE",
      "question_number": 49,
      "page": 10,
      "question_text": "A company manages 500 AWS accounts that are in an organization in AWS Organizations. The company discovers many unattached Amazon Elastic Block Store (Amazon EBS) volumes in all the accounts. The company wants to automatically tag the unattached EBS volumes for investigation.\n\nA DevOps engineer needs to deploy an AWS Lambda function to all the AWS accounts. The Lambda function must run every 30 minutes to tag all the EBS volumes that have been unattached for a period of 7 days or more.\n\nWhich solution will meet these requirements in the MOST operationally efficient manner?",
      "choices": {
        "C": "Configure a delegated administrator account for the organization. Create an AWS CloudFormation template that contains the Lambda function and an Amazon EventBridge scheduled rule to invoke the Lambda function every 30 minutes. Use CloudFormation StackSets to deploy the CloudFormation template from the delegated administrator account to all the member accounts in the organization",
        "A": "Configure a delegated administrator account for the organization. Create an AWS CloudFormation template that contains the Lambda function. Use CloudFormation StackSets to deploy the CloudFormation template from the delegated administrator account to all the member accounts in the organization. Create an Amazon EventBridge event bus in the delegated administrator account to invoke the Lambda function in each member account every 30 minutes.",
        "B": "Create a cross-account IAM role in the organization's member accounts. Attach the AWSLambda_FullAccess policy and the AWSCloudFormationFullAccess policy to the role. Create an AWS CloudFormation template that contains the Lambda function and an Amazon EventBridge scheduled rule to invoke the Lambda function every 30 minutes. Create a custom script in the organization’s management account that assumes the role and deploys the CloudFormation template to the member accounts.",
        "D": "Create a cross-account IAM role in the organization's member accounts. Attach the AmazonS3FullAccess policy and the AWSCodeDeployDeployerAccess policy to the role. Use AWS CodeDeploy to assume the role to deploy the Lambda function from the organization's management account. Configure an Amazon EventBridge scheduled rule in the member accounts to invoke the Lambda function every 30 minutes."
      },
      "correct_answer": "C",
      "answer_ET": "C",
      "answers_community": [
        "C (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/126988-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-11-23 09:27:00",
      "unix_timestamp": 1700728020,
      "discussion_count": 8,
      "discussion": [
        {
          "poster": "thanhnv142",
          "content": "C is correct: <The Lambda function must run every 30 minutes to tag all the EBS volumes>: we should use a combination of eventbridge and Lambda\nA: <. Create an Amazon EventBridge event bus in the delegated administrator account to invoke the Lambda function>: event bridge should be in each member account to monitor event, not in the delegated admin's account\nB and D: These options create an IAM role in every member account, which is incorrect",
          "timestamp": "1723030080.0",
          "comment_id": "1143337",
          "upvote_count": "5"
        },
        {
          "comment_id": "1175076",
          "content": "Selected Answer: C\nC make the most sense",
          "poster": "DanShone",
          "upvote_count": "3",
          "timestamp": "1726495140.0"
        },
        {
          "content": "Selected Answer: C\nNOT A: you don't want to run it for every user accounts",
          "timestamp": "1720799160.0",
          "comment_id": "1120974",
          "poster": "a54b16f",
          "upvote_count": "3"
        },
        {
          "content": "Selected Answer: C\nAgree with C",
          "comment_id": "1119607",
          "upvote_count": "3",
          "poster": "yuliaqwerty",
          "timestamp": "1720686720.0"
        },
        {
          "comment_id": "1117814",
          "timestamp": "1720548300.0",
          "poster": "davdan99",
          "upvote_count": "2",
          "content": "Why no A?"
        },
        {
          "comment_id": "1082675",
          "timestamp": "1716901740.0",
          "upvote_count": "3",
          "content": "Selected Answer: C\nC is correct",
          "poster": "zain1258"
        },
        {
          "poster": "tom_cat",
          "comment_id": "1079491",
          "upvote_count": "2",
          "timestamp": "1716568500.0",
          "content": "Selected Answer: C\nC makes sense"
        },
        {
          "timestamp": "1716445620.0",
          "upvote_count": "2",
          "poster": "vandergun",
          "content": "Selected Answer: C\nI vote C",
          "comment_id": "1078230"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:10:18.593Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "12IUBH34t1DgyeJqg32B",
      "question_number": 50,
      "page": 10,
      "question_text": "A company's production environment uses an AWS CodeDeploy blue/green deployment to deploy an application. The deployment incudes Amazon EC2 Auto Scaling groups that launch instances that run Amazon Linux 2.\n\nA working appspec.yml file exists in the code repository and contains the following text:\n\n//IMG//\n\n\nA DevOps engineer needs to ensure that a script downloads and installs a license file onto the instances before the replacement instances start to handle request traffic. The DevOps engineer adds a hooks section to the appspec.yml file.\n\nWhich hook should the DevOps engineer use to run the script that downloads and installs the license file?",
      "choices": {
        "B": "BeforeBlockTraffic",
        "A": "AfterBlockTraffic",
        "D": "DownloadBundle",
        "C": "BeforeInstall"
      },
      "correct_answer": "C",
      "answer_ET": "C",
      "answers_community": [
        "C (95%)",
        "5%"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/126989-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [
        "https://img.examtopics.com/aws-certified-devops-engineer-professional-dop-c02/image9.png"
      ],
      "answer_images": [],
      "timestamp": "2023-11-23 09:28:00",
      "unix_timestamp": 1700728080,
      "discussion_count": 6,
      "discussion": [
        {
          "poster": "thanhnv142",
          "comment_id": "1143359",
          "content": "Selected Answer: C\nC is correct: For blue/green deployment, Before install is one of several hooks that come before <the replacement instances> start to handle request traffic.\nA and B: these hooks come after the replacement instances start to handle request traffic. They are hooks from the original instance, which are two of 3 last steps.\nD: There is no such hook in blue/green deployment",
          "upvote_count": "7",
          "timestamp": "1723031220.0"
        },
        {
          "content": "Selected Answer: C\nA & B are not available for replacement instances - https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html#reference-appspec-file-structure-hooks-availability\nD - \"Reserved for CodeDeploy operations. Cannot be used to run scripts.\"",
          "timestamp": "1716569220.0",
          "poster": "tom_cat",
          "upvote_count": "5",
          "comment_id": "1079498"
        },
        {
          "content": "Selected Answer: A\nAfterBlockTraffic - A is correct",
          "comment_id": "1585069",
          "upvote_count": "1",
          "timestamp": "1752098220.0",
          "poster": "ET2025"
        },
        {
          "upvote_count": "2",
          "timestamp": "1726495020.0",
          "content": "Selected Answer: C\nC is correct",
          "poster": "DanShone",
          "comment_id": "1175073"
        },
        {
          "content": "Selected Answer: C\nis C: A DevOps engineer needs to ensure that a script downloads and \"installs\" a license file onto the instances \"before\" the replacement instances start to handle request traffic",
          "upvote_count": "2",
          "comment_id": "1126542",
          "timestamp": "1721371860.0",
          "poster": "twogyt"
        },
        {
          "content": "Selected Answer: C\nC should be correct",
          "comment_id": "1078231",
          "upvote_count": "3",
          "poster": "vandergun",
          "timestamp": "1716445680.0"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:10:18.593Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "MUciPQePQCJ3sT15kDrP",
      "question_number": 51,
      "page": 11,
      "question_text": "A company has an application that includes AWS Lambda functions. The Lambda functions run Python code that is stored in an AWS CodeCommit repository. The company has recently experienced failures in the production environment because of an error in the Python code. An engineer has written unit tests for the Lambda functions to help avoid releasing any future defects into the production environment.\n\nThe company's DevOps team needs to implement a solution to integrate the unit tests into an existing AWS CodePipeline pipeline. The solution must produce reports about the unit tests for the company to view.\n\nWhich solution will meet these requirements?",
      "choices": {
        "A": "Associate the CodeCommit repository with Amazon CodeGuru Reviewer. Create a new AWS CodeBuild project. In the CodePipeline pipeline, configure a test stage that uses the new CodeBuild project. Create a buildspec.yml file in the CodeCommit repository. In the buildspec yml file, define the actions to run a CodeGuru review.",
        "B": "Create a new AWS CodeBuild project. In the CodePipeline pipeline, configure a test stage that uses the new CodeBuild project. Create a CodeBuild report group. Create a buildspec.yml file in the CodeCommit repository. In the buildspec.yml file, define the actions to run the unit tests with an output of JUNITXML in the build phase section. Configure the test reports to be uploaded to the new CodeBuild report group.",
        "C": "Create a new AWS CodeArtifact repository. Create a new AWS CodeBuild project. In the CodePipeline pipeline, configure a test stage that uses the new CodeBuild project. Create an appspec.yml file in the original CodeCommit repository. In the appspec.yml file, define the actions to run the unit tests with an output of CUCUMBERJSON in the build phase section. Configure the tests reports to be sent to the new CodeArtifact repository.",
        "D": "Create a new AWS CodeBuild project. In the CodePipeline pipeline, configure a test stage that uses the new CodeBuild project. Create a new Amazon S3 bucket. Create a buildspec.yml file in the CodeCommit repository. In the buildspec yml file, define the actions to run the unit tests with an output of HTML in the phases section. In the reports section, upload the test reports to the S3 bucket."
      },
      "correct_answer": "B",
      "answer_ET": "B",
      "answers_community": [
        "B (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/126990-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-11-23 09:30:00",
      "unix_timestamp": 1700728200,
      "discussion_count": 5,
      "discussion": [
        {
          "poster": "thanhnv142",
          "timestamp": "1723031760.0",
          "content": "Selected Answer: B\nB is correct: for unit test, we need codebuild \nA: codeguru is for code analysis, not unit test\nC: This option mentions pushing reports to CodeArtifact repository, which is incorrect\nD: This option push reports to S3, which is incorrect. We should upload report to codebuild report group",
          "comment_id": "1143370",
          "upvote_count": "6"
        },
        {
          "timestamp": "1716902100.0",
          "content": "Selected Answer: B\nB is correct",
          "comment_id": "1082681",
          "poster": "zain1258",
          "upvote_count": "3"
        },
        {
          "upvote_count": "3",
          "timestamp": "1716810000.0",
          "poster": "KobraKai",
          "content": "I think B as per link:\nhttps://docs.aws.amazon.com/codebuild/latest/userguide/test-reporting.html",
          "comment_id": "1081596"
        },
        {
          "content": "Selected Answer: B\nI think it should be B",
          "poster": "tom_cat",
          "comment_id": "1079503",
          "upvote_count": "3",
          "timestamp": "1716569580.0"
        },
        {
          "comment_id": "1078234",
          "timestamp": "1716445800.0",
          "upvote_count": "4",
          "content": "Selected Answer: B\nB is corrected",
          "poster": "vandergun"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:10:28.998Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "Pl21LgtMNFYenPlbu2af",
      "question_number": 52,
      "page": 11,
      "question_text": "A company manages multiple AWS accounts in AWS Organizations. The company’s security policy states that AWS account root user credentials for member accounts must not be used. The company monitors access to the root user credentials.\n\nA recent alert shows that the root user in a member account launched an Amazon EC2 instance. A DevOps engineer must create an SCP at the organization's root level that will prevent the root user in member accounts from making any AWS service API calls.\n\nWhich SCP will meet these requirements?",
      "choices": {
        "C": "",
        "B": "",
        "D": "",
        "A": ""
      },
      "correct_answer": "C",
      "answer_ET": "C",
      "answers_community": [
        "C (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/127130-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-11-24 20:00:00",
      "unix_timestamp": 1700852400,
      "discussion_count": 7,
      "discussion": [
        {
          "upvote_count": "8",
          "timestamp": "1700852400.0",
          "poster": "tom_cat",
          "comment_id": "1079511",
          "content": "Selected Answer: C\nI believe it should be C\nhttps://docs.aws.amazon.com/organizations/latest/userguide/best-practices_member-acct.html#bp_member-acct_use-scp"
        },
        {
          "comment_id": "1143377",
          "content": "Selected Answer: C\nC is correct: < will prevent the root user in member accounts> this means deny action\nA and D: irrelevant (mention allow statement)\nB: scp does not have principal element. only condition",
          "timestamp": "1707314940.0",
          "poster": "thanhnv142",
          "upvote_count": "7"
        },
        {
          "content": "Selected Answer: C\nA slightly more consise version of \"C\" is a \"strongly recommended\" control to deny root access in member accounts. See the example:\nhttps://docs.aws.amazon.com/controltower/latest/controlreference/strongly-recommended-controls.html#disallow-root-auser-actions",
          "timestamp": "1719354180.0",
          "upvote_count": "1",
          "poster": "Gomer",
          "comment_id": "1237113"
        },
        {
          "comment_id": "1200810",
          "poster": "c3518fc",
          "content": "Selected Answer: C\nhttps://docs.aws.amazon.com/organizations/latest/userguide/best-practices_member-acct.html#bp_member-acct_use-scp",
          "upvote_count": "1",
          "timestamp": "1713888600.0"
        },
        {
          "upvote_count": "1",
          "content": "Selected Answer: C\nC is correct",
          "poster": "DanShone",
          "comment_id": "1175068",
          "timestamp": "1710604380.0"
        },
        {
          "comment_id": "1091497",
          "timestamp": "1702108020.0",
          "content": "It's C, based on the documentation :\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps_examples_general.html#example-scp-root-user",
          "poster": "manman7",
          "upvote_count": "4"
        },
        {
          "content": "Selected Answer: C\nC looks correct",
          "timestamp": "1701184560.0",
          "poster": "zain1258",
          "comment_id": "1082682",
          "upvote_count": "2"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:10:28.998Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "6h54gWD2VcT7U8G0zOl4",
      "question_number": 53,
      "page": 11,
      "question_text": "A company uses AWS and has a VPC that contains critical compute infrastructure with predictable traffic patterns. The company has configured VPC flow logs that are published to a log group in Amazon CloudWatch Logs.\n\nThe company's DevOps team needs to configure a monitoring solution for the VPC flow logs to identify anomalies in network traffic to the VPC over time. If the monitoring solution detects an anomaly, the company needs the ability to initiate a response to the anomaly.\n\nHow should the DevOps team configure the monitoring solution to meet these requirements?",
      "choices": {
        "B": "Create an Amazon Kinesis Data Firehose delivery stream that delivers events to an Amazon S3 bucket. Subscribe the log group to the delivery stream. Configure Amazon Lookout for Metrics to monitor the data in the S3 bucket for anomalies. Create an AWS Lambda function to run in response to Lookout for Metrics anomaly findings. Configure the Lambda function to publish to the default Amazon EventBridge event bus.",
        "D": "Create an Amazon Kinesis data stream. Subscribe the log group to the data stream. Create an AWS Lambda function to detect log anomalies. Configure the Lambda function to write to the default Amazon EventBridge event bus if the Lambda function detects an anomaly. Set the Lambda function as the processor for the data stream.",
        "C": "Create an AWS Lambda function to detect anomalies. Configure the Lambda function to publish an event to the default Amazon EventBridge event bus if the Lambda function detects an anomaly. Subscribe the Lambda function to the log group.",
        "A": "Create an Amazon Kinesis data stream. Subscribe the log group to the data stream. Configure Amazon Kinesis Data Analytics to detect log anomalies in the data stream. Create an AWS Lambda function to use as the output of the data stream. Configure the Lambda function to write to the default Amazon EventBridge event bus in the event of an anomaly finding."
      },
      "correct_answer": "B",
      "answer_ET": "B",
      "answers_community": [
        "B (69%)",
        "A (31%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/127013-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-11-23 13:26:00",
      "unix_timestamp": 1700742360,
      "discussion_count": 24,
      "discussion": [
        {
          "content": "Selected Answer: B\nI think it's B, Amazon Lookout for metrics can detect anomalies from S3 bucket and trigger Lambda\nhttps://aws.amazon.com/lookout-for-metrics/",
          "upvote_count": "7",
          "comment_id": "1106313",
          "timestamp": "1703618040.0",
          "poster": "giovanna_mag"
        },
        {
          "timestamp": "1738975500.0",
          "content": "Selected Answer: A\nUsing KDA for anomaly detection, means you can use the built-in RCF (Random Cut Forest) ML algorithm. \nOption B: Firehose and S3 adds latency. Further, Amazon Lookout for Metrics is more suitable for business metrics than network traffic. \nOptions C&D require you to implement your own Lambda function, which means more error prone and more maintenance.",
          "upvote_count": "1",
          "comment_id": "1353168",
          "poster": "ce0df07"
        },
        {
          "content": "Selected Answer: B\n- Data Streaming: Use Amazon Kinesis Data Firehose to deliver VPC flow logs from CloudWatch Logs to an Amazon S3 bucket.\n- Anomaly Detection: Amazon Lookout for Metrics will monitor the data in the S3 bucket and automatically detect anomalies in the network traffic.\n- Event Response: When Lookout for Metrics detects an anomaly, it triggers an AWS Lambda function. The Lambda function will then publish an event to the Amazon EventBridge event bus, which can further initiate automated responses, notifications, or alerts.",
          "poster": "jamesf",
          "comment_id": "1260177",
          "timestamp": "1722673020.0",
          "upvote_count": "4"
        },
        {
          "poster": "trungtd",
          "upvote_count": "2",
          "content": "Selected Answer: B\nAlthough option A uses Kinesis Data Analytics for anomaly detection, setting up and maintaining custom analytics and anomaly detection logic is more complex and less efficient compared to using a managed service like Lookout for Metrics.",
          "comment_id": "1245198",
          "timestamp": "1720567620.0"
        },
        {
          "poster": "xdkonorek2",
          "upvote_count": "2",
          "content": "Selected Answer: B\nA is wrong because kinesis data analytics output must be either kinesis data stream or firehose, can't be lambda directly so there is a missing component",
          "comment_id": "1241623",
          "timestamp": "1720035780.0"
        },
        {
          "poster": "Gomer",
          "upvote_count": "3",
          "content": "I've reviewed most of the comments, and it seems like everyone is just repeating themselves. I've \"googled\" and looked at the references. I found examples of both kinesis data streams, kinesis data analytics and firehose. The one step in \"A\" I have a problem with is \"Create an AWS Lambda function to use as the output of the data stream.\" How can Lambda be an output of a data stream \"over time\"? I don't think you can identify an anomaly \"over time\" unless you've got persistent storage for the data (which can be reparsed as necessary to compare past with present). I'm leaning towards \"B\" unless someone can convince me otherwise (and not by just repeating what others have already said).",
          "comment_id": "1237142",
          "timestamp": "1719362340.0"
        },
        {
          "comment_id": "1232721",
          "comments": [
            {
              "content": "I see the \"over time\" requirement as implying some ability to parse the past with the present in order for ML to assess an anomaly. I don't see the words \"real time\" in the requirements. The \"over time\" requirement is not specific enough, but until there are more specifics, it would be reasonable to presume it means your trying to discover current anomalies by comparing traffic from against days, weeks or months ago.",
              "upvote_count": "1",
              "poster": "Gomer",
              "comment_id": "1237144",
              "timestamp": "1719363000.0"
            }
          ],
          "timestamp": "1718774640.0",
          "upvote_count": "2",
          "content": "Selected Answer: A\nOption B involves using Amazon Lookout for Metrics, which is not designed for real-time anomaly detection.",
          "poster": "tsangckl"
        },
        {
          "content": "Selected Answer: B\ni think B",
          "poster": "seetpt",
          "upvote_count": "2",
          "comment_id": "1205485",
          "timestamp": "1714650720.0"
        },
        {
          "comment_id": "1200819",
          "upvote_count": "3",
          "content": "Selected Answer: B\nLookout for Metrics automatically detects and diagnoses anomalies (outliers from the norm) in business and operational data. It’s a fully managed ML service, which uses specialized ML models to detect anomalies based on the characteristics of your data. You don’t need ML experience to use Lookout for Metrics.\n\nKinesis Data Analytics Studio provides an interactive notebook experience powered by Apache Zeppelin and Apache Flink to analyze streaming data. It also helps productionize your analytics application by building and deploying code as a Kinesis data analytics application straight from the notebook. https://aws.amazon.com/blogs/machine-learning/smart-city-traffic-anomaly-detection-using-amazon-lookout-for-metrics-and-amazon-kinesis-data-analytics-studio/",
          "poster": "c3518fc",
          "timestamp": "1713889560.0"
        },
        {
          "upvote_count": "1",
          "comment_id": "1184607",
          "content": "Selected Answer: A\nA. If you google \"detecting anomalies in vpc flow logs\" every article suggests Kinesis Data Analytics",
          "timestamp": "1711611360.0",
          "poster": "stoy123"
        },
        {
          "upvote_count": "2",
          "poster": "CloudHandsOn",
          "timestamp": "1710761700.0",
          "content": "Selected Answer: A\nI'll go with A. Mainly because Kinesis data analytics has anomoly detection using a random cut forest function: https://docs.aws.amazon.com/kinesisanalytics/latest/dev/app-anomaly-detection.html",
          "comment_id": "1176411"
        },
        {
          "content": "Selected Answer: B\nB - Amazon Lookout for Metrics Automatically detect anomalies within metrics and identify their root causes. So would fit the requirements",
          "timestamp": "1710604260.0",
          "poster": "DanShone",
          "comment_id": "1175067",
          "upvote_count": "3"
        },
        {
          "timestamp": "1710583140.0",
          "poster": "ogerber",
          "comment_id": "1174889",
          "upvote_count": "1",
          "content": "Selected Answer: A\nOption A is preferable for scenarios requiring real-time processing and anomaly detection in streaming data, such as VPC flow logs, with the capability to quickly initiate responses to detected anomalies. It offers a more streamlined and immediate approach to monitoring and responding to network traffic anomalies, making it highly suitable for the company's needs regarding their critical compute infrastructure with predictable traffic patterns.\n\nOption B might still be considered if the company's workflow is more adapted to batch processing and the delays inherent in data delivery and processing are acceptable. However, for immediate anomaly detection and response, Option A stands out as the more appropriate solution."
        },
        {
          "comment_id": "1164416",
          "content": "Selected Answer: A\nKinesis Data Firehose determines how often to write to S3 by buffer settings, which is not realtime enough to handle VPC flow log, which can be fatal depending on the content of the `CRITICAL compute infrastructure`. Kinesis Data Analytics has machine learning solutions such as RANDOM_CUT_FOREST in addition to fixed detection by normal SQL.",
          "poster": "dzn",
          "upvote_count": "3",
          "timestamp": "1709421780.0"
        },
        {
          "timestamp": "1708967400.0",
          "poster": "fdoxxx",
          "upvote_count": "3",
          "content": "Option B is the most suitable for the scenario.\nKinesis Data Firehose: It allows the streaming of data to an S3 bucket, providing a durable storage solution.\nLookout for Metrics: It is designed to detect anomalies in your data and can be configured to monitor the data stored in the S3 bucket for anomalies.",
          "comment_id": "1159956"
        },
        {
          "timestamp": "1708901280.0",
          "upvote_count": "4",
          "content": "Question keyword :\n- predictable traffic patterns\n- anomalies\n\nThus, B.",
          "poster": "Seoyong",
          "comment_id": "1159245"
        },
        {
          "upvote_count": "2",
          "timestamp": "1708435380.0",
          "poster": "kyuhuck",
          "content": "Selected Answer: A\nb is not corret Option A is the most suitable approach to meet the requirements. It leverages Amazon Kinesis Data Stream for real-time data ingestion, Amazon Kinesis Data Analytics for efficient and scalable anomaly detection in real-time, and AWS Lambda to initiate a response when anomalies are detected. This setup provides a robust, scalable, and real-time monitoring solution for VPC flow logs, with the ability to initiate responses to anomalies through integration with Amazon EventBridge.",
          "comment_id": "1154738"
        },
        {
          "upvote_count": "4",
          "timestamp": "1707315420.0",
          "poster": "thanhnv142",
          "content": "Selected Answer: B\nB is correct: <needs to configure a monitoring solution for the VPC flow logs to identify anomalies> means Amazon Lookout for Metrics. \nA, C and D dont mention Amazon Lookout for Metrics",
          "comment_id": "1143386"
        },
        {
          "timestamp": "1704986640.0",
          "comment_id": "1119895",
          "poster": "a54b16f",
          "content": "Selected Answer: B\nBoth A (RCF) and B should detect anomalies. A didn't mention using RCF, also Kinesis data stream usually refers to real-time detection, while firehose is not about real-time. There is a keyword \"overtime\" in the description, so B.",
          "upvote_count": "3"
        },
        {
          "upvote_count": "3",
          "comment_id": "1117837",
          "timestamp": "1704832980.0",
          "content": "Selected Answer: B\nI will go for B",
          "poster": "davdan99"
        },
        {
          "timestamp": "1703168040.0",
          "upvote_count": "4",
          "poster": "bnagaraja9099",
          "comment_id": "1102596",
          "content": "Selected Answer: B\nAmazon lookout , AI anomalies identification"
        },
        {
          "upvote_count": "3",
          "comment_id": "1098050",
          "content": "It could be B by using Lookout for metrics. Indeed, it should be even easier to configure the detection",
          "poster": "a16a848",
          "timestamp": "1702721040.0"
        },
        {
          "poster": "zain1258",
          "timestamp": "1701184620.0",
          "comment_id": "1082685",
          "content": "Selected Answer: A\nA is correct",
          "upvote_count": "2"
        },
        {
          "upvote_count": "3",
          "timestamp": "1700742360.0",
          "content": "Selected Answer: A\nhttps://docs.aws.amazon.com/kinesisanalytics/latest/dev/app-anomaly-detection.html A should be correct",
          "poster": "vandergun",
          "comment_id": "1078417"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:10:28.998Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "DcUgfMaNVsbsZaa0pcSN",
      "question_number": 54,
      "page": 11,
      "question_text": "AnyCompany is using AWS Organizations to create and manage multiple AWS accounts. AnyCompany recently acquired a smaller company, Example Corp. During the acquisition process, Example Corp's single AWS account joined AnyCompany's management account through an Organizations invitation. AnyCompany moved the new member account under an OU that is dedicated to Example Corp.\n\nAnyCompany's DevOps engineer has an IAM user that assumes a role that is named OrganizationAccountAccessRole to access member accounts. This role is configured with a full access policy. When the DevOps engineer tries to use the AWS Management Console to assume the role in Example Corp's new member account, the DevOps engineer receives the following error message: \"Invalid information in one or more fields. Check your information or contact your administrator.\"\n\nWhich solution will give the DevOps engineer access to the new member account?",
      "choices": {
        "D": "In the new member account, edit the trust policy for the OrganizationAccountAccessRole IAM role. Grant the management account permission to assume the role.",
        "B": "In the management account, create a new SCP. In the SCP, grant the DevOps engineer's IAM user full access to all resources in the new member account. Attach the SCP to the OU that contains the new member account.",
        "C": "In the new member account, create a new IAM role that is named OrganizationAccountAccessRole. Attach the AdministratorAccess AWS managed policy to the role. In the role's trust policy, grant the management account permission to assume the role.",
        "A": "In the management account, grant the DevOps engineer's IAM user permission to assume the OrganizationAccountAccessRole IAM role in the new member account."
      },
      "correct_answer": "C",
      "answer_ET": "C",
      "answers_community": [
        "C (63%)",
        "D (35%)",
        "2%"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/126991-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-11-23 10:00:00",
      "unix_timestamp": 1700730000,
      "discussion_count": 19,
      "discussion": [
        {
          "comment_id": "1079947",
          "timestamp": "1700909940.0",
          "poster": "radev",
          "content": "Selected Answer: C\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_accounts_access.html#orgs_manage_accounts_create-cross-account-role",
          "upvote_count": "6"
        },
        {
          "upvote_count": "6",
          "content": "Selected Answer: C\nC is correct: <assume the role in Example Corp's new member account> means this role has not been properly configured (or even not created)\nA: only mention assuming the role, not create it.\nB: scp has nothing to do here\nD: only mention create trust relationship",
          "comment_id": "1143392",
          "timestamp": "1707315780.0",
          "poster": "thanhnv142"
        },
        {
          "content": "Selected Answer: C\nsorry for this reason:\nn the Example Corp account, manually create an IAM role named OrganizationAccountAccessRole.\nAttach the AdministratorAccess AWS managed policy to it.\nSet the trust policy to allow the management account to assume the role:\n{\n \"Version\": \"2012-10-17\",\n \"Statement\": [\n {\n \"Effect\": \"Allow\",\n \"Principal\": {\n \"AWS\": \"arn:aws:iam::<management-account-id>:root\"\n },\n \"Action\": \"sts:AssumeRole\"\n }\n ]\n}\nC is correct — because the account was invited, not created, and therefore the role must be manually created.",
          "timestamp": "1747320720.0",
          "comment_id": "1569075",
          "poster": "nickp84",
          "upvote_count": "1"
        },
        {
          "upvote_count": "1",
          "content": "Selected Answer: D\noption C:Creating a new role with the correct trust policy and permissions would allow the DevOps engineer to assume it.\nIf the original OrganizationAccountAccessRole was deleted or misconfigured, this could be a fallback.",
          "timestamp": "1747320540.0",
          "comment_id": "1569074",
          "poster": "nickp84"
        },
        {
          "comments": [
            {
              "content": "The role would not be automatically created in accounts that are added through invitation (as opposed to accounts created within the organization).",
              "timestamp": "1738976220.0",
              "poster": "ce0df07",
              "upvote_count": "1",
              "comment_id": "1353170"
            }
          ],
          "upvote_count": "2",
          "content": "Selected Answer: D\nThe role is already there. Why create a new one?",
          "poster": "AC2021",
          "timestamp": "1737679080.0",
          "comment_id": "1345736"
        },
        {
          "timestamp": "1734854880.0",
          "comment_id": "1330308",
          "content": "Selected Answer: D\nCorrect Answer is D \nRole Trust Policy Issue:\nWhen a new account is invited and joins an AWS Organization, the OrganizationAccountAccessRole is typically created automatically.\nThis role allows the management account to access member accounts, but its trust policy must explicitly grant the management account permission to assume the role.\nIf this trust policy is not configured correctly, the management account cannot assume the role, leading to the error message.",
          "upvote_count": "4",
          "poster": "Simba84",
          "comments": [
            {
              "timestamp": "1738976280.0",
              "upvote_count": "1",
              "poster": "ce0df07",
              "content": "Incorrect. See https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_accounts_access.html#orgs_manage_accounts_create-cross-account-role\n\"When you create a member account using AWS Organizations, Organizations automatically creates an IAM role in the account that grants administrator access to the management account. For invited member accounts, you must manually create the role.\"",
              "comment_id": "1353171"
            }
          ]
        },
        {
          "timestamp": "1733241780.0",
          "comment_id": "1321439",
          "content": "Selected Answer: C\nI've spent like 30 mins, and now I've got the most full explanation.\n\nCorrect answer is \"C\"\nWhile \"D\" is NOT FULLY describing what needs to be done (so it's wrong).\n\nThe thing you need to know to answer this question is the following:\n* if account is generated (meaning NEW account CREATED) within the Org, then this account will automatically have a proper role \"OrganizationAccountAccessRole\"\n* if account is invited (meaning EXISTING account ADDED) to Org, then this account will NOT have such role\n\nQuestion says, that Management Account tries to assume a role called \"OrganizationAccountAccessRole\" from member account, but it gets an error saying like \"there is no such thing which you request\".\n\nSo to fix an error you need:\n1) Create a IAM Role \"OrganizationAccountAccessRole\" in a member account\n2) Give it FullAccess Policy\n3) Allow Management Account to assume this role via its Trust Relationship",
          "upvote_count": "5",
          "poster": "eugene2owl"
        },
        {
          "upvote_count": "2",
          "comment_id": "1312491",
          "content": "member accounts that you invite to join your organization do not automatically get an administrator role created.\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_accounts_create-cross-account-role.html",
          "timestamp": "1731659760.0",
          "poster": "hamzaBennis"
        },
        {
          "upvote_count": "3",
          "comment_id": "1307673",
          "content": "Selected Answer: D\n\"IAM user that assumes a role that is named OrganizationAccountAccessRole\", the role is already there",
          "poster": "VerRi",
          "timestamp": "1730869260.0"
        },
        {
          "comment_id": "1288395",
          "content": "Selected Answer: D\nThe question states that the role already exists with full access policy. This role exists in the new member account. We need the IAM user from the management account the ability to assume it.",
          "upvote_count": "3",
          "timestamp": "1727152200.0",
          "poster": "heff_bezos"
        },
        {
          "content": "Selected Answer: D\nThis role is created by default in member accounts. See:\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_accounts_access.html",
          "comments": [
            {
              "poster": "heff_bezos",
              "comment_id": "1288400",
              "content": "\"By default, if you create a member account as part of your organization, AWS automatically creates a role in the account that grants administrator permissions to IAM users in the management account who can assume the role. By default, that role is named OrganizationAccountAccessRole\"\n\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_accounts_create-cross-account-role.html",
              "timestamp": "1727152500.0",
              "upvote_count": "1"
            }
          ],
          "poster": "aws_god",
          "upvote_count": "2",
          "timestamp": "1725880440.0",
          "comment_id": "1280840"
        },
        {
          "comment_id": "1200823",
          "poster": "c3518fc",
          "upvote_count": "4",
          "content": "Selected Answer: C\nTo create an AWS Organizations administrator role in a member account\nSign in to the IAM console at https://console.aws.amazon.com/iam/. You must sign in as an IAM user, assume an IAM role, or sign in as the root user (not recommended) in the member account. The user or role must have permission to create IAM roles and policies.\n\nIn the IAM console, navigate to Roles and then choose Create role.\n\nChoose AWS account, and then select Another AWS account.\n\nEnter the 12-digit account ID number of the management account that you want to grant administrator access to. Under Options, please note the following:\n\nOn the Add permissions page, choose the AWS managed policy named AdministratorAccess and then choose. https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_accounts_access.html#orgs_manage_accounts_create-cross-account-role",
          "timestamp": "1713890280.0"
        },
        {
          "comment_id": "1192066",
          "poster": "Andy11234912",
          "upvote_count": "2",
          "comments": [
            {
              "upvote_count": "1",
              "poster": "Jay_2pt0_1",
              "content": "From reading the question, I'm not sure.",
              "comment_id": "1232322",
              "timestamp": "1718698980.0"
            }
          ],
          "content": "Selected Answer: D\nnot c, the role is already created",
          "timestamp": "1712645460.0"
        },
        {
          "timestamp": "1712573280.0",
          "upvote_count": "1",
          "comment_id": "1191520",
          "content": "D.. the role is already created, what is needed is just update the trust policy",
          "poster": "sirronido"
        },
        {
          "poster": "DanShone",
          "comment_id": "1175064",
          "upvote_count": "1",
          "content": "C is correct",
          "timestamp": "1710604140.0"
        },
        {
          "comment_id": "1125151",
          "poster": "twogyt",
          "content": "Selected Answer: C\nC is correct",
          "upvote_count": "4",
          "timestamp": "1705510620.0"
        },
        {
          "poster": "zain1258",
          "comment_id": "1082686",
          "upvote_count": "4",
          "timestamp": "1701184680.0",
          "content": "Selected Answer: C\nC is correct"
        },
        {
          "comments": [
            {
              "poster": "tom_cat",
              "comment_id": "1079523",
              "timestamp": "1700853240.0",
              "upvote_count": "2",
              "content": "So I believe it's C."
            }
          ],
          "upvote_count": "2",
          "timestamp": "1700853180.0",
          "comment_id": "1079521",
          "content": "For invited accounts the OrganizationAccountAccessRole needs to be created:\nMember accounts that you invite to join your organization do not automatically get an administrator role created. You have to do this manually, as shown in the following procedure. This essentially duplicates the role automatically set up for created accounts. We recommend that you use the same name, OrganizationAccountAccessRole, for your manually created roles for consistency and ease of remembering.",
          "poster": "tom_cat"
        },
        {
          "upvote_count": "1",
          "content": "Selected Answer: A\nA should be correct",
          "timestamp": "1700730000.0",
          "comment_id": "1078271",
          "poster": "vandergun"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:10:28.998Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "DvpoQAtgDxoXYYPB3wYQ",
      "question_number": 55,
      "page": 11,
      "question_text": "A DevOps engineer is designing an application that integrates with a legacy REST API. The application has an AWS Lambda function that reads records from an Amazon Kinesis data stream. The Lambda function sends the records to the legacy REST API.\n\nApproximately 10% of the records that the Lambda function sends from the Kinesis data stream have data errors and must be processed manually. The Lambda function event source configuration has an Amazon Simple Queue Service (Amazon SQS) dead-letter queue as an on-failure destination. The DevOps engineer has configured the Lambda function to process records in batches and has implemented retries in case of failure.\n\nDuring testing, the DevOps engineer notices that the dead-letter queue contains many records that have no data errors and that already have been processed by the legacy REST API. The DevOps engineer needs to configure the Lambda function's event source options to reduce the number of errorless records that are sent to the dead-letter queue.\n\nWhich solution will meet these requirements?",
      "choices": {
        "D": "Decrease the maximum age of record.",
        "C": "Increase the concurrent batches per shard.",
        "B": "Configure the setting to split the batch when an error occurs.",
        "A": "Increase the retry attempts."
      },
      "correct_answer": "B",
      "answer_ET": "B",
      "answers_community": [
        "B (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/126992-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-11-23 10:02:00",
      "unix_timestamp": 1700730120,
      "discussion_count": 6,
      "discussion": [
        {
          "comments": [
            {
              "upvote_count": "1",
              "poster": "Gomer",
              "comment_id": "1237182",
              "timestamp": "1719375240.0",
              "content": "Seemingly very good explanation, though I had trouble finding any references other than this:\n\"BisectBatchOnFunctionError\" \"If the function returns an error, split the batch in two and retry. The default value is false.\"\naws lambda update-event-source-mapping --bisect-batch-on-function-error [...]"
            }
          ],
          "timestamp": "1713892560.0",
          "comment_id": "1200856",
          "content": "Selected Answer: B\nWhen consuming records from a Kinesis data stream using AWS Lambda, the function can process records in batches. By default, if any record in the batch fails to process, the entire batch is sent to the dead-letter queue.\nTo avoid sending errorless records to the dead-letter queue, the Lambda function's event source options should be configured to split the batch when an error occurs. This setting is called batchWindow and can be configured in the event source mapping for the Lambda function.\nWhen batchWindow is set to TRIM_HORIZON, the Lambda function will split the batch at the first record that causes an error and send only the failed records to the dead-letter queue. The remaining errorless records in the batch will continue to be processed by the function.",
          "poster": "c3518fc",
          "upvote_count": "11"
        },
        {
          "poster": "zolthar_z",
          "upvote_count": "7",
          "content": "Selected Answer: B\nB: https://docs.aws.amazon.com/lambda/latest/dg/with-kinesis.html#services-kinesis-eventsourcemapping",
          "comment_id": "1084518",
          "timestamp": "1701359580.0"
        },
        {
          "upvote_count": "2",
          "content": "B: \nhttps://aws.amazon.com/blogs/big-data/best-practices-for-consuming-amazon-kinesis-data-streams-using-aws-lambda/",
          "poster": "tinyshare",
          "comment_id": "1263642",
          "timestamp": "1723326660.0"
        },
        {
          "comment_id": "1143419",
          "timestamp": "1707317160.0",
          "content": "B is correct: <(Amazon SQS) dead-letter queue as an on-failure destination>: split the batch into 2 parts: success ones and error ones. error ones come to dead queue",
          "upvote_count": "3",
          "poster": "thanhnv142"
        },
        {
          "timestamp": "1700764860.0",
          "comment_id": "1078660",
          "content": "Selected Answer: B\nB is correct",
          "poster": "zain1258",
          "upvote_count": "4"
        },
        {
          "upvote_count": "5",
          "content": "Selected Answer: B\nB is corrected",
          "timestamp": "1700730120.0",
          "poster": "vandergun",
          "comment_id": "1078272"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:10:28.998Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "iCxhcM3ceeZEK78hQWyR",
      "question_number": 56,
      "page": 12,
      "question_text": "A company has microservices running in AWS Lambda that read data from Amazon DynamoDB. The Lambda code is manually deployed by developers after successful testing. The company now needs the tests and deployments be automated and run in the cloud. Additionally, traffic to the new versions of each microservice should be incrementally shifted over time after deployment.\n\nWhat solution meets all the requirements, ensuring the MOST developer velocity?",
      "choices": {
        "C": "Create an AWS CodePipeline configuration and set up the source code step to trigger when code is pushed. Set up the build step to use AWS CodeBuild to run the tests. Set up an AWS CodeDeploy configuration to deploy, then select the CodeDeployDefault.LambdaLinear10PercentEvery3Minutes option.",
        "D": "Use the AWS CLI to set up a post-commit hook that uploads the code to an Amazon S3 bucket after tests have passed Set up an S3 event trigger that runs a Lambda function that deploys the new version. Use an interval in the Lambda function to deploy the code over time at the required percentage.",
        "B": "Create an AWS CodeBuild configuration that triggers when the test code is pushed. Use AWS CloudFormation to trigger an AWS CodePipeline configuration that deploys the new Lambda versions and specifies the traffic shift percentage and interval.",
        "A": "Create an AWS CodePipeline configuration and set up a post-commit hook to trigger the pipeline after tests have passed. Use AWS CodeDeploy and create a Canary deployment configuration that specifies the percentage of traffic and interval."
      },
      "correct_answer": "C",
      "answer_ET": "C",
      "answers_community": [
        "C (97%)",
        "3%"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/129671-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-12-29 12:53:00",
      "unix_timestamp": 1703850780,
      "discussion_count": 11,
      "discussion": [
        {
          "poster": "c3518fc",
          "timestamp": "1729704120.0",
          "upvote_count": "6",
          "content": "Selected Answer: C\nThis solution provides the following benefits:\n\nAutomation: The entire process, from code push to testing and deployment, is automated, reducing manual effort and increasing developer velocity.\nIntegration: By using AWS CodePipeline, CodeBuild, and CodeDeploy, you leverage fully managed services that are designed to work together seamlessly.\nIncremental Deployment: The CodeDeployDefault.LambdaLinear10PercentEvery3Minutes option ensures a smooth and controlled migration of traffic to the new versions of your microservices, minimizing the risk of downtime or disruption.",
          "comment_id": "1200870"
        },
        {
          "comment_id": "1109396",
          "timestamp": "1719713700.0",
          "content": "Selected Answer: C\nAgree C is correct",
          "poster": "PrasannaBalaji",
          "upvote_count": "6"
        },
        {
          "timestamp": "1748866860.0",
          "comment_id": "1574171",
          "content": "Selected Answer: C\nAny mention of automated testing and the answer must include CodeBuild",
          "poster": "92a2133",
          "upvote_count": "1"
        },
        {
          "comment_id": "1175059",
          "content": "Selected Answer: C\nC is correct",
          "timestamp": "1726494300.0",
          "poster": "DanShone",
          "upvote_count": "3"
        },
        {
          "comment_id": "1167250",
          "poster": "Shasha1",
          "content": "C \nThere is no 'pre-commit' hook option in the Lambda deployment hook (Canary); only 'before allowing traffic' and 'after allowing traffic' options are available. Therefore, the 'LambdaLinear10PercentEvery3Minutes' option, which is a canary deployment method, enables a linear deployment strategy, gradually shifting traffic to the new versions at a rate of 10% every 3 minutes.\nhttps://medium.com/@Da_vidgf/canary-deployments-in-serverless-applications-b0f47fa9b409",
          "upvote_count": "3",
          "timestamp": "1725627240.0"
        },
        {
          "poster": "Chelseajcole",
          "content": "if it is canary, why not a?",
          "comment_id": "1148408",
          "timestamp": "1723474560.0",
          "upvote_count": "1"
        },
        {
          "comments": [
            {
              "timestamp": "1745168160.0",
              "poster": "GripZA",
              "comment_id": "1562264",
              "upvote_count": "1",
              "content": "canary won't meet requirements here since it will shift all traffic after time completes, whereas linear the amount of the traffic routed to the new version will be incremented according to the provided percentage and interval."
            }
          ],
          "timestamp": "1723034880.0",
          "upvote_count": "1",
          "content": "A is correct: canary deployment",
          "comment_id": "1143421",
          "poster": "thanhnv142"
        },
        {
          "content": "Selected Answer: C\nc is correct",
          "comment_id": "1125157",
          "upvote_count": "4",
          "timestamp": "1721228700.0",
          "poster": "twogyt"
        },
        {
          "poster": "a54b16f",
          "upvote_count": "4",
          "content": "Selected Answer: C\nB is wrong, why would you trigger a pipeline when TEST code is pushed",
          "timestamp": "1720800420.0",
          "comment_id": "1120992"
        },
        {
          "poster": "csG13",
          "comment_id": "1108734",
          "upvote_count": "4",
          "timestamp": "1719661800.0",
          "content": "Selected Answer: C\nAnswer is C, canary deployment"
        },
        {
          "content": "Selected Answer: B\nB is correct",
          "poster": "PrasannaBalaji",
          "timestamp": "1719654780.0",
          "upvote_count": "1",
          "comment_id": "1108647"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:10:39.583Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "sgYJeQDqIDrL5YwLxv05",
      "question_number": 57,
      "page": 12,
      "question_text": "To run an application, a DevOps engineer launches an Amazon EC2 instance with public IP addresses in a public subnet. A user data script obtains the application artifacts and installs them on the instances upon launch. A change to the security classification of the application now requires the instances to run with no access to the internet. While the instances launch successfully and show as healthy, the application does not seem to be installed.\nWhich of the following should successfully install the application while complying with the new rule?",
      "choices": {
        "D": "Create a security group for the application instances and allow only outbound traffic to the artifact repository. Remove the security group rule once the install is complete.",
        "B": "Set up a NAT gateway. Deploy the EC2 instances to a private subnet. Update the private subnet's route table to use the NAT gateway as the default route.",
        "C": "Publish the application artifacts to an Amazon S3 bucket and create a VPC endpoint for S3. Assign an IAM instance profile to the EC2 instances so they can read the application artifacts from the S3 bucket.",
        "A": "Launch the instances in a public subnet with Elastic IP addresses attached. Once the application is installed and running, run a script to disassociate the Elastic IP addresses afterwards."
      },
      "correct_answer": "C",
      "answer_ET": "C",
      "answers_community": [
        "C (91%)",
        "9%"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/105514-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-04-07 15:19:00",
      "unix_timestamp": 1680873540,
      "discussion_count": 26,
      "discussion": [
        {
          "content": "Selected Answer: C\nC - in the answer \nThough we can use both B and C , since we only want to download to package at the time of initialization . So there is no need to have continuous access to internet . Therefore, it is cheap and optimal to use S3 .",
          "comment_id": "1101209",
          "upvote_count": "8",
          "timestamp": "1703046120.0",
          "poster": "z_inderjot"
        },
        {
          "timestamp": "1722959940.0",
          "comment_id": "1261753",
          "upvote_count": "1",
          "poster": "namtp",
          "content": "Selected Answer: C\nC is the correct answer.\nno access to the internet but connect to aws services => private endpoint"
        },
        {
          "content": "C is correct: all other options utilize internet connections",
          "upvote_count": "3",
          "comment_id": "1133817",
          "timestamp": "1706415600.0",
          "poster": "thanhnv142"
        },
        {
          "timestamp": "1702190820.0",
          "upvote_count": "3",
          "content": "Selected Answer: C\nC is the correct one.\n\nall other option will allow internet access which is not compliance with the reqs",
          "comment_id": "1092307",
          "poster": "harithzainudin"
        },
        {
          "upvote_count": "3",
          "content": "Selected Answer: C\nC: Can't be B because with the NAT the EC2 still has internet access",
          "poster": "zolthar_z",
          "comment_id": "1073537",
          "timestamp": "1700245980.0"
        },
        {
          "upvote_count": "2",
          "poster": "robertohyena",
          "comments": [
            {
              "poster": "rowanwally",
              "comment_id": "1069831",
              "content": "is the dump still valid?",
              "upvote_count": "1",
              "timestamp": "1699916820.0"
            }
          ],
          "content": "C is the correct answer.\n\nA B D are not correct.\nKeywords:\n- requires the instances to run with no access to the internet",
          "comment_id": "1067894",
          "timestamp": "1699712820.0"
        },
        {
          "poster": "bosmanx",
          "content": "Selected Answer: C\nB is incorrect, the new policy is \"no access to the internet\"",
          "comment_id": "1062711",
          "timestamp": "1699170900.0",
          "upvote_count": "2"
        },
        {
          "content": "C is the answer. B would enable internet access from the instance.",
          "comment_id": "1056190",
          "poster": "DevopsNoob",
          "upvote_count": "1",
          "timestamp": "1698496380.0"
        },
        {
          "comment_id": "1020802",
          "timestamp": "1695993120.0",
          "upvote_count": "1",
          "content": "C is correct and B, which is specifically for NAT. in question they have asked that no internet access from the instance, so If we enable NAT then from outside no one can access the instance but internet will be accessible on the instance using NAT.",
          "poster": "Ffida"
        },
        {
          "upvote_count": "1",
          "content": "C is correct\nB: \"instances to run with no access to the internet.\" so you can not use NAT",
          "comment_id": "1016036",
          "timestamp": "1695574200.0",
          "poster": "ataince"
        },
        {
          "upvote_count": "2",
          "comment_id": "1000084",
          "timestamp": "1693965480.0",
          "poster": "DaddyDee",
          "content": "C is the answer, you can use artifacts in s3 with vpc endpoints. With a gateway endpoint, you can access Amazon S3 from your VPC, without requiring an internet gateway or NAT device for your VPC, and with no additional cost.\nhttps://repost.aws/knowledge-center/ec2-systems-manager-vpc-endpoints\nhttps://docs.aws.amazon.com/vpc/latest/privatelink/vpc-endpoints-s3.html"
        },
        {
          "poster": "rahulsingha2112",
          "upvote_count": "1",
          "comment_id": "991560",
          "content": "C is correct as solution required no internet access",
          "timestamp": "1693152600.0"
        },
        {
          "upvote_count": "1",
          "comment_id": "978180",
          "timestamp": "1691717040.0",
          "content": "Correct C.",
          "poster": "ggrodskiy"
        },
        {
          "upvote_count": "1",
          "poster": "madperro",
          "comment_id": "918169",
          "timestamp": "1686222720.0",
          "content": "Selected Answer: C\nC is the answer. B gives the instances access to the Internet."
        },
        {
          "poster": "rdoty",
          "content": "Selected Answer: C\nDef C, all others include access to the internet",
          "comment_id": "910538",
          "upvote_count": "1",
          "timestamp": "1685472060.0"
        },
        {
          "content": "This is supposed to be a Choose two answer. BC",
          "timestamp": "1685073360.0",
          "poster": "ProfXsamson",
          "comment_id": "907022",
          "upvote_count": "1"
        },
        {
          "content": "NAT GW for me",
          "timestamp": "1684821120.0",
          "upvote_count": "1",
          "comment_id": "904605",
          "poster": "Akaza"
        },
        {
          "timestamp": "1684513200.0",
          "comment_id": "902105",
          "poster": "lunt",
          "upvote_count": "2",
          "comments": [
            {
              "timestamp": "1691657340.0",
              "content": "Indeed, you can connect to a private repository in S3, in which you have previously included the application artifacts, through a VPC endpoints (adding it as a destination in the routing table for traffic destined from the VPC to Amazon S3). This VPC endpoint can be: an \"gateway endpoint\" or an \"interface endpoint\"\n\nhttps://docs.aws.amazon.com/vpc/latest/privatelink/vpc-endpoints-s3.html",
              "poster": "Fco_Javier",
              "comment_id": "977444",
              "upvote_count": "1"
            }
          ],
          "content": "Selected Answer: C\n\"no access to internet\" > no access means ACD are out.\nPoint of this question to make you think you must have Internet to complete the install, thsi is not true as you can always download the packages & DL from a repo or S3.\nC is correct."
        },
        {
          "timestamp": "1683523620.0",
          "poster": "ParagSanyashiv",
          "comment_id": "891831",
          "upvote_count": "1",
          "content": "Selected Answer: C\nC is a secure way of achieving the scenario and will match the security compliance."
        },
        {
          "content": "Selected Answer: C\nThe correct answer is C.\n\nCreating a VPC endpoint for Amazon S3 allows the EC2 instances to access the application artifacts in the S3 bucket without going through the internet, thus meeting the new security requirement of running the instances with no internet access. Assigning an IAM instance profile to the EC2 instances allows them to read the application artifacts from the S3 bucket.",
          "timestamp": "1682986080.0",
          "upvote_count": "1",
          "poster": "haazybanj",
          "comment_id": "886841"
        },
        {
          "poster": "Mail1964",
          "timestamp": "1682596020.0",
          "upvote_count": "2",
          "content": "Selected Answer: C\nThe question states - \"no internet access\". for me that only leaves C.",
          "comment_id": "882593"
        },
        {
          "comments": [
            {
              "comment_id": "877553",
              "poster": "Dimidrol",
              "upvote_count": "1",
              "timestamp": "1682189460.0",
              "content": "Very strange, if you have requirement to run instance without internet you"
            }
          ],
          "content": "Option B is the best solution as it provides a secure way for the instances to access the internet and the application artifacts without compromising security. By deploying the instances in a private subnet and setting up a NAT gateway, the instances can access the internet through the NAT gateway, which acts as a proxy, while not having direct access to the internet. The NAT gateway allows outbound internet connectivity, but inbound traffic is not allowed, so it complies with the new security rule",
          "timestamp": "1681503420.0",
          "poster": "alce2020",
          "comment_id": "870454",
          "upvote_count": "3"
        },
        {
          "comment_id": "869773",
          "content": "Selected Answer: C\nwith nat gateway we will have access to the internet",
          "upvote_count": "2",
          "poster": "henryyvr",
          "timestamp": "1681420260.0"
        },
        {
          "upvote_count": "2",
          "comment_id": "864641",
          "content": "Selected Answer: C\nC for me, with nat gateway we will have access to the internet",
          "poster": "Dimidrol",
          "timestamp": "1680952920.0"
        },
        {
          "comment_id": "864193",
          "timestamp": "1680895680.0",
          "poster": "kassem77",
          "upvote_count": "1",
          "content": "Option B is the correct solution to install the application while complying with the new rule.\nBy setting up a NAT gateway and deploying the EC2 instances to a private subnet, the instances will not have direct access to the internet, which satisfies the new security requirement."
        },
        {
          "timestamp": "1680873540.0",
          "poster": "ele",
          "upvote_count": "3",
          "comment_id": "863910",
          "comments": [
            {
              "poster": "bcx",
              "timestamp": "1685456520.0",
              "content": "No access to the Internet is a requirement. NAT gateway is accessing the Internet. So that invalidates B.",
              "comment_id": "910311",
              "upvote_count": "2"
            }
          ],
          "content": "Selected Answer: B\nuse Nat GW to access Internet"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:10:39.583Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "1pxLkJN4lSzAwQ8twago",
      "question_number": 58,
      "page": 12,
      "question_text": "A company is building a web and mobile application that uses a serverless architecture powered by AWS Lambda and Amazon API Gateway. The company wants to fully automate the backend Lambda deployment based on code that is pushed to the appropriate environment branch in an AWS CodeCommit repository.\n\nThe deployment must have the following:\n\n• Separate environment pipelines for testing and production\n• Automatic deployment that occurs for test environments only\n\nWhich steps should be taken to meet these requirements?",
      "choices": {
        "A": "Configure a new AWS CodePipeline service. Create a CodeCommit repository for each environment. Set up CodePipeline to retrieve the source code from the appropriate repository. Set up the deployment step to deploy the Lambda functions with AWS CloudFormation.",
        "D": "Create an AWS CodeBuild configuration for test and production environments. Configure the production pipeline to have a manual approval step. Create one CodeCommit repository with a branch for each environment. Push the Lambda function code to an Amazon S3 bucket. Set up the deployment step to deploy the Lambda functions from the S3 bucket.",
        "B": "Create two AWS CodePipeline configurations for test and production environments. Configure the production pipeline to have a manual approval step. Create a CodeCommit repository for each environment. Set up each CodePipeline to retrieve the source code from the appropriate repository. Set up the deployment step to deploy the Lambda functions with AWS CloudFormation.",
        "C": "Create two AWS CodePipeline configurations for test and production environments. Configure the production pipeline to have a manual approval step. Create one CodeCommit repository with a branch for each environment. Set up each CodePipeline to retrieve the source code from the appropriate branch in the repository. Set up the deployment step to deploy the Lambda functions with AWS CloudFormation."
      },
      "correct_answer": "C",
      "answer_ET": "C",
      "answers_community": [
        "C (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/129690-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-12-29 14:57:00",
      "unix_timestamp": 1703858220,
      "discussion_count": 4,
      "discussion": [
        {
          "upvote_count": "6",
          "content": "Selected Answer: C\nC is correct: <Separate environment pipelines for testing and production> means codepipeline <code that is pushed to the appropriate environment branch in an AWS CodeCommit repository.> means code CodeCommit\nA: no mention of creating Separate env for test and dev\nB: <Create a CodeCommit repository for each environment> should not do this. We should create a branch for each env\nD: no mention of code pipelines",
          "timestamp": "1723035180.0",
          "comment_id": "1143426",
          "poster": "thanhnv142"
        },
        {
          "upvote_count": "3",
          "timestamp": "1729704540.0",
          "comment_id": "1200876",
          "poster": "c3518fc",
          "content": "Selected Answer: C\nBy creating two CodePipeline configurations, using a single CodeCommit repository with branches for each environment, and deploying Lambda functions with CloudFormation, this solution meets the requirements while following best practices for source code management, continuous delivery, and infrastructure as code."
        },
        {
          "upvote_count": "3",
          "content": "Selected Answer: C\nC is correct\nFirst, A&B both are in-correct: As a basic policy - do not create a repo for the same code for multiple environments. Always create a branch from the same repo. The strategy is wrong for A&B.\nNow C&D: D uses Lambda function with s3, whereas C uses code pipeline to store and build. Using code pipeline is a smart choice rather than using S3 as a code pipeline that offers better branching strategy and controls. I will go with ‘C”.",
          "timestamp": "1719713940.0",
          "poster": "PrasannaBalaji",
          "comment_id": "1109399"
        },
        {
          "content": "Selected Answer: C\nIt’s C - unique env and also distinct resources in aws codepipeline would result to pull from both repos on every update of either repo.",
          "poster": "csG13",
          "upvote_count": "2",
          "timestamp": "1719662220.0",
          "comment_id": "1108740"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:10:39.583Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "m2yrl4UDreSdQYv4EVxg",
      "question_number": 59,
      "page": 12,
      "question_text": "A DevOps engineer wants to find a solution to migrate an application from on premises to AWS. The application is running on Linux and needs to run on specific versions of Apache Tomcat, HAProxy, and Varnish Cache to function properly. The application's operating system-level parameters require tuning. The solution must include a way to automate the deployment of new application versions. The infrastructure should be scalable and faulty servers should be replaced automatically.\n\nWhich solution should the DevOps engineer use?",
      "choices": {
        "D": "Upload the application code to an AWS CodeCommit repository with an appspec.yml file to configure and install the necessary software. Create an AWS CodeDeploy deployment group associated with an Amazon EC2 Auto Scaling group. Create an AWS CodePipeline pipeline that uses CodeCommit as a source and CodeDeploy as a deployment provider.",
        "C": "Upload the application code to an AWS CodeCommit repository with a set of .ebextensions files to configure and install the software. Create an AWS Elastic Beanstalk worker tier environment that uses the Tomcat solution stack. Create an AWS CodePipeline pipeline that uses CodeCommit as a source and Elastic Beanstalk as a deployment provider.",
        "B": "Upload the application code to an AWS CodeCommit repository with a saved configuration file to configure and install the software. Create an AWS Elastic Beanstalk web server tier and a load balanced-type environment that uses the Tomcat solution stack. Create an AWS CodePipeline pipeline that uses CodeCommit as a source and Elastic Beanstalk as a deployment provider.",
        "A": "Upload the application as a Docker image that contains all the necessary software to Amazon ECR. Create an Amazon ECS cluster using an AWS Fargate launch type and an Auto Scaling group. Create an AWS CodePipeline pipeline that uses Amazon ECR as a source and Amazon ECS as a deployment provider."
      },
      "correct_answer": "D",
      "answer_ET": "D",
      "answers_community": [
        "D (90%)",
        "8%"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/129698-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-12-29 15:36:00",
      "unix_timestamp": 1703860560,
      "discussion_count": 16,
      "discussion": [
        {
          "comment_id": "1108785",
          "timestamp": "1703860560.0",
          "upvote_count": "7",
          "content": "Selected Answer: D\nI go with D, simply because with Fargate you have very limited access to the OS.",
          "poster": "csG13"
        },
        {
          "content": "Selected Answer: D\nWhile option A is a strong candidate due to its serverless nature and ease of deployment, option D is the most suitable solution given the need for specific software versions, OS-level tuning, and the requirement for a scalable and fault-tolerant infrastructure. Option D provides the necessary control over the software environment and infrastructure configuration, along with the benefits of automation and scalability.",
          "timestamp": "1711024800.0",
          "comment_id": "1179218",
          "poster": "CloudHandsOn",
          "upvote_count": "6"
        },
        {
          "upvote_count": "1",
          "content": "Selected Answer: D\nCodeCommit + CodeDeploy + EC2 Auto Scaling",
          "comment_id": "1585228",
          "timestamp": "1752142680.0",
          "poster": "Jonalb"
        },
        {
          "poster": "ce0df07",
          "timestamp": "1738996620.0",
          "comment_id": "1353259",
          "upvote_count": "1",
          "content": "Selected Answer: B\nOption B is clearly the preferred option:\n- Uses Elastic Beanstalk web server tier with load balancing\n- Provides managed Tomcat platform\n- Supports configuration files for software installation\n- Includes auto-scaling and health monitoring\n- Uses CodePipeline for automated deployments\n- Appropriate for web applications\nOption A: Containerization created unnecessary overhead and it will turn up more expensive\nOption C: Worker tier is for background processes, not suitable for web applications\nOption D: \n- More manual configuration required\n- Less managed service features\n- No built-in platform support\n- More complex to maintain"
        },
        {
          "upvote_count": "2",
          "poster": "zijo",
          "timestamp": "1735855680.0",
          "content": "Selected Answer: D\nAWS Fargate does not allow direct tuning of Linux OS-level parameters because it is a fully managed, serverless compute engine for containers. With Fargate, AWS abstracts the underlying infrastructure, including the operating system, and does not expose granular OS-level configurations.",
          "comment_id": "1335777"
        },
        {
          "upvote_count": "2",
          "content": "Selected Answer: D\nkeywords: \n- specific versions of Apache Tomcat, HAProxy, and Varnish Cache \n- operating system-level parameters \n\nNot A as Fargate is serverless",
          "timestamp": "1722914700.0",
          "comment_id": "1261362",
          "poster": "jamesf"
        },
        {
          "comment_id": "1197157",
          "content": "Selected Answer: D\nKey point: The application's operating system-level parameters require tuning",
          "timestamp": "1713349380.0",
          "upvote_count": "5",
          "poster": "didek1986"
        },
        {
          "comment_id": "1194832",
          "poster": "dkp",
          "content": "Selected Answer: D\nill go with D",
          "upvote_count": "3",
          "timestamp": "1713000600.0"
        },
        {
          "comments": [
            {
              "poster": "jamesf",
              "content": "Me too, I still go for D but seen like new update for AWS Fargate have some change",
              "timestamp": "1722243240.0",
              "upvote_count": "1",
              "comment_id": "1257355"
            }
          ],
          "poster": "devakram",
          "upvote_count": "3",
          "content": "I doubt this question will come up since both A and D are correct: https://aws.amazon.com/blogs/containers/announcing-additional-linux-controls-for-amazon-ecs-tasks-on-aws-fargate/\n\nthey need to add some words to the question tat force you to choose either A or D",
          "comment_id": "1194795",
          "timestamp": "1712996340.0"
        },
        {
          "upvote_count": "1",
          "timestamp": "1711460940.0",
          "poster": "stoy123",
          "comment_id": "1183348",
          "content": "Selected Answer: A\nAnswer A"
        },
        {
          "comment_id": "1175038",
          "timestamp": "1710600660.0",
          "content": "Selected Answer: D\nD - The application's operating system-level parameters require tuning - means option D is the only answer. A,B and C dont allow this os level tuning",
          "comments": [
            {
              "timestamp": "1713893880.0",
              "comment_id": "1200882",
              "upvote_count": "1",
              "poster": "c3518fc",
              "content": "Today we’re excited to announce that customers can now tune Linux kernel parameters in ECS tasks on AWS Fargate. Tuning Linux kernel parameters can help customers optimize their network throughput when running containerized network proxies or achieve higher levels of workload resilience by terminating stale connections. This launch provides parity for ECS tasks launched on AWS Fargate and Amazon EC2 container instances. https://aws.amazon.com/blogs/containers/announcing-additional-linux-controls-for-amazon-ecs-tasks-on-aws-fargate/"
            }
          ],
          "poster": "DanShone",
          "upvote_count": "3"
        },
        {
          "comment_id": "1165241",
          "upvote_count": "1",
          "poster": "dzn",
          "content": "Selected Answer: A\nAWS Fargate can tune linux kernel parameters.\nhttps://docs.aws.amazon.com/AmazonECS/latest/developerguide/task_definition_parameters.html",
          "timestamp": "1709518500.0"
        },
        {
          "upvote_count": "5",
          "timestamp": "1707489300.0",
          "content": "Selected Answer: D\nD is correct: <The application's operating system-level parameters require tuning> means the user need controls over os-level\nA: AWS Fargate and Docker dont provide os-level controls\nB and C: Beanstalk does not support Varnish Cache and no os-level controls provided",
          "poster": "thanhnv142",
          "comment_id": "1145551"
        },
        {
          "upvote_count": "2",
          "content": "Selected Answer: A\nThe question only needs Linux OS, its not asking for any extra customizations. AWS fargate allows you to choose the linux platform - and you can package the specific packages in a docker container.",
          "comment_id": "1131700",
          "timestamp": "1706189460.0",
          "poster": "sksegha"
        },
        {
          "content": "Selected Answer: D\nIt need specific versions of Apache, so, B and C are out, it requires OS level customization, so no Fargate (A)",
          "upvote_count": "4",
          "timestamp": "1705083240.0",
          "poster": "a54b16f",
          "comment_id": "1120997"
        },
        {
          "comment_id": "1109922",
          "timestamp": "1703964720.0",
          "upvote_count": "5",
          "poster": "zolthar_z",
          "content": "Selected Answer: D\nD: you can't customize fargate OS, Also the docker images only ocntains the application software"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:10:39.583Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "Q7uO4dWMgsvrn6dArvr8",
      "question_number": 60,
      "page": 12,
      "question_text": "A DevOps engineer is using AWS CodeDeploy across a fleet of Amazon EC2 instances in an EC2 Auto Scaling group. The associated CodeDeploy deployment group, which is integrated with EC2 Auto Scaling, is configured to perform in-place deployments with CodeDeployDefault.OneAtATime. During an ongoing new deployment, the engineer discovers that, although the overall deployment finished successfully, two out of five instances have the previous application revision deployed. The other three instances have the newest application revision.\n\nWhat is likely causing this issue?",
      "choices": {
        "D": "EC2 Auto Scaling launched two new instances while the new deployment had not yet finished, causing the previous version to be deployed on the affected instances.",
        "A": "The two affected instances failed to fetch the new deployment.",
        "C": "The CodeDeploy agent was not installed in two affected instances.",
        "B": "A failed AfterInstall lifecycle event hook caused the CodeDeploy agent to roll back to the previous version on the affected instances."
      },
      "correct_answer": "D",
      "answer_ET": "D",
      "answers_community": [
        "D (89%)",
        "11%"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/129672-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-12-29 13:03:00",
      "unix_timestamp": 1703851380,
      "discussion_count": 8,
      "discussion": [
        {
          "content": "Selected Answer: D\nD is correct: In-place deployment search for all available agents at the time of the deployment and update the app version. If there are new instances launched after the search, they would be omitted and they fetch the lastest app version available, which is the previous revision\nA and B: If this happened, the other three would be affected as well\nC: If code deploy agents were not installed, no version would be installed on the two instances",
          "comment_id": "1145576",
          "poster": "thanhnv142",
          "timestamp": "1723207860.0",
          "upvote_count": "6"
        },
        {
          "comment_id": "1132663",
          "upvote_count": "5",
          "poster": "promo286",
          "comments": [
            {
              "upvote_count": "1",
              "poster": "govindrk",
              "timestamp": "1723454880.0",
              "comment_id": "1148014",
              "content": "A. The explanation provided for D summarizes A- The two affected instances failed to fetch the new deployment."
            }
          ],
          "content": "Selected Answer: D\nD. EC2 Auto Scaling launched two new instances while the new deployment had not yet finished, causing the previous version to be deployed on the affected instances.\n\nExplanation:\n\nIn an EC2 Auto Scaling group, when a deployment is in progress and new instances are launched, they may receive the previous version of the application if the deployment has not yet completed. This is because the new instances join the Auto Scaling group and need to fetch the latest revision during the deployment process. If the deployment has not finished when the new instances are launched, they will fetch the current revision available in the Auto Scaling group, which might be the previous version.",
          "timestamp": "1722004800.0"
        },
        {
          "poster": "nickp84",
          "comment_id": "1568486",
          "upvote_count": "1",
          "content": "Selected Answer: D\nB. Failed lifecycle event: Would cause a rollback, but that would typically be marked as a deployment failure, not success.",
          "timestamp": "1747065120.0"
        },
        {
          "timestamp": "1734903480.0",
          "comment_id": "1330587",
          "poster": "youonebe",
          "content": "Selected Answer: B\nB is correct.\n\nThe AfterInstall lifecycle event is executed after the application revision is installed on the EC2 instance. If an error occurs during this phase, the CodeDeploy agent can trigger a rollback to the previous application version. Given that the deployment was successful for three instances and not for the others, it’s possible that the AfterInstall hook failed on the two instances that still have the previous version. In this case, the CodeDeploy agent would have automatically rolled back to the last successful application revision for those two instances.\n\nwhy D is wrong? new instances launched by Auto Scaling would have the current revision applied during the deployment process, not the previous revision. Therefore, it’s unlikely that the new instances would have the previous version unless something else occurred.",
          "upvote_count": "2"
        },
        {
          "content": "Selected Answer: B\n, B. A failed AfterInstall lifecycle event hook caused the CodeDeploy agent to roll back to the previous version on the affected instances seems to be the most plausible explanation. It accounts for the scenario where the deployment was successful overall, but specific instances reverted to the previous application revision due to issues encountered during post-installation steps. It's important for the DevOps engineer to review the deployment logs, especially focusing on lifecycle event hooks and their outcomes, to confirm this hypothesis and take corrective actions.",
          "poster": "kyuhuck",
          "comment_id": "1152746",
          "timestamp": "1723911960.0",
          "upvote_count": "1"
        },
        {
          "poster": "a54b16f",
          "upvote_count": "4",
          "timestamp": "1720800900.0",
          "comment_id": "1120999",
          "content": "Selected Answer: D\nonly D makes sense"
        },
        {
          "upvote_count": "5",
          "content": "Selected Answer: D\nGot to be D. The rest choices would impact the entire ASG and not only two out of the five intances.",
          "timestamp": "1719664740.0",
          "comment_id": "1108787",
          "poster": "csG13"
        },
        {
          "content": "Selected Answer: D\nD is correct \nhttps://docs.aws.amazon.com/codedeploy/latest/userguide/troubleshooting-auto-scaling.html#troubleshooting-auto-scaling-provision-termination-loo",
          "poster": "PrasannaBalaji",
          "timestamp": "1719655380.0",
          "upvote_count": "3",
          "comment_id": "1108654"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:10:39.583Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "3tBOTA4ImewxNfn9sMKQ",
      "question_number": 61,
      "page": 13,
      "question_text": "A security team is concerned that a developer can unintentionally attach an Elastic IP address to an Amazon EC2 instance in production. No developer should be allowed to attach an Elastic IP address to an instance. The security team must be notified if any production server has an Elastic IP address at any time.\n\nHow can this task be automated?",
      "choices": {
        "C": "Ensure that all IAM groups associated with developers do not have associate-address permissions. Create a scheduled AWS Lambda function to check whether an Elastic IP address is associated with any instance tagged as production, and alert the security team if an instance has an Elastic IP address associated with it.",
        "A": "Use Amazon Athena to query AWS CloudTrail logs to check for any associate-address attempts. Create an AWS Lambda function to disassociate the Elastic IP address from the instance, and alert the security team.",
        "D": "Create an AWS Config rule to check that all production instances have EC2 IAM roles that include deny associate-address permissions. Verify whether there is an Elastic IP address associated with any instance, and alert the security team if an instance has an Elastic IP address associated with it.",
        "B": "Attach an IAM policy to the developers' IAM group to deny associate-address permissions. Create a custom AWS Config rule to check whether an Elastic IP address is associated with any instance tagged as production, and alert the security team."
      },
      "correct_answer": "B",
      "answer_ET": "B",
      "answers_community": [
        "B (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/129673-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-12-29 13:05:00",
      "unix_timestamp": 1703851500,
      "discussion_count": 7,
      "discussion": [
        {
          "poster": "thanhnv142",
          "upvote_count": "7",
          "timestamp": "1707491220.0",
          "content": "Selected Answer: B\nB is correct: < Attach an IAM policy to the developers' IAM group to deny associate-address permissions>: means we can deny all address-assosiate attempts\nA: AWS CloudTrail logs is used for monitoring users' actions. Though it would reveal associate-address attempts, it would not trigger AWS lambda to disassosiate the IPs\nC: <Ensure that all IAM groups associated with developers do not have associate-address permissions>: This is unnecessary and can be done more easily with option B. \nD: <check that all production instances have EC2 IAM roles>: We dont need to check the role of the EC2, we need to handle the role of developers.\n\nSummary: D is irrelevant while A and C, though can achive the requirements, consume more efforts and resources.",
          "comment_id": "1145586"
        },
        {
          "upvote_count": "1",
          "poster": "zijo",
          "content": "Selected Answer: B\nAWS Config provides the eip-attached managed rule to evaluate whether all allocated Elastic IPs are associated with a resource.",
          "comment_id": "1335837",
          "timestamp": "1735866240.0"
        },
        {
          "content": "For what it's worth:\n{\n \"Statement\": [\n {\n \"Action\": [\n \"ec2:AssociateAddress\",\n \"ec2:DisassociateAddress\"\n ],\n \"Effect\": \"Deny\",\n \"Resource\": \"*\"\n }\n ]\n}",
          "comment_id": "1237753",
          "timestamp": "1719436380.0",
          "poster": "Gomer",
          "upvote_count": "2"
        },
        {
          "content": "Selected Answer: B\nanswer B",
          "poster": "dkp",
          "timestamp": "1713001380.0",
          "upvote_count": "3",
          "comment_id": "1194842"
        },
        {
          "poster": "a54b16f",
          "comment_id": "1121000",
          "upvote_count": "3",
          "content": "Selected Answer: B\nso easy, almost copy/paste from the two requirements listed inside the question",
          "timestamp": "1705083540.0"
        },
        {
          "content": "Selected Answer: B\nIt's B, the only who meets the question criteria.",
          "comment_id": "1108789",
          "upvote_count": "4",
          "poster": "csG13",
          "timestamp": "1703860920.0"
        },
        {
          "comment_id": "1108655",
          "timestamp": "1703851500.0",
          "content": "Selected Answer: B\nB is correct",
          "poster": "PrasannaBalaji",
          "upvote_count": "3"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:10:50.026Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "ZY4ilVQEc6jyY15de6Bx",
      "question_number": 62,
      "page": 13,
      "question_text": "A company is using AWS Organizations to create separate AWS accounts for each of its departments. The company needs to automate the following tasks:\n\n• Update the Linux AMIs with new patches periodically and generate a golden image\n• Install a new version of Chef agents in the golden image, if available\n• Provide the newly generated AMIs to the department's accounts\n\nWhich solution meets these requirements with the LEAST management overhead?",
      "choices": {
        "D": "Use Amazon EC2 Image Builder to create an image pipeline that consists of the base Linux AMI and components to install the Chef agent. Create a parameter in AWS Systems Manager Parameter Store to store the new AMI ID that can be referenced by the department's accounts.",
        "B": "Use Amazon EC2 Image Builder to create an image pipeline that consists of the base Linux AMI and components to install the Chef agent. Use AWS Resource Access Manager to share EC2 Image Builder images with the department's accounts.",
        "A": "Write a script to launch an Amazon EC2 instance from the previous golden image. Apply the patch updates. Install the new version of the Chef agent, generate a new golden image, and then modify the AMI permissions to share only the new image with the department's accounts.",
        "C": "Use an AWS Systems Manager Automation runbook to update the Linux AMI by using the previous image. Provide the URL for the script that will update the Chef agent. Use AWS Organizations to replace the previous golden image in the department's accounts."
      },
      "correct_answer": "B",
      "answer_ET": "B",
      "answers_community": [
        "B (86%)",
        "9%"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/129674-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-12-29 13:09:00",
      "unix_timestamp": 1703851740,
      "discussion_count": 12,
      "discussion": [
        {
          "content": "Selected Answer: B\nB is correct: <generate a golden image> means we need EC2 image builder for an automated pipeline to build a golden image\nA and C: no mention of EC2 image builder\nD: This option utilizes SSM to share the image, which is not correct. We need AWS resource sharing to share resources cross-account",
          "poster": "thanhnv142",
          "comment_id": "1145589",
          "timestamp": "1723209360.0",
          "upvote_count": "5"
        },
        {
          "upvote_count": "5",
          "timestamp": "1719728340.0",
          "comment_id": "1110386",
          "poster": "d262e67",
          "content": "Selected Answer: B\nBuilder Image to streamline the AMI baking process and use RAM to easily share the AMI among the whole organization or select accounts.\n\nhttps://docs.aws.amazon.com/imagebuilder/latest/userguide/manage-shared-resources.html#manage-shared-resources-using"
        },
        {
          "poster": "Jonalb",
          "content": "Selected Answer: B\nB is correct: <generate a golden image> means we need EC2 image builder for an automated pipeline to build a golden image",
          "timestamp": "1752142920.0",
          "upvote_count": "1",
          "comment_id": "1585229"
        },
        {
          "content": "Selected Answer: D\nit automates the AMI creation process and provides a low-maintenance way for departments to access the latest AMI ID.",
          "upvote_count": "2",
          "comment_id": "1411985",
          "timestamp": "1743329340.0",
          "poster": "Srikantha"
        },
        {
          "content": "Selected Answer: B\nBy leveraging EC2 Image Builder and RAM, solution B provides a fully automated and centralized approach to creating, updating, and sharing golden images with the department's accounts, minimizing manual effort and management overhead.",
          "timestamp": "1729706040.0",
          "upvote_count": "4",
          "poster": "c3518fc",
          "comment_id": "1200893"
        },
        {
          "timestamp": "1728813420.0",
          "comment_id": "1194847",
          "poster": "dkp",
          "upvote_count": "3",
          "content": "Selected Answer: B\nill go with B"
        },
        {
          "poster": "DanShone",
          "content": "Selected Answer: B\nB - The function of Amazon EC2 Image Builder is to build you golden AMIs",
          "comment_id": "1175035",
          "upvote_count": "3",
          "timestamp": "1726490640.0"
        },
        {
          "poster": "denccc",
          "comment_id": "1125183",
          "upvote_count": "2",
          "timestamp": "1721230920.0",
          "content": "This should be B"
        },
        {
          "comment_id": "1121008",
          "timestamp": "1720801440.0",
          "upvote_count": "5",
          "content": "Selected Answer: B\nOnly B provided solution for sharing AMI image",
          "poster": "a54b16f"
        },
        {
          "timestamp": "1720603620.0",
          "content": "Selected Answer: B\nGoing for B",
          "upvote_count": "4",
          "comment_id": "1118506",
          "poster": "davdan99"
        },
        {
          "timestamp": "1719665220.0",
          "comments": [
            {
              "comment_id": "1109203",
              "content": "Reading it again, B is a better option",
              "upvote_count": "2",
              "timestamp": "1719692640.0",
              "poster": "csG13"
            }
          ],
          "content": "Selected Answer: C\nLeast management overhead is provided by C",
          "upvote_count": "2",
          "comment_id": "1108794",
          "poster": "csG13"
        },
        {
          "timestamp": "1719655740.0",
          "upvote_count": "1",
          "comment_id": "1108659",
          "poster": "PrasannaBalaji",
          "content": "Selected Answer: C\nIt looks like C is the right one.\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/sharingamis-explicit.html"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:10:50.026Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "rDHbMJ3ICcZF8VdFWvCO",
      "question_number": 63,
      "page": 13,
      "question_text": "A company has a mission-critical application on AWS that uses automatic scaling. The company wants the deployment lifecycle to meet the following parameters:\n\n• The application must be deployed one instance at a time to ensure the remaining fleet continues to serve traffic.\n• The application is CPU intensive and must be closely monitored.\n• The deployment must automatically roll back if the CPU utilization of the deployment instance exceeds 85%.\n\nWhich solution will meet these requirements?",
      "choices": {
        "B": "Use AWS CodeDeploy with Amazon EC2 Auto Scaling Configure an alarm tied to the CPU utilization metric. Use the CodeDeployDefault OneAtAtime configuration as a deployment strategy. Configure automatic rollbacks within the deployment group to roll back the deployment if the alarm thresholds are breached.",
        "D": "Use AWS Systems Manager to perform a blue/green deployment with Amazon EC2 Auto Scaling. Configure an alarm tied to the CPU utilization metric. Deploy updates one at a time. Configure automatic rollbacks within the Auto Scaling group to roll back the deployment if the alarm thresholds are breached.",
        "C": "Use AWS Elastic Beanstalk for load balancing and AWS Auto Scaling. Configure an alarm tied to the CPU utilization metric. Configure rolling deployments with a fixed batch size of one instance. Enable enhanced health to monitor the status of the deployment and roll back based on the alarm previously created.",
        "A": "Use AWS CloudFormation to create an AWS Step Functions state machine and Auto Scaling lifecycle hooks to move to one instance at a time into a wait state. Use AWS Systems Manager automation to deploy the update to each instance and move it back into the Auto Scaling group using the heartbeat timeout."
      },
      "correct_answer": "B",
      "answer_ET": "B",
      "answers_community": [
        "B (95%)",
        "5%"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/129675-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-12-29 13:12:00",
      "unix_timestamp": 1703851920,
      "discussion_count": 9,
      "discussion": [
        {
          "poster": "thanhnv142",
          "upvote_count": "5",
          "content": "Selected Answer: B\nB is correct: < must be deployed one instance at a time> means codedeploy, which provides this option\nA: AWS Step Functions state machine does not provide deployment Functions\nC: Beanstalk does not work with EC2\nD: AWS SSM does not provides deployment Functions",
          "timestamp": "1707495720.0",
          "comment_id": "1145633"
        },
        {
          "comments": [
            {
              "upvote_count": "3",
              "content": "Sorry still B\nAlthough Beanstalk can use CPU utilization to auto scale\nBeanstalk can not use CPU utilization to roll back. The roll back is a manual re-deployment of the latest working version.",
              "timestamp": "1732512600.0",
              "poster": "tinyshare",
              "comment_id": "1317337"
            }
          ],
          "poster": "tinyshare",
          "content": "Selected Answer: C\nWhy not C? Beanstalk can use CPU utilization metric for Auto Scaling.",
          "timestamp": "1732511760.0",
          "upvote_count": "1",
          "comment_id": "1317333"
        },
        {
          "timestamp": "1722244080.0",
          "poster": "jamesf",
          "upvote_count": "2",
          "content": "Selected Answer: B\nB correct - CodeDeployDefault OneAtAtime configuration (One at a time, but not AllAtATime)",
          "comment_id": "1257360"
        },
        {
          "timestamp": "1719440820.0",
          "comment_id": "1237776",
          "poster": "Gomer",
          "upvote_count": "3",
          "content": "\"B\"\n\"You can now monitor and automatically react to changes in your AWS CodeDeploy deployments using Amazon CloudWatch alarms.\"\n\"Using CloudWatch with CodeDeploy, you can monitor metrics for Amazon EC2 instances or Auto Scaling groups being managed by CodeDeploy and then invoke an action if the metric you are tracking crosses a certain threshold for a defined period of time.\"\n\"You can monitor metrics such as instance CPU utilization.\"\n\"If the alarm is activated, CloudWatch initiates actions such as sending a notification to Amazon Simple Notification Service, stopping a CodeDeploy deployment, or changing the state of an instance (e.g. reboot, terminate, recover).\"\n\"CodeDeploy now also lets you automatically roll back a deployment when a deployment fails or when a CloudWatch alarm is activated.\"\nhttps://aws.amazon.com/about-aws/whats-new/2016/09/aws-codedeploy-introduces-deployment-monitoring-with-amazon-cloudwatch-alarms-and-automatic-deployment-rollback/"
        },
        {
          "upvote_count": "3",
          "comment_id": "1200894",
          "content": "Selected Answer: B\nBy using AWS CodeDeploy with Amazon EC2 Auto Scaling, configuring the CodeDeployDefault.OneAtAtime deployment strategy, and setting up automatic rollbacks based on a CloudWatch alarm for CPU utilization, this solution meets all the specified requirements. It ensures a controlled deployment process, monitors the CPU-intensive application, and automatically rolls back the deployment if the CPU utilization threshold is breached, providing a reliable and automated deployment lifecycle for the mission-critical application.",
          "timestamp": "1713894960.0",
          "poster": "c3518fc"
        },
        {
          "content": "Selected Answer: B\nanswer B",
          "comment_id": "1194852",
          "timestamp": "1713002580.0",
          "poster": "dkp",
          "upvote_count": "3"
        },
        {
          "comments": [
            {
              "content": "Beanstalk can not use CPU utilization to roll back. The roll back is a manual re-deployment of the latest working version.\nhttps://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html",
              "timestamp": "1732512960.0",
              "poster": "tinyshare",
              "upvote_count": "1",
              "comment_id": "1317338"
            }
          ],
          "timestamp": "1704886620.0",
          "upvote_count": "3",
          "content": "Why not BeansTalk?",
          "poster": "davdan99",
          "comment_id": "1118517"
        },
        {
          "content": "Selected Answer: B\nIt's B - the only one that fulfils all the requirements.",
          "poster": "csG13",
          "timestamp": "1703861580.0",
          "comment_id": "1108801",
          "upvote_count": "4"
        },
        {
          "upvote_count": "3",
          "timestamp": "1703851920.0",
          "content": "Selected Answer: B\nB is correct",
          "comment_id": "1108660",
          "poster": "PrasannaBalaji"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:10:50.026Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "HH7UsYm9LSzlIbgNo6SS",
      "question_number": 64,
      "page": 13,
      "question_text": "A company has a single developer writing code for an automated deployment pipeline. The developer is storing source code in an Amazon S3 bucket for each project. The company wants to add more developers to the team but is concerned about code conflicts and lost work. The company also wants to build a test environment to deploy newer versions of code for testing and allow developers to automatically deploy to both environments when code is changed in the repository.\n\nWhat is the MOST efficient way to meet these requirements?",
      "choices": {
        "D": "Enable versioning and branching on each S3 bucket, use the main branch for production code, and create a testing branch for code deployed to testing. Have developers use each branch for developing in each environment.",
        "A": "Create an AWS CodeCommit repository for each project, use the main branch for production code, and create a testing branch for code deployed to testing. Use feature branches to develop new features and pull requests to merge code to testing and main branches.",
        "B": "Create another S3 bucket for each project for testing code, and use an AWS Lambda function to promote code changes between testing and production buckets. Enable versioning on all buckets to prevent code conflicts.",
        "C": "Create an AWS CodeCommit repository for each project, and use the main branch for production and test code with different deployment pipelines for each environment. Use feature branches to develop new features."
      },
      "correct_answer": "A",
      "answer_ET": "A",
      "answers_community": [
        "A (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/129676-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-12-29 13:15:00",
      "unix_timestamp": 1703852100,
      "discussion_count": 8,
      "discussion": [
        {
          "timestamp": "1723213560.0",
          "upvote_count": "5",
          "comment_id": "1145637",
          "poster": "thanhnv142",
          "content": "Selected Answer: A\nA is correct: < The developer is storing source code in an Amazon S3 bucket for each project> and < The company wants to add more developers to the team but is concerned about code conflicts and lost work> means we need codecommit\nB and D: no mention of code commnit\nC: Should not use the main branch for both production and test"
        },
        {
          "comment_id": "1200896",
          "content": "Selected Answer: A\nBy leveraging AWS CodeCommit and following Git branching best practices, the company can efficiently manage code changes, facilitate collaboration among developers, and automate deployments to both production and testing environments. This solution provides a scalable and organized approach to software development and deployment, while minimizing the risk of code conflicts and lost work.",
          "timestamp": "1729706340.0",
          "poster": "c3518fc",
          "upvote_count": "3"
        },
        {
          "poster": "dkp",
          "content": "Selected Answer: A\nanswer is A",
          "upvote_count": "3",
          "comment_id": "1194915",
          "timestamp": "1728818940.0"
        },
        {
          "comment_id": "1175033",
          "content": "Selected Answer: A\nA is correct",
          "upvote_count": "3",
          "timestamp": "1726490460.0",
          "poster": "DanShone"
        },
        {
          "comment_id": "1121011",
          "timestamp": "1720801920.0",
          "poster": "a54b16f",
          "content": "Selected Answer: A\ntypical branching strategy",
          "upvote_count": "3"
        },
        {
          "comment_id": "1110391",
          "poster": "d262e67",
          "content": "Selected Answer: A\nDefinitely A. C lacks proper strategy for pull requests for code merging. In general, it's a proper patter in software development.",
          "timestamp": "1719729060.0",
          "upvote_count": "4"
        },
        {
          "timestamp": "1719665880.0",
          "comment_id": "1108809",
          "poster": "csG13",
          "content": "Selected Answer: A\nIt's A, since C suggests to merge to main branch non-production code. Although it's a working pattern, A is definitely more industry standard and safer.",
          "upvote_count": "4"
        },
        {
          "poster": "PrasannaBalaji",
          "content": "Correct answer is A.\nS3 not good choice , so eliminating B & D",
          "timestamp": "1719656100.0",
          "upvote_count": "2",
          "comment_id": "1108662"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:10:50.026Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "yEnXExRIg9dDCXTssqX9",
      "question_number": 65,
      "page": 13,
      "question_text": "A DevOps engineer notices that all Amazon EC2 instances running behind an Application Load Balancer in an Auto Scaling group are failing to respond to user requests. The EC2 instances are also failing target group HTTP health checks.\n\nUpon inspection, the engineer notices the application process was not running in any EC2 instances. There are a significant number of out of memory messages in the system logs. The engineer needs to improve the resilience of the application to cope with a potential application memory leak. Monitoring and notifications should be enabled to alert when there is an issue.\n\nWhich combination of actions will meet these requirements? (Choose two.)",
      "choices": {
        "D": "Enable the available memory consumption metric within the Amazon CloudWatch dashboard for the entire Auto Scaling group. Create an alarm when the memory utilization is high. Associate an Amazon SNS topic to the alarm to receive notifications when the alarm goes off.",
        "E": "Use the Amazon CloudWatch agent to collect the memory utilization of the EC2 instances in the Auto Scaling group. Create an alarm when the memory utilization is high and associate an Amazon SNS topic to receive a notification.",
        "C": "Change the target group health checks from HTTP to TCP to check if the port where the application is listening is reachable.",
        "A": "Change the Auto Scaling configuration to replace the instances when they fail the load balancer's health checks.",
        "B": "Change the target group health check HealthCheckIntervalSeconds parameter to reduce the interval between health checks."
      },
      "correct_answer": "AE",
      "answer_ET": "AE",
      "answers_community": [
        "AE (87%)",
        "13%"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/129679-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-12-29 13:25:00",
      "unix_timestamp": 1703852700,
      "discussion_count": 12,
      "discussion": [
        {
          "comment_id": "1145666",
          "timestamp": "1723218000.0",
          "upvote_count": "5",
          "content": "Selected Answer: AE\nA and E are correct: If there is failed instance, we should replace it with a new one to restart. Use cloudwatch agent to monitor metrics\nB: this does not do anything\nC: should not change from HTTP to TCP\nD: Cloudwatch work with agents, about which this option does not mention",
          "poster": "thanhnv142"
        },
        {
          "content": "Selected Answer: AE\nFirst choice is A, that is sure because one requirement is resiliency.\n\nFor second choice, D vs E:\nBy default, CloudWatch metrics for EC2 do not include memory utilization. We must install the CloudWatch agent to collect memory metrics which is mentioned in E. So E is correct.",
          "upvote_count": "1",
          "poster": "lovekiller",
          "timestamp": "1738673460.0",
          "comment_id": "1351350"
        },
        {
          "poster": "zijo",
          "upvote_count": "2",
          "comment_id": "1336184",
          "timestamp": "1735945080.0",
          "content": "Selected Answer: AE\nAn Auto Scaling Group (ASG) can replace EC2 instances by default when Application Load Balancer (ALB) health checks fail, but only if ALB health checks are explicitly enabled in the ASG configuration. By default, an ASG uses EC2 instance status checks for health monitoring. If you want the ASG to replace instances based on ALB health check results, you need to configure it to use ELB/ALB health checks."
        },
        {
          "upvote_count": "4",
          "content": "Selected Answer: BE\nfrom doc:\n\nWhen Amazon EC2 Auto Scaling determines that an InService instance is unhealthy, it replaces it with a new instance to maintain the desired capacity of the group.\n\nso you can't actually have a ASG that doesn't replace unhealthy instance so A doesn't make sense, but B makes sense since faster replacement will improve the resilience.",
          "poster": "xdkonorek2",
          "comment_id": "1206594",
          "timestamp": "1730748960.0"
        },
        {
          "content": "Selected Answer: AE\nAnswer is A&E",
          "timestamp": "1728819240.0",
          "poster": "dkp",
          "comment_id": "1194920",
          "upvote_count": "3"
        },
        {
          "poster": "DanShone",
          "upvote_count": "3",
          "content": "Selected Answer: AE\nA - Autoscaling to replace teh EC2s\nE - CloudWatch agent to monitor Memory",
          "timestamp": "1726490400.0",
          "comment_id": "1175028"
        },
        {
          "timestamp": "1726294260.0",
          "comment_id": "1173228",
          "upvote_count": "2",
          "content": "I choose AE, but I am not comfortable with E, because CW doesn't by default give memory metrics, it requires customization.",
          "poster": "Jaguaroooo"
        },
        {
          "timestamp": "1720802100.0",
          "poster": "a54b16f",
          "content": "Selected Answer: AE\nIt's A & E",
          "comment_id": "1121013",
          "upvote_count": "3"
        },
        {
          "poster": "davdan99",
          "upvote_count": "3",
          "content": "Selected Answer: AE\nWe don't have memory metrics for autoscaling, here is the list of metrics \nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/appinsights-metrics-ec2.html#appinsights-metrics-ec2-linux\n\nHere is the list of metrics from cloudwatch agent \n\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/appinsights-metrics-ec2.html#appinsights-metrics-ec2-linux",
          "comment_id": "1118533",
          "timestamp": "1720606680.0"
        },
        {
          "poster": "d262e67",
          "upvote_count": "2",
          "content": "Selected Answer: AE\nMemory monitoring requires an agent and auto scaling needs the self-healing feature turned on.",
          "comment_id": "1110393",
          "timestamp": "1719729240.0"
        },
        {
          "upvote_count": "3",
          "timestamp": "1719666180.0",
          "content": "Selected Answer: AE\nIt's A & E - no memory can be added in Cloudwatch dashboard, needs to be exported using Cloudwatch agent first.",
          "comment_id": "1108815",
          "poster": "csG13"
        },
        {
          "comment_id": "1108666",
          "timestamp": "1719656700.0",
          "poster": "PrasannaBalaji",
          "content": "Selected Answer: AE\nI'll go with A, E\nB is wrong because it does not talk about problem solving\nC is wrong because it does not talk about problem solving http/tcp \nD is wrong because of notifications when the alarm goes off",
          "upvote_count": "2"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:10:50.026Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "KvtAOii8XlMAwGoeaWup",
      "question_number": 66,
      "page": 14,
      "question_text": "An ecommerce company uses a large number of Amazon Elastic Block Store (Amazon EBS) backed Amazon EC2 instances. To decrease manual work across all the instances, a DevOps engineer is tasked with automating restart actions when EC2 instance retirement events are scheduled.\n\nHow can this be accomplished?",
      "choices": {
        "D": "Set up an AWS Health Amazon EventBridge rule to run AWS Systems Manager Automation runbooks that stop and start the EC2 instance when a retirement scheduled event occurs.",
        "B": "Enable EC2 Auto Recovery on all of the instances. Create an AWS Config rule to limit the recovery to occur during a maintenance window only.",
        "C": "Reboot all EC2 instances during an approved maintenance window that is outside of standard business hours. Set up Amazon CloudWatch alarms to send a notification in case any instance is failing EC2 instance status checks.",
        "A": "Create a scheduled Amazon EventBridge rule to run an AWS Systems Manager Automation runbook that checks if any EC2 instances are scheduled for retirement once a week. If the instance is scheduled for retirement, the runbook will hibernate the instance."
      },
      "correct_answer": "D",
      "answer_ET": "D",
      "answers_community": [
        "D (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/129680-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-12-29 13:37:00",
      "unix_timestamp": 1703853420,
      "discussion_count": 9,
      "discussion": [
        {
          "timestamp": "1723218780.0",
          "upvote_count": "7",
          "content": "Selected Answer: D\nD is correct: <automating restart actions when EC2 instance retirement events are scheduled>: these even occurs when the underlying infra that runs the instances needs to be repaired. To deal with this, just need to stop and start the instance to re-locate the instance to a different physical machine\nA: Hibernate would not work for this scenario\nB: This would also bring back the instance. However, AWS config rule cannot limit the recovery. It only reports, not actions\nC: This option is a manual work, not automated one",
          "poster": "thanhnv142",
          "comment_id": "1145674"
        },
        {
          "content": "Selected Answer: D\nD - Retirement relates to AWS Health",
          "timestamp": "1726490280.0",
          "poster": "DanShone",
          "upvote_count": "5",
          "comment_id": "1175025"
        },
        {
          "content": "Selected Answer: D\nBy leveraging AWS Health events, Amazon EventBridge, and AWS Systems Manager Automation runbooks, you can create an automated and event-driven solution that responds to EC2 instance retirement events in a timely and consistent manner, minimizing manual effort and reducing the risk of service disruptions.",
          "upvote_count": "5",
          "timestamp": "1729737060.0",
          "poster": "c3518fc",
          "comment_id": "1200900"
        },
        {
          "timestamp": "1728820380.0",
          "comment_id": "1194929",
          "content": "Selected Answer: D\nanswer D",
          "poster": "dkp",
          "upvote_count": "3"
        },
        {
          "timestamp": "1722557460.0",
          "comment_id": "1138053",
          "content": "D is correct",
          "poster": "sarlos",
          "upvote_count": "2"
        },
        {
          "upvote_count": "3",
          "comment_id": "1121014",
          "timestamp": "1720802160.0",
          "poster": "a54b16f",
          "content": "Selected Answer: D\nretirement is tied with Health"
        },
        {
          "content": "Answer is D. Once a week is a joke :D",
          "timestamp": "1719775320.0",
          "comment_id": "1110818",
          "poster": "kabary",
          "upvote_count": "3"
        },
        {
          "poster": "csG13",
          "timestamp": "1719666360.0",
          "content": "Selected Answer: D\nIt 's D",
          "comment_id": "1108822",
          "upvote_count": "3"
        },
        {
          "poster": "PrasannaBalaji",
          "content": "Selected Answer: D\nD is correct\nhttps://aws.amazon.com/blogs/mt/automate-remediation-actions-for-amazon-ec2-notifications-and-beyond-using-ec2-systems-manager-automation-and-aws-health/",
          "timestamp": "1719657420.0",
          "upvote_count": "3",
          "comment_id": "1108671"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:11:00.513Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "rF68VUHlFXmKFzIgCuvE",
      "question_number": 67,
      "page": 14,
      "question_text": "A company manages AWS accounts for application teams in AWS Control Tower. Individual application teams are responsible for securing their respective AWS accounts.\n\nA DevOps engineer needs to enable Amazon GuardDuty for all AWS accounts in which the application teams have not already enabled GuardDuty. The DevOps engineer is using AWS CloudFormation StackSets from the AWS Control Tower management account.\n\nHow should the DevOps engineer configure the CloudFormation template to prevent failure during the StackSets deployment?",
      "choices": {
        "D": "Manually discover the list of AWS account IDs where GuardDuty is not enabled. Use the CloudFormation Fn::ImportValue intrinsic function to import the list of account IDs into the CloudFormation template to skip deployment for the listed AWS accounts.",
        "B": "Use the Conditions section of the CloudFormation template to enable GuardDuty in accounts where GuardDuty is not already enabled.",
        "A": "Create a CloudFormation custom resource that invokes an AWS Lambda function. Configure the Lambda function to conditionally enable GuardDuty if GuardDuty is not already enabled in the accounts.",
        "C": "Use the CloudFormation Fn::GetAtt intrinsic function to check whether GuardDuty is already enabled. If GuardDuty is not already enabled, use the Resources section of the CloudFormation template to enable GuardDuty."
      },
      "correct_answer": "A",
      "answer_ET": "A",
      "answers_community": [
        "A (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/129706-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-12-29 16:10:00",
      "unix_timestamp": 1703862600,
      "discussion_count": 7,
      "discussion": [
        {
          "timestamp": "1723220700.0",
          "upvote_count": "7",
          "comment_id": "1145689",
          "content": "Selected Answer: A\nA is correct: <configure the CloudFormation template> is the requirement of the question. By default, cloudformation doesnot support turning on Guarduty. To turn it on, need to use ACF template in combination with lambda. \nA: perfectly correct\nB: no mention of lambda\nC: Fn::GetAtt intrinsic: This is used to check only. No mention of using lambda to enable Guarduty\nD: This might work. However, a manual approach is not recommeded",
          "poster": "thanhnv142"
        },
        {
          "comment_id": "1110397",
          "upvote_count": "5",
          "timestamp": "1719729600.0",
          "poster": "d262e67",
          "content": "Selected Answer: A\nA is correct. Conditions are designed to decide whether or not create resources. GetAtt is to retrieve the value of an attribute from a resource in the same template. and manual processes are usually not good."
        },
        {
          "comment_id": "1194936",
          "timestamp": "1728820740.0",
          "content": "Selected Answer: A\nanswer A",
          "upvote_count": "2",
          "poster": "dkp"
        },
        {
          "comment_id": "1175023",
          "poster": "DanShone",
          "content": "Selected Answer: A\nA is correct",
          "timestamp": "1726490160.0",
          "upvote_count": "2"
        },
        {
          "content": "Selected Answer: A\nstandard pattern: use lambda to conditional DO something",
          "comment_id": "1121015",
          "poster": "a54b16f",
          "upvote_count": "2",
          "timestamp": "1720802280.0"
        },
        {
          "timestamp": "1719746280.0",
          "poster": "PrasannaBalaji",
          "upvote_count": "2",
          "content": "Selected Answer: A\nA is correct",
          "comment_id": "1109406"
        },
        {
          "comment_id": "1108829",
          "timestamp": "1719666600.0",
          "upvote_count": "5",
          "content": "Selected Answer: A\nIt's a standard pattern, so A\n\nHere is a reference: \nhttps://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/enable-amazon-guardduty-conditionally-by-using-aws-cloudformation-templates.html",
          "poster": "csG13"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:11:00.513Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "9EnTaeKYQsdA4ppB9Tso",
      "question_number": 68,
      "page": 14,
      "question_text": "A development team is using AWS CodeCommit to version control application code and AWS CodePipeline to orchestrate software deployments. The team has decided to use a remote main branch as the trigger for the pipeline to integrate code changes. A developer has pushed code changes to the CodeCommit repository, but noticed that the pipeline had no reaction, even after 10 minutes.\nWhich of the following actions should be taken to troubleshoot this issue?",
      "choices": {
        "D": "Check to see if the pipeline failed to start because of CodeCommit errors in Amazon CloudWatch Logs.",
        "B": "Check that the CodePipeline service role has permission to access the CodeCommit repository.",
        "C": "Check that the developer’s IAM role has permission to push to the CodeCommit repository.",
        "A": "Check that an Amazon EventBridge rule has been created for the main branch to trigger the pipeline."
      },
      "correct_answer": "A",
      "answer_ET": "A",
      "answers_community": [
        "A (60%)",
        "B (37%)",
        "3%"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/105519-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-04-07 15:39:00",
      "unix_timestamp": 1680874740,
      "discussion_count": 53,
      "discussion": [
        {
          "upvote_count": "21",
          "poster": "Dushank",
          "comment_id": "1009523",
          "content": "Selected Answer: B\nA: EventBridge rules are not a requirement for CodePipeline to trigger from a CodeCommit repository. CodePipeline directly integrates with CodeCommit without needing EventBridge.\n\nB: is a likely cause. The CodePipeline service role needs permissions to access the CodeCommit repository in order to start the pipeline execution when new code is pushed.\n\nC:If the developer was able to push code changes to the CodeCommit repository, then their IAM role permissions with respect to CodeCommit are likely fine. This isn't the issue.\n\nD:If the pipeline didn't start, CloudWatch Logs could give insights. However, these logs will only exist if the pipeline actually attempted to start but failed. If the pipeline never started, checking logs won't help.\n\nGiven these options, Option B: is the correct answer.",
          "timestamp": "1694920320.0"
        },
        {
          "upvote_count": "13",
          "poster": "a54b16f",
          "comment_id": "1123373",
          "timestamp": "1705323840.0",
          "content": "Selected Answer: A\nB would throw out \"Permission denied\" error immediately, rather than no reaction for 10 minutes."
        },
        {
          "comment_id": "1561008",
          "upvote_count": "2",
          "poster": "MarcosSantos",
          "content": "Selected Answer: A\nGood question! I'm used CodePipeline in a particular lab in last year and now I checked the existence of eventbridge rules created by Codepipeline, impressive.",
          "timestamp": "1744763520.0"
        },
        {
          "comment_id": "1559992",
          "content": "Selected Answer: A\nCodePipeline uses Amazon EventBridge (formerly CloudWatch Events) to trigger a pipeline when changes are pushed to a source like CodeCommit.\nIf no EventBridge rule is in place (or it's misconfigured), the pipeline won’t be triggered, even if code was pushed to the correct branch.\nThe rule must match the repository name, branch, and event type (e.g., CodeCommit Repository State Change → referenceUpdated).",
          "poster": "Srikantha",
          "upvote_count": "1",
          "timestamp": "1744417440.0"
        },
        {
          "upvote_count": "2",
          "poster": "spring21",
          "content": "Selected Answer: A\nChange detection:\nWhen you set up a CodePipeline with a CodeCommit source, the default behavior is to use an EventBridge rule to detect changes in the repository, eliminating the need for manual polling mechanisms",
          "timestamp": "1734980100.0",
          "comment_id": "1330910"
        },
        {
          "content": "Selected Answer: B\nThe question states that the developer has pushed the change to the repository, which means CodeCommit has no issue, the pipeline doesn't rely on EvertBridge to trigger.",
          "comment_id": "1328662",
          "timestamp": "1734548400.0",
          "upvote_count": "1",
          "poster": "ZinggieG87"
        },
        {
          "poster": "Serial_X25",
          "upvote_count": "1",
          "timestamp": "1731587760.0",
          "comment_id": "1311915",
          "content": "Selected Answer: A\nCodePipeline connects to third-party source providers directly using CodeConnections, https://docs.aws.amazon.com/codepipeline/latest/userguide/pipelines-connections.html, but it should use EventBridge for CodeCommit, https://docs.aws.amazon.com/codepipeline/latest/userguide/triggering.html."
        },
        {
          "comment_id": "1308633",
          "timestamp": "1731037680.0",
          "content": "Selected Answer: A\nA. Check that an Amazon EventBridge rule has been created for the main branch to trigger the pipeline.\nThis approach directly addresses the most likely cause: a missing or misconfigured EventBridge rule that prevents CodePipeline from starting in response to changes in the CodeCommit repository.",
          "upvote_count": "1",
          "poster": "Jonalb"
        },
        {
          "poster": "jamesf",
          "comment_id": "1254767",
          "timestamp": "1721888940.0",
          "upvote_count": "3",
          "content": "Selected Answer: A\nA - EventBridge rule is one of the recommended ways to configure CodePipeline to automatically trigger based on changes in a CodeCommit repository\n\nB - if \"Permission denied\", the error message should prompt immediately, rather than no reaction for 10 minutes for the pipeline. Mean the pipeline not even start\n\nReference:\nhttps://docs.aws.amazon.com/codepipeline/latest/userguide/pipelines-about-starting.html#change-detection-methods\nhttps://docs.aws.amazon.com/codepipeline/latest/userguide/tutorials-simple-codecommit.html"
        },
        {
          "poster": "trungtd",
          "content": "Selected Answer: A\nEventBridge rule is one of the recommended ways to configure CodePipeline to automatically trigger based on changes in a CodeCommit repository",
          "timestamp": "1721086740.0",
          "comment_id": "1248603",
          "upvote_count": "2"
        },
        {
          "timestamp": "1719563760.0",
          "upvote_count": "1",
          "comment_id": "1238639",
          "content": "Selected Answer: A\nFor CodePipeline to be triggered by changes in a CodeCommit repository, an EventBridge rule (formerly CloudWatch Events rule) needs to be set up. This rule listens for specific events (like commits to the main branch) and triggers the pipeline accordingly.",
          "poster": "Mordans"
        },
        {
          "timestamp": "1719358140.0",
          "comment_id": "1237130",
          "content": "Selected Answer: B\nIt's B.",
          "poster": "aefuen1",
          "upvote_count": "1",
          "comments": [
            {
              "content": "The answer is not B because if the CodePipeline service linked role didn't have permissions to access CodeCommit, you will get a \"Permissions denied\" error immediately but the question said you didn't get any reaction in 10 minutes so the only possible scenario we would be dealing with here is not having an EventBridge rule that triggers the pipeline. When you use the console to create or edit a pipeline, the change detection resources are created for you. If you use the AWS CLI to create the pipeline, you must create the additional resources yourself. \n\nReference: https://docs.aws.amazon.com/codepipeline/latest/userguide/pipelines-create.html",
              "timestamp": "1723811940.0",
              "poster": "flaacko",
              "upvote_count": "1",
              "comment_id": "1267068"
            }
          ]
        },
        {
          "timestamp": "1718524920.0",
          "content": "Selected Answer: A\nthe answer is A because if codepipeline has no access to codecommit pipeline is triggered and source stage fails with:\n```\nThe service role or action role doesn’t have the permissions required to access the AWS CodeCommit repository named random-repo. Update the IAM role permissions, and then try again\n```",
          "comment_id": "1231256",
          "upvote_count": "1",
          "poster": "xdkonorek2"
        },
        {
          "poster": "k23319",
          "timestamp": "1717106100.0",
          "comment_id": "1221832",
          "upvote_count": "1",
          "content": "Selected Answer: B\nB is right."
        },
        {
          "comments": [
            {
              "comment_id": "1214024",
              "poster": "vn_thanhtung",
              "content": "B wrong.",
              "upvote_count": "2",
              "timestamp": "1716161460.0"
            }
          ],
          "upvote_count": "2",
          "comment_id": "1210730",
          "content": "Selected Answer: B\nJust voting to fix the results, because clearly its B, as explained by top 2 comments here.",
          "timestamp": "1715579520.0",
          "poster": "liuyomz"
        },
        {
          "comment_id": "1209528",
          "poster": "c3518fc",
          "timestamp": "1715373480.0",
          "content": "Selected Answer: B\nThe first step in troubleshooting this issue should be to check that the CodePipeline service role has the required permissions to access the CodeCommit repository. If the permissions are correct, then you can proceed with other troubleshooting steps, such as checking the CloudWatch Logs for any errors or failures.",
          "upvote_count": "1"
        },
        {
          "comment_id": "1193828",
          "comments": [
            {
              "poster": "vn_thanhtung",
              "upvote_count": "2",
              "comments": [
                {
                  "upvote_count": "2",
                  "poster": "vn_thanhtung",
                  "content": "When you create a pipeline from CodePipeline during the step-by-step it creates a CloudWatch Event rule for a given branch and repo\nlike this:\n{\n\"source\": [\n\"aws.codecommit\"\n],\n\"detail-type\": [\n\"CodeCommit Repository State Change\"\n],\n\"resources\": [\n\"arn:aws:codecommit:us-east-1:xxxxx:repo-name\"\n],\n\"detail\": {\n\"event\": [\n\"referenceCreated\",\n\"referenceUpdated\"\n],\n\"referenceType\": [\n\"branch\"\n],\n\"referenceName\": [\n\"master\"\n]\n}",
                  "comment_id": "1205459",
                  "timestamp": "1714647540.0"
                }
              ],
              "timestamp": "1714647480.0",
              "comment_id": "1205458",
              "content": "You are wrong, correct answer is A"
            }
          ],
          "content": "Selected Answer: B\nNot sure why most people here are even considering A. CodePipeline does not use Amazon EventBridge to trigger pipeline executions based on changes in CodeCommit repositories. Instead, it directly integrates with CodeCommit and monitors repository changes internally.",
          "poster": "c3518fc",
          "upvote_count": "4",
          "timestamp": "1712841240.0"
        },
        {
          "upvote_count": "4",
          "timestamp": "1711108740.0",
          "poster": "ogerber",
          "comment_id": "1180038",
          "content": "Selected Answer: A\n\"After you select the repository name and branch, a message displays the Amazon CloudWatch Events rule to be created for this pipeline.\"\nhttps://docs.aws.amazon.com/codepipeline/latest/userguide/tutorials-simple-codecommit.html"
        },
        {
          "poster": "alexleely",
          "upvote_count": "3",
          "timestamp": "1710740820.0",
          "comment_id": "1176282",
          "content": "Selected Answer: A\nI highly believe it is A.\nEven though CodePipeline directly integrates with CodeCommit, this integration automatically creates a EventBridge rule for you if it is created through the console. \nhttps://docs.aws.amazon.com/codepipeline/latest/userguide/pipelines-trigger-source-repo-changes-console.html\nSince we know that there is no reaction from the pipeline, it would mean that it wasn't triggered at all. \n\nB is about permission which would have thrown an error in the console at that stage, but to even start the first stage, it needs to be trigger first which for the case here.\nC shouldn't be the answer as the question already said that it was pushed into the repository."
        },
        {
          "upvote_count": "1",
          "content": "Selected Answer: B\nBBBBBBBBBBBBB",
          "timestamp": "1710648780.0",
          "comment_id": "1175576",
          "poster": "cas_tori"
        },
        {
          "content": "Selected Answer: A\nAmazon EventBridge (recommended). This is the default for pipelines with an CodeCommit source created or edited in the console.\nhttps://docs.aws.amazon.com/codepipeline/latest/userguide/pipelines-about-starting.html#change-detection-methods",
          "comment_id": "1162800",
          "upvote_count": "3",
          "poster": "vmahilevskyi",
          "timestamp": "1709224620.0"
        },
        {
          "upvote_count": "2",
          "comment_id": "1161600",
          "poster": "Vitalydt",
          "timestamp": "1709122320.0",
          "content": "Selected Answer: A\nA is most accurate IMO"
        },
        {
          "poster": "zijo",
          "comment_id": "1161019",
          "upvote_count": "2",
          "content": "I think the answer is B\nB. Check that the CodePipeline service role has permission to access the CodeCommit repository.\nBy default CodePipeline polls repository for changes even if there is no EventBridge configured. But if the CodePipeline Service Role has no permissions to access the CodeCommit repository it will throw error and CodePipeline will have no reaction.",
          "timestamp": "1709071440.0"
        },
        {
          "content": "Selected Answer: A\nhttps://docs.aws.amazon.com/codepipeline/latest/userguide/pipelines-about-starting.html",
          "poster": "vortegon",
          "timestamp": "1706653020.0",
          "comment_id": "1136222",
          "upvote_count": "1"
        },
        {
          "poster": "thanhnv142",
          "upvote_count": "4",
          "comment_id": "1133828",
          "content": "A is correct\nD: irrelevant\nB and C is wrong definitely: if there were permission errors, the pipeline should have started and threw out error messages. The question states that no reaction from the pipeline, which indicates it never started.",
          "timestamp": "1706416800.0"
        },
        {
          "timestamp": "1706416200.0",
          "content": "A is good: If lack of permissions occur, there is error noti immediately",
          "comment_id": "1133822",
          "upvote_count": "2",
          "poster": "thanhnv142"
        },
        {
          "timestamp": "1705492020.0",
          "content": "Selected Answer: A\nB. CodePipeline service role's permissions: If this were the problem, errors would likely occur during pipeline execution, not trigger failure.",
          "upvote_count": "2",
          "comment_id": "1124917",
          "poster": "promo286"
        },
        {
          "poster": "Jay_2pt0_1",
          "timestamp": "1704021960.0",
          "content": "The answer should be 'B'. EventBridget shouldn't even be a factor.",
          "upvote_count": "1",
          "comment_id": "1110482"
        },
        {
          "poster": "svjl",
          "comment_id": "1078472",
          "timestamp": "1700748660.0",
          "upvote_count": "2",
          "content": "A- CodeCommit uses EventBridge by default: https://docs.aws.amazon.com/codepipeline/latest/userguide/pipelines-about-starting.html"
        },
        {
          "upvote_count": "3",
          "content": "Selected Answer: A\nOption A focuses on verifying the setup of the trigger for the pipeline, particularly checking if the Amazon EventBridge rule (or any other trigger mechanism) for the main branch has been correctly configured to initiate the pipeline upon changes. This directly targets the trigger mechanism and its association with the repository, which is the likely cause of the issue in this context.",
          "poster": "koenigParas2324",
          "comment_id": "1075415",
          "timestamp": "1700486220.0"
        },
        {
          "poster": "zolthar_z",
          "upvote_count": "2",
          "content": "Selected Answer: A\nA: The questions indicates that the pipeline was running ok, but it failed once the trigger branch was changed to main",
          "timestamp": "1700246580.0",
          "comment_id": "1073550"
        },
        {
          "content": "Selected Answer: A\nSince everything was working and it is no longer working, it is A .\nThere are some new limitation that inactive pipeline can only be triggered manually or from eventbridge",
          "poster": "hoomaan",
          "comment_id": "1066421",
          "upvote_count": "1",
          "timestamp": "1699535160.0"
        },
        {
          "upvote_count": "3",
          "timestamp": "1698325260.0",
          "poster": "rlf",
          "comment_id": "1054582",
          "content": "Answer is A. \nhttps://docs.aws.amazon.com/codepipeline/latest/userguide/triggering.html\nThe change detection method defaults to starting the pipeline by polling the source. You should disable periodic checks and create the change detection rule manually."
        },
        {
          "content": "Selected Answer: A\nA: is correct because It says that the team uses a remote branch, CodeCommit is not directly integrated in the pipeline => So we need an EventBridge rule to trigger the pipeline => Hence we should troubleshoot the event rule => A",
          "timestamp": "1698237300.0",
          "poster": "Cervus18",
          "upvote_count": "1",
          "comment_id": "1053718"
        },
        {
          "content": "Selected Answer: A\nAWS CodeCommit and AWS CodePipeline have been in use and working together, it's reasonable to assume that the service role for CodePipeline is already correctly configured to access CodeCommit, which would rule out option B.\n\nIn that case, option A becomes a more likely culprit. If the team has just decided to use a remote main branch as the trigger for the pipeline, they would need to set up an Amazon EventBridge rule to listen for changes on that branch and trigger the pipeline. The pipeline would not be triggered if this rule is missing or misconfigured, even if changes were pushed to the main branch.\n\nSo, in this context, option A is the most likely issue to investigate.",
          "comment_id": "1004263",
          "upvote_count": "2",
          "poster": "Jonfernz",
          "timestamp": "1694373180.0"
        },
        {
          "poster": "Aestebance",
          "upvote_count": "1",
          "timestamp": "1694083920.0",
          "content": "Selected Answer: A\nIf would be B, then user would receive an error. Correct is A.",
          "comment_id": "1001438"
        },
        {
          "upvote_count": "2",
          "comment_id": "997747",
          "content": "Selected Answer: B\nB. Check that the CodePipeline service role has permission to access the CodeCommit repository.\nOption A is incorrect because an Amazon EventBridge rule is not required to trigger a CodePipeline pipeline. The pipeline can be triggered by a variety of events, including changes to a CodeCommit repository.\n\nOption C is incorrect because the developer's IAM role only needs permission to push to the CodeCommit repository. The CodePipeline service role needs permission to access the CodeCommit repository in order to trigger the pipeline.\n\nOption D is incorrect because the pipeline logs would only show errors if the pipeline actually started. Since the pipeline did not start, there would be no errors in the logs.",
          "poster": "BaburTurk",
          "timestamp": "1693755720.0"
        },
        {
          "content": "Selected Answer: D\nThe first step in troubleshooting this issue is to check to see if the pipeline failed to start because of CodeCommit errors in Amazon CloudWatch Logs.",
          "upvote_count": "1",
          "timestamp": "1693141200.0",
          "poster": "jason7",
          "comment_id": "991462"
        },
        {
          "poster": "SVGoogle89",
          "timestamp": "1691431980.0",
          "upvote_count": "1",
          "comment_id": "974894",
          "content": "A\nfor B, it would error out"
        },
        {
          "poster": "gigi_devops",
          "content": "B is false because the statement says ''the pipeline did not react''. A permission issue would generate an error message at the pipeline level.",
          "comment_id": "968624",
          "timestamp": "1690849320.0",
          "upvote_count": "2"
        },
        {
          "upvote_count": "1",
          "timestamp": "1688913060.0",
          "content": "It´s a. Show that tutorial :) https://awstut.com/en/2022/08/14/use-codepipeline-to-trigger-codecommit-pushes-to-push-images-to-ecr-en/",
          "comment_id": "947332",
          "poster": "Just_Ninja"
        },
        {
          "comment_id": "921271",
          "upvote_count": "4",
          "poster": "Mail1964",
          "content": "Selected Answer: A\nThe Q states that they are already using codecommit and code pipeline, so do we assume from that, the service role is already configured without issues which rules out B, so it is A.",
          "timestamp": "1686560220.0",
          "comments": [
            {
              "timestamp": "1691314920.0",
              "comment_id": "973690",
              "upvote_count": "2",
              "poster": "Aja1",
              "content": "If the EventBridge rule exists and is configured correctly, the pipeline should be triggered when the developer pushes code changes to the main branch."
            }
          ]
        },
        {
          "timestamp": "1686223440.0",
          "poster": "madperro",
          "upvote_count": "4",
          "comment_id": "918186",
          "comments": [
            {
              "content": "It says nothing was triggered.",
              "comment_id": "1209525",
              "timestamp": "1715372880.0",
              "poster": "c3518fc",
              "upvote_count": "1"
            }
          ],
          "content": "Selected Answer: A\nCodePipeline creates a service role when creating a pipeline so B is unlikely. But it need EventBridge rule to trigger pipeline after CodeCommit changes.\nA is a correct answer.\nhttps://docs.aws.amazon.com/codepipeline/latest/userguide/pipelines-about-starting.html\nhttps://docs.aws.amazon.com/codepipeline/latest/userguide/triggering.html"
        },
        {
          "content": "Selected Answer: D\nOption A, checking that an Amazon EventBridge rule has been created for the main branch to trigger the pipeline, is not the correct action in this scenario.\nOption B, checking The pipeline's service role grants it the necessary permissions, and if there were permission issues, an error would be encountered when attempting to create or configure the pipeline.\nOption C, checking that the developer's IAM role has permission to push to the CodeCommit repository, is also not directly related to the issue. The pipeline triggering is based on code changes to the repository, \nOption D is the correct action because checking the CloudWatch Logs can help identify any errors or issues related to CodeCommit. It's possible that the pipeline failed to start due to errors in accessing or retrieving code changes from the CodeCommit repository. Reviewing the CloudWatch Logs associated with the pipeline can provide insights into the underlying cause of the issue.",
          "timestamp": "1685607180.0",
          "poster": "Bassel",
          "comment_id": "911882",
          "upvote_count": "2"
        },
        {
          "poster": "bcx",
          "timestamp": "1685456820.0",
          "upvote_count": "2",
          "content": "Selected Answer: A\nB is incorrect because you have to trigger pipeline build first in order for CodePipeline to start trying to access the CodeCommit repository. the question makes it clear that the pipeline is not started at all, it literally says that the pipeline had no reaction! An error because the pipeline cannot access the repository would be a reaction.",
          "comment_id": "910317"
        },
        {
          "comment_id": "906402",
          "content": "Selected Answer: A\nA is the correct answer.\n\nThe permission only be checked in the \"Source\" stage, the pipeline didn't react to the code changes which means the issue occurs in the trigger, so EventBridge should be first checked in this case.",
          "timestamp": "1684992480.0",
          "poster": "qan1257",
          "upvote_count": "2"
        },
        {
          "content": "Selected Answer: B\nB is the correct answer.",
          "upvote_count": "2",
          "poster": "ParagSanyashiv",
          "timestamp": "1683523620.0",
          "comment_id": "891832"
        },
        {
          "upvote_count": "2",
          "content": "Selected Answer: B\nB. Check that the CodePipeline service role has permission to access the CodeCommit repository.\n\nIf the pipeline did not react to the code changes pushed to the CodeCommit repository, it is likely that the CodePipeline service role does not have sufficient permissions to access the repository. This can be resolved by granting the role the necessary permissions to access the repository. Checking that an Amazon EventBridge rule has been created for the main branch to trigger the pipeline (option A) is also important, but only after ensuring that the CodePipeline service role has the required permissions.",
          "timestamp": "1682986260.0",
          "poster": "haazybanj",
          "comment_id": "886849"
        },
        {
          "timestamp": "1681503600.0",
          "comment_id": "870456",
          "upvote_count": "1",
          "content": "B. Check that the CodePipeline service role has permission to access the CodeCommit repository",
          "poster": "alce2020"
        },
        {
          "poster": "jqso234",
          "timestamp": "1681493220.0",
          "upvote_count": "2",
          "comment_id": "870361",
          "content": "Here is the link to the whitepaper:\nhttps://d1.awsstatic.com/whitepapers/DevOps/practices-for-deploying-aws-codepipeline.pdf\n\nTherefore, option B, which recommends checking that the CodePipeline service role has permission to access the CodeCommit repository, is the correct action to take to troubleshoot this issue."
        },
        {
          "poster": "henryyvr",
          "comment_id": "869777",
          "content": "Selected Answer: A\nNo errors on the pipeline because EventBridge did not successfully target it",
          "upvote_count": "1",
          "timestamp": "1681420980.0"
        },
        {
          "comment_id": "868714",
          "timestamp": "1681324980.0",
          "upvote_count": "3",
          "content": "Selected Answer: B\nI think it's B CodePipeline automatically triggers CodeDeploy, so you don't need to use EventBridge.",
          "poster": "ataince"
        },
        {
          "content": "Selected Answer: A\nopt for A, even B is also possible",
          "timestamp": "1680874740.0",
          "upvote_count": "1",
          "comment_id": "863935",
          "poster": "ele"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:11:00.513Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "AVpSTh1TiotSRAlbEKQr",
      "question_number": 69,
      "page": 14,
      "question_text": "A company has an AWS Control Tower landing zone. The company's DevOps team creates a workload OU. A development OU and a production OU are nested under the workload OU. The company grants users full access to the company's AWS accounts to deploy applications.\n\nThe DevOps team needs to allow only a specific management IAM role to manage the IAM roles and policies of any AWS accounts in only the production OU.\n\nWhich combination of steps will meet these requirements? (Choose two.)",
      "choices": {
        "E": "Create an SCP that denies IAM related actions with a condition to exclude the management IAM role. Attach the SCP to the production OU.",
        "D": "Create an SCP that denies IAM related actions with a condition to exclude the management IAM role. Attach the SCP to the workload OU.",
        "B": "Ensure that the FullAWSAccess SCP is applied at the organization root.",
        "C": "Create an SCP that allows IAM related actions. Attach the SCP to the development OU.",
        "A": "Create an SCP that denies full access with a condition to exclude the management IAM role for the organization root."
      },
      "correct_answer": "BE",
      "answer_ET": "BE",
      "answers_community": [
        "BE (93%)",
        "7%"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/129829-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-12-30 06:02:00",
      "unix_timestamp": 1703912520,
      "discussion_count": 10,
      "discussion": [
        {
          "content": "Selected Answer: BE\nYou need to understand how SCP inheritance works in AWS. The way it works for Deny policies is different that allow policies. \n\nAllow polices are passing down to children ONLY if they don't have an allow policy.\n\nDeny policies always pass down to children.\n\nThat's why there is always an SCP set to the Root to allow everything by default. If you limit this policy, the whole organization will be limited, not matter what other policies are saying for the other OUs. So it's not A. It's not D because it restricts the wrong OU.",
          "upvote_count": "11",
          "comment_id": "1110401",
          "timestamp": "1704012840.0",
          "poster": "d262e67"
        },
        {
          "timestamp": "1715174700.0",
          "comments": [
            {
              "comment_id": "1317346",
              "content": "The organization root is NOT the top level OU. The question specifies the workload OU has full access, but not the organization root. So you still need B.",
              "timestamp": "1732516200.0",
              "upvote_count": "1",
              "poster": "tinyshare"
            }
          ],
          "content": "CE\nFullAWSAccess is applied be default, no need to check it since the question did not say it has been removed.\nFor an Action to be permitted it has to be allowed from the Root OUs all the way to the accounts.",
          "upvote_count": "1",
          "comment_id": "1208398",
          "poster": "MalonJay"
        },
        {
          "content": "Selected Answer: BE\nANS: B&E",
          "upvote_count": "2",
          "poster": "dkp",
          "timestamp": "1713009780.0",
          "comment_id": "1194937"
        },
        {
          "timestamp": "1710599700.0",
          "content": "Selected Answer: BE\nB & E are correct",
          "upvote_count": "2",
          "comment_id": "1175022",
          "poster": "DanShone"
        },
        {
          "comment_id": "1149199",
          "upvote_count": "3",
          "timestamp": "1707832980.0",
          "poster": "Ramdi1",
          "content": "Selected Answer: BE\nB and E are correct because he requirement for dev ou user should still be able to do what they need to"
        },
        {
          "upvote_count": "3",
          "poster": "thanhnv142",
          "comment_id": "1145695",
          "timestamp": "1707503580.0",
          "content": "Selected Answer: BE\nB and E are correct: \nA: this does not make sense. It would mess with permissions for all OUs\nC: The question requires <only the production OU>: we need to target the production OU, not development OU\nD: <Attach the SCP to the workload OU>: we need to target only the production OU. This option affects both dev and prod OUS"
        },
        {
          "content": "B & E it is",
          "timestamp": "1705749540.0",
          "comment_id": "1127232",
          "upvote_count": "1",
          "poster": "denccc"
        },
        {
          "upvote_count": "2",
          "comment_id": "1121020",
          "content": "Selected Answer: BE\nA is wrong, we only want to limit production OU, development OU users should be able to do anything",
          "timestamp": "1705084980.0",
          "poster": "a54b16f"
        },
        {
          "content": "Selected Answer: BE\nAnswer is B & E.\n\nA is not correct because it would prevent the developers team to access the Developer OU. That wouldn't make sense.",
          "upvote_count": "2",
          "poster": "kabary",
          "comment_id": "1110830",
          "timestamp": "1704059400.0"
        },
        {
          "comment_id": "1109409",
          "poster": "PrasannaBalaji",
          "upvote_count": "2",
          "content": "Selected Answer: AE\nA and E",
          "timestamp": "1703912520.0"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:11:00.513Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "onxFvYchrMYWk5fhsUC5",
      "question_number": 70,
      "page": 14,
      "question_text": "A company hired a penetration tester to simulate an internal security breach. The tester performed port scans on the company's Amazon EC2 instances. The company's security measures did not detect the port scans.\n\nThe company needs a solution that automatically provides notification when port scans are performed on EC2 instances. The company creates and subscribes to an Amazon Simple Notification Service (Amazon SNS) topic.\n\nWhat should the company do next to meet the requirement?",
      "choices": {
        "C": "Ensure that Amazon Inspector is enabled. Create an Amazon EventBridge event for detected CVEs that cause open port vulnerabilities. Connect the event to the SNS topic.",
        "A": "Ensure that Amazon GuardDuty is enabled. Create an Amazon CloudWatch alarm for detected EC2 and port scan findings. Connect the alarm to the SNS topic.",
        "B": "Ensure that Amazon Inspector is enabled. Create an Amazon EventBridge event for detected network reachability findings that indicate port scans. Connect the event to the SNS topic.",
        "D": "Ensure that AWS CloudTrail is enabled. Create an AWS Lambda function to analyze the CloudTrail logs for unusual amounts of traffic from an IP address range. Connect the Lambda function to the SNS topic."
      },
      "correct_answer": "A",
      "answer_ET": "A",
      "answers_community": [
        "A (96%)",
        "4%"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/129708-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-12-29 16:32:00",
      "unix_timestamp": 1703863920,
      "discussion_count": 11,
      "discussion": [
        {
          "upvote_count": "2",
          "poster": "YucelFuat",
          "comment_id": "1280121",
          "content": "Selected Answer: A\n- GuardDuty is focused on real-time threat detection and alerting, while Inspector is focused on vulnerability scanning and remediation. \n- GuardDuty operates continuously in the background, whereas Inspector is typically run on-demand or scheduled for specific workloads.",
          "timestamp": "1725738600.0"
        },
        {
          "timestamp": "1724247240.0",
          "upvote_count": "1",
          "poster": "flaacko",
          "content": "The Answer is A. C is wrong because while you can use Inspector to detect open ports and software vulnerabilities, you can't use it to detect port scanning.",
          "comment_id": "1270154"
        },
        {
          "upvote_count": "2",
          "timestamp": "1719534720.0",
          "poster": "Gomer",
          "content": "Selected Answer: A\nPer ChatGPT \"AWS offers several services and features that can help detect port scans:\"\n\"GuardDuty\" (using VPC Flow Logs), \"WAF\", and \"Network Firewall\"\nWas able to also provide references\nhttps://aws.amazon.com/blogs/aws/amazon-guardduty-continuous-security-monitoring-threat-detection/",
          "comment_id": "1238437"
        },
        {
          "content": "Bad question.\n\nAlthough you can do it via GuardDuty, the answer doesn't mention the required VPC flow logs.\nThere is no mention online of how to create a CloudWatch ALARM for GuardDuty only CloudWatch events.",
          "timestamp": "1715086920.0",
          "comment_id": "1207879",
          "upvote_count": "1",
          "poster": "that1guy"
        },
        {
          "timestamp": "1713010380.0",
          "content": "Selected Answer: A\nanswer a is correct",
          "comment_id": "1194946",
          "upvote_count": "2",
          "poster": "dkp"
        },
        {
          "timestamp": "1707641280.0",
          "upvote_count": "4",
          "comment_id": "1147067",
          "content": "Selected Answer: A\nA is correct: To detect port scans in real time, we need Guarduty, not inspector\nB, C and D: no mention of Guarduty",
          "poster": "thanhnv142"
        },
        {
          "comment_id": "1121022",
          "poster": "a54b16f",
          "upvote_count": "4",
          "timestamp": "1705085100.0",
          "content": "Selected Answer: A\nonly GuardDuty would detect port scanning activities"
        },
        {
          "content": "Selected Answer: A\nhttps://medium.com/aws-architech/use-case-aws-inspector-vs-guardduty-3662bf80767a",
          "poster": "davdan99",
          "upvote_count": "2",
          "comment_id": "1118560",
          "timestamp": "1704891660.0"
        },
        {
          "comment_id": "1110853",
          "timestamp": "1704062220.0",
          "upvote_count": "2",
          "content": "Selected Answer: A\nGuardDuty should be the answer as it best detects whether a port scan has happened on an EC2 instances; we don't care about whether the port is open or not, we care if it was scanned.",
          "poster": "kabary"
        },
        {
          "comment_id": "1110418",
          "upvote_count": "4",
          "poster": "d262e67",
          "content": "Selected Answer: A\nInspector is designed to find vulnerabilities across EC2 servers and detect open ports. It doesn't detect port scans against EC2 servers. The reachability analyzer mentioned below is the port scanner itself. I doesn't detect other port scanners.\n\nhttps://aws.amazon.com/blogs/security/amazon-inspector-assess-network-exposure-ec2-instances-aws-network-reachability-assessments/\n\nGuardDuty on the other hand draws upon traffic logs to find specious activities such as port scans in a form of a finding.",
          "timestamp": "1704016080.0"
        },
        {
          "timestamp": "1703863920.0",
          "comment_id": "1108849",
          "content": "Selected Answer: B\nIt's B - here is a reference for the network reachability package: \n\nhttps://aws.amazon.com/blogs/security/amazon-inspector-assess-network-exposure-ec2-instances-aws-network-reachability-assessments/",
          "upvote_count": "1",
          "comments": [
            {
              "poster": "kabary",
              "comment_id": "1110855",
              "upvote_count": "2",
              "content": "AWS inspector doesn't detect whether a PenTester performed a port scan against an EC2. It only detects open port vulnerabilities. You need a system that detects a threat which is by definition GuardDuty",
              "timestamp": "1704062640.0"
            }
          ],
          "poster": "csG13"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:11:00.513Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "crnuFT2vDT4LuRJ2z65S",
      "question_number": 71,
      "page": 15,
      "question_text": "A company runs applications in an Amazon Elastic Kubernetes Service (Amazon EKS) cluster. The EKS cluster uses an Application Load Balancer to route traffic to the applications that run in the cluster.\n\nA new application that was migrated to the EKS cluster is performing poorly. All the other applications in the EKS cluster maintain appropriate operation. The new application scales out horizontally to the preconfigured maximum number of pods immediately upon deployment, before any user traffic routes to the web application.\n\nWhich solution will resolve the scaling behavior of the web application in the EKS cluster?",
      "choices": {
        "B": "Implement the Vertical Pod Autoscaler in the EKS cluster.",
        "A": "Implement the Horizontal Pod Autoscaler in the EKS cluster.",
        "C": "Implement the Cluster Autoscaler.",
        "D": "Implement the AWS Load Balancer Controller in the EKS cluster."
      },
      "correct_answer": "B",
      "answer_ET": "B",
      "answers_community": [
        "B (62%)",
        "A (35%)",
        "3%"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/129682-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-12-29 13:53:00",
      "unix_timestamp": 1703854380,
      "discussion_count": 23,
      "discussion": [
        {
          "timestamp": "1711007100.0",
          "poster": "vmahilevskyi",
          "comment_id": "1179040",
          "upvote_count": "10",
          "content": "Selected Answer: B\nIn my opinion A is incorrect because \"The new application scales out horizontally to the preconfigured maximum number of pods immediately upon deployment\" means that the Horizontal Pod Autoscaler is already implemented but doesn't resolve the issue with poor performance. This may indicate inappropriate resource allocation. \nBut Vertical Pod Autoscaler will help us \"right size\" our application. \nSo, for me it's B.",
          "comments": [
            {
              "poster": "moonj",
              "timestamp": "1746752880.0",
              "upvote_count": "2",
              "content": "But the question is asking \"Which solution will resolve the scaling behavior\" but not for the performance, then should be A?",
              "comment_id": "1567570"
            }
          ]
        },
        {
          "poster": "a54b16f",
          "content": "Selected Answer: B\nscaled out to maximum when there is no user traffic: this means that the configured pod instance is wrong-sized, for example, need more memory or CPU.",
          "timestamp": "1705349580.0",
          "comment_id": "1123634",
          "upvote_count": "8"
        },
        {
          "poster": "nickp84",
          "timestamp": "1747068960.0",
          "comment_id": "1568497",
          "upvote_count": "1",
          "content": "Selected Answer: A\nB. Vertical Pod Autoscaler adjusts the CPU/memory resources per pod, not the number of pods. It won’t solve premature scaling out."
        },
        {
          "comment_id": "1562279",
          "timestamp": "1745173020.0",
          "poster": "GripZA",
          "upvote_count": "1",
          "content": "Selected Answer: A\nHPA dynamically adjusts the number of pods based on actual metrics, like CPU utilization, memory, or custom metrics. in this case, the app is likely using a static replica count, or has a misconfigured initial replica setting or resource requests that trigger autoscaling early. implementing HPA would ensure pods scale only when needed, based on real usage, e.g CPU > 70%"
        },
        {
          "timestamp": "1743330240.0",
          "content": "Selected Answer: A\nIf scaled \"Before\" traffic is flew, then option A to control autoscaling",
          "comment_id": "1411987",
          "poster": "Srikantha",
          "upvote_count": "1"
        },
        {
          "content": "Selected Answer: C\nEnable Cluster Autoscaler:\n\nEnsure that the Cluster Autoscaler is enabled and configured to scale the number of nodes based on the actual demand.\nVerify that the Cluster Autoscaler is not causing unnecessary scaling by reviewing its configuration and logs.",
          "poster": "DKM",
          "comment_id": "1399770",
          "upvote_count": "1",
          "timestamp": "1742235960.0"
        },
        {
          "content": "Selected Answer: A\nFor stateless applications with variable traffic, start with HPA.\nFor stateful or resource-intensive applications, start with VPA.",
          "comment_id": "1323755",
          "poster": "spring21",
          "upvote_count": "2",
          "timestamp": "1733688720.0"
        },
        {
          "comment_id": "1307699",
          "poster": "VerRi",
          "timestamp": "1730878080.0",
          "upvote_count": "3",
          "content": "Selected Answer: B\nThe keywords here are \"immediately upon deployment\"\nIf the pods scale to the max immediately, it indicates that the pods may not have enough resources to handle the workload"
        },
        {
          "timestamp": "1723489800.0",
          "content": "Selected Answer: A\nThe app is currently deployed as Deployment with a set of replicas which explains that it scales to the maximum set without any traffic.\n\nIt needs HPA to scale up in response to traffic not Vertical Pod Autoscaler which is in response to adding more cpu/mem resources to already running pods",
          "upvote_count": "4",
          "poster": "auxwww",
          "comment_id": "1264776"
        },
        {
          "timestamp": "1722306180.0",
          "upvote_count": "4",
          "poster": "jamesf",
          "content": "Selected Answer: B\nShould be B\nkeywords: \"scaled out to maximum before user traffic route to web application\" \n- this means that the configured pod instance is wrong-sized before user traffic, which need more cpu or memory. \n\nincrease CPU/memory for resources - Vertical Pod Autoscaler\nincrease pod for traffic - Horizontal Pod Autoscaler",
          "comment_id": "1257782"
        },
        {
          "comment_id": "1207880",
          "poster": "that1guy",
          "timestamp": "1715087220.0",
          "content": "Selected Answer: A\nIt's A, in Kubernetes you can specific the number of pod replicas without the use of HPA.\n\n\"The new application scales out horizontally to the preconfigured maximum number of pods\n\nThis would imply that they are doing it statically currently.",
          "upvote_count": "3"
        },
        {
          "comment_id": "1205491",
          "content": "Selected Answer: B\nB for me",
          "poster": "seetpt",
          "timestamp": "1714651380.0",
          "upvote_count": "3"
        },
        {
          "comment_id": "1194950",
          "upvote_count": "1",
          "poster": "dkp",
          "content": "Selected Answer: A\nAns: A\nHorizontal Pod Autoscaler (Option A) is the most appropriate solution for adjusting the number of pods based on CPU and memory utilization.\n\nVertical Pod Autoscaler (Option B) adjusts the CPU and memory reservations for pods, which might not directly address the scaling behavior issue in this scenario.",
          "timestamp": "1713011160.0"
        },
        {
          "timestamp": "1711836780.0",
          "content": "Selected Answer: A\nA.\nAssuming that the first part of the question is related to replicas, which is the max number controlled by the deployment, the Replicaset will set it to the maximum\nKind: deployment\nReplicas:4\nwhich means this is NOT necessarily HPA, just replica set. So, there is a need to configure the HPA properly, BASED on CPU other them STATiC",
          "upvote_count": "2",
          "comments": [
            {
              "upvote_count": "1",
              "content": "but then, when it comes to resources, seems to be B, so changing my to B",
              "timestamp": "1711837080.0",
              "poster": "WhyIronMan",
              "comment_id": "1186421"
            }
          ],
          "comment_id": "1186420",
          "poster": "WhyIronMan"
        },
        {
          "upvote_count": "4",
          "poster": "ogerber",
          "timestamp": "1710589560.0",
          "comment_id": "1174940",
          "content": "Selected Answer: B\nGiven the nature of the problem — scaling out to the maximum number of pods prematurely, the issue appears to be related to resource allocation rather than the need to handle increased traffic/load. Option B (Implement the Vertical Pod Autoscaler in the EKS cluster) is more likely to address the underlying issue by optimizing the resource allocation for the pods, which could prevent the unnecessary immediate scale-out of the application. VPA adjusts pod resources to match their needs more accurately, potentially mitigating the need for immediate horizontal scaling."
        },
        {
          "upvote_count": "2",
          "poster": "thanhnv142",
          "timestamp": "1707651900.0",
          "comment_id": "1147193",
          "content": "Selected Answer: A\nA is correct: There are problem with horizontal scale so we need to implement it properly"
        },
        {
          "comment_id": "1114582",
          "poster": "matanasov",
          "content": "Selected Answer: A\nThe key here is \"preconfigured maximum number of pods\". The HPA should be configured to scale the number of pods up based on actual CPU utilization, not based on a preconfigured maximum number of pods.",
          "timestamp": "1704467160.0",
          "upvote_count": "2"
        },
        {
          "poster": "zolthar_z",
          "comment_id": "1111949",
          "upvote_count": "2",
          "content": "B: Can't be A because without traffic the horizontal scaling is full, you need to increase the CPU capacity, https://docs.aws.amazon.com/eks/latest/userguide/vertical-pod-autoscaler.html",
          "timestamp": "1704204780.0"
        },
        {
          "poster": "GokSK",
          "timestamp": "1704093600.0",
          "comment_id": "1111003",
          "upvote_count": "1",
          "content": "Selected Answer: A\nWe need to note this like - new application that was migrated to the EKS cluster is performing poorly\n\nHere, due to the new application, cluster is performing poorly. So, we need to only focus on this pod rather overall capacity. so, C will not be the right option as it will take care of entire pods. A is the right way.\n\nThe Horizontal Pod AutoScaler (HPA) is typically more suitable for adjusting the number of replicas (pods) based on metrics like CPU utilization, ensuring that the application scales in or out based on demand"
        },
        {
          "comment_id": "1110796",
          "poster": "hisdlodskfe",
          "upvote_count": "1",
          "content": "It needs more node, so that the answer is C",
          "timestamp": "1704053460.0"
        },
        {
          "comment_id": "1110426",
          "timestamp": "1704016800.0",
          "poster": "d262e67",
          "upvote_count": "5",
          "content": "Selected Answer: B\nIf the new application in the Amazon EKS cluster is performing poorly despite scaling out to the maximum number of pods, it's possible that the issue might be related to the resources allocated to each individual pod, rather than the number of pods."
        },
        {
          "comment_id": "1108855",
          "poster": "csG13",
          "content": "Selected Answer: A\nIt's A - horizontal pod autoscaler can scale in or horizontally based on CPU utilisation. \n\nhttps://docs.aws.amazon.com/eks/latest/userguide/horizontal-pod-autoscaler.html",
          "timestamp": "1703864280.0",
          "upvote_count": "1"
        },
        {
          "upvote_count": "1",
          "timestamp": "1703854380.0",
          "content": "Selected Answer: C\nC is correct",
          "poster": "PrasannaBalaji",
          "comment_id": "1108680"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:11:11.130Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "1MlD4RS72xStbIvyS7vw",
      "question_number": 72,
      "page": 15,
      "question_text": "A company has an AWS Control Tower landing zone that manages its organization in AWS Organizations. The company created an OU structure that is based on the company's requirements. The company's DevOps team has established the core accounts for the solution and an account for all centralized AWS CloudFormation and AWS Service Catalog solutions.\n\nThe company wants to offer a series of customizations that an account can request through AWS Control Tower.\n\nWhich combination of steps will meet these requirements? (Choose three.)",
      "choices": {
        "D": "Create a CloudFormation stack set for each CloudFormation template. Enable automatic deployment for each stack set. Create a CloudFormation stack instance that targets specific OUs.",
        "E": "Deploy the Customizations for AWS Control Tower (CfCT) CloudFormation stack.",
        "C": "Create a Service Catalog product for each CloudFormation template.",
        "B": "Create an IAM role that is named AWSControlTowerBlueprintAccess. Configure the role with a trust policy that allows the AWSControlTowerAdmin role in the management account to assume the role. Attach the AWSServiceCatalogAdminFullAccess IAM policy to the AWSControlTowerBlueprintAccess role.",
        "F": "Create a CloudFormation template that contains the resources for each customization.",
        "A": "Enable trusted access for CloudFormation with Organizations by using service-managed permissions."
      },
      "correct_answer": "BCF",
      "answer_ET": "BCF",
      "answers_community": [
        "BCF (78%)",
        "9%",
        "9%"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/129985-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-12-31 11:25:00",
      "unix_timestamp": 1704018300,
      "discussion_count": 14,
      "discussion": [
        {
          "comment_id": "1568500",
          "content": "Selected Answer: BEF\nB. Create an IAM role named AWSControlTowerBlueprintAccess...\n Required to enable CfCT to deploy customizations to accounts.\nThe trust relationship ensures that the management account can assume the role in target accounts for customization deployments.\nE. Deploy the CfCT CloudFormation stack\n This is the core mechanism for enabling customizations across your Control Tower-managed environment.\n CfCT provisions and manages CloudFormation templates and optionally Service Catalog products across OUs and accounts.\n F. Create a CloudFormation template for each customization\nThese templates define the infrastructure or policies you want to roll out to Control Tower accounts.",
          "poster": "nickp84",
          "timestamp": "1747069560.0",
          "upvote_count": "1"
        },
        {
          "poster": "Srikantha",
          "content": "Selected Answer: ACE\nThis approach ensures automation, governance, and self-service capability with the least operational overhead",
          "upvote_count": "1",
          "timestamp": "1743330660.0",
          "comment_id": "1411990"
        },
        {
          "content": "Selected Answer: BCE\nCfCT provides a framework that automates the deployment of custom resources and policies across your AWS Control Tower environment.\n\nCreating and deploying individual CloudFormation templates for each customization requires significant manual effort.",
          "comment_id": "1326108",
          "timestamp": "1734091200.0",
          "upvote_count": "1",
          "poster": "Slays"
        },
        {
          "upvote_count": "1",
          "comment_id": "1307718",
          "poster": "VerRi",
          "content": "Selected Answer: ACE\n\"customisations that an account can request\"\nIt is an ongoing customisation, CfCt does better than AFC.",
          "timestamp": "1730881500.0"
        },
        {
          "upvote_count": "1",
          "poster": "awsarchitect5",
          "timestamp": "1730347020.0",
          "comment_id": "1305283",
          "content": "Selected Answer: BCE\nCfCT provides predefined stacksets that extend Control Tower. Offers a series of customization."
        },
        {
          "content": "I think the correct answer is ACE.\n1- Enable trusted access for CloudFormation with Organizations to allow cross-account deployments.\n2- Create Service Catalog products for each CloudFormation template to provide a self-service portal.\n3- Deploy the Customizations for AWS Control Tower (CfCT) stack to automate and manage customizations across your organization.",
          "timestamp": "1721903040.0",
          "upvote_count": "1",
          "poster": "hzaki",
          "comment_id": "1254913"
        },
        {
          "poster": "Gomer",
          "upvote_count": "3",
          "comment_id": "1238474",
          "timestamp": "1719547740.0",
          "content": "Selected Answer: BCF\n\"Steps to set up Account Factory for the customization process\":\n1. \"Create the required role....\"\n - \"The role must be named AWSControlTowerBlueprintAccess.\"\n - \"The AWSControlTowerBlueprintAccess role must be set up to grant trust to\" [...]\n - \"The role named AWSControlTowerAdmin in the AWS Control Tower management account.\"\n - AWS Control Tower requires that the managed policy named AWSServiceCatalogAdminFullAccess must be attached to the AWSControlTowerBlueprintAccess role. (Required permissions policy)\n2. \"Create the AWS Service Catalog product...\"\n[...]\nhttps://docs.aws.amazon.com/controltower/latest/userguide/af-customization-page.html\nhttps://docs.aws.amazon.com/controltower/latest/userguide/afc-setup-steps.html"
        },
        {
          "poster": "dkp",
          "comment_id": "1195138",
          "timestamp": "1713042360.0",
          "upvote_count": "1",
          "content": "Selected Answer: BCF\nans: BCF"
        },
        {
          "content": "Selected Answer: BCF\nB,C,F is the only answer",
          "comment_id": "1186422",
          "poster": "WhyIronMan",
          "timestamp": "1711837140.0",
          "upvote_count": "1"
        },
        {
          "timestamp": "1707652500.0",
          "poster": "thanhnv142",
          "content": "Selected Answer: BCF\nBCF are correct: <an account for all centralized AWS CloudFormation > means we need to set up a role for this accoun\nA: Trusted access is for assuming role only\nD: no mention of customization\nE: CFCT is needed only when we need to apply to best practice",
          "upvote_count": "4",
          "comment_id": "1147198"
        },
        {
          "content": "BCF are correct: <an account for all centralized AWS CloudFormation > means we need to set up a role for this accoun\nE: CFCT is for when we need to apply to best practice",
          "comment_id": "1147192",
          "timestamp": "1707651840.0",
          "poster": "thanhnv142",
          "upvote_count": "1"
        },
        {
          "timestamp": "1705049340.0",
          "upvote_count": "2",
          "comment_id": "1120582",
          "poster": "yuliaqwerty",
          "content": "Selected Answer: BCF\nanswer BCF"
        },
        {
          "poster": "kabary",
          "upvote_count": "3",
          "content": "Selected Answer: BCF\nhttps://docs.aws.amazon.com/controltower/latest/userguide/afc-setup-steps.html",
          "timestamp": "1704144480.0",
          "comment_id": "1111441"
        },
        {
          "upvote_count": "4",
          "timestamp": "1704018300.0",
          "comment_id": "1110451",
          "content": "Selected Answer: BCF\nhttps://docs.aws.amazon.com/controltower/latest/userguide/afc-setup-steps.html",
          "poster": "d262e67"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:11:11.131Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "8CkLxL6xUiZ5oCBbpLyD",
      "question_number": 73,
      "page": 15,
      "question_text": "A company runs a workload on Amazon EC2 instances. The company needs a control that requires the use of Instance Metadata Service Version 2 (IMDSv2) on all EC2 instances in the AWS account. If an EC2 instance does not prevent the use of Instance Metadata Service Version 1 (IMDSv1), the EC2 instance must be terminated.\n\nWhich solution will meet these requirements?",
      "choices": {
        "C": "Set up Amazon Inspector in the account. Configure Amazon Inspector to activate deep inspection for EC2 instances. Create an Amazon EventBridge rule for an Inspector2 finding. Set an AWS Lambda function as the target to terminate the instance.",
        "A": "Set up AWS Config in the account. Use a managed rule to check EC2 instances. Configure the rule to remediate the findings by using AWS Systems Manager Automation to terminate the instance.",
        "B": "Create a permissions boundary that prevents the ec2:RunInstance action if the ec2:MetadataHttpTokens condition key is not set to a value of required. Attach the permissions boundary to the IAM role that was used to launch the instance.",
        "D": "Create an Amazon EventBridge rule for the EC2 instance launch successful event. Send the event to an AWS Lambda function to inspect the EC2 metadata and to terminate the instance."
      },
      "correct_answer": "A",
      "answer_ET": "A",
      "answers_community": [
        "A (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/129710-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-12-29 16:58:00",
      "unix_timestamp": 1703865480,
      "discussion_count": 8,
      "discussion": [
        {
          "content": "Selected Answer: A\nAWS Config can do this using the managed ec2-imdsv2-check rule.\n\nHere is a reference:\nhttps://docs.aws.amazon.com/config/latest/developerguide/ec2-imdsv2-check.html",
          "upvote_count": "8",
          "timestamp": "1703865480.0",
          "comment_id": "1108867",
          "poster": "csG13"
        },
        {
          "timestamp": "1724248140.0",
          "content": "Using the ec2-imdsv2-check AWS Config managed rule, you can Checks if your Amazon EC2 instance metadata version is configured with Instance Metadata Service Version 2 (IMDSv2).",
          "poster": "flaacko",
          "comment_id": "1270166",
          "upvote_count": "1"
        },
        {
          "content": "Selected Answer: A\nAWS Config is a service that enables you to assess, audit, and evaluate the configurations of your AWS resources. It provides managed rules that you can use to evaluate the configuration settings of your resources against desired configurations.\nIn this case, you can use the AWS Config managed rule \"ec2-imdsv2-check\" to evaluate whether your EC2 instances are using the Instance Metadata Service Version 2 (IMDSv2) or not. This rule checks if the EC2 instances have the HTTP token request enabled for the Instance Metadata Service (IMDS), which is a requirement for using IMDSv2.\nIf an EC2 instance is found to be non-compliant with the rule (i.e., not using IMDSv2), AWS Config can be configured to automatically remediate the non-compliant resource. You can set up AWS Systems Manager Automation to terminate the non-compliant EC2 instance as the remediation action.",
          "comment_id": "1201458",
          "upvote_count": "2",
          "timestamp": "1713970200.0",
          "poster": "c3518fc"
        },
        {
          "timestamp": "1710597180.0",
          "comment_id": "1174985",
          "upvote_count": "1",
          "poster": "DanShone",
          "content": "Selected Answer: A\nA - AWS Config"
        },
        {
          "poster": "DanShone",
          "comment_id": "1174984",
          "timestamp": "1710597120.0",
          "content": "A - AWS Config",
          "upvote_count": "1"
        },
        {
          "poster": "thanhnv142",
          "upvote_count": "3",
          "comment_id": "1147196",
          "content": "Selected Answer: A\nA is correct: use Config to monitor and SSM Automation to terminate instances\nB: permission boundary cannot spot the need-to-terminate instances\nC: Inspector is for vul scanning\nD: EC2 instance launch successful event wont provide sufficient information",
          "timestamp": "1707652380.0"
        },
        {
          "timestamp": "1704063780.0",
          "poster": "kabary",
          "comment_id": "1110862",
          "upvote_count": "1",
          "content": "Selected Answer: A\nAnswer A"
        },
        {
          "content": "Selected Answer: A\nOnly viable option",
          "upvote_count": "1",
          "poster": "d262e67",
          "comment_id": "1110457",
          "timestamp": "1704018660.0"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:11:11.131Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "33Wqwn2v6qW953NIRkcz",
      "question_number": 74,
      "page": 15,
      "question_text": "A company builds an application that uses an Application Load Balancer in front of Amazon EC2 instances that are in an Auto Scaling group. The application is stateless. The Auto Scaling group uses a custom AMI that is fully prebuilt. The EC2 instances do not have a custom bootstrapping process.\n\nThe AMI that the Auto Scaling group uses was recently deleted. The Auto Scaling group's scaling activities show failures because the AMI ID does not exist.\n\nWhich combination of steps should a DevOps engineer take to meet these requirements? (Choose three.)",
      "choices": {
        "B": "Update the Auto Scaling group to use the new launch template.",
        "D": "Increase the Auto Scaling group's desired capacity by 1.",
        "E": "Create a new AMI from a running EC2 instance in the Auto Scaling group.",
        "F": "Create a new AMI by copying the most recent public AMI of the operating system that the EC2 instances use.",
        "A": "Create a new launch template that uses the new AMI.",
        "C": "Reduce the Auto Scaling group's desired capacity to 0."
      },
      "correct_answer": "ABE",
      "answer_ET": "ABE",
      "answers_community": [
        "ABE (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/129715-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-12-29 17:07:00",
      "unix_timestamp": 1703866020,
      "discussion_count": 10,
      "discussion": [
        {
          "content": "Selected Answer: ABE\nABE\nother options not related to solve problem",
          "poster": "jamesf",
          "upvote_count": "1",
          "timestamp": "1722307080.0",
          "comment_id": "1257789"
        },
        {
          "timestamp": "1713042900.0",
          "upvote_count": "2",
          "content": "Selected Answer: ABE\nans ABE",
          "poster": "dkp",
          "comment_id": "1195141"
        },
        {
          "content": "Selected Answer: ABE\nA,B,E\nThe others do not actually help solve the problem",
          "poster": "WhyIronMan",
          "timestamp": "1711837380.0",
          "upvote_count": "1",
          "comment_id": "1186424"
        },
        {
          "content": "Selected Answer: ABE\nABE\nE - Create new AMI from existing EC2 (we can do this as they are stateless)\nA - New lanuch template with the new AMI\nB - Tell autoscaling to use this new launch template",
          "poster": "DanShone",
          "timestamp": "1710595500.0",
          "upvote_count": "2",
          "comment_id": "1174971"
        },
        {
          "comment_id": "1147200",
          "poster": "thanhnv142",
          "timestamp": "1707652920.0",
          "upvote_count": "4",
          "content": "Selected Answer: ABE\nABE are correct: <The AMI that the Auto Scaling group uses was recently deleted> means we need a new AMI image from a runnung EC2 instance\nC and D: irrelevant, auto Scaling group's desired capacity has nothing to do here\nF: This option doesnt make sense"
        },
        {
          "timestamp": "1706360220.0",
          "content": "Selected Answer: ABE\nTo address the issue of the deleted AMI in the Auto Scaling group, you can follow these steps:\n\nCreate a new AMI from a running EC2 instance in the Auto Scaling group. (Option E)\n\nThis will ensure that you have a new AMI based on the current state of one of the running instances in the group.\nCreate a new launch template that uses the new AMI. (Option A)\n\nAfter creating the new AMI, update the launch template to use this new AMI. Launch templates provide a versioned and more structured way to define the launch configuration for your Auto Scaling group.\nUpdate the Auto Scaling group to use the new launch template. (Option B)\n\nUpdate the Auto Scaling group to use the new launch template that includes the new AMI. This will ensure that new instances launched by the Auto Scaling group will use the updated configuration.",
          "upvote_count": "2",
          "poster": "promo286",
          "comment_id": "1133349"
        },
        {
          "timestamp": "1706192640.0",
          "content": "Selected Answer: ABE\nCreate a new AMI from running instance, Update launch template to use new AMI, update ASG to use new launch template",
          "comment_id": "1131746",
          "upvote_count": "1",
          "poster": "sksegha"
        },
        {
          "poster": "a54b16f",
          "content": "Selected Answer: ABE\ncreate a new image and use it",
          "upvote_count": "1",
          "comment_id": "1121032",
          "timestamp": "1705086120.0"
        },
        {
          "content": "Selected Answer: ABE\nOnly viable steps",
          "timestamp": "1704060660.0",
          "poster": "d262e67",
          "upvote_count": "1",
          "comment_id": "1110846"
        },
        {
          "timestamp": "1703866020.0",
          "poster": "csG13",
          "content": "Selected Answer: ABE\nCreate a new AMI from a running instance in ASG. Use it to create a new launch template, finally update ASG to use the new template.",
          "comment_id": "1108877",
          "upvote_count": "2",
          "comments": [
            {
              "poster": "csG13",
              "comment_id": "1108882",
              "upvote_count": "1",
              "content": "btw, creating a new launch template in this case can also mean a new version of it.",
              "timestamp": "1703866140.0"
            }
          ]
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:11:11.131Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "W69J7rN2bgyNXbLBygjz",
      "question_number": 75,
      "page": 15,
      "question_text": "A company deploys a web application on Amazon EC2 instances that are behind an Application Load Balancer (ALB). The company stores the application code in an AWS CodeCommit repository. When code is merged to the main branch, an AWS Lambda function invokes an AWS CodeBuild project. The CodeBuild project packages the code, stores the packaged code in AWS CodeArtifact, and invokes AWS Systems Manager Run Command to deploy the packaged code to the EC2 instances.\n\nPrevious deployments have resulted in defects, EC2 instances that are not running the latest version of the packaged code, and inconsistencies between instances.\n\nWhich combination of actions should a DevOps engineer take to implement a more reliable deployment solution? (Choose two.)",
      "choices": {
        "A": "Create a pipeline in AWS CodePipeline that uses the CodeCommit repository as a source provider. Configure pipeline stages that run the CodeBuild project in parallel to build and test the application. In the pipeline, pass the CodeBuild project output artifact to an AWS CodeDeploy action.",
        "B": "Create a pipeline in AWS CodePipeline that uses the CodeCommit repository as a source provider. Create separate pipeline stages that run a CodeBuild project to build and then test the application. In the pipeline, pass the CodeBuild project output artifact to an AWS CodeDeploy action.",
        "E": "Create an Amazon S3 bucket. Modify the CodeBuild project to store the packages in the S3 bucket instead of in CodeArtifact. Use deploy actions in CodeDeploy to deploy the artifact to the EC2 instances.",
        "D": "Create individual Lambda functions that use AWS CodeDeploy instead of Systems Manager to run build, test, and deploy actions.",
        "C": "Create an AWS CodeDeploy application and a deployment group to deploy the packaged code to the EC2 instances. Configure the ALB for the deployment group."
      },
      "correct_answer": "BC",
      "answer_ET": "BC",
      "answers_community": [
        "BC (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/129720-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-12-29 17:21:00",
      "unix_timestamp": 1703866860,
      "discussion_count": 11,
      "discussion": [
        {
          "upvote_count": "1",
          "content": "Selected Answer: BC\nBC\nCodePipeline, CodeCommit, CodeDeploy",
          "poster": "jamesf",
          "timestamp": "1722307260.0",
          "comment_id": "1257791"
        },
        {
          "comment_id": "1195143",
          "timestamp": "1713044040.0",
          "content": "Selected Answer: BC\nB (Sequential Stages) if:\nBuild and test stages have dependencies (tests rely on build output).\nYou need manual approvals or gates between stages for control.",
          "poster": "dkp",
          "upvote_count": "2"
        },
        {
          "timestamp": "1711837680.0",
          "upvote_count": "1",
          "content": "Selected Answer: BC\nB,C\nA is wrong because doesn't make sense to do it in parallel since it'll cause more problems.\nD and E are dumb",
          "poster": "WhyIronMan",
          "comment_id": "1186427"
        },
        {
          "upvote_count": "1",
          "poster": "DanShone",
          "content": "Selected Answer: BC\nB and C\nCodePipeline and CodeDeploy",
          "comment_id": "1174970",
          "timestamp": "1710595380.0"
        },
        {
          "comment_id": "1147201",
          "content": "Selected Answer: BC\nB and C are correct: We need to use codedeploy to build instead of using codebuild\nA: <run the CodeBuild project in parallel> - this is in correct. Should run the pipiline respectedly\nD: Should not use lambda\nE: Codeartifact is good, no need to change to S3",
          "upvote_count": "4",
          "poster": "thanhnv142",
          "timestamp": "1707653280.0"
        },
        {
          "poster": "denccc",
          "upvote_count": "1",
          "comment_id": "1125202",
          "content": "B & C BeCause",
          "timestamp": "1705514880.0"
        },
        {
          "comment_id": "1120721",
          "timestamp": "1705059240.0",
          "poster": "yuliaqwerty",
          "content": "Selected Answer: BC\nAnswer B and C",
          "upvote_count": "1"
        },
        {
          "poster": "kabary",
          "content": "Selected Answer: BC\nB & C for sure.",
          "comment_id": "1110881",
          "timestamp": "1704065760.0",
          "upvote_count": "1"
        },
        {
          "upvote_count": "1",
          "comment_id": "1110848",
          "content": "Selected Answer: BC\nB. because in this case sequential approach is more reliable and ensures consistency.\nC. Because in the only possible next step in the process.",
          "poster": "d262e67",
          "timestamp": "1704061020.0"
        },
        {
          "comment_id": "1109419",
          "upvote_count": "1",
          "poster": "PrasannaBalaji",
          "content": "Selected Answer: BC\nB and C",
          "timestamp": "1703913240.0"
        },
        {
          "timestamp": "1703866860.0",
          "upvote_count": "1",
          "poster": "csG13",
          "content": "Selected Answer: BC\nUse Codepipeline to orchestrate the build and codedeploy to deploy it on EC2.",
          "comment_id": "1108896"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:11:11.131Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "mEiCfOD5LRRx2rRgUSOb",
      "question_number": 76,
      "page": 16,
      "question_text": "A company uses an organization in AWS Organizations to manage its AWS accounts. The company's automation account contains a CI/CD pipeline that creates and configures new AWS accounts.\n\nThe company has a group of internal service teams that provide services to accounts in the organization. The service teams operate out of a set of services accounts. The service teams want to receive an AWS CloudTrail event in their services accounts when the CreateAccount API call creates a new account.\n\nHow should the company share this CloudTrail event with the service accounts?",
      "choices": {
        "D": "Create a custom Amazon EventBridge event bus in the automation account. Create an EventBridge rule and policy that connects the custom event bus to the default event buses in the services accounts.",
        "C": "Create a custom Amazon EventBridge event bus in the automation account and the services accounts. Create an EventBridge rule and policy that connects the custom event buses that are in the automation account and the services accounts.",
        "B": "Create a custom Amazon EventBridge event bus in the services accounts. Update the custom event bus to allow events from the automation account. Create an EventBridge rule in the services account that directly listens to CloudTrail events from the automation account.",
        "A": "Create an Amazon EventBridge rule in the automation account to send account creation events to the default event bus in the services accounts. Update the default event bus in the services accounts to allow events from the automation account."
      },
      "correct_answer": "A",
      "answer_ET": "A",
      "answers_community": [
        "A (85%)",
        "B (15%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/129727-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-12-29 17:46:00",
      "unix_timestamp": 1703868360,
      "discussion_count": 11,
      "discussion": [
        {
          "comment_id": "1147267",
          "poster": "thanhnv142",
          "content": "Selected Answer: A\nA is correct: We need account creation events and this option provides us with exactly that\nB: < Create an EventBridge rule in the services account that directly listens to CloudTrail events>: This does not make sense. We should apply rule to eventbus to send event\nC and D: Both options send all events, not just account creation events",
          "upvote_count": "5",
          "timestamp": "1723377300.0"
        },
        {
          "comment_id": "1111522",
          "content": "Selected Answer: A\nA is right. \"Create an Amazon EventBridge rule in the automation account to send account creation events to the default event bus in the services accounts\": propagation of provision events to the service accounts. \"Update the default event bus in the services accounts to allow events from the automation account.\": correct\n\n\nB. \"Create a custom Amazon EventBridge event bus in the services accounts. Update the custom event bus to allow events from the automation account.\": correct however \"Create an EventBridge rule in the services account that directly listens to CloudTrail events from the automation account.\": Why do you create a rule in the services account listening the events from automation account, in opposite, the rule should be created in the automation account to push the events to the bus in the services account.",
          "upvote_count": "5",
          "timestamp": "1719873360.0",
          "poster": "ozansenturk"
        },
        {
          "comment_id": "1206979",
          "upvote_count": "1",
          "timestamp": "1730834100.0",
          "comments": [
            {
              "comment_id": "1214802",
              "poster": "vn_thanhtung",
              "upvote_count": "2",
              "timestamp": "1732183260.0",
              "content": "https://aws.amazon.com/vi/blogs/aws/new-cross-account-delivery-of-cloudwatch-events/\nOption B Wrong. Answer is A"
            }
          ],
          "poster": "xdkonorek2",
          "content": "Selected Answer: B\nI'm voting B\n\nA - there could be more than 5 accounts: https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-rules-best-practices.html - \"While you can specify up to five targets for a given rule\"\n\nIt's perfectly viable to create rule in one account to receive events from second account."
        },
        {
          "poster": "c3518fc",
          "upvote_count": "1",
          "comment_id": "1201479",
          "timestamp": "1729783800.0",
          "content": "Selected Answer: B\nThe steps to configure EventBridge to send events to or receive events from an event bus in a different account include the following:\nOn the receiver account, edit the permissions on an event bus to allow specified AWS accounts, an organization, or all AWS accounts to send events to the receiver account.\nOn the sender account, set up one or more rules that have the receiver account's event bus as the target.\nIf the sender account inherits permissions to send events from an AWS Organization, the sender account also must have an IAM role with policies that enable it to send events to the receiver account. If you use the AWS Management Console to create the rule that targets the event bus in the receiver account, the role is created automatically. \nOn the receiver account, set up one or more rules that match events that come from the sender account."
        },
        {
          "poster": "dkp",
          "content": "Selected Answer: A\nanswer A",
          "upvote_count": "3",
          "timestamp": "1728856140.0",
          "comment_id": "1195144"
        },
        {
          "comment_id": "1184640",
          "poster": "stoy123",
          "comments": [
            {
              "poster": "stoy123",
              "timestamp": "1727505660.0",
              "upvote_count": "1",
              "content": "I mean B",
              "comment_id": "1184642"
            }
          ],
          "upvote_count": "3",
          "timestamp": "1727505600.0",
          "content": "Selected Answer: A\nof course its A!\n(CloudTrail events) ---EventBridge rule---> [automation account default EventBridge event bus] ---allow---> [service accounts custom EventBridge event bus]"
        },
        {
          "poster": "6f258dd",
          "timestamp": "1721130660.0",
          "upvote_count": "2",
          "content": "Selected Answer: A\nits A, rest don't include account creation.",
          "comment_id": "1124258"
        },
        {
          "timestamp": "1720804860.0",
          "comment_id": "1121048",
          "upvote_count": "2",
          "poster": "a54b16f",
          "content": "Selected Answer: A\nB is wrong, the event is in automation account. It lacks the step to send the event from automation to service account."
        },
        {
          "comment_id": "1118745",
          "upvote_count": "2",
          "comments": [
            {
              "content": "The link you pasted clearly says it is D.",
              "timestamp": "1723722180.0",
              "poster": "govindrk",
              "upvote_count": "1",
              "comment_id": "1151037"
            }
          ],
          "content": "Selected Answer: A\nGo for A \n\nhttps://docs.aws.amazon.com/eventbridge/latest/userguide/eb-cross-account.html",
          "poster": "davdan99",
          "timestamp": "1720617600.0"
        },
        {
          "timestamp": "1719863220.0",
          "upvote_count": "1",
          "poster": "kabary",
          "comment_id": "1111450",
          "content": "Selected Answer: B\nI will go with B.\n\nGiven that \"listening directly to CloudTrail\" is mentioned in the below AWS documentation in bullet point number 8:\n\nhttps://aws.amazon.com/blogs/machine-learning/onboard-users-to-amazon-sagemaker-studio-with-active-directory-group-specific-iam-roles/"
        },
        {
          "timestamp": "1719672360.0",
          "comments": [
            {
              "comment_id": "1108925",
              "upvote_count": "2",
              "poster": "csG13",
              "timestamp": "1719673260.0",
              "content": "reading it again, I'm more inclined to A given that B says about eventbridge rule listening *directly* from Cloudtrail"
            }
          ],
          "comment_id": "1108914",
          "upvote_count": "1",
          "content": "Selected Answer: B\nIt's B - create an Eventbridge rule in the source account, and point the rule to a custom event bus in the service accounts.",
          "poster": "csG13"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:11:21.554Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "o9lCYacuXGiwyGaVPtqd",
      "question_number": 77,
      "page": 16,
      "question_text": "A DevOps engineer is building a solution that uses Amazon Simple Queue Service (Amazon SQS) standard queues. The solution also includes an AWS Lambda function and an Amazon DynamoDB table. The Lambda function pulls content from an SQS queue event source and writes the content to the DynamoDB table.\n\nThe solution must maximize the scalability of Lambda and must prevent successfully processed SQS messages from being processed multiple times.\n\nWhich solution will meet these requirements?",
      "choices": {
        "D": "Set the queue visibility timeout on the Lambda function's event source mapping to account for invocation throttling of the Lambda function.",
        "A": "Decrease the batch window to 1 second when configuring the Lambda function's event source mapping.",
        "C": "Include the ReportBatchItemFailures value in the FunctionResponseTypes list in the Lambda function's event source mapping.",
        "B": "Decrease the batch size to 1 when configuring the Lambda function's event source mapping."
      },
      "correct_answer": "C",
      "answer_ET": "C",
      "answers_community": [
        "C (86%)",
        "7%"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/129832-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-12-30 06:20:00",
      "unix_timestamp": 1703913600,
      "discussion_count": 14,
      "discussion": [
        {
          "content": "Selected Answer: C\nIt's C, here is a reference: \n\nhttps://docs.aws.amazon.com/lambda/latest/dg/with-sqs.html#services-sqs-batchfailurereporting",
          "poster": "csG13",
          "upvote_count": "6",
          "timestamp": "1703918580.0",
          "comment_id": "1109484",
          "comments": [
            {
              "comment_id": "1249345",
              "upvote_count": "2",
              "poster": "noisonnoiton",
              "content": "link updated:\n\nhttps://docs.aws.amazon.com/lambda/latest/dg/services-sqs-errorhandling.html",
              "timestamp": "1721179980.0"
            }
          ]
        },
        {
          "timestamp": "1734919740.0",
          "upvote_count": "1",
          "content": "Selected Answer: D\nC does not prevent multiple message processing",
          "comment_id": "1330652",
          "poster": "youonebe"
        },
        {
          "comment_id": "1330651",
          "timestamp": "1734919620.0",
          "upvote_count": "1",
          "content": "Selected Answer: D\nThe visibility timeout determines how long SQS will \"hide\" a message after Lambda begins processing it. The key here is ensuring that the message is not available for another Lambda function invocation until the current one completes successfully. The visibility timeout should be at least as long as the Lambda function's maximum processing time, ensuring that the same message is not processed again while it is being handled.",
          "poster": "youonebe"
        },
        {
          "content": "To prevent Lambda from processing a message multiple times, you can either configure your event source mapping to include batch item failures in your function response, or you can use the DeleteMessage API to remove messages from the queue as your Lambda function successfully processes them.\n\nTo avoid reprocessing successfully processed messages in a failed batch, you can configure your event source mapping to make only the failed messages visible again. This is called a partial batch response. To turn on partial batch responses, specify ReportBatchItemFailures for the FunctionResponseTypes action when configuring your event source mapping. This lets your function return a partial success, which can help reduce the number of unnecessary retries on records.\n\nSource: https://docs.aws.amazon.com/lambda/latest/dg/services-sqs-errorhandling.html#services-sqs-batchfailurereporting",
          "timestamp": "1724250240.0",
          "upvote_count": "2",
          "comment_id": "1270185",
          "poster": "flaacko"
        },
        {
          "comments": [
            {
              "poster": "Gomer",
              "comment_id": "1238940",
              "content": "The URL with \"services-sqs-batchfailurereporting\" pointer seems to be invalid now. I think the preceeding URL replaced it.",
              "timestamp": "1719610560.0",
              "upvote_count": "1"
            }
          ],
          "timestamp": "1719610500.0",
          "poster": "Gomer",
          "content": "Selected Answer: C\n\"Implementing partial batch responses\nWhen \"Lambda function encounters an error while processing a batch, all messages\"... \"become visible in the queue\"... \"including messages that Lambda processed successfully.\"\n\"...your function can end up processing the same message several times.\n\"To avoid reprocessing successfully processed messages in a failed batch\" \"configure your event source mapping to make only the failed messages visible again.\"\n\"To turn on partial batch responses, specify ReportBatchItemFailures for the FunctionResponseTypes action when configuring your event source mapping.\"\n\"This lets your function return a partial success, which can help reduce the number of unnecessary retries on records.\"\nhttps://docs.aws.amazon.com/lambda/latest/dg/services-sqs-errorhandling.html",
          "upvote_count": "1",
          "comment_id": "1238939"
        },
        {
          "comment_id": "1211759",
          "upvote_count": "1",
          "timestamp": "1715749200.0",
          "content": "Selected Answer: B\nC doesn't address the \"maximize the scalability of Lambda\" while B addresses both,\nwhile batch size is 1, you either fail or success",
          "poster": "misako"
        },
        {
          "upvote_count": "4",
          "comment_id": "1201487",
          "poster": "c3518fc",
          "timestamp": "1713973380.0",
          "content": "Selected Answer: C\nTo avoid reprocessing successfully processed messages in a failed batch, you can configure your event source mapping to make only the failed messages visible again. This is called a partial batch response. To turn on partial batch responses, specify ReportBatchItemFailures for the FunctionResponseTypes action when configuring your event source mapping. This lets your function return a partial success, which can help reduce the number of unnecessary retries on records. https://docs.aws.amazon.com/lambda/latest/dg/with-sqs.html#services-sqs-batchfailurereporting"
        },
        {
          "timestamp": "1713045480.0",
          "poster": "dkp",
          "upvote_count": "2",
          "comment_id": "1195146",
          "content": "Selected Answer: C\nanswer C"
        },
        {
          "timestamp": "1710595140.0",
          "upvote_count": "2",
          "content": "Selected Answer: C\nhttps://docs.aws.amazon.com/lambda/latest/dg/with-sqs.html#services-sqs-batchfailurereporting",
          "poster": "DanShone",
          "comment_id": "1174967"
        },
        {
          "poster": "thanhnv142",
          "upvote_count": "4",
          "timestamp": "1707660300.0",
          "content": "Selected Answer: C\nC is correct. We need ReportBatchItemFailures to return only failed items \nA: batch window is the interval process time\nB: batch size is the size of the job\nD: queue visibility timeout is about re-process",
          "comment_id": "1147276"
        },
        {
          "upvote_count": "2",
          "content": "Selected Answer: C\nhttps://docs.aws.amazon.com/lambda/latest/dg/with-sqs.html#services-sqs-batchfailurereporting",
          "timestamp": "1704156300.0",
          "comment_id": "1111526",
          "poster": "ozansenturk"
        },
        {
          "timestamp": "1704062160.0",
          "comment_id": "1110852",
          "upvote_count": "4",
          "poster": "d262e67",
          "content": "Selected Answer: C\nLambda process messages in batches. If one message in the batch fails the whole batch considered failed and all messages in the batch return to the queue. For example if batch has 10 messages and message 5 and 7 failed to get processed, all 10 messages will return to the queue. So, successfully processed messages can get processed again.\n\nNow to prevent this to happen you have two ways (used to be one)\n1. Write your code in a way to identify processed messages and delete them manually from SQS.\n2. Partial batch response that returns only the messages that were failed to be processed (supported since Dec 2021).\n\nReferences:\nhttps://docs.aws.amazon.com/lambda/latest/dg/with-sqs.html\nhttps://docs.aws.amazon.com/lambda/latest/dg/with-sqs.html#services-sqs-batchfailurereporting"
        },
        {
          "content": "C is correct.",
          "poster": "hisdlodskfe",
          "upvote_count": "1",
          "timestamp": "1704057300.0",
          "comment_id": "1110817"
        },
        {
          "content": "Selected Answer: B\nB is correct",
          "timestamp": "1703913600.0",
          "upvote_count": "1",
          "comment_id": "1109422",
          "poster": "PrasannaBalaji"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:11:21.554Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "9lDo90LuTrMU1BqKHxau",
      "question_number": 78,
      "page": 16,
      "question_text": "A company has a new AWS account that teams will use to deploy various applications. The teams will create many Amazon S3 buckets for application-specific purposes and to store AWS CloudTrail logs. The company has enabled Amazon Macie for the account.\n\nA DevOps engineer needs to optimize the Macie costs for the account without compromising the account's functionality.\n\nWhich solutions will meet these requirements? (Choose two.)",
      "choices": {
        "A": "Exclude S3 buckets that contain CloudTrail logs from automated discovery.",
        "D": "Configure discovery jobs to include S3 objects based on the last modified criterion.",
        "E": "Configure discovery jobs to include S3 objects that are tagged as production only.",
        "B": "Exclude S3 buckets that have public read access from automated discovery.",
        "C": "Configure scheduled daily discovery jobs for all S3 buckets in the account."
      },
      "correct_answer": "AD",
      "answer_ET": "AD",
      "answers_community": [
        "AD (84%)",
        "Other"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/129683-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-12-29 14:17:00",
      "unix_timestamp": 1703855820,
      "discussion_count": 15,
      "discussion": [
        {
          "upvote_count": "3",
          "timestamp": "1729785240.0",
          "content": "Selected Answer: AD\nMake your sensitive data discovery jobs as targeted and specific as possible in their scope by using the Object criteria",
          "comment_id": "1201493",
          "poster": "c3518fc"
        },
        {
          "poster": "dkp",
          "timestamp": "1728857520.0",
          "upvote_count": "3",
          "comment_id": "1195148",
          "content": "Selected Answer: AD\nA&D \nOptions to make discovery jobs more targeted include:\nInclude objects by using the “last modified” criterion \nDon’t scan CloudTrail logs \nConsider using random object sampling \nInclude objects with specific extensions, tags, or storage size with specific tag key/value pairs such as Environment: Production.\nConsider scheduling jobs based on how long objects live in your S3 buckets"
        },
        {
          "upvote_count": "2",
          "comment_id": "1194827",
          "poster": "devakram",
          "content": "Selected Answer: AD\nhttps://aws.amazon.com/blogs/security/how-to-use-amazon-macie-to-reduce-the-cost-of-discovering-sensitive-data/\n\nOptions to make discovery jobs more targeted include:\n\n Include objects by using the “last modified” criterion — \n Consider using random object sampling — \n Include objects with specific extensions, tags, or storage size —",
          "timestamp": "1728811200.0"
        },
        {
          "timestamp": "1727728440.0",
          "upvote_count": "2",
          "poster": "WhyIronMan",
          "comment_id": "1186431",
          "content": "Selected Answer: AD\nA - No need to scan these\nD - Reduce costs but not functionallity"
        },
        {
          "timestamp": "1726485060.0",
          "upvote_count": "1",
          "comment_id": "1174965",
          "poster": "DanShone",
          "content": "Selected Answer: AD\nA - No need to scan these\nD - Reduce costs but not functionallity"
        },
        {
          "poster": "Diego1414",
          "content": "Selected Answer: AD\nAD - Correct\n\nhttps://aws.amazon.com/blogs/security/how-to-use-amazon-macie-to-reduce-the-cost-of-discovering-sensitive-data/",
          "upvote_count": "1",
          "comment_id": "1158126",
          "timestamp": "1724521980.0"
        },
        {
          "timestamp": "1723378680.0",
          "content": "Selected Answer: AD\nA and D are correct: \nA: We dont need to scan Cloudtrail logs, so this is good\nB: Excluding S3 that have public read is just wrong\nC: We have excluded cloudtrail logs S3, so scanning all S3 is not correct\nD: This is good\nE: <Amazon S3 buckets for application-specific purposes and to store AWS CloudTrail logs> means that these S3 buckets are used to store logs and for productions only. Therefore, there will be no production tag, because all of them are production S3 bukets",
          "upvote_count": "2",
          "comment_id": "1147284",
          "poster": "thanhnv142"
        },
        {
          "comment_id": "1121051",
          "content": "Selected Answer: AD\nE sounds right, but the question is about how to optimize, so E would make sense it mentioned skipping non-prod log, or scan prod data only",
          "timestamp": "1720805280.0",
          "upvote_count": "1",
          "poster": "a54b16f"
        },
        {
          "comment_id": "1120836",
          "timestamp": "1720787220.0",
          "upvote_count": "1",
          "poster": "yuliaqwerty",
          "content": "Selected Answer: AD\nAnswer AD"
        },
        {
          "upvote_count": "3",
          "comment_id": "1111530",
          "timestamp": "1719874440.0",
          "content": "Selected Answer: AD\nDon’t scan CloudTrail logs, Include objects by using the “last modified” criterion :https://aws.amazon.com/blogs/security/how-to-use-amazon-macie-to-reduce-the-cost-of-discovering-sensitive-data/",
          "poster": "ozansenturk"
        },
        {
          "timestamp": "1719784440.0",
          "poster": "kabary",
          "content": "Selected Answer: AE\nAnswer is A & E.",
          "upvote_count": "1",
          "comment_id": "1110889"
        },
        {
          "poster": "d262e67",
          "content": "Selected Answer: AD\nBetween D and E: Since the question didn't give any details I picked the broader option. Plus the question mentioned that the account is new, so the team would probably know when the account was created and they can use the last modified criteria. But nowhere mentions the organization's tagging policy. Maybe there is no production tag.",
          "upvote_count": "1",
          "timestamp": "1719781140.0",
          "comment_id": "1110861"
        },
        {
          "content": "Selected Answer: AE\nIt's A & E. See her for reference: https://aws.amazon.com/blogs/security/how-to-use-amazon-macie-to-reduce-the-cost-of-discovering-sensitive-data/",
          "poster": "csG13",
          "timestamp": "1719740940.0",
          "upvote_count": "2",
          "comment_id": "1109700"
        },
        {
          "content": "Selected Answer: AD\nA and D is correct",
          "poster": "komorebi",
          "timestamp": "1719734100.0",
          "comment_id": "1109585",
          "upvote_count": "1"
        },
        {
          "content": "Selected Answer: BE\nB and E is correct",
          "comment_id": "1108702",
          "comments": [],
          "upvote_count": "1",
          "poster": "PrasannaBalaji",
          "timestamp": "1719659820.0"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:11:21.554Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "2w9qP4BcfDZm7PJ7N3C1",
      "question_number": 79,
      "page": 16,
      "question_text": "A company's developers use Amazon EC2 instances as remote workstations. The company is concerned that users can create or modify EC2 security groups to allow unrestricted inbound access.\nA DevOps engineer needs to develop a solution to detect when users create unrestricted security group rules. The solution must detect changes to security group rules in near real time, remove unrestricted rules, and send email notifications to the security team. The DevOps engineer has created an AWS Lambda function that checks for security group ID from input, removes rules that grant unrestricted access, and sends notifications through Amazon Simple Notification Service (Amazon SNS).\nWhat should the DevOps engineer do next to meet the requirements?",
      "choices": {
        "A": "Configure the Lambda function to be invoked by the SNS topic. Create an AWS CloudTrail subscription for the SNS topic. Configure a subscription filter for security group modification events.",
        "B": "Create an Amazon EventBridge scheduled rule to invoke the Lambda function. Define a schedule pattern that runs the Lambda function every hour.",
        "D": "Create an Amazon EventBridge custom event bus that subscribes to events from all AWS services. Configure the Lambda function to be invoked by the custom event bus.",
        "C": "Create an Amazon EventBridge event rule that has the default event bus as the source. Define the rule’s event pattern to match EC2 security group creation and modification events. Configure the rule to invoke the Lambda function."
      },
      "correct_answer": "C",
      "answer_ET": "C",
      "answers_community": [
        "C (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/105520-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-04-07 15:48:00",
      "unix_timestamp": 1680875280,
      "discussion_count": 10,
      "discussion": [
        {
          "timestamp": "1722170460.0",
          "comment_id": "1134175",
          "upvote_count": "6",
          "poster": "thanhnv142",
          "content": "C is correct:\nA: lambda should be invoked by Eventbridge\nB: we need to act when there is events, not schedully\nD: subscribing to events from ALL AWS services incurs a huge cost"
        },
        {
          "poster": "01037",
          "timestamp": "1731339540.0",
          "content": "Selected Answer: C\nC of course.\nBut A seems working, and does Aws Config work in this situation?",
          "upvote_count": "1",
          "comment_id": "1209807"
        },
        {
          "comment_id": "1209531",
          "poster": "c3518fc",
          "content": "Selected Answer: C\nBy creating an EventBridge event rule with the appropriate event pattern and configuring it to invoke the Lambda function, the DevOps engineer can effectively detect security group rule changes in near real-time, remove unrestricted rules, and send notifications to the security team. This solution leverages the event-driven architecture of EventBridge and the serverless execution of AWS Lambda, providing a scalable and efficient way to meet the company's security requirements.",
          "upvote_count": "3",
          "timestamp": "1731278940.0"
        },
        {
          "upvote_count": "2",
          "content": "selected answer:C",
          "poster": "meriemheni",
          "timestamp": "1719259680.0",
          "comment_id": "1104877"
        },
        {
          "upvote_count": "4",
          "timestamp": "1702060140.0",
          "poster": "madperro",
          "comment_id": "918476",
          "content": "Selected Answer: C\nC the default bus includes events from AWS services.\nhttps://docs.aws.amazon.com/eventbridge/latest/userguide/eb-event-bus.html"
        },
        {
          "upvote_count": "4",
          "poster": "bcx",
          "comment_id": "910327",
          "content": "Selected Answer: C\nWrong answers:\nA. SNS is used here to send a notification post-facto\nB. The question requires \"near real time\", an hour is not \"near real time\"\nD. AWS events come on the default event bus, you do not need a custom event bus",
          "timestamp": "1701362220.0",
          "comments": [
            {
              "content": "The default event bus in each account receives events from AWS services.\n\nA custom event bus sends events to or receives events from a different account.\n\nA custom event bus sends events to or receives events from a different Region to aggregate events in a single location.\n\nA partner event bus receives events from a SaaS partner.",
              "upvote_count": "4",
              "poster": "Aja1",
              "timestamp": "1707221580.0",
              "comment_id": "973713"
            }
          ]
        },
        {
          "poster": "haazybanj",
          "comment_id": "886862",
          "upvote_count": "4",
          "content": "Selected Answer: C\nTo meet the requirements, the DevOps engineer should create an Amazon EventBridge event rule that has the default event bus as the source. The rule's event pattern should match EC2 security group creation and modification events, and it should be configured to invoke the Lambda function. This solution will allow for near real-time detection of security group rule changes and will trigger the Lambda function to remove any unrestricted rules and send email notifications to the security team.",
          "timestamp": "1698891480.0"
        },
        {
          "upvote_count": "2",
          "comment_id": "870458",
          "content": "C is the answer",
          "timestamp": "1697314980.0",
          "poster": "alce2020"
        },
        {
          "poster": "5aga",
          "upvote_count": "4",
          "comment_id": "869976",
          "content": "Selected Answer: C\nC. Create an Amazon EventBridge event rule that has the default event bus as the source. Define the rule’s event pattern to match EC2 security group creation and modification events. Configure the rule to invoke the Lambda function.\n\nThe solution requires near real-time detection of changes to security group rules and immediate action to remove unrestricted rules and send email notifications to the security team. The AWS Lambda function created by the DevOps engineer can perform these actions, but it needs to be invoked whenever a security group rule is modified.\n\nAmazon EventBridge is a serverless event bus service that can receive and process events from various AWS services, including Amazon EC2 and Amazon SNS. An EventBridge event rule with the default event bus as the source can be created to match EC2 security group creation and modification events. This rule can then be configured to invoke the Lambda function, which can remove unrestricted rules and send email notifications to the security team.",
          "timestamp": "1697261820.0"
        },
        {
          "upvote_count": "3",
          "timestamp": "1696686480.0",
          "poster": "ele",
          "content": "Selected Answer: C\nhttps://repost.aws/knowledge-center/monitor-security-group-changes-ec2",
          "comment_id": "863942"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:11:21.554Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "fKt8itxnQ3yqgL9nB9nE",
      "question_number": 80,
      "page": 16,
      "question_text": "A company uses an organization in AWS Organizations to manage its AWS accounts. The company recently acquired another company that has standalone AWS accounts. The acquiring company's DevOps team needs to consolidate the administration of the AWS accounts for both companies and retain full administrative control of the accounts. The DevOps team also needs to collect and group findings across all the accounts to implement and maintain a security posture.\n\nWhich combination of steps should the DevOps team take to meet these requirements? (Choose two.)",
      "choices": {
        "D": "Use AWS Firewall Manager to collect and group findings across all accounts. Enable all features for the organization. Designate an account in the organization as the delegated administrator account for Firewall Manager.",
        "E": "Use Amazon Inspector to collect and group findings across all accounts. Designate an account in the organization as the delegated administrator account for Amazon Inspector.",
        "B": "Invite the acquired company's AWS accounts to join the organization. Create the OrganizationAccountAccessRole IAM role in the invited accounts. Grant permission to the management account to assume the role.",
        "A": "Invite the acquired company's AWS accounts to join the organization. Create an SCP that has full administrative privileges. Attach the SCP to the management account.",
        "C": "Use AWS Security Hub to collect and group findings across all accounts. Use Security Hub to automatically detect new accounts as the accounts are added to the organization."
      },
      "correct_answer": "BC",
      "answer_ET": "BC",
      "answers_community": [
        "BC (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/129732-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-12-29 18:35:00",
      "unix_timestamp": 1703871300,
      "discussion_count": 7,
      "discussion": [
        {
          "content": "Selected Answer: BC\nB and C are correct: <needs to consolidate the administration of the AWS accounts for both companies and retain full administrative control of the accounts> means we need to invite the new accounts to the existing AWS organization. <collect and group findings across all the accounts> means security hub\nA: <Attach the SCP to the management account.>: this is incorrect\nD: Firewall manager is use to centrally manage all FWs, not to collect and group findings\nE: Inspector is used for vulnerability scanning only",
          "poster": "thanhnv142",
          "upvote_count": "6",
          "timestamp": "1723378980.0",
          "comment_id": "1147287"
        },
        {
          "poster": "c3518fc",
          "content": "Selected Answer: BC\nB) https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_accounts_invites.html\nC) https://docs.aws.amazon.com/securityhub/latest/userguide/securityhub-concepts.html",
          "upvote_count": "2",
          "timestamp": "1729786020.0",
          "comment_id": "1201496"
        },
        {
          "content": "Selected Answer: BC\nanswer is BC",
          "comment_id": "1195150",
          "upvote_count": "2",
          "poster": "dkp",
          "timestamp": "1728857880.0"
        },
        {
          "comment_id": "1174964",
          "content": "Selected Answer: BC\nB - Add accounts to the org\nC - collect and group findings",
          "upvote_count": "2",
          "poster": "DanShone",
          "timestamp": "1726484940.0"
        },
        {
          "upvote_count": "4",
          "timestamp": "1720788180.0",
          "content": "Selected Answer: BC\nAnswer B and C https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_accounts_invites.html",
          "poster": "yuliaqwerty",
          "comment_id": "1120853"
        },
        {
          "upvote_count": "1",
          "timestamp": "1719781380.0",
          "comment_id": "1110863",
          "content": "Selected Answer: BC\nThey seem correct",
          "poster": "d262e67"
        },
        {
          "poster": "csG13",
          "timestamp": "1719675300.0",
          "comment_id": "1108952",
          "content": "Selected Answer: BC\nB is required to access from the management account the new account.\nC will provide an aggregate view of all the security findings.",
          "upvote_count": "3"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:11:21.554Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "l90wmLMdxCHmavEwyhR4",
      "question_number": 81,
      "page": 17,
      "question_text": "A company has an application and a CI/CD pipeline. The CI/CD pipeline consists of an AWS CodePipeline pipeline and an AWS CodeBuild project. The CodeBuild project runs tests against the application as part of the build process and outputs a test report. The company must keep the test reports for 90 days.\n\nWhich solution will meet these requirements?",
      "choices": {
        "B": "Add a report group in the CodeBuild project buildspec file with the appropriate path and format for the reports. Create an Amazon S3 bucket to store the reports. Configure an Amazon EventBridge rule that invokes an AWS Lambda function to copy the reports to the S3 bucket when a build is completed. Create an S3 Lifecycle rule to expire the objects after 90 days.",
        "D": "Add a report group in the CodeBuild project buildspec file with the appropriate path and format for the reports. Create an Amazon S3 bucket to store the reports. Configure the report group as an artifact in the CodeBuild project buildspec file. Configure the S3 bucket as the artifact destination. Set the object expiration to 90 days.",
        "A": "Add a new stage in the CodePipeline pipeline after the stage that contains the CodeBuild project. Create an Amazon S3 bucket to store the reports. Configure an S3 deploy action type in the new CodePipeline stage with the appropriate path and format for the reports.",
        "C": "Add a new stage in the CodePipeline pipeline. Configure a test action type with the appropriate path and format for the reports. Configure the report expiration time to be 90 days in the CodeBuild project buildspec file."
      },
      "correct_answer": "B",
      "answer_ET": "B",
      "answers_community": [
        "B (65%)",
        "D (35%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/129734-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-12-29 18:54:00",
      "unix_timestamp": 1703872440,
      "discussion_count": 21,
      "discussion": [
        {
          "timestamp": "1708182960.0",
          "upvote_count": "10",
          "comment_id": "1152637",
          "content": "Selected Answer: D\nBest Option: Option D appears to be the most straightforward and effective solution that meets the requirements. It simplifies the process by utilizing CodeBuild's feature to directly send reports to an S3 bucket configured as the artifact destination. By setting the object expiration to 90 days in the S3 bucket settings, it fulfills the requirement to keep the test reports for 90 days. This option does not require additional services for moving the reports to S3, assuming the CodeBuild report group configuration allows for direct report storage in S3 with specified retention policies.",
          "poster": "kyuhuck"
        },
        {
          "timestamp": "1710297300.0",
          "content": "Selected Answer: B\nquestion key word : \nhow to expire objects in S3.\n\nonly S3 Lifecycle rule can expire objects.",
          "poster": "Seoyong",
          "upvote_count": "7",
          "comment_id": "1172198",
          "comments": [
            {
              "comment_id": "1210243",
              "content": "I think this is the most valid point.",
              "upvote_count": "2",
              "timestamp": "1715522280.0",
              "poster": "b0gdan433"
            }
          ]
        },
        {
          "timestamp": "1745179740.0",
          "upvote_count": "1",
          "poster": "GripZA",
          "comment_id": "1562321",
          "content": "Selected Answer: D\nTorn between B and D TBH, looking at documentation:\n\"If you want to upload the raw data of your test report results to an Amazon S3 bucket:\nSelect Export to Amazon S3.\" \n\nHere it clearly mentions test report results, not export as artifact. \n\nThen a little further down when referring to encryption, doc says \"Disable artifact encryption to disable encryption. You might choose this if you want to share your test results, or publish them to a static website. \"\n\nNow referring to it as an artifact...\n\nBut this changes if you specifically configure them as artifacts in the buildspec file."
        },
        {
          "content": "Selected Answer: D\nphases:\n build:\n commands:\n - echo \"Running tests...\"\n - run_tests_command\nartifacts:\n files:\n - path/to/test_report/*\n discard-paths: no\n base-directory: path/to/test_report",
          "poster": "spring21",
          "timestamp": "1733707320.0",
          "upvote_count": "1",
          "comment_id": "1323820"
        },
        {
          "poster": "xdkonorek2",
          "timestamp": "1714936500.0",
          "content": "Selected Answer: D\nI think D even though this sentence sounds ridiculous: \"Configure the report group as an artifact in the CodeBuild project buildspec file\" they probably meant that report should be sent as an artifact and that's correct.",
          "comments": [
            {
              "comment_id": "1270196",
              "content": "D is incorrect because it talks about storing report groups as artifacts but since report groups are not CodeBuild artifacts, this cannot be done.",
              "timestamp": "1724251980.0",
              "upvote_count": "2",
              "poster": "flaacko"
            }
          ],
          "comment_id": "1207042",
          "upvote_count": "2"
        },
        {
          "poster": "seetpt",
          "comment_id": "1205496",
          "timestamp": "1714651800.0",
          "upvote_count": "2",
          "content": "Selected Answer: B\nI think B"
        },
        {
          "comment_id": "1201507",
          "upvote_count": "5",
          "timestamp": "1713975540.0",
          "content": "Selected Answer: B\nNot sure why we are configuring the report group as an artifact in the CodeBuild project buildspec file. Doesn't make sense and we use S3 lifecycle to expire objects automatically.",
          "poster": "c3518fc"
        },
        {
          "poster": "dkp",
          "upvote_count": "5",
          "timestamp": "1713047880.0",
          "comment_id": "1195153",
          "content": "Selected Answer: B\nBoth B & D seem to work, but object expiration settings still need to set the lifecycle rule manually for option D"
        },
        {
          "timestamp": "1711477020.0",
          "comment_id": "1183511",
          "poster": "stoy123",
          "content": "Selected Answer: D\nanswer D",
          "upvote_count": "1"
        },
        {
          "poster": "CloudHandsOn",
          "comment_id": "1180727",
          "timestamp": "1711184700.0",
          "content": "Selected Answer: D\nOptions B and D both provide viable solutions to meet the requirements. However, D offers a more direct and simplified approach by leveraging the capabilities of AWS CodeBuild and Amazon S3, including the use of S3 Lifecycle policies for managing the expiration of the test reports. Option B is also a valid solution but involves additional components like EventBridge and Lambda, which might not be necessary for this specific requirement. Therefore, D is the recommended solution for its simplicity and direct alignment with the requirements.",
          "upvote_count": "1"
        },
        {
          "upvote_count": "2",
          "poster": "master9",
          "timestamp": "1710209220.0",
          "comment_id": "1171394",
          "content": "Selected Answer: D\nrtifacts:\n files:\n - '**/*'\n name: '<artifact-name>'\n artifactPrefix: '<path-prefix>'\n discardPaths: yes\n baseDirectory: 'test-reports'\n reports:\n reportGroupName:\n files:\n - '**/*'\n baseDirectory: 'test-reports'\n\nReplace <path-prefix> with the desired path prefix within the S3 bucket."
        },
        {
          "poster": "dzn",
          "upvote_count": "2",
          "timestamp": "1709619840.0",
          "comment_id": "1166253",
          "content": "Selected Answer: D\nAWS Lambda is not necessary. Test report files specified in the `base-directory` and `files` in the buildspec.yml reports section are uploaded to S3 by specifying them in the artifacts section as well. This is mean of `Configure the report group as an artifact in the CodeBuild project buildspec file.`"
        },
        {
          "timestamp": "1707731160.0",
          "upvote_count": "4",
          "poster": "thanhnv142",
          "comment_id": "1147948",
          "content": "Selected Answer: B\nB is correct: <build process and outputs a test report.> means we need report group in codebuild and store report in S3\nA: No mention of report group in codebuild \nC: No mention of s3 and report group\nD: report group is not the same as an artifact"
        },
        {
          "upvote_count": "4",
          "comment_id": "1126083",
          "content": "Selected Answer: B\nB is correct",
          "timestamp": "1705599300.0",
          "poster": "twogyt"
        },
        {
          "comments": [
            {
              "content": "B is not exactly use this pattern, but using eventBridge also works",
              "poster": "a54b16f",
              "comment_id": "1121062",
              "timestamp": "1705088280.0",
              "upvote_count": "1"
            }
          ],
          "comment_id": "1121061",
          "timestamp": "1705088100.0",
          "content": "Selected Answer: B\nPattern: run lambda inside PostBuild to zip up unit test result folder and copy to S3",
          "upvote_count": "3",
          "poster": "a54b16f"
        },
        {
          "upvote_count": "4",
          "timestamp": "1704993900.0",
          "poster": "a54b16f",
          "comment_id": "1120007",
          "content": "Selected Answer: B\nhttps://malsouli.medium.com/10-smart-ways-to-use-aws-codebuild-22a8ee0d9302 \n\nThe test reports have to be handled separately , either via postbuild, or via eventbridge as suggested in B"
        },
        {
          "timestamp": "1704810060.0",
          "content": "Selected Answer: B\ndefinitely B",
          "poster": "JayF88",
          "upvote_count": "4",
          "comment_id": "1117558"
        },
        {
          "comment_id": "1111716",
          "content": "Selected Answer: B\n\"B. Add a report group in the CodeBuild project buildspec file with the appropriate path and format for the reports. Create an Amazon S3 bucket to store the reports. Configure an Amazon EventBridge rule that invokes an AWS Lambda function to copy the reports to the S3 bucket when a build is completed. Create an S3 Lifecycle rule to expire the objects after 90 days.\": A test report expires 30 days after it is created. You cannot view an expired test report, but you can export the test results to raw test result files in an S3 bucket. Exported raw test files do not expire. https://docs.aws.amazon.com/codebuild/latest/userguide/test-report.html when you export a test group to s3, it is zipped https://docs.aws.amazon.com/codebuild/latest/userguide/test-report-group-create-cli.html",
          "timestamp": "1704184140.0",
          "upvote_count": "5",
          "poster": "ozansenturk"
        },
        {
          "poster": "d262e67",
          "upvote_count": "4",
          "comment_id": "1110878",
          "content": "Selected Answer: B\nD might sound a better option but the problem is that \"report groups\" cannot be a part of artifacts. If you check the Buildspec syntax in the link below, you'll see that artifacts are separated from report groups. Under artifacts you cannot specify the arn of the report group.\n\nhttps://docs.aws.amazon.com/codebuild/latest/userguide/build-spec-ref.html#reports-buildspec-file",
          "timestamp": "1704064920.0"
        },
        {
          "upvote_count": "2",
          "poster": "PrasannaBalaji",
          "comment_id": "1109424",
          "timestamp": "1703914140.0",
          "content": "Selected Answer: D\nD is correct."
        },
        {
          "upvote_count": "3",
          "comment_id": "1108965",
          "timestamp": "1703872440.0",
          "content": "Selected Answer: D\nit's D",
          "poster": "csG13"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:11:32.017Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "weR8WvCCDW5iQyYxTAYV",
      "question_number": 82,
      "page": 17,
      "question_text": "A company uses an Amazon API Gateway regional REST API to host its application API. The REST API has a custom domain. The REST API's default endpoint is deactivated.\n\nThe company's internal teams consume the API. The company wants to use mutual TLS between the API and the internal teams as an additional layer of authentication.\n\nWhich combination of steps will meet these requirements? (Choose two.)",
      "choices": {
        "E": "Upload the root private certificate authority (CA) certificate to an Amazon S3 bucket. Configure the API Gateway mutual TLS to use the private CA certificate that is stored in the S3 bucket as the trust store.",
        "C": "Upload the provisioned client certificate to an Amazon S3 bucket. Configure the API Gateway mutual TLS to use the client certificate that is stored in the S3 bucket as the trust store.",
        "D": "Upload the provisioned client certificate private key to an Amazon S3 bucket. Configure the API Gateway mutual TLS to use the private key that is stored in the S3 bucket as the trust store.",
        "B": "Provision a client certificate that is signed by a public certificate authority (CA). Import the certificate into AWS Certificate Manager (ACM).",
        "A": "Use AWS Certificate Manager (ACM) to create a private certificate authority (CA). Provision a client certificate that is signed by the private CA."
      },
      "correct_answer": "AE",
      "answer_ET": "AE",
      "answers_community": [
        "AE (79%)",
        "AC (15%)",
        "6%"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/129685-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-12-29 14:29:00",
      "unix_timestamp": 1703856540,
      "discussion_count": 13,
      "discussion": [
        {
          "comments": [
            {
              "upvote_count": "1",
              "timestamp": "1719783480.0",
              "content": "https://docs.aws.amazon.com/apigateway/latest/developerguide/rest-api-mutual-tls.html",
              "comment_id": "1110883",
              "poster": "d262e67"
            }
          ],
          "poster": "d262e67",
          "comment_id": "1110882",
          "timestamp": "1719783480.0",
          "content": "Selected Answer: AE\nA. Because it's only for internal teams.\nE. Because the truststore dictates which CAs to trust. If you have intermediate CAs those also need to be present in the S3 bucket.",
          "upvote_count": "6"
        },
        {
          "content": "Selected Answer: AE\nC. Upload client certificate to S3\n– The trust store should contain the CA certificate, not individual client certificates. API Gateway validates the client cert against the CA, not by comparing the cert directly.",
          "poster": "nickp84",
          "comment_id": "1568506",
          "timestamp": "1747071240.0",
          "upvote_count": "1"
        },
        {
          "poster": "Jay_2pt0_1",
          "comment_id": "1208772",
          "content": "Selected Answer: AE\nA. use ACM to generate cert\nE. See https://aws.amazon.com/blogs/compute/introducing-mutual-tls-authentication-for-amazon-api-gateway/",
          "timestamp": "1731149280.0",
          "upvote_count": "2"
        },
        {
          "timestamp": "1729168560.0",
          "comment_id": "1197235",
          "upvote_count": "4",
          "content": "Selected Answer: AE\nC is incorrect because the trust store should contain the root CA certificate, not the client certificate.\nRoot CA certificate is used to validate the client certificates (can be many) presented by the clients. If the client certificate itself is in the trust store, it would mean that only that specific client is trusted, which is not practical in a scenario where there are multiple clients (read it as company's internal teams).",
          "poster": "didek1986"
        },
        {
          "upvote_count": "1",
          "content": "Selected Answer: AC\nA and C. Details are everything in an Investigation...\nWhat API Gateway needs is the Client Certificate generated by option A and not the CA",
          "comment_id": "1186452",
          "timestamp": "1727644860.0",
          "poster": "WhyIronMan"
        },
        {
          "content": "Selected Answer: AE\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/rest-api-mutual-tls.html\nAfter reading the above documentation I would determine A & E",
          "poster": "DanShone",
          "comment_id": "1174962",
          "timestamp": "1726484520.0",
          "upvote_count": "4"
        },
        {
          "upvote_count": "1",
          "comment_id": "1173565",
          "timestamp": "1726323840.0",
          "poster": "Nano803",
          "content": "Selected Answer: AC\nCheck this article, https://docs.aws.amazon.com/apigateway/latest/developerguide/rest-api-mutual-tls.html. You need to upload the truststore to an Amazon S3 bucket in a single file"
        },
        {
          "comment_id": "1149212",
          "upvote_count": "3",
          "timestamp": "1723551780.0",
          "content": "Selected Answer: AE\nAfter reading this I would suggest A & E",
          "poster": "Ramdi1"
        },
        {
          "poster": "thanhnv142",
          "comment_id": "1147954",
          "upvote_count": "1",
          "content": "Selected Answer: AC\nA and C is correct: \nA: we prefer AWS service more than a public one, which is B\nB: The reason is explained in option a\nC: Upload the provisioned to S3 bucket. \nD: should not upload private key to anywhere. \nE: This option has no connection to option A.",
          "timestamp": "1723449240.0"
        },
        {
          "timestamp": "1722963660.0",
          "content": "Selected Answer: BE\nDetails can be found here: https://aws.amazon.com/blogs/compute/introducing-mutual-tls-authentication-for-amazon-api-gateway/",
          "comment_id": "1142499",
          "poster": "Spavanko",
          "upvote_count": "2"
        },
        {
          "content": "Selected Answer: AE\nyou need to use ROOT CA , or whatever the certificated being used to sign other certificate in truststore,",
          "timestamp": "1720712040.0",
          "poster": "a54b16f",
          "comment_id": "1120016",
          "upvote_count": "3"
        },
        {
          "timestamp": "1719785580.0",
          "poster": "kabary",
          "comments": [
            {
              "content": "Option C and Option D involve uploading the client certificate or its private key to an S3 bucket and configuring the API Gateway to use them as the trust store. This is not a recommended practice as it exposes sensitive information to potential security risks. The trust store for mutual TLS should typically involve the CA certificate or a certificate chain that verifies the client certificates, not the client certificates or private keys themselves.",
              "upvote_count": "2",
              "comment_id": "1115033",
              "timestamp": "1720245600.0",
              "poster": "matanasov"
            }
          ],
          "content": "Selected Answer: AC\nYou shall NEVER upload private cert or key to an S3 bucket. This is a bad practise and hence C.\n\nI also choose A because you need private cert between the internal teams and the API.",
          "upvote_count": "2",
          "comment_id": "1110897"
        },
        {
          "timestamp": "1719660540.0",
          "content": "Selected Answer: AE\nA and E",
          "poster": "PrasannaBalaji",
          "upvote_count": "3",
          "comment_id": "1108714"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:11:32.017Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "VdwgbPo3mS7itU0RyAKl",
      "question_number": 83,
      "page": 17,
      "question_text": "A company uses AWS Directory Service for Microsoft Active Directory as its identity provider (IdP). The company requires all infrastructure to be defined and deployed by AWS CloudFormation.\n\nA DevOps engineer needs to create a fleet of Windows-based Amazon EC2 instances to host an application. The DevOps engineer has created a CloudFormation template that contains an EC2 launch template, IAM role, EC2 security group, and EC2 Auto Scaling group. The DevOps engineer must implement a solution that joins all EC2 instances to the domain of the AWS Managed Microsoft AD directory.\n\nWhich solution will meet these requirements with the MOST operational efficiency?",
      "choices": {
        "A": "In the CloudFormation template, create an AWS::SSM::Document resource that joins the EC2 instance to the AWS Managed Microsoft AD domain by using the parameters for the existing directory. Update the launch template to include the SSMAssociation property to use the new SSM document. Attach the AmazonSSMManagedInstanceCore and AmazonSSMDirectoryServiceAccess AWS managed policies to the IAM role that the EC2 instances use.",
        "C": "Store the existing AWS Managed Microsoft AD domain connection details in AWS Secrets Manager. In the CloudFormation template, create an AWS::SSM::Association resource to associate the AWS-CreateManagedWindowsInstanceWithApproval Automation runbook with the EC2 Auto Scaling group. Pass the ARNs for the parameters from Secrets Manager to join the domain. Attach the AmazonSSMDirectoryServiceAccess and SecretsManagerReadWrite AWS managed policies to the IAM role that the EC2 instances use.",
        "B": "In the CloudFormation template, update the launch template to include specific tags that propagate on launch. Create an AWS::SSM::Association resource to associate the AWS-JoinDirectoryServiceDomain Automation runbook with the EC2 instances that have the specified tags. Define the required parameters to join the AWS Managed Microsoft AD directory. Attach the AmazonSSMManagedInstanceCore and AmazonSSMDirectoryServiceAccess AWS managed policies to the IAM role that the EC2 instances use.",
        "D": "Store the existing AWS Managed Microsoft AD domain administrator credentials in AWS Secrets Manager. In the CloudFormation template, update the EC2 launch template to include user data. Configure the user data to pull the administrator credentials from Secrets Manager and to join the AWS Managed Microsoft AD domain. Attach the AmazonSSMManagedInstanceCore and SecretsManagerReadWrite AWS managed policies to the IAM role that the EC2 instances use."
      },
      "correct_answer": "B",
      "answer_ET": "B",
      "answers_community": [
        "B (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/129833-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-12-30 06:32:00",
      "unix_timestamp": 1703914320,
      "discussion_count": 8,
      "discussion": [
        {
          "poster": "aws_god",
          "comment_id": "1281388",
          "timestamp": "1725949260.0",
          "upvote_count": "1",
          "content": "Selected Answer: B\nB is correct - https://docs.aws.amazon.com/directoryservice/latest/admin-guide/step4_test_ec2_access.html"
        },
        {
          "comment_id": "1195161",
          "upvote_count": "2",
          "timestamp": "1713050340.0",
          "content": "Selected Answer: B\nans is B",
          "poster": "dkp"
        },
        {
          "upvote_count": "3",
          "comment_id": "1147959",
          "poster": "thanhnv142",
          "timestamp": "1707732360.0",
          "content": "Selected Answer: B\nB is correct: we need to use AWS:SSM::Document with the AWS-JoinDirectoryServiceDomain automation runbook for this task\nA: no mention of the name of runbook to join domain \nC: AWS-CreateManagedWindowsInstanceWithApproval Automation runbook is used for creating a windows instance, not to join domain\nD: no mention of AWS::SSM::Document"
        },
        {
          "upvote_count": "2",
          "poster": "twogyt",
          "comment_id": "1126087",
          "timestamp": "1705599840.0",
          "content": "Selected Answer: B\nB is correct"
        },
        {
          "content": "Selected Answer: B\nkeyword: JoinDirectoryServiceDomain",
          "upvote_count": "2",
          "comment_id": "1121071",
          "timestamp": "1705088880.0",
          "poster": "a54b16f"
        },
        {
          "poster": "d262e67",
          "timestamp": "1704066720.0",
          "upvote_count": "2",
          "content": "Selected Answer: B\nMust be B:\nhttps://docs.aws.amazon.com/systems-manager/latest/userguide/walkthrough-powershell.html#walkthrough-powershell-domain-join",
          "comment_id": "1110887"
        },
        {
          "upvote_count": "2",
          "poster": "csG13",
          "comment_id": "1109509",
          "timestamp": "1703923500.0",
          "content": "Selected Answer: B\nIt’s B"
        },
        {
          "content": "Selected Answer: B\nB is correct",
          "timestamp": "1703914320.0",
          "comment_id": "1109426",
          "upvote_count": "2",
          "poster": "PrasannaBalaji"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:11:32.017Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "5sv1i2wd6KWAiPEpQwe5",
      "question_number": 84,
      "page": 17,
      "question_text": "A company uses AWS Organizations to manage its AWS accounts. The company has a root OU that has a child OU. The root OU has an SCP that allows all actions on all resources. The child OU has an SCP that allows all actions for Amazon DynamoDB and AWS Lambda, and denies all other actions.\n\nThe company has an AWS account that is named vendor-data in the child OU. A DevOps engineer has an IAM user that is attached to the Administrator Access IAM policy in the vendor-data account. The DevOps engineer attempts to launch an Amazon EC2 instance in the vendor-data account but receives an access denied error.\n\nWhich change should the DevOps engineer make to launch the EC2 instance in the vendor-data account?",
      "choices": {
        "B": "Create a new SCP that allows all actions for Amazon EC2. Attach the SCP to the vendor-data account.",
        "D": "Create a new SCP that allows all actions for Amazon EC2. Attach the SCP to the root OU.",
        "C": "Update the SCP in the child OU to allow all actions for Amazon EC2.",
        "A": "Attach the AmazonEC2FullAccess IAM policy to the IAM user."
      },
      "correct_answer": "C",
      "answer_ET": "C",
      "answers_community": [
        "C (72%)",
        "B (28%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/129736-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-12-29 19:07:00",
      "unix_timestamp": 1703873220,
      "discussion_count": 14,
      "discussion": [
        {
          "timestamp": "1721905620.0",
          "comments": [
            {
              "poster": "Srikantha",
              "upvote_count": "1",
              "content": "Deny takes precedence",
              "timestamp": "1743332100.0",
              "comment_id": "1411997"
            }
          ],
          "poster": "ericphl",
          "upvote_count": "7",
          "content": "Selected Answer: B\nI vote B. can't understand why B is not correct answer. SCP can be attached to account.\n\nFor the C, it is possible. but the potential risk is it's not only allow all EC2 action on \"vendor-data\" account, but also allow all EC2 actions in other account under the child OU. which is not a best practice.",
          "comment_id": "1254943"
        },
        {
          "timestamp": "1703873220.0",
          "comment_id": "1108979",
          "upvote_count": "5",
          "content": "Selected Answer: C\nIt's C - Allow must be explicit from root all the way down to the account level. Since it's not specified in the OU the only way to make it available to vendor-account is to change the OU policy.",
          "poster": "csG13"
        },
        {
          "content": "Selected Answer: C\nThis ensures that IAM policies in the vendor-data account can grant EC2 permissions, resolving the issue.",
          "poster": "Srikantha",
          "upvote_count": "1",
          "comment_id": "1411998",
          "timestamp": "1743332160.0"
        },
        {
          "upvote_count": "2",
          "comment_id": "1266841",
          "poster": "Abilash2605",
          "content": "Answer is C. You can attach SCP to vendor-data account. however there is deny rule at OU level and that will apply and without updating that your SCP at vendor data account is not useful. As the account will inherit SCP applied at OU.",
          "timestamp": "1723786860.0"
        },
        {
          "content": "Selected Answer: C\nB - Incorrect IMO - The question doesn't ask about taking away anything currently allowed in the existing SCP",
          "poster": "auxwww",
          "comment_id": "1264763",
          "timestamp": "1723487760.0",
          "upvote_count": "1"
        },
        {
          "upvote_count": "3",
          "timestamp": "1713976620.0",
          "content": "Selected Answer: C\nBy updating the SCP in the child OU to allow all actions for Amazon EC2, the DevOps engineer can grant the necessary permissions to launch EC2 instances in the vendor-data account while maintaining the desired restrictions for other services and accounts within the child OU.",
          "comment_id": "1201516",
          "poster": "c3518fc"
        },
        {
          "comment_id": "1195162",
          "poster": "dkp",
          "upvote_count": "3",
          "content": "Selected Answer: C\nanswer is C",
          "timestamp": "1713050520.0"
        },
        {
          "comment_id": "1186455",
          "content": "Selected Answer: C\nC, details are everything during an investigation",
          "upvote_count": "2",
          "timestamp": "1711841280.0",
          "poster": "WhyIronMan"
        },
        {
          "poster": "stoy123",
          "comment_id": "1183529",
          "timestamp": "1711478640.0",
          "upvote_count": "2",
          "comments": [
            {
              "comment_id": "1184653",
              "timestamp": "1711616820.0",
              "poster": "stoy123",
              "upvote_count": "2",
              "content": "Edit: C is correct"
            }
          ],
          "content": "Selected Answer: B\nB is the correct answer!!!!"
        },
        {
          "timestamp": "1710593640.0",
          "poster": "DanShone",
          "upvote_count": "2",
          "content": "Selected Answer: C\nC is correct",
          "comment_id": "1174960"
        },
        {
          "content": "Selected Answer: C\nC is correct: \nA: We need to modify SCP not IAM policy\nB: SCP is attached to OUs, not account\nD: This option changes nothing, as the roout OU has already allowed all actions",
          "upvote_count": "3",
          "comments": [],
          "comment_id": "1147960",
          "timestamp": "1707732600.0",
          "poster": "thanhnv142"
        },
        {
          "upvote_count": "1",
          "poster": "a54b16f",
          "comment_id": "1121072",
          "timestamp": "1705089000.0",
          "content": "Selected Answer: C\nupdate policy to include EC2"
        },
        {
          "comment_id": "1110891",
          "poster": "d262e67",
          "timestamp": "1704066960.0",
          "upvote_count": "1",
          "content": "Selected Answer: C\nThe only correct option"
        },
        {
          "comment_id": "1109429",
          "upvote_count": "1",
          "timestamp": "1703914380.0",
          "content": "Selected Answer: C\nC is correct",
          "poster": "PrasannaBalaji"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:11:32.017Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "Zrps2jWJOm2f3suzNwal",
      "question_number": 85,
      "page": 17,
      "question_text": "A company's security policies require the use of security hardened AMIs in production environments. A DevOps engineer has used EC2 Image Builder to create a pipeline that builds the AMIs on a recurring schedule.\n\nThe DevOps engineer needs to update the launch templates of the company's Auto Scaling groups. The Auto Scaling groups must use the newest AMIs during the launch of Amazon EC2 instances.\n\nWhich solution will meet these requirements with the MOST operational efficiency?",
      "choices": {
        "A": "Configure an Amazon EventBridge rule to receive new AMI events from Image Builder. Target an AWS Systems Manager Run Command document that updates the launch templates of the Auto Scaling groups with the newest AMI ID.",
        "C": "Configure the launch template to use a value from AWS Systems Manager Parameter Store for the AMI ID. Configure the Image Builder pipeline to update the Parameter Store value with the newest AMI ID.",
        "B": "Configure an Amazon EventBridge rule to receive new AMI events from Image Builder. Target an AWS Lambda function that updates the launch templates of the Auto Scaling groups with the newest AMI ID.",
        "D": "Configure the Image Builder distribution settings to update the launch templates with the newest AMI IConfigure the Auto Scaling groups to use the newest version of the launch template."
      },
      "correct_answer": "D",
      "answer_ET": "D",
      "answers_community": [
        "D (62%)",
        "C (26%)",
        "12%"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/129737-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-12-29 19:17:00",
      "unix_timestamp": 1703873820,
      "discussion_count": 18,
      "discussion": [
        {
          "poster": "thanhnv142",
          "upvote_count": "7",
          "comment_id": "1147969",
          "content": "Selected Answer: D\nD is correct: Image builder has a built-in that allow updating EC2 launch template\nA: AWS Systems Manager Run Command document is used for running scripts on EC2, not to update\nB: Lambda is used for other tasks, not this one\nC: This seems to be a feasible option, but we can update the launch template directly without using parameter store",
          "timestamp": "1707733020.0"
        },
        {
          "content": "Selected Answer: D\nDefinitely D according to this:\nhttps://docs.aws.amazon.com/imagebuilder/latest/userguide/dist-using-launch-template.html",
          "upvote_count": "6",
          "timestamp": "1704067200.0",
          "poster": "d262e67",
          "comment_id": "1110893"
        },
        {
          "timestamp": "1747380420.0",
          "comment_id": "1569289",
          "poster": "nickp84",
          "upvote_count": "1",
          "content": "Selected Answer: C\nLaunch templates can reference an SSM Parameter Store value for the AMI ID.\nWhen Image Builder updates that parameter, new EC2 instances launched by the Auto Scaling group will automatically use the new AMI—no need to update the launch template or its version.\nThis is fully supported, automated, and operationally efficient."
        },
        {
          "upvote_count": "2",
          "content": "Selected Answer: C\nUsing Parameter Store as a centralized AMI reference ensures that new instances always use the latest AMI with minimal operational overhead.",
          "timestamp": "1743332220.0",
          "comment_id": "1412001",
          "poster": "Srikantha"
        },
        {
          "timestamp": "1724670960.0",
          "comment_id": "1272645",
          "poster": "f4bi4n",
          "content": "Selected Answer: C\nI would go with C even when D seems to be the primary choice.\nIn D, you would need to maintain all of the launchTemplateConfigurations in a list, which means there is a lot of overhead. \nWith C this is not the case",
          "upvote_count": "2"
        },
        {
          "comment_id": "1240399",
          "timestamp": "1719869940.0",
          "content": "Answers B, C, and D can work. I'm leaning towards \"D\", but I'm witholding a formal vote for now. It appears the \"correct\" answer may depend on how you interpret requirements.\nNOT B: EventBridge/Lamba can work, but not as simple as D or C. It DOES \"update the launch templates of the company's Auto Scaling groups.\"\nNOT C: Answer C can work and is fairly simple, but it DOES NOT \"update the launch templates of the company's Auto Scaling groups\", because it does not need to, which could be argued is \"operationally efficient\".\nYES D: Seems like simple solution. ASG does need to be updated, but I don't know if that means defining someting like an $LATEST AMI alias (pointer) in ASG, or if ASG actually needs to be updated for each new version of Launch template. This solution could be more complex than C:.",
          "poster": "Gomer",
          "upvote_count": "1"
        },
        {
          "poster": "TEC1",
          "content": "Selected Answer: C\nC: This involves configuring the launch template to reference the AMI ID stored in the AWS Systems Manager Parameter Store. The EC2 Image Builder pipeline is then set up to update this Parameter Store value each time a new AMI is built. By doing so, the launch template always points to the latest AMI without requiring manual updates each time a new AMI is built. This approach automates the update process and ensures that Auto Scaling groups always use the most recent and secure AMIs, with minimal manual intervention and operational overhead.",
          "upvote_count": "2",
          "comment_id": "1210995",
          "timestamp": "1715621460.0"
        },
        {
          "timestamp": "1713050700.0",
          "content": "Selected Answer: D\nans is D",
          "comment_id": "1195163",
          "poster": "dkp",
          "upvote_count": "2"
        },
        {
          "comment_id": "1186458",
          "poster": "WhyIronMan",
          "content": "Selected Answer: D\nD is the correct and best practice suggested by aws\nhttps://docs.aws.amazon.com/imagebuilder/latest/userguide/dist-using-launch-template.html",
          "timestamp": "1711841520.0",
          "upvote_count": "4"
        },
        {
          "upvote_count": "2",
          "timestamp": "1711478940.0",
          "comment_id": "1183533",
          "poster": "stoy123",
          "content": "Selected Answer: B\nanswer B"
        },
        {
          "poster": "Seoyong",
          "content": "Selected Answer: B\nC is not efficiency .\nhttps://aws.amazon.com/blogs/compute/tracking-the-latest-server-images-in-amazon-ec2-image-builder-pipelines/",
          "comment_id": "1181435",
          "upvote_count": "1",
          "timestamp": "1711275120.0"
        },
        {
          "comment_id": "1175074",
          "upvote_count": "2",
          "poster": "ogerber",
          "content": "Selected Answer: D\nits D, 100%\nConfigure the Image Builder distribution settings to update the launch templates with the newest AMI IConfigure the Auto Scaling groups to use the newest version of the launch template.",
          "timestamp": "1710604620.0"
        },
        {
          "comments": [
            {
              "poster": "WhyIronMan",
              "upvote_count": "2",
              "comment_id": "1186459",
              "content": "don't trust chat gpt to help you pass exam, studying is the right way. \nQuestion says \"Which solution will meet these requirements with the MOST operational efficiency?\" you are adding more steps than it needs in D.\n\nOption C involves using Systems Manager Parameter Store to manage the AMI ID, but it requires manual updates to the Parameter Store value, which may not be as efficient or automated as directly configuring Image Builder to update the launch templates \n\nremember that Parameter store is not supported in distribution settings of image builder",
              "timestamp": "1711841760.0"
            }
          ],
          "content": "Selected Answer: C\nadd Explanation 'c' cause = chat gpt4.0 = c and i think\nThe most operationally efficient solution is to use AWS systems manager parameter store1 to store the ami di and reference it in the launch template2. this way, the launch template does not nned to be updated event titme a new ami is created by image buider, instead the image builder prpeline, can update the parameter store value with the newest ami id3,j and the auto scaling gorup can launch instances using the lastest value from parameter store",
          "timestamp": "1708335300.0",
          "poster": "kyuhuck",
          "comment_id": "1153805",
          "upvote_count": "2"
        },
        {
          "poster": "kyuhuck",
          "comment_id": "1153730",
          "upvote_count": "2",
          "timestamp": "1708321620.0",
          "content": "Selected Answer: C\nGiven these options, C represents the most operationally efficient solution that meets the requirements. It automates the process of using the newest AMIs for EC2 instance launches within Auto Scaling groups by leveraging the AWS Systems Manager Parameter Store and EC2 Image Builder. This method ensures that the Auto Scaling groups always use the latest security-hardened AMIs without needing to manually update launch templates for each new AMI release, thereby streamlining operations and maintaining compliance with the company's security policies."
        },
        {
          "comment_id": "1121084",
          "timestamp": "1705089780.0",
          "upvote_count": "3",
          "content": "Selected Answer: D\nD is correct. Actually C is also a valid option to pass AMI ID into launch template, but it has lots of limitations and not used in enterprise environment",
          "poster": "a54b16f"
        },
        {
          "comment_id": "1110902",
          "timestamp": "1704069060.0",
          "content": "Selected Answer: D\nAnswer is D.",
          "poster": "kabary",
          "upvote_count": "2"
        },
        {
          "comment_id": "1109431",
          "content": "D is correct",
          "poster": "PrasannaBalaji",
          "upvote_count": "1",
          "timestamp": "1703914500.0"
        },
        {
          "poster": "csG13",
          "comments": [
            {
              "comment_id": "1108988",
              "poster": "csG13",
              "upvote_count": "1",
              "timestamp": "1703874120.0",
              "content": "now that I think twice about it, D seems to be the most operationally efficient. I change my answer to D."
            }
          ],
          "upvote_count": "2",
          "timestamp": "1703873820.0",
          "comment_id": "1108986",
          "content": "Selected Answer: B\nB seems like an option"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:11:32.017Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "9rbUVKV1pQwG6gLjP2DI",
      "question_number": 86,
      "page": 18,
      "question_text": "A company has configured an Amazon S3 event source on an AWS Lambda function. The company needs the Lambda function to run when a new object is created or an existing object is modified in a particular S3 bucket. The Lambda function will use the S3 bucket name and the S3 object key of the incoming event to read the contents of the created or modified S3 object. The Lambda function will parse the contents and save the parsed contents to an Amazon DynamoDB table.\n\nThe Lambda function's execution role has permissions to read from the S3 bucket and to write to the DynamoDB table. During testing, a DevOps engineer discovers that the Lambda function does not run when objects are added to the S3 bucket or when existing objects are modified.\n\nWhich solution will resolve this problem?",
      "choices": {
        "D": "Provision space in the /tmp folder of the Lambda function to give the function the ability to process large files from the S3 bucket.",
        "A": "Increase the memory of the Lambda function to give the function the ability to process large files from the S3 bucket.",
        "B": "Create a resource policy on the Lambda function to grant Amazon S3 the permission to invoke the Lambda function for the S3 bucket.",
        "C": "Configure an Amazon Simple Queue Service (Amazon SQS) queue as an OnFailure destination for the Lambda function."
      },
      "correct_answer": "B",
      "answer_ET": "B",
      "answers_community": [
        "B (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/129686-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-12-29 14:40:00",
      "unix_timestamp": 1703857200,
      "discussion_count": 7,
      "discussion": [
        {
          "upvote_count": "6",
          "timestamp": "1707733440.0",
          "poster": "thanhnv142",
          "content": "Selected Answer: B\nB: is correct: Need to add a permission in lambda resource-based policy, which allow S3 to invode lambda\nA: If there is insufficient memory, lambda still runs. This case is about lambda not running at all \nC: We dont need SQS for dead-letter queue here\nD: Lambda does not run in a test, which proves that the problem does not lie in disk space because in tests, testers usually wont use large objects",
          "comment_id": "1147970"
        },
        {
          "upvote_count": "1",
          "poster": "jamesf",
          "content": "Selected Answer: B\nB due to permission issue",
          "comment_id": "1257905",
          "timestamp": "1722319320.0"
        },
        {
          "upvote_count": "2",
          "comment_id": "1195166",
          "timestamp": "1713052080.0",
          "content": "Selected Answer: B\nb is correct",
          "poster": "dkp"
        },
        {
          "timestamp": "1710588120.0",
          "poster": "DanShone",
          "comment_id": "1174929",
          "content": "Selected Answer: B\nB - Is correct\nA C and D have no relevance to the problem",
          "upvote_count": "2"
        },
        {
          "timestamp": "1705600260.0",
          "comment_id": "1126092",
          "content": "Selected Answer: B\nb is correct",
          "upvote_count": "3",
          "poster": "twogyt"
        },
        {
          "content": "Selected Answer: B\nLambda must allow S3 to invoke it",
          "comment_id": "1110894",
          "poster": "d262e67",
          "upvote_count": "3",
          "timestamp": "1704067380.0"
        },
        {
          "comment_id": "1108723",
          "content": "Selected Answer: B\nB is correct",
          "timestamp": "1703857200.0",
          "poster": "PrasannaBalaji",
          "upvote_count": "4"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:11:42.473Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "z1UtXJsrVznljLc2fsSl",
      "question_number": 87,
      "page": 18,
      "question_text": "A company has deployed a critical application in two AWS Regions. The application uses an Application Load Balancer (ALB) in both Regions. The company has Amazon Route 53 alias DNS records for both ALBs.\n\nThe company uses Amazon Route 53 Application Recovery Controller to ensure that the application can fail over between the two Regions. The Route 53 ARC configuration includes a routing control for both Regions. The company uses Route 53 ARC to perform quarterly disaster recovery (DR) tests.\n\nDuring the most recent DR test, a DevOps engineer accidentally turned off both routing controls. The company needs to ensure that at least one routing control is turned on at all times.\n\nWhich solution will meet these requirements?",
      "choices": {
        "B": "In Route 53 ARC, create a new gating safety rule. Apply the assertion safety rule to the two routing controls. Configure the rule with the OR type with a threshold of 1.",
        "D": "In Route 53 ARC, create a new resource set. Configure the resource set with an AWS::Route53RecoveryReadiness::DNSTargetResource resource type. Add the domain names of the two Route 53 alias DNS records as the target resource. Create a new readiness check for the resource set.",
        "C": "In Route 53 ARC, create a new resource set. Configure the resource set with an AWS::Route53::HealthCheck resource type. Specify the ARNs of the two routing controls as the target resource. Create a new readiness check for the resource set.",
        "A": "In Route 53 ARC, create a new assertion safety rule. Apply the assertion safety rule to the two routing controls. Configure the rule with the ATLEAST type with a threshold of 1."
      },
      "correct_answer": "A",
      "answer_ET": "A",
      "answers_community": [
        "A (94%)",
        "6%"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/129834-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-12-30 06:36:00",
      "unix_timestamp": 1703914560,
      "discussion_count": 8,
      "discussion": [
        {
          "poster": "c3518fc",
          "comment_id": "1201774",
          "upvote_count": "3",
          "timestamp": "1729836180.0",
          "content": "Selected Answer: A\nhttps://docs.aws.amazon.com/r53recovery/latest/dg/routing-control.safety-rules.html"
        },
        {
          "poster": "dkp",
          "comment_id": "1195167",
          "timestamp": "1728864180.0",
          "upvote_count": "1",
          "content": "answer is A"
        },
        {
          "comment_id": "1186447",
          "upvote_count": "1",
          "content": "Selected Answer: B\nB is correct\nOption D is incorrect due to a few reasons:\n\n1. No Automation for Report Copying: Unlike Option B, there's no mention of any automated process to copy the reports to the S3 bucket. It relies solely on configuring the S3 bucket as an artifact destination. Without automation, someone would have to manually manage the copying process, which is less efficient and prone to errors.\n\n2. Expiration: While Option D mentions setting object expiration to 90 days, it doesn't specify how this would be achieved. S3 Lifecycle rules are typically used to manage object expiration, but there's no mention of setting up such a rule in this option.",
          "timestamp": "1727644080.0",
          "poster": "WhyIronMan"
        },
        {
          "comments": [
            {
              "poster": "thanhnv142",
              "comment_id": "1147988",
              "upvote_count": "1",
              "content": "C and D are irrelevant. They are about creating new resource",
              "timestamp": "1723451880.0"
            }
          ],
          "poster": "thanhnv142",
          "upvote_count": "3",
          "timestamp": "1723451880.0",
          "content": "Selected Answer: A\nA is correct: assertion rule to make sure that atleast on gate is always open. This rules are basically things that users cannot do or only allow to do\nB: This gating rule is basically an on-off swith for a set of ARCs. If there is a controller that we dont want to turn off, specify this rule. This rule might help us achive the goal of the question. However, this requires we specify the exact name of the controller that should not be turned off. Meanwhile, the question requires that any controller can be turned off but at least one must be up and running. Therefore, this is not the right option",
          "comment_id": "1147986"
        },
        {
          "comment_id": "1126098",
          "timestamp": "1721318400.0",
          "upvote_count": "2",
          "content": "Selected Answer: A\nA is correct : https://docs.aws.amazon.com/r53recovery/latest/dg/routing-control.safety-rules.html",
          "poster": "twogyt"
        },
        {
          "comment_id": "1120877",
          "upvote_count": "2",
          "content": "Selected Answer: A\nanswer A https://docs.aws.amazon.com/r53recovery/latest/dg/routing-control.safety-rules.html",
          "timestamp": "1720790160.0",
          "poster": "yuliaqwerty"
        },
        {
          "poster": "csG13",
          "comment_id": "1109495",
          "upvote_count": "3",
          "content": "Selected Answer: A\nIt's an assertion rule with ATLEAST threshold set to 1. So, A.\n\nHere is a reference: \nhttps://docs.aws.amazon.com/r53recovery/latest/dg/routing-control.safety-rules.html\nhttps://docs.aws.amazon.com/r53recovery/latest/dg/getting-started-cli-routing.html#getting-started-cli-routing.safety",
          "timestamp": "1719724680.0"
        },
        {
          "content": "Selected Answer: A\nA is correct",
          "upvote_count": "2",
          "timestamp": "1719718560.0",
          "comment_id": "1109432",
          "poster": "PrasannaBalaji"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:11:42.473Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "9IhOS44iTYNB9zVpBwoQ",
      "question_number": 88,
      "page": 18,
      "question_text": "A healthcare services company is concerned about the growing costs of software licensing for an application for monitoring patient wellness. The company wants to create an audit process to ensure that the application is running exclusively on Amazon EC2 Dedicated Hosts. A DevOps engineer must create a workflow to audit the application to ensure compliance.\n\nWhat steps should the engineer take to meet this requirement with the LEAST administrative overhead?",
      "choices": {
        "B": "Use custom Java code running on an EC2 instance. Set up EC2 Auto Scaling for the instance depending on the number of instances to be checked. Send the list of noncompliant EC2 instance IDs to an Amazon SQS queue. Set up another worker instance to process instance IDs from the SQS queue and write them to Amazon DynamoDUse an AWS Lambda function to terminate noncompliant instance IDs obtained from the queue, and send them to an Amazon SNS email topic for distribution.",
        "D": "Use AWS CloudTrail. Identify all EC2 instances to be audited by analyzing all calls to the EC2 RunCommand API action. Invoke an AWS Lambda function that analyzes the host placement of the instance. Store the EC2 instance ID of noncompliant resources in an Amazon RDS for MySQL DB instance. Generate a report by querying the RDS instance and exporting the query results to a CSV text file.",
        "A": "Use AWS Systems Manager Configuration Compliance. Use calls to the put-compliance-items API action to scan and build a database of noncompliant EC2 instances based on their host placement configuration. Use an Amazon DynamoDB table to store these instance IDs for fast access. Generate a report through Systems Manager by calling the list-compliance-summaries API action.",
        "C": "Use AWS Config. Identify all EC2 instances to be audited by enabling Config Recording on all Amazon EC2 resources for the region. Create a custom AWS Config rule that triggers an AWS Lambda function by using the \"config-rule-change -triggered\" blueprint. Modify the Lambda evaluateCompliance() function to verify host placement to return a NON_COMPLIANT result if the instance is not running on an EC2 Dedicated Host. Use the AWS Config report to address noncompliant instances."
      },
      "correct_answer": "C",
      "answer_ET": "C",
      "answers_community": [
        "C (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/129687-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-12-29 14:44:00",
      "unix_timestamp": 1703857440,
      "discussion_count": 4,
      "discussion": [
        {
          "upvote_count": "1",
          "timestamp": "1730556780.0",
          "content": "Selected Answer: C\nC for me",
          "poster": "seetpt",
          "comment_id": "1205501"
        },
        {
          "timestamp": "1723452300.0",
          "content": "Selected Answer: C\nC is correct: Using Config is the right way\nA: <Use an Amazon DynamoDB table to store these instance IDs for fast access> DynamoDB is used primarily for storing web section data, not to store these IDs\nB: Should not use custom Java code on Ec2. Additionally, This option mention terminating non-compliance ones, which is incorrect. We only need an audit workflow\nD: Cloud trail is for auditing user activities, not to check non-compliance EC2 instacnes",
          "comment_id": "1147989",
          "poster": "thanhnv142",
          "upvote_count": "4"
        },
        {
          "poster": "csG13",
          "comment_id": "1109496",
          "upvote_count": "2",
          "content": "Selected Answer: C\nIt's C",
          "timestamp": "1719724800.0"
        },
        {
          "timestamp": "1719661440.0",
          "content": "Selected Answer: C\nC is correct",
          "comment_id": "1108730",
          "poster": "PrasannaBalaji",
          "upvote_count": "2"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:11:42.473Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "FPU2CQEnzP4xkG71zmsl",
      "question_number": 89,
      "page": 18,
      "question_text": "A DevOps engineer is planning to deploy a Ruby-based application to production. The application needs to interact with an Amazon RDS for MySQL database and should have automatic scaling and high availability. The stored data in the database is critical and should persist regardless of the state of the application stack.\n\nThe DevOps engineer needs to set up an automated deployment strategy for the application with automatic rollbacks. The solution also must alert the application team when a deployment fails.\n\nWhich combination of steps will meet these requirements? (Choose three.)",
      "choices": {
        "B": "Deploy the application on AWS Elastic Beanstalk. Deploy a separate Amazon RDS for MySQL DB instance outside of Elastic Beanstalk.",
        "C": "Configure a notification email address that alerts the application team in the AWS Elastic Beanstalk configuration.",
        "A": "Deploy the application on AWS Elastic Beanstalk. Deploy an Amazon RDS for MySQL DB instance as part of the Elastic Beanstalk configuration.",
        "E": "Use the immutable deployment method to deploy new application versions.",
        "D": "Configure an Amazon EventBridge rule to monitor AWS Health events. Use an Amazon Simple Notification Service (Amazon SNS) topic as a target to alert the application team.",
        "F": "Use the rolling deployment method to deploy new application versions."
      },
      "correct_answer": "BCE",
      "answer_ET": "BCE",
      "answers_community": [
        "BCE (83%)",
        "Other"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/129688-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-12-29 14:46:00",
      "unix_timestamp": 1703857560,
      "discussion_count": 12,
      "discussion": [
        {
          "poster": "csG13",
          "content": "Selected Answer: BCE\nMove RDS out of Beanstalk as it is critical. AWS Health can't check deployment health but services only.",
          "comment_id": "1109010",
          "upvote_count": "8",
          "timestamp": "1719679800.0"
        },
        {
          "poster": "c3518fc",
          "upvote_count": "2",
          "timestamp": "1729837800.0",
          "comment_id": "1201783",
          "content": "Selected Answer: BCE\nhttps://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.managing.sns.html"
        },
        {
          "timestamp": "1728865980.0",
          "poster": "dkp",
          "upvote_count": "4",
          "content": "Selected Answer: BCE\nBCE\nImmutable deployments perform an immutable update to launch a full set of new instances running the new version of the application in a separate Auto Scaling group, alongside the instances running the old version. Immutable deployments can prevent issues caused by partially completed rolling deployments. If the new instances don't pass health checks, Elastic Beanstalk terminates them, leaving the original instances untouched.",
          "comment_id": "1195170"
        },
        {
          "comment_id": "1186465",
          "content": "Selected Answer: BCE\nB, C, E is the correct answer since\nA. its is not a good practice deploy database layer with EB\nD. is not directly related to the deployment strategy for the application and does not address the requirement for automatic rollbacks.\nF. In a rolling deployment, the application version is gradually deployed across the existing instances in the environment. Each instance is updated one at a time, and the application remains available throughout the deployment process. This approach updates the application in a phased manner, minimizing downtime but potentially complicating rollback if issues arise.",
          "timestamp": "1727646660.0",
          "upvote_count": "2",
          "poster": "WhyIronMan"
        },
        {
          "timestamp": "1727370180.0",
          "poster": "stoy123",
          "content": "Selected Answer: BDE\nBDE for sure",
          "comments": [
            {
              "poster": "stoy123",
              "timestamp": "1727508000.0",
              "content": "BCE is correct",
              "comment_id": "1184660",
              "upvote_count": "1"
            }
          ],
          "comment_id": "1183544",
          "upvote_count": "1"
        },
        {
          "upvote_count": "2",
          "content": "Selected Answer: BCE\nBCE is correct",
          "comment_id": "1175098",
          "timestamp": "1726496820.0",
          "poster": "ogerber"
        },
        {
          "poster": "master9",
          "timestamp": "1726101300.0",
          "upvote_count": "1",
          "content": "Selected Answer: BDE\nWS Elastic Beanstalk itself does not directly provide email notification capabilities. However, you can integrate AWS Elastic Beanstalk with other AWS services to achieve email notifications.",
          "comment_id": "1171401"
        },
        {
          "upvote_count": "4",
          "poster": "Diego1414",
          "timestamp": "1724527560.0",
          "comment_id": "1158167",
          "content": "Selected Answer: BCE\nBCE - \nB - Makes sense as you want the RDS to persist.\nC- You can set an email notification during the Elastic Beanstalk configuration\nE - Immutable for roll back as previous versions persists"
        },
        {
          "comments": [
            {
              "poster": "rijub2022",
              "upvote_count": "4",
              "timestamp": "1724330280.0",
              "comment_id": "1156437",
              "content": "Apologies it will be BCE.\nBeanstalk can directly send notifications via SNS: https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.managing.sns.html"
            }
          ],
          "comment_id": "1156431",
          "upvote_count": "1",
          "poster": "rijub2022",
          "content": "Selected Answer: BDE\nBDE seems correct",
          "timestamp": "1724330040.0"
        },
        {
          "content": "Selected Answer: BDE\nBD and E are correct:\nA: Beanstalk does not support Amazon RDS for MySQL DB instance\nB: We need to deploy a separate Amazon RDS for MySQL DB instance \nC: Beanstalk does not send notification to email address. We need SNS\nD: This option monitor service health and send to SNS\nE: Immutable allow roll back because it does not delete the original one\nF: This replaces the original one, rolling out the new version gradually. Therefore, we cannot roll back",
          "timestamp": "1723453020.0",
          "poster": "thanhnv142",
          "comment_id": "1147997",
          "upvote_count": "1"
        },
        {
          "upvote_count": "3",
          "timestamp": "1719788220.0",
          "content": "Selected Answer: BCE\nSame reasons as csG13",
          "poster": "d262e67",
          "comment_id": "1110911"
        },
        {
          "upvote_count": "1",
          "comment_id": "1108732",
          "poster": "PrasannaBalaji",
          "timestamp": "1719661560.0",
          "content": "Selected Answer: ADE\nADE is correct"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:11:42.473Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "rGSXaf9eUPgEDlTzKt7z",
      "question_number": 90,
      "page": 18,
      "question_text": "A DevOps engineer is creating an AWS CloudFormation template to deploy a web service. The web service will run on Amazon EC2 instances in a private subnet behind an Application Load Balancer (ALB). The DevOps engineer must ensure that the service can accept requests from clients that have IPv6 addresses.\nWhat should the DevOps engineer do with the CloudFormation template so that IPv6 clients can access the web service?",
      "choices": {
        "D": "Add an IPv6 CIDR block to the VPC and subnets for the ALB. Create a listener on port 443. and specify the dualstack IP address type on the ALB. Create a target group, and add the EC2 instances as targets. Associate the target group with the ALB.",
        "A": "Add an IPv6 CIDR block to the VPC and the private subnet for the EC2 instances. Create route table entries for the IPv6 network, use EC2 instance types that support IPv6, and assign IPv6 addresses to each EC2 instance.",
        "C": "Replace the ALB with a Network Load Balancer (NLB). Add an IPv6 CIDR block to the VPC and subnets for the NLB, and assign the NLB an IPv6 Elastic IP address.",
        "B": "Assign each EC2 instance an IPv6 Elastic IP address. Create a target group, and add the EC2 instances as targets. Create a listener on port 443 of the ALB, and associate the target group with the ALB."
      },
      "correct_answer": "D",
      "answer_ET": "D",
      "answers_community": [
        "D (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/105239-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-04-05 04:09:00",
      "unix_timestamp": 1680660540,
      "discussion_count": 17,
      "discussion": [
        {
          "comment_id": "904299",
          "poster": "levster",
          "upvote_count": "7",
          "content": "D\n\"To support IPv6, configure your Application Load Balancers or Network Load Balancers with the “dualstack” IP address type. This means that clients can communicate with the load balancers using both IPv4 and IPv6 addresses. In a dual-stack IP address type, the DNS name of the load balancer provides both IPv4 and IPv6 addresses, and creates A and AAAA records respectively. \"\n\nhttps://docs.aws.amazon.com/whitepapers/latest/ipv6-on-aws/scaling-the-dual-stack-network-design-in-aws.html",
          "timestamp": "1684780560.0"
        },
        {
          "upvote_count": "3",
          "poster": "01037",
          "content": "Selected Answer: D\nBut why is port 443 necessary?",
          "comments": [
            {
              "content": "Port 443 is the TCP port for HTTPS which a secured or encrypted version of HTTP. To enable the ALB handle HTTPS traffic having a listener on port 443 is necessary.",
              "comment_id": "1267088",
              "upvote_count": "2",
              "poster": "flaacko",
              "timestamp": "1723812960.0"
            }
          ],
          "timestamp": "1715436540.0",
          "comment_id": "1209824"
        },
        {
          "content": "Selected Answer: D\nThe correct answer is D. Add an IPv6 CIDR block to the VPC and subnets for the ALB. Create a listener on port 443. and specify the dualstack IP address type on the ALB. Create a target group, and add the EC2 instances as targets. Associate the target group with the ALB.",
          "poster": "c3518fc",
          "comment_id": "1209532",
          "upvote_count": "1",
          "timestamp": "1715374740.0"
        },
        {
          "timestamp": "1709137740.0",
          "content": "Why is the need for port 443 reference on D and D has no reference to private subnet. That makes me think the answer is A, but A has no reference to ALB.",
          "comment_id": "1161794",
          "poster": "zijo",
          "upvote_count": "2"
        },
        {
          "content": "D is correct: use dual stack + listener on 443\nA: no mention of the ALB\nB: no mention of adding dualstack IP to ALB\nC: cannot replace the ALB",
          "comment_id": "1134178",
          "upvote_count": "4",
          "poster": "thanhnv142",
          "timestamp": "1706453100.0"
        },
        {
          "content": "definitely D",
          "timestamp": "1704605640.0",
          "upvote_count": "2",
          "poster": "sksegha",
          "comment_id": "1115608"
        },
        {
          "upvote_count": "2",
          "poster": "Jamshif01",
          "comment_id": "1104885",
          "timestamp": "1703456700.0",
          "content": "keyword is \"Dualstack\""
        },
        {
          "poster": "z_inderjot",
          "timestamp": "1703047800.0",
          "content": "Selected Answer: D\nD is correct , To enable ALB to deal with Ipv6 requests , vpc should enable for dual stack, by configuring a ipv6 cidr , and ALB subnet should also adhere to the same , by having ipv4 and 6 cidr \nB is incorrect , we can assisg any public ip to instance , since it is in private subnet .",
          "upvote_count": "3",
          "comment_id": "1101229"
        },
        {
          "upvote_count": "1",
          "comment_id": "918477",
          "content": "Selected Answer: D\nD is the correct answer. C is wrong, we don't need Elastic IPs for a private app.",
          "timestamp": "1686242100.0",
          "poster": "madperro"
        },
        {
          "content": "Selected Answer: D\nD i answer",
          "upvote_count": "1",
          "poster": "Rick365",
          "timestamp": "1685511060.0",
          "comment_id": "910842"
        },
        {
          "upvote_count": "4",
          "timestamp": "1685457660.0",
          "poster": "bcx",
          "content": "I would say it is D. The backend instances serving the data can be IPv4. The ALB shoult serve IPv6 to the public (which is what is required by the question). So the only place in the VPC that needs IPv6 are the ALB subnets.",
          "comment_id": "910335"
        },
        {
          "poster": "ParagSanyashiv",
          "comment_id": "891933",
          "upvote_count": "1",
          "content": "Selected Answer: D\nD is the correct answer in this case",
          "timestamp": "1683535500.0"
        },
        {
          "content": "Selected Answer: D\nTo allow IPv6 clients to access the web service running on Amazon EC2 instances in a private subnet behind an Application Load Balancer (ALB) using an AWS CloudFormation template, the DevOps engineer should choose option D:\n\nAdd an IPv6 CIDR block to the VPC and subnets for the ALB. Create a listener on port 443, and specify the dualstack IP address type on the ALB. Create a target group, add the EC2 instances as targets, and associate the target group with the ALB.\n\nThe dualstack IP address type enables the ALB to support both IPv4 and IPv6 traffic. By adding an IPv6 CIDR block to the VPC and subnets for the ALB, the VPC automatically assigns an IPv6 address to the ALB.",
          "upvote_count": "4",
          "timestamp": "1682986860.0",
          "poster": "haazybanj",
          "comment_id": "886869"
        },
        {
          "comment_id": "882035",
          "content": "D\nhttps://repost.aws/ja/knowledge-center/elb-configure-with-ipv6",
          "timestamp": "1682544900.0",
          "upvote_count": "4",
          "poster": "gdtypk"
        },
        {
          "content": "answer is D",
          "upvote_count": "1",
          "poster": "alce2020",
          "comment_id": "870459",
          "timestamp": "1681503840.0"
        },
        {
          "upvote_count": "2",
          "poster": "ele",
          "content": "Selected Answer: D\nD right",
          "timestamp": "1680875520.0",
          "comment_id": "863945"
        },
        {
          "content": "I think D",
          "comment_id": "861687",
          "timestamp": "1680660540.0",
          "poster": "lqpO_Oqpl",
          "upvote_count": "1"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:11:42.473Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "FI6DK3wGeCtcwRiJiPWn",
      "question_number": 91,
      "page": 19,
      "question_text": "A company is using AWS CodePipeline to deploy an application. According to a new guideline, a member of the company's security team must sign off on any application changes before the changes are deployed into production. The approval must be recorded and retained.\n\nWhich combination of actions will meet these requirements? (Choose two.)",
      "choices": {
        "A": "Configure CodePipeline to write actions to Amazon CloudWatch Logs.",
        "B": "Configure CodePipeline to write actions to an Amazon S3 bucket at the end of each pipeline stage.",
        "C": "Create an AWS CloudTrail trail to deliver logs to Amazon S3.",
        "E": "Create a CodePipeline manual approval action before the deployment step. Create a policy that grants the security team access to approve manual approval stages.",
        "D": "Create a CodePipeline custom action to invoke an AWS Lambda function for approval. Create a policy that gives the security team access to manage CodePipeline custom actions."
      },
      "correct_answer": "CE",
      "answer_ET": "CE",
      "answers_community": [
        "CE (83%)",
        "Other"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/129689-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-12-29 14:48:00",
      "unix_timestamp": 1703857680,
      "discussion_count": 12,
      "discussion": [
        {
          "poster": "thanhnv142",
          "timestamp": "1723453260.0",
          "upvote_count": "6",
          "content": "Selected Answer: CE\nC and E are correct: \nA: Cloudwatch Logs is to store logs from AWS resources like EC2, not codepipeline\nB: We dont need to store codepipeline actions in S3. \nC: We need to monitor users'actions, so using cloudtrail to store logs to S3 is the recommended one\nD: We should not invoke AWS lambda for approval\nE: This is the recommended one",
          "comment_id": "1148001"
        },
        {
          "content": "Selected Answer: AE\nCloudTrail tracks API activity in your AWS environment, but it does not specifically capture manual approval actions within CodePipeline. CloudTrail can help you audit changes to resources but is not suited for tracking the specific approval process within CodePipeline itself.",
          "comment_id": "1330897",
          "poster": "youonebe",
          "timestamp": "1734977100.0",
          "upvote_count": "2"
        },
        {
          "poster": "c3518fc",
          "comment_id": "1201787",
          "upvote_count": "3",
          "timestamp": "1729838220.0",
          "content": "Selected Answer: CE\nhttps://docs.aws.amazon.com/codepipeline/latest/userguide/monitoring-cloudtrail-logs.html"
        },
        {
          "timestamp": "1728866400.0",
          "comment_id": "1195173",
          "content": "ans ce",
          "upvote_count": "1",
          "poster": "dkp"
        },
        {
          "content": "Selected Answer: CE\nC- Logging, since Cloudwatch Logs and writelogs to S3 can not capture the Approval that only CloudTrail can\nE - Manual Approval Step is natively supported by codepipeline, no need to make it more complex with anything",
          "upvote_count": "3",
          "comment_id": "1186467",
          "timestamp": "1727647080.0",
          "poster": "WhyIronMan"
        },
        {
          "comment_id": "1174923",
          "poster": "DanShone",
          "timestamp": "1726477920.0",
          "upvote_count": "2",
          "content": "Selected Answer: CE\nC- Logging\nE - Manual Approval Step"
        },
        {
          "poster": "davdan99",
          "comment_id": "1118929",
          "timestamp": "1720629420.0",
          "content": "Selected Answer: CE\nhttps://stelligent.com/2019/06/11/aws-codepipeline-approval-gate-tracking/",
          "upvote_count": "2"
        },
        {
          "poster": "zolthar_z",
          "upvote_count": "4",
          "timestamp": "1719938100.0",
          "comment_id": "1112140",
          "content": "Selected Answer: CE\nC and E: The approval process is an AWS API Event and this is managed by CloudTrail\n\nhttps://docs.aws.amazon.com/codepipeline/latest/userguide/incident-response.html"
        },
        {
          "comment_id": "1111783",
          "poster": "ozansenturk",
          "content": "Selected Answer: CE\nCE:\nAWS CodePipeline is integrated with AWS CloudTrail, a service that provides a record of actions taken by a user, role, or an AWS service in CodePipeline;. CloudTrail captures all API calls for CodePipeline as events. The calls captured include calls from the CodePipeline console and code calls to the CodePipeline API operations. If you create a trail, you can enable continuous delivery of CloudTrail events to an Amazon S3 bucket, including events for CodePipeline. If you don't configure a trail, you can still view the most recent events in the CloudTrail console in Event history. Using the information collected by CloudTrail, you can determine the request that was made to CodePipeline, the IP address from which the request was made, who made the request, when it was made, and additional details.\n\nhttps://docs.aws.amazon.com/codepipeline/latest/userguide/monitoring-cloudtrail-logs.html",
          "timestamp": "1719909420.0",
          "upvote_count": "2"
        },
        {
          "upvote_count": "3",
          "timestamp": "1719788940.0",
          "content": "Selected Answer: CE\nC. because actions performed by the security team are api calls. And api calls go into CloudTrail, if you want to retain them we have to send them into an S3 bucket.\n\nhttps://docs.aws.amazon.com/codepipeline/latest/userguide/monitoring-cloudtrail-logs.html",
          "poster": "d262e67",
          "comment_id": "1110914"
        },
        {
          "poster": "GokSK",
          "content": "Selected Answer: AE\nE is for Manual Approval\nA is for recorded and retained",
          "comment_id": "1109726",
          "upvote_count": "2",
          "timestamp": "1719743460.0"
        },
        {
          "content": "Selected Answer: DE\nD and E is correct",
          "comments": [
            {
              "poster": "WhyIronMan",
              "comment_id": "1186466",
              "content": "Option D does not address the need \"The approval must be recorded and retained.\"",
              "timestamp": "1727646960.0",
              "upvote_count": "1"
            }
          ],
          "upvote_count": "1",
          "poster": "PrasannaBalaji",
          "comment_id": "1108733",
          "timestamp": "1719661680.0"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:11:52.906Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "4NEx2AioKINpEenojkHz",
      "question_number": 92,
      "page": 19,
      "question_text": "A company requires its internal business teams to launch resources through pre-approved AWS CloudFormation templates only. The security team requires automated monitoring when resources drift from their expected state.\n\nWhich strategy should be used to meet these requirements?",
      "choices": {
        "A": "Allow users to deploy CloudFormation stacks using a CloudFormation service role only. Use CloudFormation drift detection to detect when resources have drifted from their expected state.",
        "C": "Allow users to deploy CloudFormation stacks using AWS Service Catalog only. Enforce the use of a launch constraint. Use AWS Config rules to detect when resources have drifted from their expected state.",
        "D": "Allow users to deploy CloudFormation stacks using AWS Service Catalog only. Enforce the use of a template constraint. Use Amazon EventBridge notifications to detect when resources have drifted from their expected state.",
        "B": "Allow users to deploy CloudFormation stacks using a CloudFormation service role only. Use AWS Config rules to detect when resources have drifted from their expected state."
      },
      "correct_answer": "C",
      "answer_ET": "C",
      "answers_community": [
        "C (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/129738-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-12-29 19:57:00",
      "unix_timestamp": 1703876220,
      "discussion_count": 11,
      "discussion": [
        {
          "poster": "Jordarlu",
          "upvote_count": "2",
          "comment_id": "1294846",
          "timestamp": "1728413700.0",
          "content": "Selected Answer: C\nIn the Option A, Drift detection must be run manually or scheduled, which doesn't fully meet the requirement for \"automated monitoring.\""
        },
        {
          "content": "Selected Answer: C\nKeypoint: AWS Config for drift detection",
          "poster": "jamesf",
          "upvote_count": "1",
          "timestamp": "1722321060.0",
          "comment_id": "1257922"
        },
        {
          "content": "Selected Answer: C\nChecks if the actual configuration of a AWS CloudFormation (AWS CloudFormation) stack differs, or has drifted, from the expected configuration. A stack is considered to have drifted if one or more of its resources differ from their expected configuration. https://docs.aws.amazon.com/config/latest/developerguide/cloudformation-stack-drift-detection-check.html",
          "poster": "c3518fc",
          "comment_id": "1201886",
          "upvote_count": "3",
          "timestamp": "1714037700.0"
        },
        {
          "content": "Selected Answer: C\nanwer c",
          "poster": "dkp",
          "timestamp": "1713055740.0",
          "comment_id": "1195174",
          "upvote_count": "2"
        },
        {
          "upvote_count": "3",
          "timestamp": "1710587280.0",
          "poster": "DanShone",
          "content": "Selected Answer: C\nC - Service Catalog + AWS Config",
          "comment_id": "1174922"
        },
        {
          "content": "Selected Answer: C\nC is correct: <pre-approved AWS CloudFormation templates only> means we need service catalog\nA and B: < Allow users to deploy CloudFormation stacks using a CloudFormation service role only>: With service role, users can modify anything in the template\nD: Eventbridge cannot detect drift",
          "upvote_count": "3",
          "comment_id": "1148005",
          "timestamp": "1707736140.0",
          "poster": "thanhnv142"
        },
        {
          "upvote_count": "2",
          "content": "Selected Answer: C\nUse config for drift detection",
          "comment_id": "1126161",
          "timestamp": "1705609560.0",
          "poster": "twogyt"
        },
        {
          "poster": "a54b16f",
          "content": "Selected Answer: C\nConfig for drift detection",
          "comment_id": "1121089",
          "upvote_count": "3",
          "timestamp": "1705090440.0"
        },
        {
          "comment_id": "1110918",
          "poster": "d262e67",
          "timestamp": "1704071820.0",
          "upvote_count": "4",
          "content": "Selected Answer: C\nYou can use AWS Managed Config cloudformation-stack-drift-detection-check rule to evaluate drift in CloudFormation stacks.\n\nhttps://docs.aws.amazon.com/config/latest/developerguide/cloudformation-stack-drift-detection-check.html"
        },
        {
          "timestamp": "1703915040.0",
          "upvote_count": "1",
          "comment_id": "1109438",
          "content": "Selected Answer: C\nC is correct",
          "poster": "PrasannaBalaji"
        },
        {
          "timestamp": "1703876220.0",
          "content": "Selected Answer: C\nit's C",
          "upvote_count": "1",
          "poster": "csG13",
          "comment_id": "1109017"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:11:52.906Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "Xi9AZAh0AQMaefvqvnI1",
      "question_number": 93,
      "page": 19,
      "question_text": "A company has multiple development groups working in a single shared AWS account. The senior manager of the groups wants to be alerted via a third-party API call when the creation of resources approaches the service limits for the account.\n\nWhich solution will accomplish this with the LEAST amount of development effort?",
      "choices": {
        "B": "Deploy an AWS Lambda function that refreshes AWS Trusted Advisor checks, and configure an Amazon EventBridge rule to run the Lambda function periodically. Create another EventBridge rule with an event pattern matching Trusted Advisor events and a target Lambda function. In the target Lambda function, notify the senior manager.",
        "D": "Add an AWS Config custom rule that runs periodically, checks the AWS service limit status, and streams notifications to an Amazon Simple Notification Service (Amazon SNS) topic. Deploy an AWS Lambda function that notifies the senior manager, and subscribe the Lambda function to the SNS topic.",
        "C": "Deploy an AWS Lambda function that refreshes AWS Health Dashboard checks, and configure an Amazon EventBridge rule to run the Lambda function periodically. Create another EventBridge rule with an event pattern matching Health Dashboard events and a target Lambda function. In the target Lambda function, notify the senior manager.",
        "A": "Create an Amazon EventBridge rule that runs periodically and targets an AWS Lambda function. Within the Lambda function, evaluate the current state of the AWS environment and compare deployed resource values to resource limits on the account. Notify the senior manager if the account is approaching a service limit."
      },
      "correct_answer": "B",
      "answer_ET": "B",
      "answers_community": [
        "B (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/129692-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-12-29 15:22:00",
      "unix_timestamp": 1703859720,
      "discussion_count": 9,
      "discussion": [
        {
          "comment_id": "1201896",
          "upvote_count": "4",
          "content": "Selected Answer: B\nUnderstanding your service limits (and how close you are to them) is an important part of managing your AWS deployments – continuous monitoring allows you to request limit increases or shut down resources before the limit is reached.\n\nOne of the easiest ways to do this is via AWS Trusted Advisor’s Service Limit Dashboard, which currently covers 39 limits across 10 services. https://aws.amazon.com/blogs/mt/monitoring-service-limits-with-trusted-advisor-and-amazon-cloudwatch/",
          "poster": "c3518fc",
          "timestamp": "1729850520.0"
        },
        {
          "comment_id": "1195181",
          "timestamp": "1728867480.0",
          "upvote_count": "2",
          "content": "Selected Answer: B\nanswer b",
          "poster": "dkp"
        },
        {
          "content": "Selected Answer: B\nB - Trusted Advisor",
          "timestamp": "1726477620.0",
          "poster": "DanShone",
          "comment_id": "1174921",
          "upvote_count": "2"
        },
        {
          "upvote_count": "3",
          "content": "Selected Answer: B\nTrusted advisor",
          "comment_id": "1126173",
          "poster": "twogyt",
          "timestamp": "1721327520.0"
        },
        {
          "upvote_count": "3",
          "poster": "a54b16f",
          "content": "Selected Answer: B\nservice quote limit === trusted advisor",
          "timestamp": "1720808160.0",
          "comment_id": "1121091"
        },
        {
          "upvote_count": "2",
          "timestamp": "1720791180.0",
          "content": "Selected Answer: B\nAnswer B https://aws.amazon.com/blogs/mt/monitoring-service-limits-with-trusted-advisor-and-amazon-cloudwatch/",
          "poster": "yuliaqwerty",
          "comment_id": "1120885"
        },
        {
          "upvote_count": "1",
          "content": "Selected Answer: B\nTrusted advisor checks the service limits",
          "poster": "d262e67",
          "timestamp": "1719789600.0",
          "comment_id": "1110919"
        },
        {
          "poster": "csG13",
          "upvote_count": "1",
          "content": "Selected Answer: B\nIt's B",
          "timestamp": "1719680340.0",
          "comment_id": "1109018"
        },
        {
          "content": "Selected Answer: B\nB is correct",
          "poster": "PrasannaBalaji",
          "comment_id": "1108772",
          "upvote_count": "1",
          "timestamp": "1719663720.0"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:11:52.906Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "dmgpX7dZUeZvKRwXhbEh",
      "question_number": 94,
      "page": 19,
      "question_text": "A DevOps engineer is setting up a container-based architecture. The engineer has decided to use AWS CloudFormation to automatically provision an Amazon ECS cluster and an Amazon EC2 Auto Scaling group to launch the EC2 container instances. After successfully creating the CloudFormation stack, the engineer noticed that, even though the ECS cluster and the EC2 instances were created successfully and the stack finished the creation, the EC2 instances were associating with a different cluster.\n\nHow should the DevOps engineer update the CloudFormation template to resolve this issue?",
      "choices": {
        "D": "Reference the ECS cluster in the AWS::CloudFormation::CustomResource resource to trigger an AWS Lambda function that registers the EC2 instances with the appropriate ECS cluster.",
        "A": "Reference the EC2 instances in the AWS::ECS::Cluster resource and reference the ECS cluster in the AWS::ECS::Service resource.",
        "B": "Reference the ECS cluster in the AWS::AutoScaling::LaunchConfiguration resource of the UserData property.",
        "C": "Reference the ECS cluster in the AWS::EC2::Instance resource of the UserData property."
      },
      "correct_answer": "B",
      "answer_ET": "B",
      "answers_community": [
        "B (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/129693-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-12-29 15:26:00",
      "unix_timestamp": 1703859960,
      "discussion_count": 9,
      "discussion": [
        {
          "content": "Selected Answer: B\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-ecs-cluster.html",
          "poster": "Jonalb",
          "upvote_count": "1",
          "comment_id": "1585367",
          "timestamp": "1752170400.0"
        },
        {
          "content": "Selected Answer: B\nB is correct according to this block\n\"UserData\": {\n \"Fn::Base64\": {\n \"Fn::Sub\": \"#!/bin/bash -xe\\n echo ECS_CLUSTER=${ECSCluster} >> /etc/ecs/ecs.config\\n yum install -y aws-cfn-bootstrap\\n /opt/aws/bin/cfn-init -v --stack ${AWS::StackId} --resource ContainerInstances --configsets full_install --region ${AWS::Region} &\\n\"\n }\n }, \nin https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-ecs-cluster.html",
          "comment_id": "1316226",
          "timestamp": "1732269000.0",
          "upvote_count": "1",
          "poster": "Aquaman"
        },
        {
          "upvote_count": "1",
          "content": "Selected Answer: B\nLink the Auto Scaling Group with ECS Cluster",
          "poster": "jamesf",
          "timestamp": "1722321420.0",
          "comment_id": "1257929"
        },
        {
          "upvote_count": "1",
          "poster": "seetpt",
          "content": "Selected Answer: B\nB for me",
          "comment_id": "1205503",
          "timestamp": "1714652100.0"
        },
        {
          "content": "Selected Answer: B\nanswer B",
          "comment_id": "1195184",
          "poster": "dkp",
          "upvote_count": "2",
          "timestamp": "1713056880.0"
        },
        {
          "content": "B is correct: <even though the ECS cluster and the EC2 instances were created successfully and the stack finished the creation, the EC2 instances were associating with a different cluster> means we need to link the auto scaling group to the ECS cluster\nA: incorrect. AWS::ECS::Cluster creates an ECS cluster. AWS::ECS::Service creates its services. However, it does not link the EC2 auto scaling group to the ECS cluster\nC: incorrect. AWS::EC2::Instance creates an EC2 instance\nD: incorrect. AWS::CloudFormation::CustomResource creates CustomResource",
          "comment_id": "1148085",
          "upvote_count": "3",
          "timestamp": "1707740340.0",
          "poster": "thanhnv142"
        },
        {
          "timestamp": "1705091280.0",
          "comment_id": "1121096",
          "poster": "a54b16f",
          "content": "Selected Answer: B\nB, here is a sample code https://github.com/thinegan/cloudformation-project2/blob/master/infrastructure/ecs-autoscaling-appserver.yaml",
          "upvote_count": "3"
        },
        {
          "upvote_count": "3",
          "poster": "ozansenturk",
          "timestamp": "1704193740.0",
          "comment_id": "1111797",
          "content": "Selected Answer: B\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-autoscaling-launchconfiguration.html#aws-resource-autoscaling-launchconfiguration--examples"
        },
        {
          "content": "Selected Answer: B\nB is correct",
          "timestamp": "1703859960.0",
          "upvote_count": "2",
          "poster": "PrasannaBalaji",
          "comment_id": "1108775"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:11:52.906Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "8AumhS5UYYnevZjRSfVz",
      "question_number": 95,
      "page": 19,
      "question_text": "A DevOps engineer is implementing governance controls for a company that requires its infrastructure to be housed within the United States. The engineer must restrict which AWS Regions can be used, and ensure an alert is sent as soon as possible if any activity outside the governance policy takes place. The controls should be automatically enabled on any new Region outside the United States (US).\n\nWhich combination of actions will meet these requirements? (Choose two.)",
      "choices": {
        "A": "Create an AWS Organizations SCP that denies access to all non-global services in non-US Regions. Attach the policy to the root of the organization.",
        "B": "Configure AWS CloudTrail to send logs to Amazon CloudWatch Logs and enable it for all Regions. Use a CloudWatch Logs metric filter to send an alert on any service activity in non-US Regions.",
        "C": "Use an AWS Lambda function that checks for AWS service activity and deploy it to all Regions. Write an Amazon EventBridge rule that runs the Lambda function every hour, sending an alert if activity is found in a non-US Region.",
        "E": "Write an SCP using the aws:RequestedRegion condition key limiting access to US Regions. Apply the policy to all users, groups, and roles.",
        "D": "Use an AWS Lambda function to query Amazon Inspector to look for service activity in non-US Regions and send alerts if any activity is found."
      },
      "correct_answer": "AB",
      "answer_ET": "AB",
      "answers_community": [
        "AB (91%)",
        "9%"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/129694-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-12-29 15:29:00",
      "unix_timestamp": 1703860140,
      "discussion_count": 10,
      "discussion": [
        {
          "poster": "kiwtirApp",
          "comment_id": "1215906",
          "content": "Selected Answer: AB\nOption A suggests creating an AWS Organizations SCP that denies access to all non-global services in non-US Regions. This is a valid approach.\nOption B recommends configuring AWS CloudTrail to send logs to Amazon CloudWatch Logs and enabling it for all Regions.",
          "timestamp": "1732307700.0",
          "upvote_count": "2"
        },
        {
          "timestamp": "1730556900.0",
          "upvote_count": "1",
          "content": "Selected Answer: AB\nAB for me",
          "comment_id": "1205504",
          "poster": "seetpt"
        },
        {
          "upvote_count": "2",
          "poster": "dkp",
          "content": "Selected Answer: AB\nA& B are correct answer",
          "timestamp": "1728868200.0",
          "comment_id": "1195186"
        },
        {
          "upvote_count": "4",
          "timestamp": "1726477440.0",
          "comment_id": "1174919",
          "content": "Selected Answer: AB\nA - SCP to restrict \nB - CloudTrail to monitor",
          "poster": "DanShone"
        },
        {
          "poster": "thanhnv142",
          "comments": [
            {
              "comment_id": "1215902",
              "upvote_count": "1",
              "poster": "kiwtirApp",
              "timestamp": "1732307460.0",
              "content": "Your logic for SCP not applying to users, groups and roles is incorrect. SCP can be applied to users and roles. Groups will therefore be indirectly affected."
            }
          ],
          "timestamp": "1723458240.0",
          "upvote_count": "4",
          "comment_id": "1148091",
          "content": "Selected Answer: AB\nA and B are correct: SCP to restrict and AWS cloutrail to monitor\nC: Lambda cannot check AWS service activity\nD: AWS inspector has nothing to do here\nE: <Apply the policy to all users, groups, and roles>: cannot assign a SCP to all users, groups and roles."
        },
        {
          "upvote_count": "2",
          "content": "Selected Answer: AB\nIt's A and B",
          "comment_id": "1126177",
          "poster": "twogyt",
          "timestamp": "1721327760.0"
        },
        {
          "comment_id": "1110924",
          "upvote_count": "2",
          "content": "Selected Answer: AB\nA & B are correct.",
          "timestamp": "1719790200.0",
          "poster": "d262e67"
        },
        {
          "timestamp": "1719707520.0",
          "content": "Selected Answer: AB\nA & B Correct\nhttps://www.examtopics.com/discussions/amazon/view/47872-exam-aws-devops-engineer-professional-topic-1-question-260/",
          "upvote_count": "2",
          "poster": "Alagong",
          "comment_id": "1109343"
        },
        {
          "content": "Selected Answer: AB\nIt's A & B",
          "timestamp": "1719680460.0",
          "upvote_count": "2",
          "poster": "csG13",
          "comment_id": "1109020"
        },
        {
          "timestamp": "1719664140.0",
          "upvote_count": "2",
          "content": "Selected Answer: BC\nB & C correct",
          "comment_id": "1108778",
          "poster": "PrasannaBalaji"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:11:52.906Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "VqyYpaRbusKvVvbBX4en",
      "question_number": 96,
      "page": 20,
      "question_text": "A company sells products through an ecommerce web application. The company wants a dashboard that shows a pie chart of product transaction details. The company wants to integrate the dashboard with the company's existing Amazon CloudWatch dashboards.\n\nWhich solution will meet these requirements with the MOST operational efficiency?",
      "choices": {
        "A": "Update the ecommerce application to emit a JSON object to a CloudWatch log group for each processed transaction. Use CloudWatch Logs Insights to query the log group and to visualize the results in a pie chart format. Attach the results to the desired CloudWatch dashboard.",
        "C": "Update the ecommerce application to use AWS X-Ray for instrumentation. Create a new X-Ray subsegment. Add an annotation for each processed transaction. Use X-Ray traces to query the data and to visualize the results in a pie chart format. Attach the results to the desired CloudWatch dashboard.",
        "B": "Update the ecommerce application to emit a JSON object to an Amazon S3 bucket for each processed transaction. Use Amazon Athena to query the S3 bucket and to visualize the results in a pie chart format. Export the results from Athena. Attach the results to the desired CloudWatch dashboard.",
        "D": "Update the ecommerce application to emit a JSON object to a CloudWatch log group for each processed transaction. Create an AWS Lambda function to aggregate and write the results to Amazon DynamoDB. Create a Lambda subscription filter for the log file. Attach the results to the desired CloudWatch dashboard."
      },
      "correct_answer": "A",
      "answer_ET": "A",
      "answers_community": [
        "A (92%)",
        "4%"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/129696-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-12-29 15:36:00",
      "unix_timestamp": 1703860560,
      "discussion_count": 12,
      "discussion": [
        {
          "timestamp": "1722921780.0",
          "content": "Selected Answer: A\nPie Chart support by Cloudwatch \nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph_a_metric.html",
          "upvote_count": "1",
          "comment_id": "1261460",
          "poster": "jamesf"
        },
        {
          "poster": "dkp",
          "content": "Selected Answer: A\nA: the pie chart can be achieved using cloud watch log insights",
          "timestamp": "1713057300.0",
          "upvote_count": "2",
          "comment_id": "1195187"
        },
        {
          "content": "Selected Answer: A\nA is obvious",
          "upvote_count": "2",
          "poster": "WhyIronMan",
          "comment_id": "1186471",
          "timestamp": "1711843620.0"
        },
        {
          "upvote_count": "3",
          "timestamp": "1710586980.0",
          "poster": "DanShone",
          "comment_id": "1174916",
          "content": "Selected Answer: A\nA is the correct answer"
        },
        {
          "comment_id": "1174047",
          "upvote_count": "1",
          "poster": "master9",
          "content": "Selected Answer: B\nupdate in January 2022, Amazon CloudWatch does not provide native support for generating pie charts directly within the CloudWatch console. However, you can achieve this by exporting CloudWatch Logs to Amazon S3 and then analyzing the logs using other AWS services or third-party tools that support pie chart visualization.",
          "timestamp": "1710478680.0"
        },
        {
          "upvote_count": "4",
          "timestamp": "1707741120.0",
          "comment_id": "1148101",
          "content": "Selected Answer: A\nA is correct: <a pie chart of product transaction details> means we need to collect application logs and analyze the log. The recommended way is to use cloudwatch logs and cloudwatch logs insights\nB: Should not use S3. We should use cloudwatch logs so that we can integrate easily to the existing cloudwatch dashboards\nC: X-ray is for troubleshooting, which is short-term, not for long-term app monitoring\nD: Should not use lambda and should not write the results to dynamodb, which is primarily used to store session data\nhttps://www.examtopics.com/exams/amazon/aws-certified-devops-engineer-professional-dop-c02/view/1/#",
          "poster": "thanhnv142"
        },
        {
          "comment_id": "1121099",
          "content": "Selected Answer: A\ncloudwatch insight can generate graph",
          "poster": "a54b16f",
          "timestamp": "1705091640.0",
          "upvote_count": "2"
        },
        {
          "upvote_count": "2",
          "content": "Selected Answer: A\nGo For A \nhttps://www.delenamalan.co.za/til/2021-06-07-cloudwatch-insights-graph.html",
          "poster": "davdan99",
          "comment_id": "1118973",
          "timestamp": "1704914340.0"
        },
        {
          "content": "Selected Answer: A\nMust be A",
          "comment_id": "1110925",
          "poster": "d262e67",
          "timestamp": "1704072780.0",
          "upvote_count": "2"
        },
        {
          "upvote_count": "2",
          "content": "Selected Answer: A\nit's A",
          "timestamp": "1703876700.0",
          "comment_id": "1109023",
          "poster": "csG13"
        },
        {
          "comment_id": "1108788",
          "timestamp": "1703860920.0",
          "content": "Selected Answer: A\nSry, it should be A",
          "upvote_count": "2",
          "poster": "PrasannaBalaji"
        },
        {
          "content": "Selected Answer: D\nPrefer D",
          "timestamp": "1703860560.0",
          "upvote_count": "1",
          "comment_id": "1108782",
          "poster": "PrasannaBalaji"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:12:03.350Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "3UicRQkZBryH0E7bKi23",
      "question_number": 97,
      "page": 20,
      "question_text": "A company is launching an application. The application must use only approved AWS services. The account that runs the application was created less than 1 year ago and is assigned to an AWS Organizations OU.\n\nThe company needs to create a new Organizations account structure. The account structure must have an appropriate SCP that supports the use of only services that are currently active in the AWS account. The company will use AWS Identity and Access Management (IAM) Access Analyzer in the solution.\n\nWhich solution will meet these requirements?",
      "choices": {
        "D": "Create an SCP that allows the services that IAM Access Analyzer identifies. Create an OU for the account. Move the account into the new OU. Attach the new SCP to the management account. Detach the default FullAWSAccess SCP from the new OU.",
        "B": "Create an SCP that denies the services that IAM Access Analyzer identifies. Create an OU for the account. Move the account into the new OU. Attach the new SCP to the new OU.",
        "A": "Create an SCP that allows the services that IAM Access Analyzer identifies. Create an OU for the account. Move the account into the new OU. Attach the new SCP to the new OU. Detach the default FullAWSAccess SCP from the new OU.",
        "C": "Create an SCP that allows the services that IAM Access Analyzer identifies. Attach the new SCP to the organization's root."
      },
      "correct_answer": "A",
      "answer_ET": "A",
      "answers_community": [
        "A (88%)",
        "13%"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/129699-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-12-29 15:44:00",
      "unix_timestamp": 1703861040,
      "discussion_count": 10,
      "discussion": [
        {
          "comment_id": "1257945",
          "timestamp": "1722322560.0",
          "poster": "jamesf",
          "upvote_count": "1",
          "content": "Selected Answer: A\nThis question and answers are confusing but I think \"Management Account\" in option D and \"the account\" are different accounts. \nHence, A is correct."
        },
        {
          "poster": "dkp",
          "timestamp": "1713057600.0",
          "comment_id": "1195189",
          "content": "Selected Answer: A\nAnswer A",
          "upvote_count": "2"
        },
        {
          "timestamp": "1710586920.0",
          "comment_id": "1174915",
          "poster": "DanShone",
          "content": "Selected Answer: A\nA is correct",
          "upvote_count": "3"
        },
        {
          "comment_id": "1148105",
          "comments": [
            {
              "comment_id": "1148126",
              "poster": "thanhnv142",
              "upvote_count": "3",
              "content": "Correct: D - We can attach SCP to an account. But it only affects an account. We need to impose the scp rule on the entire accounts in the new OU",
              "timestamp": "1707744300.0"
            }
          ],
          "content": "Selected Answer: A\nA is correct: <AWS Identity and Access Management (IAM) Access Analyzer> is a solution for least privilege, which is allow some, deny all. So we need to defy allowed permissions and then remove the <default FullAWSAccess>\nB: least privilege is allow some, deny all, not allow all, deny some. \nC: The step mentioned would have no effect. The root already had default FullAWSAccess SCP. Allowing some more services does not change anything\nD: <Attach the new SCP to the management account>: Cannot attach a SCP to an account",
          "timestamp": "1707741840.0",
          "upvote_count": "3",
          "poster": "thanhnv142"
        },
        {
          "timestamp": "1705564980.0",
          "poster": "denccc",
          "comment_id": "1125615",
          "upvote_count": "1",
          "content": "A is correct"
        },
        {
          "poster": "kabary",
          "content": "Selected Answer: A\nI agree with @d262e67.",
          "timestamp": "1704148740.0",
          "comment_id": "1111475",
          "upvote_count": "1"
        },
        {
          "poster": "d262e67",
          "comment_id": "1110929",
          "upvote_count": "3",
          "timestamp": "1704073500.0",
          "content": "Selected Answer: A\nIt's A. To those who selected D, why would you assign the SCP to the management account??? The application account goes into an OU, and the SCP must be associated with that OU, period!"
        },
        {
          "timestamp": "1703912040.0",
          "comments": [
            {
              "content": "Could you please explain more?",
              "upvote_count": "1",
              "comment_id": "1110561",
              "poster": "GokSK",
              "timestamp": "1704027960.0"
            }
          ],
          "upvote_count": "1",
          "content": "Selected Answer: D\nD is the right answer",
          "comment_id": "1109037",
          "poster": "csG13"
        },
        {
          "poster": "examaws",
          "timestamp": "1703883960.0",
          "comment_id": "1109105",
          "upvote_count": "1",
          "content": "Selected Answer: A\nA is correct.\nOption D: Attaching the SCP to the management account and detaching FullAWSAccess from the new OU may lead to unintended access restrictions for other accounts and services under the management account."
        },
        {
          "content": "Selected Answer: D\nD is correct",
          "poster": "PrasannaBalaji",
          "upvote_count": "1",
          "timestamp": "1703861040.0",
          "comment_id": "1108790"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:12:03.350Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "GYOQbvIA1tmAkJ7C6n0g",
      "question_number": 98,
      "page": 20,
      "question_text": "A company has multiple development teams in different business units that work in a shared single AWS account. All Amazon EC2 resources that are created in the account must include tags that specify who created the resources. The tagging must occur within the first hour of resource creation.\n\nA DevOps engineer needs to add tags to the created resources that include the user ID that created the resource and the cost center ID. The DevOps engineer configures an AWS Lambda function with the cost center mappings to tag the resources. The DevOps engineer also sets up AWS CloudTrail in the AWS account. An Amazon S3 bucket stores the CloudTrail event logs.\n\nWhich solution will meet the tagging requirements?",
      "choices": {
        "B": "Enable server access logging on the S3 bucket. Create an S3 event notification on the S3 bucket for s3:ObjectTagging:* events.",
        "D": "Create an Amazon EventBridge rule that uses Amazon EC2 as the event source. Configure the rule to match events delivered by CloudTrail. Configure the rule to target the Lambda function.",
        "A": "Create an S3 event notification on the S3 bucket to invoke the Lambda function for s3:ObjectTagging:Put events. Enable bucket versioning on the S3 bucket.",
        "C": "Create a recurring hourly Amazon EventBridge scheduled rule that invokes the Lambda function. Modify the Lambda function to read the logs from the S3 bucket."
      },
      "correct_answer": "D",
      "answer_ET": "D",
      "answers_community": [
        "D (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/129700-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-12-29 15:47:00",
      "unix_timestamp": 1703861220,
      "discussion_count": 7,
      "discussion": [
        {
          "comment_id": "1195193",
          "upvote_count": "2",
          "content": "Selected Answer: D\nD looks more relevant",
          "poster": "dkp",
          "timestamp": "1728869220.0"
        },
        {
          "poster": "DanShone",
          "timestamp": "1726477140.0",
          "content": "Selected Answer: D\nAnswer is D.",
          "upvote_count": "3",
          "comment_id": "1174914"
        },
        {
          "poster": "thanhnv142",
          "timestamp": "1723553400.0",
          "comment_id": "1149245",
          "content": "Selected Answer: D\nD is corect. \nA and B: irrelevant\nC: using lambda to read log is a bad idea because it takes a lot of time.",
          "upvote_count": "3"
        },
        {
          "content": "Selected Answer: D\nthe trigger event is the EC2 creation, so D",
          "timestamp": "1720809720.0",
          "upvote_count": "4",
          "comment_id": "1121104",
          "poster": "a54b16f"
        },
        {
          "timestamp": "1719867000.0",
          "content": "Selected Answer: D\nAnswer is D. \n\nThe answer must have CloudTrail for EC2 tagging.",
          "upvote_count": "2",
          "comment_id": "1111481",
          "poster": "kabary"
        },
        {
          "comment_id": "1109044",
          "upvote_count": "3",
          "content": "Selected Answer: D\nIt's D. It says within an hour so it can't be C, looping over the S3 logs may take a lot (apparently there is also consideration about the 15mins limit of lambda)",
          "poster": "csG13",
          "timestamp": "1719683820.0"
        },
        {
          "upvote_count": "1",
          "timestamp": "1719665220.0",
          "content": "C looks correct",
          "comment_id": "1108795",
          "poster": "PrasannaBalaji"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:12:03.350Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "22Jxg0AnmB497dZCxP2D",
      "question_number": 99,
      "page": 20,
      "question_text": "A company runs an application for multiple environments in a single AWS account. An AWS CodePipeline pipeline uses a development Amazon Elastic Container Service (Amazon ECS) cluster to test an image for the application from an Amazon Elastic Container Registry (Amazon ECR) repository. The pipeline promotes the image to a production ECS cluster.\n\nThe company needs to move the production cluster into a separate AWS account in the same AWS Region. The production cluster must be able to download the images over a private connection.\n\nWhich solution will meet these requirements?",
      "choices": {
        "C": "Configure ECR private image replication in the main AWS account. Activate cross-account replication. Define the destination account ID of the separate AWS account.",
        "B": "Set a repository policy on the production ECR repository in the main AWS account. Configure the repository policy to allow the production ECS tasks in the separate AWS account to pull images from the main account. Configure the production ECS task execution role to have permission to download the image from the ECR repository.",
        "A": "Use Amazon ECR VPC endpoints and an Amazon S3 gateway endpoint. In the separate AWS account, create an ECR repository. Set the repository policy to allow the production ECS tasks to pull images from the main AWS account. Configure the production ECS task execution role to have permission to download the image from the ECR repository.",
        "D": "Use Amazon ECR VPC endpoints and an Amazon S3 gateway endpoint. Set a repository policy on the production ECR repository in the main AWS account. Configure the repository policy to allow the production ECS tasks in the separate AWS account to pull images from the main account. Configure the production ECS task execution role to have permission to download the image from the ECR repository."
      },
      "correct_answer": "D",
      "answer_ET": "D",
      "answers_community": [
        "D (78%)",
        "C (17%)",
        "4%"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/129701-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-12-29 15:50:00",
      "unix_timestamp": 1703861400,
      "discussion_count": 15,
      "discussion": [
        {
          "timestamp": "1743333540.0",
          "upvote_count": "1",
          "comment_id": "1412043",
          "content": "Selected Answer: B\nIt allows the production ECS tasks in a different AWS account to pull images from the ECR repository in the main AWS account with the proper access control set via repository policies and ECS task execution role permissions",
          "poster": "Srikantha"
        },
        {
          "upvote_count": "1",
          "poster": "GripZA",
          "comments": [
            {
              "timestamp": "1745222820.0",
              "content": "changing to D to meet private conn requirement",
              "comment_id": "1562406",
              "upvote_count": "1",
              "poster": "GripZA"
            }
          ],
          "comment_id": "1266540",
          "timestamp": "1723736940.0",
          "content": "Selected Answer: C\n\"You can configure your Amazon ECR private registry to support the replication of your repositories. Amazon ECR supports both cross-Region and cross-account replication\"\n\nhttps://docs.aws.amazon.com/AmazonECR/latest/userguide/replication.html\n\nIf cross-account replication is enabled, then for Cross-account replication, choose the cross-account replication setting for the registry. For Destination account, enter the account ID for the destination account and one or more Destination regions to replicate to. Choose Destination account + to configure additional accounts as replication destinations.\n\nhttps://docs.aws.amazon.com/AmazonECR/latest/userguide/registry-settings-configure.html"
        },
        {
          "timestamp": "1721997660.0",
          "upvote_count": "1",
          "comment_id": "1255715",
          "content": "The correct answer is A: The company needs to move the production cluster into a separate AWS account in the same AWS Region. The repository is in a separate account, and permissions are set there, giving better isolation between environments.",
          "comments": [
            {
              "content": "I will go with option C because cross-account replication is straightforward and secure.",
              "upvote_count": "1",
              "timestamp": "1725299640.0",
              "comment_id": "1276947",
              "poster": "hzaki"
            }
          ],
          "poster": "hzaki"
        },
        {
          "comment_id": "1241692",
          "timestamp": "1720045740.0",
          "upvote_count": "1",
          "content": "Selected Answer: D\nBased the references provided, it would appear that both \"C\" and \"D\" could work to distribute an image, EXCEPT for the \"\"private connection\" requirement. It's also seems like a cleaner solution to just rely on one ECR repository, rather than replicate repo's to other accounts in same region.",
          "poster": "Gomer"
        },
        {
          "comment_id": "1201913",
          "timestamp": "1714042800.0",
          "upvote_count": "2",
          "content": "Selected Answer: D\nhttps://docs.aws.amazon.com/AmazonECR/latest/userguide/vpc-endpoints.html",
          "poster": "c3518fc"
        },
        {
          "poster": "dkp",
          "comment_id": "1195195",
          "upvote_count": "2",
          "timestamp": "1713058440.0",
          "content": "Selected Answer: D\nAns D:\nAmazon ECS tasks to pull private images from Amazon ECR, you must create a gateway endpoint for Amazon S3. The gateway endpoint is required because Amazon ECR uses Amazon S3 to store your image layers."
        },
        {
          "comment_id": "1174913",
          "content": "Selected Answer: D\nECR VPC endpoints is needed to meet \"download the images over a private connection.\"",
          "upvote_count": "2",
          "timestamp": "1710586680.0",
          "poster": "DanShone"
        },
        {
          "content": "Selected Answer: D\nUse ECR VPC endpoints is necessary to meet the below requirements.\n`download the images over a private connection.`",
          "upvote_count": "3",
          "poster": "dzn",
          "timestamp": "1709545920.0",
          "comment_id": "1165487"
        },
        {
          "comment_id": "1156799",
          "content": "Selected Answer: C\nhttps://docs.aws.amazon.com/AmazonECR/latest/userguide/replication.html",
          "timestamp": "1708647060.0",
          "upvote_count": "1",
          "poster": "testhard"
        },
        {
          "comment_id": "1148109",
          "timestamp": "1707742740.0",
          "content": "Selected Answer: C\nC is correct: ECR private image replication can allow replicate image to the new account\nA and D: both mentions S3 gw, which is unnecessary\nB: no mention of how to replicate images cross account",
          "poster": "thanhnv142",
          "upvote_count": "2"
        },
        {
          "comment_id": "1120074",
          "comments": [
            {
              "timestamp": "1705567800.0",
              "comment_id": "1125642",
              "content": "It's D, no need to create a seperate ECR repo in the other account, just update the policy of the ECR repo in the main account to allow cross-account access.",
              "upvote_count": "3",
              "poster": "denccc"
            }
          ],
          "poster": "poctest",
          "upvote_count": "1",
          "timestamp": "1704998580.0",
          "content": "Don't see the difference between A & D"
        },
        {
          "comment_id": "1111822",
          "upvote_count": "2",
          "content": "Selected Answer: D\nhttps://docs.aws.amazon.com/AmazonECR/latest/userguide/vpc-endpoints.html",
          "poster": "ozansenturk",
          "timestamp": "1704195780.0"
        },
        {
          "content": "Selected Answer: D\nAnswer is D.",
          "comment_id": "1111482",
          "poster": "kabary",
          "upvote_count": "2",
          "timestamp": "1704149520.0"
        },
        {
          "content": "Selected Answer: D\nIt's D",
          "timestamp": "1703882580.0",
          "upvote_count": "2",
          "poster": "csG13",
          "comment_id": "1109085"
        },
        {
          "timestamp": "1703861400.0",
          "poster": "PrasannaBalaji",
          "content": "Selected Answer: D\nD - Using Amazon ECR VPC endpoints ensures that the ECS tasks in both the development and production clusters can pull Docker images securely over a private connection.",
          "upvote_count": "2",
          "comment_id": "1108796"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:12:03.350Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "k42XamqEB2uQG03EQ2ux",
      "question_number": 100,
      "page": 20,
      "question_text": "A company needs to ensure that flow logs remain configured for all existing and new VPCs in its AWS account. The company uses an AWS CloudFormation stack to manage its VPCs. The company needs a solution that will work for any VPCs that any IAM user creates.\n\nWhich solution will meet these requirements?",
      "choices": {
        "C": "Turn on AWS Config. Create an AWS Config rule to check whether VPC flow logs are turned on. Configure automatic remediation to turn on VPC flow logs.",
        "B": "Create an organization in AWS Organizations. Add the company's AWS account to the organization. Create an SCP to prevent users from modifying VPC flow logs.",
        "A": "Add the AWS::EC2::FlowLog resource to the CloudFormation stack that creates the VPCs.",
        "D": "Create an IAM policy to deny the use of API calls for VPC flow logs. Attach the IAM policy to all IAM users."
      },
      "correct_answer": "C",
      "answer_ET": "C",
      "answers_community": [
        "C (94%)",
        "6%"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/129753-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-12-29 21:48:00",
      "unix_timestamp": 1703882880,
      "discussion_count": 6,
      "discussion": [
        {
          "comment_id": "1148112",
          "timestamp": "1723460520.0",
          "poster": "thanhnv142",
          "content": "Selected Answer: C\nC is correct: This will monitor and remediate all existing and new VPCs\nA: AWS::EC2::FlowLog: This is used to configured flow log, not monitor it\nB: SCP wont address existing VPCs\nD: IAM policy has nothing to do here",
          "upvote_count": "6"
        },
        {
          "poster": "dkp",
          "timestamp": "1728870060.0",
          "comment_id": "1195200",
          "content": "Selected Answer: C\noption c is correct",
          "upvote_count": "2"
        },
        {
          "comment_id": "1112239",
          "upvote_count": "4",
          "content": "Selected Answer: C\nSCPs only prevent people from changing the VPC flow log configuration. It doesn't ensure it's on.",
          "timestamp": "1719945660.0",
          "poster": "d262e67"
        },
        {
          "comment_id": "1111865",
          "poster": "ozansenturk",
          "upvote_count": "1",
          "timestamp": "1719915360.0",
          "content": "Selected Answer: B\nboth AWS config and SCP work, however, SCP is more preventive compared to proactive AWS Config. therefore, I opted B."
        },
        {
          "comment_id": "1111484",
          "upvote_count": "2",
          "timestamp": "1719867360.0",
          "content": "Selected Answer: C\nAnswer is C.",
          "poster": "kabary"
        },
        {
          "content": "Selected Answer: C\nIt's C, here is a reference how to do it:\n\nhttps://aws.amazon.com/blogs/mt/how-to-enable-vpc-flow-logs-automatically-using-aws-config-rules/",
          "upvote_count": "3",
          "timestamp": "1719686880.0",
          "comment_id": "1109088",
          "poster": "csG13"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:12:03.350Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "AkekGD8VUWFOfY2ESFlZ",
      "question_number": 101,
      "page": 21,
      "question_text": "A company uses AWS Organizations and AWS Control Tower to manage all the company's AWS accounts. The company uses the Enterprise Support plan.\nA DevOps engineer is using Account Factory for Terraform (AFT) to provision new accounts. When new accounts are provisioned, the DevOps engineer notices that the support plan for the new accounts is set to the Basic Support plan. The DevOps engineer needs to implement a solution to provision the new accounts with the Enterprise Support plan.\nWhich solution will meet these requirements?",
      "choices": {
        "C": "Add an additional value to the control_tower_parameters input to set the AWSEnterpriseSupport parameter as the organization's management account number.",
        "B": "Create an AWS Lambda function to create a ticket for AWS Support to add the account to the Enterprise Support plan. Grant the Lambda function the support:ResolveCase permission.",
        "D": "Set the aft_feature_enterprise_support feature flag to True in the AFT deployment input configuration. Redeploy AFT and apply the changes.",
        "A": "Use an AWS Config conformance pack to deploy the account-part-of-organizations AWS Config rule and to automatically remediate any noncompliant accounts."
      },
      "correct_answer": "D",
      "answer_ET": "D",
      "answers_community": [
        "D (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/105240-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-04-05 04:26:00",
      "unix_timestamp": 1680661560,
      "discussion_count": 9,
      "discussion": [
        {
          "timestamp": "1718852880.0",
          "comment_id": "1101244",
          "poster": "z_inderjot",
          "upvote_count": "11",
          "content": "Selected Answer: D\nD check docs\nhttps://docs.aws.amazon.com/controltower/latest/userguide/aft-feature-options.html#enterprise-support-option"
        },
        {
          "poster": "5aga",
          "content": "Selected Answer: D\nD. Set the aft_feature_enterprise_support feature flag to True in the AFT deployment input configuration. Redeploy AFT and apply the changes.\n\nAWS Organizations is a service that helps to manage multiple AWS accounts. AWS Control Tower is a service that makes it easy to set up and govern secure, compliant multi-account AWS environments. Account Factory for Terraform (AFT) is an AWS Control Tower feature that provisions new accounts using Terraform templates.\n\nTo provision new accounts with the Enterprise Support plan, the DevOps engineer can set the aft_feature_enterprise_support feature flag to True in the AFT deployment input configuration. This flag enables the Enterprise Support plan for newly provisioned accounts.",
          "upvote_count": "9",
          "comment_id": "869977",
          "timestamp": "1697261940.0"
        },
        {
          "upvote_count": "1",
          "comment_id": "1561576",
          "content": "Selected Answer: D\nif we use IaC (terraform, CDK), all update must realize in IaC instead of direct in console service",
          "poster": "life1991",
          "timestamp": "1744951140.0"
        },
        {
          "upvote_count": "6",
          "comment_id": "1145483",
          "poster": "thanhnv142",
          "timestamp": "1723201680.0",
          "content": "Selected Answer: D\nD is correct: < Account Fachttps://www.examtopics.com/exams/amazon/aws-certified-devops-engineer-professional-dop-c02/view/#tory for Terraform (AFT)> means we need to change AFT config\nA: AWS Config conformance pack should be used with SSM automation document to remediate\nB and C: irrelevant"
        },
        {
          "timestamp": "1702060860.0",
          "comment_id": "918481",
          "content": "Selected Answer: D\nD\nhttps://docs.aws.amazon.com/controltower/latest/userguide/aft-feature-options.html",
          "poster": "madperro",
          "upvote_count": "3"
        },
        {
          "content": "Selected Answer: D\nD. Set the aft_feature_enterprise_support feature flag to True in the AFT deployment input configuration, and then redeploy AFT to apply the changes. This flag is used to enable the Enterprise Support plan for new accounts provisioned by AFT. By default, AFT provisions accounts with the Basic Support plan. Therefore, enabling this flag will provision accounts with the Enterprise Support plan.",
          "poster": "haazybanj",
          "upvote_count": "3",
          "comment_id": "886887",
          "timestamp": "1698892200.0"
        },
        {
          "poster": "alce2020",
          "upvote_count": "2",
          "comment_id": "871148",
          "content": "D it is",
          "timestamp": "1697389860.0"
        },
        {
          "upvote_count": "1",
          "timestamp": "1696687020.0",
          "comment_id": "863955",
          "poster": "ele",
          "content": "Selected Answer: D\nD: To enable the Enterprise Support option, set the following feature flag to True in your AFT deployment input configuration.\naft_feature_enterprise_support=true\nhttps://docs.aws.amazon.com/controltower/latest/userguide/aft-feature-options.html"
        },
        {
          "timestamp": "1696472760.0",
          "comment_id": "861690",
          "upvote_count": "1",
          "poster": "lqpO_Oqpl",
          "comments": [
            {
              "timestamp": "1710853980.0",
              "upvote_count": "1",
              "content": "check this out https://controltower.aws-management.tools/automation/aft_setup/",
              "comment_id": "1011218",
              "poster": "bugincloud"
            }
          ],
          "content": "Why not C?"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:12:13.772Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "lWgHYSLhFXIYVv93p9rn",
      "question_number": 102,
      "page": 21,
      "question_text": "A company's application teams use AWS CodeCommit repositories for their applications. The application teams have repositories in multiple AWS accounts. All accounts are in an organization in AWS Organizations.\n\nEach application team uses AWS IAM Identity Center (AWS Single Sign-On) configured with an external IdP to assume a developer IAM role. The developer role allows the application teams to use Git to work with the code in the repositories.\n\nA security audit reveals that the application teams can modify the main branch in any repository. A DevOps engineer must implement a solution that allows the application teams to modify the main branch of only the repositories that they manage.\n\nWhich combination of steps will meet these requirements? (Choose three.)",
      "choices": {
        "F": "Create an IAM permissions boundary in each account. Include the following statement:",
        "E": "Attach an SCP to the accounts. Include the following statement:",
        "C": "Create an approval rule template for each account. Associate the template with all repositories. Add the \"aws:ResourceTag/access-team\": \"$ ;{aws:PrincipalTag/access-team}\" condition to the approval rule template.",
        "D": "For each CodeCommit repository, add an access-team tag that has the value set to the name of the associated team.",
        "A": "Update the SAML assertion to pass the user's team name. Update the IAM role's trust policy to add an access-team session tag that has the team name.",
        "B": "Create an approval rule template for each team in the Organizations management account. Associate the template with all the repositories. Add the developer role ARN as an approver."
      },
      "correct_answer": "ADE",
      "answer_ET": "ADE",
      "answers_community": [
        "ADE (72%)",
        "ADF (17%)",
        "11%"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/129758-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-12-29 22:04:00",
      "unix_timestamp": 1703883840,
      "discussion_count": 17,
      "discussion": [
        {
          "comment_id": "1159058",
          "content": "Selected Answer: ADE\nA- SAML Assertion\nD - Tag the resource\nE - Will work with D above ad condition is based on resource tag",
          "upvote_count": "7",
          "poster": "Diego1414",
          "timestamp": "1708890240.0"
        },
        {
          "upvote_count": "3",
          "content": "Selected Answer: ADE\nI go for ADE\n\nOption E, this SCP ensures that only users from the team that manages a repository can modify the main branch by using the access-team tag. It denies actions if the team tags do not match.",
          "timestamp": "1722323880.0",
          "comment_id": "1257960",
          "poster": "jamesf"
        },
        {
          "upvote_count": "1",
          "content": "Why are people choosing option E? \nAs per the requirement:\n\"A company's application teams use AWS CodeCommit repositories for their applications. The application teams have repositories in multiple AWS\naccounts. All accounts are in an organization in AWS Organizations.\nEach application team uses AWS IAM Identity Center (AWS Single Sign-On) configured with an external IdP to assume a developer IAM role. The\ndeveloper role allows the application teams to use Git to work with the code in the repositories\"\nShouldn't we then ALLOW GitPush, PutFile and Merge*?\nI think it should be ADF.",
          "comment_id": "1215975",
          "timestamp": "1716406980.0",
          "comments": [
            {
              "content": "F is Wrong. E is correct. This SCP ensures that only users from the team that manages a repository can modify the main branch by using the access-team tag. It denies actions if the team tags do not match.",
              "poster": "vdxiii",
              "comment_id": "1232413",
              "timestamp": "1718712600.0",
              "upvote_count": "2"
            },
            {
              "content": "Sorry, i wanted to highlight this: \n\"A DevOps engineer must implement a solution that allows the application teams to modify the main branch of only the repositories that they manage.\"\n\nThis is why it should be ADF.",
              "poster": "kiwtirApp",
              "comment_id": "1215977",
              "comments": [
                {
                  "comment_id": "1220482",
                  "upvote_count": "4",
                  "content": "they can already access all the repositories, the requirement is to scope down their access hense DENY not ALLOW",
                  "timestamp": "1716921600.0",
                  "poster": "Onolisk"
                }
              ],
              "upvote_count": "1",
              "timestamp": "1716407040.0"
            }
          ],
          "poster": "kiwtirApp"
        },
        {
          "timestamp": "1714652280.0",
          "comment_id": "1205508",
          "upvote_count": "2",
          "content": "Selected Answer: ADE\nADE for me",
          "poster": "seetpt"
        },
        {
          "timestamp": "1713059160.0",
          "poster": "dkp",
          "content": "Selected Answer: ADE\nADE seems more appropriate",
          "upvote_count": "3",
          "comment_id": "1195202"
        },
        {
          "timestamp": "1710699900.0",
          "upvote_count": "2",
          "poster": "ogerber",
          "comment_id": "1176010",
          "content": "Selected Answer: ADF\nADF, 100%",
          "comments": [
            {
              "upvote_count": "4",
              "timestamp": "1711386300.0",
              "poster": "ogerber",
              "comment_id": "1182665",
              "content": "Correction, its ADE:\nPermissions boundaries (Option F) are more granular and would be set on each IAM role individually. While they could achieve a similar effect, they are not as broad in scope as SCPs and would require setting up on every IAM role, which could be less efficient than a blanket policy across the organization with an SCP"
            }
          ]
        },
        {
          "upvote_count": "4",
          "content": "Selected Answer: ADE\nA D E\nThere no mention of an approval step being needed so rules out B & C. and F is an allow policy not deny",
          "comment_id": "1174910",
          "poster": "DanShone",
          "timestamp": "1710586500.0"
        },
        {
          "content": "ACE \nreference for option c\nhttps://docs.aws.amazon.com/codecommit/latest/userguide/how-to-create-template.html",
          "timestamp": "1710241620.0",
          "upvote_count": "1",
          "comment_id": "1171614",
          "poster": "Shasha1"
        },
        {
          "upvote_count": "2",
          "poster": "kyuhuck",
          "timestamp": "1708329180.0",
          "comment_id": "1153764",
          "content": "Selected Answer: ADF\nadf -> correct"
        },
        {
          "poster": "ghoul1221",
          "comments": [
            {
              "poster": "kyuhuck",
              "upvote_count": "2",
              "timestamp": "1708409400.0",
              "comment_id": "1154511",
              "content": "i think . adf~"
            }
          ],
          "content": "isn't ADF? \" Attach an SCP to the accounts. Include the following statement:\" scp are for the organizations no?",
          "upvote_count": "1",
          "timestamp": "1708168020.0",
          "comment_id": "1152498"
        },
        {
          "upvote_count": "3",
          "timestamp": "1707842220.0",
          "poster": "Ramdi1",
          "comment_id": "1149363",
          "content": "Selected Answer: ADE\nI will go with ADE"
        },
        {
          "upvote_count": "2",
          "content": "Selected Answer: ACE\nACE are correct:\nA: < IAM Identity Center (AWS Single Sign-On) configured with an external IdP> means we need SAML\nC and E are just similar with \"aws:ResourceTag/access-team\": \"$ ;{aws:PrincipalTag/access-team}\" condition",
          "comment_id": "1148124",
          "timestamp": "1707744120.0",
          "poster": "thanhnv142"
        },
        {
          "timestamp": "1704915720.0",
          "content": "Go For ADE, We don't need approval Rule here, And we use organizations, that's why SCP",
          "upvote_count": "2",
          "comment_id": "1118991",
          "poster": "davdan99"
        },
        {
          "poster": "d262e67",
          "timestamp": "1704229800.0",
          "content": "Selected Answer: ADE\nAs far as I know, the approval rule templates are designed to manage pull requests, not direct pushes to branches.",
          "comment_id": "1112252",
          "upvote_count": "4"
        },
        {
          "upvote_count": "2",
          "comments": [
            {
              "upvote_count": "2",
              "comment_id": "1113534",
              "poster": "ozansenturk",
              "timestamp": "1704363720.0",
              "content": "edit: ADE"
            }
          ],
          "timestamp": "1704198540.0",
          "poster": "ozansenturk",
          "content": "Selected Answer: ADF\nquestion sounds like ABAC assessment: https://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_abac-saml.html\n\napproval rule templates are good to audit pull requests and if the developer is the repo owner, he/she is free to do anything.",
          "comment_id": "1111876"
        },
        {
          "timestamp": "1704150240.0",
          "comment_id": "1111489",
          "upvote_count": "1",
          "content": "Selected Answer: ACE\nAnswer is A, C, & E.",
          "poster": "kabary"
        },
        {
          "comment_id": "1109103",
          "content": "Selected Answer: ACE\nTricky question, I'll go with A C E",
          "upvote_count": "1",
          "poster": "csG13",
          "timestamp": "1703883840.0"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:12:13.772Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "IPxuYoMbtfEKPK645L4r",
      "question_number": 103,
      "page": 21,
      "question_text": "A company uses AWS WAF to protect its cloud infrastructure. A DevOps engineer needs to give an operations team the ability to analyze log messages from AWS WAF. The operations team needs to be able to create alarms for specific patterns in the log output.\n\nWhich solution will meet these requirements with the LEAST operational overhead?",
      "choices": {
        "C": "Create an Amazon S3 bucket for the log output. Configure AWS WAF to send log outputs to the S3 bucket. Instruct the operations team to create AWS Lambda functions that detect each desired log message pattern. Configure the Lambda functions to publish to an Amazon Simple Notification Service (Amazon SNS) topic.",
        "D": "Create an Amazon S3 bucket for the log output. Configure AWS WAF to send log outputs to the S3 bucket. Use Amazon Athena to create an external table definition that fits the log message pattern. Instruct the operations team to write SQL queries and to create Amazon CloudWatch metric filters for the Athena queries.",
        "A": "Create an Amazon CloudWatch Logs log group. Configure the appropriate AWS WAF web ACL to send log messages to the log group. Instruct the operations team to create CloudWatch metric filters.",
        "B": "Create an Amazon OpenSearch Service cluster and appropriate indexes. Configure an Amazon Kinesis Data Firehose delivery stream to stream log data to the indexes. Use OpenSearch Dashboards to create filters and widgets."
      },
      "correct_answer": "A",
      "answer_ET": "A",
      "answers_community": [
        "A (89%)",
        "11%"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/129762-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-12-29 22:29:00",
      "unix_timestamp": 1703885340,
      "discussion_count": 10,
      "discussion": [
        {
          "poster": "GripZA",
          "content": "Selected Answer: A\nTo send logs to Amazon CloudWatch Logs, you create a CloudWatch Logs log group. When you enable logging in AWS WAF, you provide the log group ARN. After you enable logging for your web ACL, AWS WAF delivers logs to the CloudWatch Logs log group in log streams.\n\nhttps://docs.aws.amazon.com/waf/latest/developerguide/logging-cw-logs.html",
          "comment_id": "1266559",
          "upvote_count": "1",
          "timestamp": "1723738980.0"
        },
        {
          "upvote_count": "4",
          "poster": "c3518fc",
          "timestamp": "1714053420.0",
          "content": "Selected Answer: A\nhttps://docs.aws.amazon.com/waf/latest/developerguide/logging-management.html",
          "comment_id": "1202040"
        },
        {
          "poster": "dkp",
          "content": "Selected Answer: A\nA & D can work, least operation overheard is A",
          "comment_id": "1195204",
          "timestamp": "1713059460.0",
          "upvote_count": "2"
        },
        {
          "upvote_count": "2",
          "content": "Selected Answer: A\nLEAST operational overhead = A",
          "poster": "DanShone",
          "comment_id": "1174909",
          "timestamp": "1710586260.0"
        },
        {
          "timestamp": "1707744720.0",
          "poster": "thanhnv142",
          "upvote_count": "2",
          "comments": [
            {
              "content": "You fail to notice that the question is asking about LEAST operational overhead. Therefore, it should be A.",
              "timestamp": "1716407340.0",
              "poster": "kiwtirApp",
              "comment_id": "1215982",
              "upvote_count": "1"
            }
          ],
          "comment_id": "1148132",
          "content": "Selected Answer: D\nD is correct: We have two tasks: collect log and analyze data. S3 bucket can store log and athena is for log analysis.\nA: This options does not mention of log analysis. Additionally, AWS WAF web ACL cannot send log to AWS logs group\nB: OpenSearch Service and Amazon Kinesis Data Firehose are used for other purposes. They are high-end features and cost a lots.\nC: Should not use lambda to analys log"
        },
        {
          "upvote_count": "1",
          "content": "Selected Answer: A\ncloudwatch",
          "timestamp": "1705355580.0",
          "comment_id": "1123716",
          "poster": "a54b16f"
        },
        {
          "timestamp": "1705092780.0",
          "content": "Selected Answer: A\ncloudwatch for WAF logging",
          "upvote_count": "1",
          "poster": "a54b16f",
          "comment_id": "1121113"
        },
        {
          "content": "Selected Answer: A\nhttps://docs.aws.amazon.com/waf/latest/developerguide/logging.html",
          "poster": "ozansenturk",
          "timestamp": "1704198720.0",
          "upvote_count": "1",
          "comment_id": "1111878"
        },
        {
          "upvote_count": "1",
          "poster": "kabary",
          "comment_id": "1111490",
          "content": "Selected Answer: A\nAnswer is A based on the following AWS documentation:\n\nhttps://docs.aws.amazon.com/waf/latest/developerguide/web-acl-creating.html",
          "timestamp": "1704150480.0"
        },
        {
          "upvote_count": "3",
          "timestamp": "1703885340.0",
          "poster": "csG13",
          "content": "Selected Answer: A\nA seems to involve the least operational overhead",
          "comment_id": "1109129"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:12:13.772Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "pF85SlDTkL7O9vmk10DM",
      "question_number": 104,
      "page": 21,
      "question_text": "A software team is using AWS CodePipeline to automate its Java application release pipeline. The pipeline consists of a source stage, then a build stage, and then a deploy stage. Each stage contains a single action that has a runOrder value of 1.\n\nThe team wants to integrate unit tests into the existing release pipeline. The team needs a solution that deploys only the code changes that pass all unit tests.\n\nWhich solution will meet these requirements?",
      "choices": {
        "D": "Modify the deploy stage. Add a test action that has a runOrder value of 2. Use AWS CodeBuild as the action provider to run unit tests.",
        "A": "Modify the build stage. Add a test action that has a runOrder value of 1. Use AWS CodeDeploy as the action provider to run unit tests.",
        "B": "Modify the build stage. Add a test action that has a runOrder value of 2. Use AWS CodeBuild as the action provider to run unit tests.",
        "C": "Modify the deploy stage. Add a test action that has a runOrder value of 1. Use AWS CodeDeploy as the action provider to run unit tests."
      },
      "correct_answer": "B",
      "answer_ET": "B",
      "answers_community": [
        "B (95%)",
        "5%"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/129703-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-12-29 16:01:00",
      "unix_timestamp": 1703862060,
      "discussion_count": 10,
      "discussion": [
        {
          "timestamp": "1707744960.0",
          "content": "Selected Answer: B\nB is correct: Runorder value of 2 ensure that we do unit tests after we build artifacts. \nA: The unit tests would run in parellel with the build step, which is incorrect. We can only test after we have done building\nC and D: The unit tests would not run before the deploy step.",
          "comment_id": "1148140",
          "upvote_count": "5",
          "poster": "thanhnv142"
        },
        {
          "comment_id": "1562416",
          "upvote_count": "1",
          "timestamp": "1745225520.0",
          "content": "Selected Answer: B\n\"runOrder is a positive integer that indicates the run order of the action within the stage. Parallel actions in the stage are shown as having the same integer. For example, two actions with a run order of two will run in parallel after the first action in the stage runs.\"\n\nhttps://docs.aws.amazon.com/codepipeline/latest/userguide/action-requirements.html",
          "poster": "GripZA"
        },
        {
          "content": "Selected Answer: C\nC: (YES) aws:SecureTransport = data in transit (TLS/HTTPS)\nD: (NO) ...server-side-encryption-aws... = data at rest (in S3)",
          "comment_id": "1242328",
          "timestamp": "1720134000.0",
          "poster": "Gomer",
          "upvote_count": "1"
        },
        {
          "poster": "c3518fc",
          "content": "Selected Answer: B\nBy modifying the build stage, adding a test action with a runOrder value of 2, and using AWS CodeBuild as the action provider to run unit tests, the solution ensures that unit tests are executed as part of the build process and that only the code changes that pass all unit tests are deployed, meeting the requirements of the software team.",
          "upvote_count": "4",
          "timestamp": "1714053600.0",
          "comment_id": "1202041"
        },
        {
          "poster": "dkp",
          "timestamp": "1713059580.0",
          "upvote_count": "2",
          "content": "Selected Answer: B\nB is correct",
          "comment_id": "1195207"
        },
        {
          "content": "Selected Answer: B\nB is correct",
          "timestamp": "1710586260.0",
          "comment_id": "1174907",
          "upvote_count": "2",
          "poster": "DanShone"
        },
        {
          "content": "Selected Answer: B\nB is correct",
          "timestamp": "1709286300.0",
          "comment_id": "1163369",
          "upvote_count": "3",
          "poster": "Jonalb"
        },
        {
          "poster": "a54b16f",
          "content": "Selected Answer: B\norder of 2 would create sequence order",
          "upvote_count": "2",
          "comment_id": "1121114",
          "timestamp": "1705092840.0"
        },
        {
          "timestamp": "1703884140.0",
          "comment_id": "1109106",
          "poster": "csG13",
          "content": "Selected Answer: B\nit's definitely B",
          "upvote_count": "2"
        },
        {
          "poster": "PrasannaBalaji",
          "timestamp": "1703862060.0",
          "upvote_count": "3",
          "content": "Option B - The runOrder value of 2 ensures that the test action runs after the build action, allowing the unit tests to be executed only if the build is successful.",
          "comment_id": "1108811"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:12:13.772Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "vMw8SOSyPReO5BPqm6a9",
      "question_number": 105,
      "page": 21,
      "question_text": "A company uses an organization in AWS Organizations to manage several AWS accounts that the company's developers use. The company requires all data to be encrypted in transit.\n\nMultiple Amazon S3 buckets that were created in developer accounts allow unencrypted connections. A DevOps engineer must enforce encryption of data in transit for all existing S3 buckets that are created in accounts in the organization.\n\nWhich solution will meet these requirements?",
      "choices": {
        "B": "Use AWS CloudFormation StackSets to deploy an AWS Network Firewall firewall to each account. Route all inbound requests to the AWS environment through the firewall. Deploy a policy to block access to all inbound requests on port 80.",
        "A": "Use AWS CloudFormation StackSets to deploy an AWS Network Firewall firewall to each account. Route all outbound requests from the AWS environment through the firewall. Deploy a policy to block access to all outbound requests on port 80.",
        "C": "Turn on AWS Config for the organization. Deploy a conformance pack that uses the s3-bucket-ssl-requests-only managed rule and an AWS Systems Manager Automation runbook. Use a runbook that adds a bucket policy statement to deny access to an S3 bucket when the value of the aws:SecureTransport condition key is false.",
        "D": "Turn on AWS Config for the organization. Deploy a conformance pack that uses the s3-bucket-ssl-requests-only managed rule and an AWS Systems Manager Automation runbook. Use a runbook that adds a bucket policy statement to deny access to an S3 bucket when the value of the s3:x-amz-server-side-encryption-aws-kms-key-id condition key is null."
      },
      "correct_answer": "C",
      "answer_ET": "C",
      "answers_community": [
        "C (96%)",
        "4%"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/129704-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-12-29 16:03:00",
      "unix_timestamp": 1703862180,
      "discussion_count": 8,
      "discussion": [
        {
          "upvote_count": "7",
          "comment_id": "1111492",
          "content": "Selected Answer: C\nAnswer C 100%.\n\naws:SecureTransport condition this will be allowing only encrypted connections over HTTPS (TLS) --> THIS IS WHAT WE NEED\n\ns3:x-amz-server-side-encryption-aws-kms-key-id --> To require that a particular AWS KMS key be used to encrypt the objects in a bucket. WE DON'T NEED THIS HERE!",
          "poster": "kabary",
          "timestamp": "1704151080.0"
        },
        {
          "timestamp": "1707745140.0",
          "poster": "thanhnv142",
          "comment_id": "1148147",
          "content": "Selected Answer: C\nC is correct:\nA and B: we should not use AWS Network Firewall firewall here. It is just incorrect\nD: s3:x-amz-server-side-encryption-aws-kms-key-id: This is for data encryption at rest, not in transit",
          "upvote_count": "6"
        },
        {
          "content": "Selected Answer: C\nC is correct",
          "timestamp": "1713059760.0",
          "comment_id": "1195212",
          "poster": "dkp",
          "upvote_count": "2"
        },
        {
          "upvote_count": "3",
          "poster": "Jonalb",
          "timestamp": "1709285820.0",
          "content": "Selected Answer: C\nC is correct:",
          "comment_id": "1163363"
        },
        {
          "comment_id": "1111888",
          "content": "Selected Answer: C\nhttps://repost.aws/knowledge-center/s3-bucket-policy-for-config-rule",
          "upvote_count": "2",
          "timestamp": "1704199200.0",
          "poster": "ozansenturk"
        },
        {
          "poster": "hisdlodskfe",
          "comment_id": "1109827",
          "content": "C is correct. D is encryption for rest not transit.",
          "timestamp": "1703953080.0",
          "upvote_count": "2"
        },
        {
          "timestamp": "1703884320.0",
          "poster": "csG13",
          "comment_id": "1109110",
          "upvote_count": "2",
          "content": "Selected Answer: C\nIt's C - they want to enforce SSL (i.e., encryption of data in transit)."
        },
        {
          "comments": [
            {
              "content": "Option D is wrong as it is for server side encryption and what the question requires is the encryption of data in transit not when the data is at rest within an S3 bucket.",
              "comment_id": "1271318",
              "timestamp": "1724420040.0",
              "upvote_count": "1",
              "poster": "flaacko"
            }
          ],
          "upvote_count": "1",
          "poster": "PrasannaBalaji",
          "comment_id": "1108816",
          "timestamp": "1703862180.0",
          "content": "Selected Answer: D\noption D is correct"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:12:13.772Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "3eqyo0zd1Kml5njjjL0s",
      "question_number": 106,
      "page": 22,
      "question_text": "A company is reviewing its IAM policies. One policy written by the DevOps engineer has been flagged as too permissive. The policy is used by an AWS Lambda function that issues a stop command to Amazon EC2 instances tagged with Environment: NonProduction over the weekend. The current policy is:\n\n//IMG//\n\n\nWhat changes should the engineer make to achieve a policy of least permission? (Choose three.)",
      "choices": {
        "D": "Add the following conditional expression:",
        "B": "Change \"Resource\": \"*\"to \"Resource\": \"arn:aws:ec2:*:*:instance/*\"",
        "C": "Add the following conditional expression:",
        "A": "Add the following conditional expression:",
        "F": "Add the following conditional expression:",
        "E": "Change \"Action\": \"ec2:*\"to \"Action\": \"ec2:StopInstances\""
      },
      "correct_answer": "B",
      "answer_ET": "B",
      "answers_community": [
        "B (62%)",
        "A (19%)",
        "D (19%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/129705-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [
        "https://img.examtopics.com/aws-certified-devops-engineer-professional-dop-c02/image16.png"
      ],
      "answer_images": [],
      "timestamp": "2023-12-29 16:06:00",
      "unix_timestamp": 1703862360,
      "discussion_count": 22,
      "discussion": [
        {
          "timestamp": "1707745800.0",
          "content": "Selected Answer: B\nB, D and E are correct:\nA: This allow all any lambda func to do the task, wont make any change\nB: This allow action only on EC2, so it is correct\nC: We need to allow action on Ec2 instances tagged with NonProdction only. Using this would grant permissions to other tags as well\nD: perfectly correct\nE: Only permit stop action, so it is correct\nF: irrelevant",
          "upvote_count": "8",
          "poster": "thanhnv142",
          "comment_id": "1148157"
        },
        {
          "comment_id": "1146584",
          "upvote_count": "8",
          "content": "Selected Answer: A\nA,D,E\nprincipalType could be a condition key \nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_elements_condition.html\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_condition-keys.html",
          "timestamp": "1707592320.0",
          "poster": "vortegon"
        },
        {
          "poster": "rinip86277",
          "upvote_count": "2",
          "comment_id": "1358620",
          "content": "BDE seems correct to me.",
          "timestamp": "1739950200.0"
        },
        {
          "upvote_count": "1",
          "poster": "youonebe",
          "comment_id": "1331127",
          "timestamp": "1735046880.0",
          "content": "Selected Answer: D\nDEF - clearly"
        },
        {
          "comment_id": "1279861",
          "timestamp": "1725671220.0",
          "content": "A, B, D\nThe engineer should make the following changes to achieve a policy of least permission:\n\nA:Add a condition to ensure that the principal making the request is an AWS Lambda function. This ensures that only Lambda functions can execute this policy.\n\nB:Narrow down the resources by specifying the ARN of EC2 instances instead of allowing all resources. This ensures that the policy only affects EC2 instances.\n\nD:Add a condition to ensure that this policy only applies to EC2 instances tagged with ''Environment: NonProduction''. This ensures that production environments are not affected by this policy.",
          "upvote_count": "1",
          "poster": "3f78595"
        },
        {
          "upvote_count": "1",
          "content": "I'll go with BDE\n\nB - restrict resource from wildcard to only \"arn:aws:ec2:*:*:instance/*\"\nD - this condition limits to non Prod only\nE - limit actions to \"ec2:StopInstances\" and not all ec2 actions\n\nas for F, although YOU CAN allow access based on date/time. The typical format is:\n\n \"Condition\": {\n \"DateGreaterThan\": {\"aws:CurrentTime\": \"2020-04-01T00:00:00Z\"},\n \"DateLessThan\": {\"aws:CurrentTime\": \"2020-06-30T23:59:59Z\"}\n\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_examples_aws-dates.html",
          "poster": "GripZA",
          "comment_id": "1266566",
          "timestamp": "1723740420.0"
        },
        {
          "comment_id": "1258352",
          "poster": "kiko_zhang",
          "timestamp": "1722364740.0",
          "upvote_count": "4",
          "content": "DEF.\nwhen using E, there is no need for B.\nF: https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_examples_aws-dates.html"
        },
        {
          "poster": "jamesf",
          "comment_id": "1257969",
          "timestamp": "1722324780.0",
          "content": "Selected Answer: B\nBDE are correct.\n\nOption F is irrelevant and can use Amazon EventBridge Rule to execute the Lambda",
          "comments": [
            {
              "timestamp": "1722324900.0",
              "content": "And in question, there is keywords: least permission\nSo B is needed",
              "poster": "jamesf",
              "upvote_count": "2",
              "comment_id": "1257970"
            }
          ],
          "upvote_count": "2"
        },
        {
          "content": "Selected Answer: D\nDEF\nE is more prefect answer rather than B. \nD. restricted the ec2 using env tag as \"nonproduction\"\nF. support scope of running time which mention in the question.",
          "timestamp": "1721986140.0",
          "upvote_count": "4",
          "poster": "ericphl",
          "comment_id": "1255621"
        },
        {
          "poster": "MalonJay",
          "upvote_count": "3",
          "content": "DEF\nPrincipal condition is usually for resource policies.",
          "comment_id": "1208866",
          "timestamp": "1715260380.0"
        },
        {
          "timestamp": "1714652400.0",
          "poster": "seetpt",
          "comment_id": "1205511",
          "content": "Selected Answer: B\nBDE for me",
          "upvote_count": "4"
        },
        {
          "comment_id": "1199240",
          "timestamp": "1713629340.0",
          "upvote_count": "3",
          "content": "Selected Answer: D\nD we need non prod,E we need specific action,F we need dates restriction",
          "poster": "didek1986"
        },
        {
          "poster": "dkp",
          "content": "Selected Answer: B\nBDE seems correct",
          "comment_id": "1195218",
          "timestamp": "1713060120.0",
          "upvote_count": "2"
        },
        {
          "content": "Why is B the correct answer?",
          "timestamp": "1707823740.0",
          "poster": "soojung",
          "comment_id": "1149120",
          "upvote_count": "2"
        },
        {
          "comment_id": "1139573",
          "timestamp": "1706991600.0",
          "content": "there is no point to restrict \"Resource\": into instances, when you restricting action to \"ec2:StopInstances\". Only EC2 instance have such action. So whats the point to restrict Resource.",
          "upvote_count": "5",
          "poster": "pokemonas"
        },
        {
          "content": "Selected Answer: B\nB, D, and E.",
          "timestamp": "1706373480.0",
          "upvote_count": "2",
          "poster": "promo286",
          "comment_id": "1133493"
        },
        {
          "poster": "yuliaqwerty",
          "content": "Selected Answer: B\nVote for BDE",
          "timestamp": "1705431480.0",
          "upvote_count": "3",
          "comment_id": "1124445"
        },
        {
          "comment_id": "1121420",
          "poster": "Ola2234",
          "timestamp": "1705134900.0",
          "content": "BDE is the correct answer.",
          "upvote_count": "1"
        },
        {
          "comment_id": "1112313",
          "poster": "d262e67",
          "content": "Selected Answer: B\nB, D, and E. Principal is not for an IAM policy. And it's not possible to include weekdays in the policy.",
          "timestamp": "1704233280.0",
          "upvote_count": "2"
        },
        {
          "poster": "ozansenturk",
          "timestamp": "1704201480.0",
          "content": "Selected Answer: B\nBDE seems correct",
          "comment_id": "1111922",
          "upvote_count": "1"
        },
        {
          "timestamp": "1703885100.0",
          "comment_id": "1109123",
          "upvote_count": "2",
          "content": "Selected Answer: B\nB, D & E seem correct.",
          "poster": "csG13"
        },
        {
          "comment_id": "1108824",
          "timestamp": "1703862360.0",
          "content": "BDE is correct",
          "poster": "PrasannaBalaji",
          "upvote_count": "2"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:12:24.241Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "nPi0bOJIdASktb5BGHPs",
      "question_number": 107,
      "page": 22,
      "question_text": "A company is developing an application that will generate log events. The log events consist of five distinct metrics every one tenth of a second and produce a large amount of data.\n\nThe company needs to configure the application to write the logs to Amazon Timestream. The company will configure a daily query against the Timestream table.\n\nWhich combination of steps will meet these requirements with the FASTEST query performance? (Choose three.)",
      "choices": {
        "C": "Treat each log as a single-measure record.",
        "F": "Configure the memory store retention period to be shorter than the magnetic store retention period.",
        "B": "Write each log event as a single write operation.",
        "D": "Treat each log as a multi-measure record.",
        "E": "Configure the memory store retention period to be longer than the magnetic store retention period.",
        "A": "Use batch writes to write multiple log events in a single write operation."
      },
      "correct_answer": "ADF",
      "answer_ET": "ADF",
      "answers_community": [
        "ADF (74%)",
        "ADE (15%)",
        "11%"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/133291-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-02-07 16:58:00",
      "unix_timestamp": 1707321480,
      "discussion_count": 9,
      "discussion": [
        {
          "timestamp": "1707592620.0",
          "poster": "vortegon",
          "content": "Selected Answer: ADF\nWhile E suggests configuring the memory store retention period to be longer than the magnetic store retention period, this is typically not aimed at optimizing query performance but rather at keeping data in the faster-access memory store for longer periods, which could be beneficial for workloads requiring frequent access to recent data. However, for the scenario described, focusing on efficient data ingestion methods (A and D) and understanding the role of retention periods (F) provides a balanced approach to achieving the fastest query performance for daily queries.",
          "upvote_count": "7",
          "comment_id": "1146587"
        },
        {
          "upvote_count": "2",
          "content": "Selected Answer: ADE\nE fast performance\nA writer more throughput\n D multi measure means less records to store each data point. Faster query",
          "timestamp": "1721996280.0",
          "comment_id": "1255708",
          "poster": "auxwww"
        },
        {
          "upvote_count": "2",
          "poster": "Gomer",
          "content": "Selected Answer: ADE\nMy only hesitation is in regards to how batch writes might improve query performance, other than if the stored data is in a contiguous chunk, that could hep a query later. As far as for multi-measure and more memory, I defer to references:\nA: (YES) \"When writing data to InfluxDB, write data in batches to minimize the network overhead related to every write request.\" \nD: (YES) \"Multi-measure records results in lower query latency for most query types when compared to single-measure records.\" \nE: (YES) \"The memory store is optimized for high throughput data writes and fast point-in-time queries.\" \nF: (NO) \"The magnetic store is optimized for lower throughput late-arriving data writes, long term data storage, and fast analytical queries.\"",
          "timestamp": "1720153800.0",
          "comments": [
            {
              "upvote_count": "1",
              "timestamp": "1720154760.0",
              "poster": "Gomer",
              "comment_id": "1242481",
              "content": "Memory based storage is always going to provide \"FASTEST query performance\" compared to magnetic storage. You want faster query, provide higher ratio of memory storage compared to magnetic."
            }
          ],
          "comment_id": "1242468"
        },
        {
          "upvote_count": "3",
          "poster": "didek1986",
          "content": "Selected Answer: ADF\nADF\nA - improve write performance and efficiency\nD - query for a specific measure in a multi-measure record, Timestream only scans the relevant measure, not the entire record. This means that even though the record contains multiple measures, the query performance for a specific measure is not negatively impacted. Multi-measure record reduces the number of records that need to be written and subsequently queried, which improve query performance.\nF - memory store, which is optimised for write and query performance, is not filled with older data that is not frequently accessed",
          "comment_id": "1197773",
          "timestamp": "1713425400.0"
        },
        {
          "poster": "dkp",
          "content": "Selected Answer: ADF\nADF seems more relevant",
          "timestamp": "1713060540.0",
          "comment_id": "1195222",
          "upvote_count": "3"
        },
        {
          "timestamp": "1708893360.0",
          "poster": "Diego1414",
          "content": "Selected Answer: ADF\nADF – batch writes, Treat log as multi-measure record, Memory story should be shorter,.\nhttps://aws.amazon.com/blogs/database/improve-query-performance-and-reduce-cost-using-scheduled-queries-in-amazon-timestream/#:~:text=Improve%20query%20performance%20and%20reduce%20cost%20using%20scheduled,6%20Query%20performance%20metrics%20...%207%20Conclusion%20",
          "upvote_count": "4",
          "comment_id": "1159160"
        },
        {
          "upvote_count": "3",
          "comment_id": "1149377",
          "content": "Selected Answer: ACD\nA. Batch writes: This significantly reduces overhead associated with individual write operations and improves overall write throughput.\nC. Single-measure record: For daily queries summarizing multiple metrics, treating each log as a single record helps Timestream leverage its optimized storage and query processing for single measures.\nD. Multi-measure record: While it seems counterintuitive, Timestream performs better with multiple measures within a single record compared to separate records for each metric. This allows for efficient data retrieval and aggregation during queries.",
          "comments": [
            {
              "poster": "Ramdi1",
              "upvote_count": "1",
              "timestamp": "1707843420.0",
              "content": "Options B, E, and F are not recommended for optimal performance:\n \nB. Single write operations: This increases overhead and reduces write throughput, negating Timestream's scalability benefits.\nE. Longer memory store: While faster for recent data, it increases cost and doesn't impact daily queries focused on older, magnetic store data.\nF. Shorter memory store: Reduces cost but sacrifices potential performance gains for frequently accessed recent data, which might not be relevant for daily queries.\n \nBy combining batch writes, single-measure records, and multi-measure records, the company can achieve the fastest query performance for their daily Timestream use case.",
              "comment_id": "1149378"
            }
          ],
          "poster": "Ramdi1",
          "timestamp": "1707843420.0"
        },
        {
          "content": "Selected Answer: ADF\nA,D and F are correct:\nA: do job in batch optimize costs and performance\nB: should not do single write\nC: The app emits multiple records the same time. So it should be multi-measure record, not single one\nD: correct\nE: Should not do this\nF: correct",
          "timestamp": "1707747240.0",
          "comment_id": "1148173",
          "poster": "thanhnv142",
          "upvote_count": "3"
        },
        {
          "poster": "Chelseajcole",
          "comment_id": "1143488",
          "timestamp": "1707321480.0",
          "content": "ACE. Batch the write, write as whole and stay in memory longer",
          "upvote_count": "1"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:12:24.241Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "hNW6qancM6T4NUVKUDH8",
      "question_number": 108,
      "page": 22,
      "question_text": "A DevOps engineer has created an AWS CloudFormation template that deploys an application on Amazon EC2 instances. The EC2 instances run Amazon Linux. The application is deployed to the EC2 instances by using shell scripts that contain user data. The EC2 instances have an IAM instance profile that has an IAM role with the AmazonSSMManagedinstanceCore managed policy attached.\n\nThe DevOps engineer has modified the user data in the CloudFormation template to install a new version of the application. The engineer has also applied the stack update. However, the application was not updated on the running EC2 instances. The engineer needs to ensure that the changes to the application are installed on the running EC2 instances.\n\nWhich combination of steps will meet these requirements? (Choose two.)",
      "choices": {
        "E": "Refactor the user data command to use an AWS Systems Manager document (SSM document). Use Systems Manager State Manager to create an association between the SSM document and the EC2 instances.",
        "C": "Configure an EC2 launch template for the EC2 instances. Create a new EC2 Auto Scaling group. Associate the Auto Scaling group with the EC2 launch template. Use the AutoScalingScheduledAction update policy for the Auto Scaling group.",
        "B": "Refactor the user data commands to use the cfn-init helper script. Update the user data to install and configure the cfn-hup and cfn-init helper scripts to monitor and apply the metadata changes.",
        "D": "Refactor the user data commands to use an AWS Systems Manager document (SSM document). Add an AWS CLI command in the user data to use Systems Manager Run Command to apply the SSM document to the EC2 instances.",
        "A": "Configure the user data content to use the Multipurpose Internet Mail Extensions (MIME) multipart format. Set the scripts-user parameter to always in the text/cloud-config section."
      },
      "correct_answer": "BE",
      "answer_ET": "BE",
      "answers_community": [
        "BE (71%)",
        "BD (25%)",
        "4%"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/132880-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-02-05 17:45:00",
      "unix_timestamp": 1707151500,
      "discussion_count": 12,
      "discussion": [
        {
          "upvote_count": "9",
          "comment_id": "1146591",
          "timestamp": "1723310580.0",
          "content": "Selected Answer: BE\nB and E are the most effective in ensuring that updates to the application are installed on the running EC2 instances by leveraging CloudFormation's and AWS Systems Manager's capabilities for managing and applying updates.",
          "poster": "vortegon"
        },
        {
          "comment_id": "1412065",
          "content": "Selected Answer: BE\nThe best combination of steps to ensure the application is updated on the running EC2 instances during a CloudFormation stack update is:\n\nB. Refactor the user data commands to use the cfn-init helper script. Update the user data to install and configure the cfn-hup and cfn-init helper scripts to monitor and apply the metadata changes.\nD. Refactor the user data commands to use an AWS Systems Manager document (SSM document). Add an AWS CLI command in the user data to use Systems Manager Run Command to apply the SSM document to the EC2 instances.",
          "timestamp": "1743334320.0",
          "poster": "Srikantha",
          "upvote_count": "1"
        },
        {
          "content": "Selected Answer: BE\nB&E\nD. Systems Manager Run Command (with user data): Using Run Command within user data to apply an SSM document introduces an unnecessary step. Option E with State Manager automates the process.",
          "upvote_count": "3",
          "poster": "dkp",
          "comment_id": "1195229",
          "timestamp": "1728872400.0"
        },
        {
          "upvote_count": "2",
          "poster": "WhyIronMan",
          "content": "Selected Answer: BE\nB, E.\n\"Association\" is the key. Details are everything during an Investigation...",
          "timestamp": "1727651820.0",
          "comment_id": "1186474"
        },
        {
          "poster": "Seoyong",
          "upvote_count": "1",
          "comment_id": "1182248",
          "timestamp": "1727238120.0",
          "content": "Selected Answer: E\nUser data is executed when the system starts, not executed in runing EC2."
        },
        {
          "upvote_count": "3",
          "content": "Selected Answer: BE\nEC2 instance profile with AmazonSSMManagedinstanceCore policy doesn't have permissions to SSM Run Command, so D is incorrect. \nSo for me it's BE.",
          "timestamp": "1727167500.0",
          "comment_id": "1181453",
          "poster": "vmahilevskyi"
        },
        {
          "comment_id": "1176028",
          "content": "Selected Answer: BE\n\"Add an AWS CLI command in the user data to use Systems Manager Run Command to apply the SSM document to the EC2 instances.\" \n**Add an AWS CLI command in the ****user data*****\nit will not help to update running instances, tricky question.\nOnly Systems Manager State Manager will update the running instances in this question",
          "poster": "ogerber",
          "timestamp": "1726592100.0",
          "upvote_count": "2"
        },
        {
          "upvote_count": "1",
          "poster": "fdoxxx",
          "comment_id": "1162961",
          "timestamp": "1724953260.0",
          "content": "B and D\nA. This option is not applicable for updating applications on EC2 instances.\n\nB. Refactoring the user data commands to use the cfn-init helper script helps in handling metadata changes and applying them to the EC2 instances. This is especially useful in CloudFormation stack updates.\n\nC. Creating a new EC2 Auto Scaling group with an update policy doesn't necessarily address the application update requirement in this scenario.\n\nD. Refactoring the user data commands to use an AWS Systems Manager document and using Run Command to apply the SSM document is a valid approach for updating applications on EC2 instances.\n\nE. While using Systems Manager documents and State Manager is a valid approach, it might be more complex than needed for a straightforward update of an application on EC2 instances.\n\nTherefore, options B and D together provide a good solution for updating the application on the running EC2 instances."
        },
        {
          "upvote_count": "1",
          "poster": "Ramdi1",
          "comment_id": "1149380",
          "comments": [
            {
              "content": "Options A, C, and E are not suitable for this scenario:\n \nOption A: MIME multipart format isn't necessary for this scenario.\nOption C: While Auto Scaling offers flexibility, creating a new launch template and Auto Scaling group is unnecessary for a simple application update.\nOption E: State Manager is generally used for ongoing configuration management, not one-time deployments like this.\n \nBy using both cfn-init and SSM documents, the DevOps engineer can achieve a reliable and manageable way to update the application on the running EC2 instances.",
              "upvote_count": "1",
              "poster": "Ramdi1",
              "timestamp": "1723561200.0",
              "comment_id": "1149381"
            }
          ],
          "timestamp": "1723561140.0",
          "content": "Selected Answer: BD\nption B: cfn-init is a powerful tool for managing configuration on EC2 instances. By using cfn-init, the DevOps engineer can ensure that the new application version is installed regardless of the current state of the instances Option D: SSM documents provide a centralized and reusable way to manage configurations. By using Run Command, the engineer can trigger the application update on all instances directly from the template."
        },
        {
          "comment_id": "1148186",
          "poster": "thanhnv142",
          "upvote_count": "2",
          "timestamp": "1723465320.0",
          "content": "Selected Answer: BD\nB and D:\nA: irrelevant\nB: cfn-init is perfectly correct for this purpose\nC: irrelevant. The question does not mention autoscaling group\nD: <Systems Manager Run Command > can help install packages, so it is correct\nE: < Systems Manager State Manager> is used to maintain, not to update"
        },
        {
          "poster": "Ramdi1",
          "content": "Selected Answer: BD\nHere's why these options are correct:\nOption B: cfn-init is a powerful tool for managing configuration on EC2 instances. By using cfn-init, the DevOps engineer can ensure that the new application version is installed regardless of the current state of the instances. cfn-hup helps keep cfn-init updated with the latest configuration changes.\n \nOption D: SSM documents provide a centralized and reusable way to manage configurations. By using Run Command, the engineer can trigger the application update on all instances directly from the template. This approach allows for easier management and updates in the future.",
          "upvote_count": "2",
          "timestamp": "1723456080.0",
          "comment_id": "1148054"
        },
        {
          "content": "Selected Answer: BD\ncfn-hup to chek for updates in cloudformation and ssm run command to run commands if required for application",
          "timestamp": "1722869100.0",
          "poster": "hotblooded",
          "comment_id": "1141284",
          "upvote_count": "2"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:12:24.241Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "mN2dQ4HIJ9LAS957BC2l",
      "question_number": 109,
      "page": 22,
      "question_text": "A company is refactoring applications to use AWS. The company identifies an internal web application that needs to make Amazon S3 API calls in a specific AWS account.\n\nThe company wants to use its existing identity provider (IdP) auth.company.com for authentication. The IdP supports only OpenID Connect (OIDC). A DevOps engineer needs to secure the web application's access to the AWS account.\n\nWhich combination of steps will meet these requirements? (Choose three.)",
      "choices": {
        "C": "Create an IAM role that has a policy that allows the necessary S3 actions. Configure the role's trust policy to allow the OIDC IP to assume the role if the sts.amazon.com:aud context key is appid_from_idp.",
        "E": "Configure the web application to use the AssumeRoleWithWebIdentity API operation to retrieve temporary credentials. Use the temporary credentials to make the S3 API calls.",
        "B": "Create an IAM IdP by using the provider URL, audience, and signature from the existing IP.",
        "A": "Configure AWS IAM Identity Center (AWS Single Sign-On). Configure an IdP. Upload the IdP metadata from the existing IdP.",
        "D": "Create an IAM role that has a policy that allows the necessary S3 actions. Configure the role's trust policy to allow the OIDC IP to assume the role if the auth.company.com:aud context key is appid_from_idp.",
        "F": "Configure the web application to use the GetFederationToken API operation to retrieve temporary credentials. Use the temporary credentials to make the S3 API calls."
      },
      "correct_answer": "BDE",
      "answer_ET": "BDE",
      "answers_community": [
        "BDE (75%)",
        "13%",
        "6%"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/133261-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-02-07 12:52:00",
      "unix_timestamp": 1707306720,
      "discussion_count": 9,
      "discussion": [
        {
          "poster": "vortegon",
          "content": "Selected Answer: BDE\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_create_for-idp_oidc.html",
          "timestamp": "1707593580.0",
          "comment_id": "1146597",
          "upvote_count": "5"
        },
        {
          "comment_id": "1331245",
          "poster": "sn61613",
          "content": "Selected Answer: BCE\nBCE\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_create_for-idp_oidc.html",
          "upvote_count": "1",
          "timestamp": "1735078560.0"
        },
        {
          "comment_id": "1242978",
          "poster": "Gomer",
          "timestamp": "1720206720.0",
          "content": "Selected Answer: BDE\n\"Use OpenID Connect (OIDC) federated identity providers instead of creating\" IAM users.\" \"With an\" IdP \"you can manage\" \"user identities outside of AWS and give these external user identities permissions to access AWS resources in your account.\"\nB: (YES) \"IAM OIDC identity Providers\" \"This is useful when creating a mobile app or web application that requires access to AWS resources, but you don't want to create custom sign-in code or manage your own user identities.\"\nD: (YES) \"For OIDC providers, use the fully qualified URL of the OIDC IdP with the aud context key\" e.g.: \"Condition\": {\"StringEquals\": {\"server.example.com:aud\": \"appid_from_oidc_idp\"}}\"\nE: (YES) \"AssumeRoleWithWebIdentity\" \"Federation through a web-based\" IDP \"returns a set of temporary security credentials for federated users\" \"authenticated\" \"with a public identity provider.\" \"This operation is useful for\" \"client-based web applications that require access to AWS.\"",
          "upvote_count": "2"
        },
        {
          "content": "Selected Answer: BDE\nBDE for me",
          "comment_id": "1205513",
          "poster": "seetpt",
          "timestamp": "1714652460.0",
          "upvote_count": "1"
        },
        {
          "content": "Selected Answer: ADE\nDE is correct not sure between A & B\nA. Configure AWS IAM Identity Center (AWS Single Sign-On). Configure an IdP. Upload the IdP metadata from the existing IdP.\nPros: Integrates with AWS SSO and allows for IdP metadata upload.\nCons: AWS SSO is generally used for managing multiple AWS accounts and SSO for multiple AWS services, might be overkill for a single account and application.\nB. Create an IAM IdP by using the provider URL, audience, and signature from the existing IP.\nPros: Creates a custom IAM IdP using the existing IdP's details.\nCons: Manual configuration of IAM IdP might be error-prone and not the best practice for OIDC integration.",
          "upvote_count": "1",
          "poster": "dkp",
          "timestamp": "1713062400.0",
          "comment_id": "1195241"
        },
        {
          "comment_id": "1148217",
          "upvote_count": "4",
          "poster": "thanhnv142",
          "timestamp": "1707749280.0",
          "content": "Selected Answer: BDE\nBDE: \nA: we need to create an IDP. We dont need a AWS Single Sign-On\nB: correct\nC: we need to authen. sts.amazon.com:aud does not for authen\nD: auth.company.com:aud is for authen\nE: This used for authen AssumeRoleWithWebIdentity\nF: This is not used for authen"
        },
        {
          "comments": [
            {
              "upvote_count": "1",
              "content": "Options A, B, and F are not suitable for this scenario:\nA: AWS SSO is currently not available for public AWS accounts and wouldn't address the specific OIDC integration requirement.\nB: While creating an IAM IdP is possible, it's generally less secure than leveraging the existing, trusted IdP with OIDC support.\nF: GetFederationToken is often used with SAML-based federation and wouldn't work directly with OIDC.",
              "poster": "Ramdi1",
              "comment_id": "1148079",
              "timestamp": "1707739860.0"
            }
          ],
          "comment_id": "1148078",
          "timestamp": "1707739860.0",
          "content": "Selected Answer: CDE\nC & D: Creating an IAM role with specific S3 permissions and configuring the trust policy based on the appropriate audience (sts.amazon.com:aud or auth.company.com:aud) allows secure role assumption by the OIDC IdP on behalf of authenticated users.\nE: Using AssumeRoleWithWebIdentity fetches temporary credentials with restricted privileges, enhancing security compared to long-lived credentials.",
          "upvote_count": "1",
          "poster": "Ramdi1"
        },
        {
          "poster": "Chelseajcole",
          "content": "BDE is my answer",
          "upvote_count": "3",
          "comment_id": "1143493",
          "timestamp": "1707321780.0"
        },
        {
          "upvote_count": "1",
          "timestamp": "1707306720.0",
          "poster": "Arnaud92",
          "comment_id": "1143290",
          "content": "Selected Answer: ADE\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_create_for-idp_oidc.html"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:12:24.241Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "A2PwyLkt2OdmA4d1UggO",
      "question_number": 110,
      "page": 22,
      "question_text": "A company uses Amazon RDS for all databases in its AWS accounts. The company uses AWS Control Tower to build a landing zone that has an audit and logging account. All databases must be encrypted at rest for compliance reasons. The company's security engineer needs to receive notification about any noncompliant databases that are in the company’s accounts.\n\nWhich solution will meet these requirements with the MOST operational efficiency?",
      "choices": {
        "C": "Create a custom AWS Config rule in every account to determine whether the RDS storage is encrypted. Create an Amazon Simple Notification Service (Amazon SNS) topic in the audit account. Create an Amazon EventBidge rule to filter noncompliant events from the AWS Control Tower control (guardrail) to notify the SNS topic. Subscribe the security engineer's email address to the SNS topic.",
        "D": "Launch an Amazon C2 instance. Run an hourly cron job by using the AWS CLI to determine whether the RDS storage is encrypted in each AWS account. Store the results in an RDS database. Notify the security engineer by sending email messages from the EC2 instance when noncompliance is detected",
        "B": "Use AWS CloudFormation StackSets to deploy AWS Lambda functions to every account. Write the Lambda function code to determine whether the RDS storage is encrypted in the account the function is deployed to. Send the findings as an Amazon CloudWatch metric to the management account. Create an Amazon Simple Notification Service (Amazon SNS) topic. Create a CloudWatch alarm that notifies the SNS topic when metric thresholds are met. Subscribe the security engineer's email address to the SNS topic.",
        "A": "Use AWS Control Tower to activate the optional detective control (guardrail) to determine whether the RDS storage is encrypted. Create an Amazon Simple Notification Service (Amazon SNS) topic in the company's audit account. Create an Amazon EventBridge rule to filter noncompliant events from the AWS Control Tower control (guardrail) to notify the SNS topic. Subscribe the security engineer's email address to the SNS topic."
      },
      "correct_answer": "A",
      "answer_ET": "A",
      "answers_community": [
        "A (79%)",
        "C (21%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/132881-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-02-05 17:48:00",
      "unix_timestamp": 1707151680,
      "discussion_count": 11,
      "discussion": [
        {
          "comment_id": "1258026",
          "poster": "jamesf",
          "content": "Selected Answer: A\nKeywords: Control Tower\nA company uses Amazon RDS for all databases in its AWS accounts. The company uses AWS Control Tower",
          "timestamp": "1722327720.0",
          "upvote_count": "2"
        },
        {
          "upvote_count": "3",
          "poster": "didek1986",
          "timestamp": "1713630120.0",
          "comment_id": "1199251",
          "content": "Selected Answer: A\nA\nhttps://docs.aws.amazon.com/controltower/latest/userguide/strongly-recommended-controls.html#disallow-rds-storage-unencrypted"
        },
        {
          "poster": "dkp",
          "content": "Selected Answer: A\nmost efficient way is A",
          "comment_id": "1195243",
          "upvote_count": "3",
          "timestamp": "1713062820.0"
        },
        {
          "comment_id": "1174884",
          "content": "Selected Answer: A\nA - least operational overhead",
          "upvote_count": "3",
          "poster": "DanShone",
          "timestamp": "1710582360.0"
        },
        {
          "poster": "sejar",
          "content": "Selected Answer: C\nGuardrail uses AWS Config for compliance detection",
          "upvote_count": "3",
          "comment_id": "1169435",
          "timestamp": "1709984760.0"
        },
        {
          "poster": "Diego1414",
          "timestamp": "1708895640.0",
          "upvote_count": "2",
          "comment_id": "1159199",
          "content": "Selected Answer: C\nAnswer: C - https://docs.aws.amazon.com/controltower/latest/userguide/strongly-recommended-controls.html#disallow-rds-storage-unencrypted"
        },
        {
          "content": "Selected Answer: A\nA is correct: we need guardraild to detect non-compliances\nB and D: no mention of guardrail.\nC: Though this option mentions guardrail, it uses AWS Config to detect non-compliances",
          "comment_id": "1148227",
          "poster": "thanhnv142",
          "comments": [
            {
              "content": "correction: C",
              "timestamp": "1707836400.0",
              "poster": "thanhnv142",
              "comment_id": "1149261",
              "upvote_count": "1"
            }
          ],
          "upvote_count": "4",
          "timestamp": "1707749520.0"
        },
        {
          "timestamp": "1707739800.0",
          "content": "Selected Answer: A\nLeverages existing infrastructure: It utilizes native AWS Control Tower functionality for compliance checks and integrates seamlessly with SNS for notifications.\nCentralized management: Configuration and monitoring are done in the audit account, eliminating the need for individual resources in each account.\nScalability: Handles future account growth without manual intervention.",
          "comment_id": "1148075",
          "poster": "Ramdi1",
          "upvote_count": "4"
        },
        {
          "poster": "vortegon",
          "content": "Selected Answer: A\nhttps://docs.aws.amazon.com/controltower/latest/userguide/strongly-recommended-controls.html#disallow-rds-storage-unencrypted",
          "comment_id": "1146601",
          "timestamp": "1707594060.0",
          "upvote_count": "4"
        },
        {
          "content": "Selected Answer: A\nhttps://docs.aws.amazon.com/controltower/latest/userguide/strongly-recommended-controls.html#disallow-rds-storage-unencrypted",
          "comment_id": "1143288",
          "timestamp": "1707306660.0",
          "poster": "Arnaud92",
          "upvote_count": "3"
        },
        {
          "timestamp": "1707151680.0",
          "comment_id": "1141286",
          "poster": "hotblooded",
          "upvote_count": "2",
          "comments": [
            {
              "content": "The key here is most operational efficiency. Option C says create a rule in every account. Control tower is more efficient in this regard.",
              "timestamp": "1735287660.0",
              "comment_id": "1332310",
              "poster": "Slays",
              "upvote_count": "2"
            },
            {
              "poster": "Chelseajcole",
              "timestamp": "1707328620.0",
              "comment_id": "1143616",
              "upvote_count": "2",
              "content": "thanks for the summary"
            }
          ],
          "content": "Selected Answer: C\nCompliance == Aws config"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:12:24.241Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "trAYtVxgp5uwm04ch2jt",
      "question_number": 111,
      "page": 23,
      "question_text": "A company is migrating from its on-premises data center to AWS. The company currently uses a custom on-premises Cl/CD pipeline solution to build and package software.\n\nThe company wants its software packages and dependent public repositories to be available in AWS CodeArtifact to facilitate the creation of application-specific pipelines.\n\nWhich combination of steps should the company take to update the CI/CD pipeline solution and to configure CodeArtifact with the LEAST operational overhead? (Choose two.)",
      "choices": {
        "D": "For each public repository, create a CodeArutact repository that is configured with an external connection. Configure the dependent repositories as upstream public repositories.",
        "C": "Create a new Amazon S3 bucket. Generate a presigned URL that allows the PutObject request. Update the on-premises CI/CD pipeline to use the presigned URL to publish the packages from the on-premises location to the S3 bucket. Create an AWS Lambda function that runs when packages are created in the bucket through a put command. Configure the Lambda function to publish the packages to CodeArtifact.",
        "E": "Create a Codeartitact repository that is configured with a set of external connections to the public repositories. Configure the external connections to be downstream of the repository.",
        "B": "Create an AWS Identity and Access Management Roles Anywhere trust anchor. Create an IAM role that allows CodeArtifact actions and that has a trust relationship on the trust anchor. Update the on-premises CI/CD pipeline to assume the new IAM role and to publish the packages to CodeArtifact.",
        "A": "Update the C1ICD pipeline to create a VM image that contains newly packaged software. Use AWS Import/Export to make the VM image available as an Amazon EC2 AMI. Launch the AMI with an attached IAM instance profile that allows CodeArtifact actions. Use AWS CLI commands to publish the packages to a CodeArtifact repository."
      },
      "correct_answer": "BD",
      "answer_ET": "BD",
      "answers_community": [
        "BD (95%)",
        "5%"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/133267-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-02-07 13:18:00",
      "unix_timestamp": 1707308280,
      "discussion_count": 8,
      "discussion": [
        {
          "content": "Selected Answer: BD\nB and D are correct: <wants its software packages and dependent public repositories to be available in AWS CodeArtifact >: we need to push onprem artifact to CodeArtifact with IAM Anywhere Role and create an upstream for public repositories\nA: irrelevant\nB: correct\nC: irrelevant\nD: correct\nE: there is no downstream in CodeArtifact",
          "upvote_count": "6",
          "comment_id": "1148238",
          "timestamp": "1723467600.0",
          "poster": "thanhnv142"
        },
        {
          "poster": "GripZA",
          "timestamp": "1745230260.0",
          "comment_id": "1562425",
          "upvote_count": "2",
          "content": "Selected Answer: BD\nB: IAM Roles Anywhere lets workloads outside AWS (on‑prem servers, VMs, containers) exchange X.509 certificates for short‑lived AWS credentials (just like an EC2 role)\n\nD: a CodeArtifact external connection is a managed link to a public upstream, you only configure one external connection per repository—CodeArtifact handles mirroring and caching. you then create a second, “downstream” CodeArtifact repo (your private repo) and add the external‑connected repos as an upstream"
        },
        {
          "upvote_count": "1",
          "comment_id": "1331140",
          "poster": "youonebe",
          "timestamp": "1735049100.0",
          "content": "Selected Answer: BD\nAWS IAM Roles Anywhere is a feature that allows workloads running outside of AWS, such as on-premises servers, containers, and applications, to access AWS resources using temporary security credentials obtained by assuming an IAM role."
        },
        {
          "upvote_count": "2",
          "comment_id": "1195245",
          "timestamp": "1728874440.0",
          "poster": "dkp",
          "content": "Selected Answer: BD\nANS B&D"
        },
        {
          "content": "Selected Answer: BD\nB & D\nB - https://docs.aws.amazon.com/rolesanywhere/latest/userguide/getting-started.html\nD - Best practice for external connections is to have one repository per domain with an external connection to a given public repository.",
          "timestamp": "1726472700.0",
          "upvote_count": "4",
          "poster": "DanShone",
          "comment_id": "1174883"
        },
        {
          "comment_id": "1148068",
          "content": "Selected Answer: BD\nB & D\n\nThe other options have drawbacks:\nA:\nComplex setup: Requires VM image creation, import, and AMI launching, adding unnecessary complexity.\nSecurity concerns: Using EC2 instances might introduce security risks compared to IAM roles.\nInefficient publishing: Relies on manual CLI commands for publishing, less automated than other options.",
          "timestamp": "1723456980.0",
          "poster": "Ramdi1",
          "upvote_count": "3",
          "comments": [
            {
              "content": "B:\nMinimal infrastructure: Only requires an IAM role and trust anchor setup in AWS, without creating additional resources like VMs or S3 buckets.\nSecure access: Leverages IAM for secure communication between the on-premises pipeline and CodeArtifact.\nDirect publishing: Enables direct package publishing from the pipeline to CodeArtifact.\n \nD:\nCentralized management: Manages public repositories through a single CodeArtifact repository with external connections.\nAutomatic updates: Upstream repository changes are automatically reflected in CodeArtifact.\nReduced bandwidth: Packages stored in public repositories, minimizing data transfer to AWS.",
              "timestamp": "1723456980.0",
              "poster": "Ramdi1",
              "comment_id": "1148070",
              "upvote_count": "1"
            }
          ]
        },
        {
          "comment_id": "1146607",
          "content": "Selected Answer: BD\nhttps://www.pulumi.com/ai/answers/bddaepm6EeuDs9du1MVtC8/aws-codeartifact-and-iam-roles-setup",
          "upvote_count": "2",
          "poster": "vortegon",
          "timestamp": "1723312140.0"
        },
        {
          "comments": [
            {
              "comment_id": "1143314",
              "content": "I meant B :-) (not A)",
              "poster": "Arnaud92",
              "timestamp": "1723026000.0",
              "upvote_count": "1"
            }
          ],
          "poster": "Arnaud92",
          "upvote_count": "1",
          "timestamp": "1723025880.0",
          "content": "Selected Answer: AD\nD. In CodeArtifact, the intended way to use external connections is to have one repository per domain with an external connection to a given public repository.\n\nA. Using aws codeartifact with rolesanywhere is the LEAST operational overhead => https://www.pulumi.com/ai/answers/bddaepm6EeuDs9du1MVtC8/aws-codeartifact-and-iam-roles-setup",
          "comment_id": "1143312"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:12:34.663Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "OvtWCdtShiwP3X9uFgGs",
      "question_number": 112,
      "page": 23,
      "question_text": "A company provides an application to customers. The application has an Amazon API Gateway REST API that invokes an AWS Lambda function. On initialization, the Lambda function loads a large amount of data from an Amazon DynamoDB table. The data load process results in long cold-start times of 8-10 seconds. The DynamoDB table has DynamoDB Accelerator (DAX) configured.\nCustomers report that the application intermittently takes a long time to respond to requests. The application receives thousands of requests throughout the day. In the middle of the day, the application experiences 10 times more requests than at any other time of the day. Near the end of the day, the application's request volume decreases to 10% of its normal total.\nA DevOps engineer needs to reduce the latency of the Lambda function at all times of the day.\nWhich solution will meet these requirements?",
      "choices": {
        "B": "Configure reserved concurrency on the Lambda function with a concurrency value of 0.",
        "C": "Configure provisioned concurrency on the Lambda function. Configure AWS Application Auto Scaling on the Lambda function with provisioned concurrency values set to a minimum of 1 and a maximum of 100.",
        "A": "Configure provisioned concurrency on the Lambda function with a concurrency value of 1. Delete the DAX cluster for the DynamoDB table.",
        "D": "Configure reserved concurrency on the Lambda function. Configure AWS Application Auto Scaling on the API Gateway API with a reserved concurrency maximum value of 100."
      },
      "correct_answer": "C",
      "answer_ET": "C",
      "answers_community": [
        "C (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/105230-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-04-05 01:29:00",
      "unix_timestamp": 1680650940,
      "discussion_count": 14,
      "discussion": [
        {
          "timestamp": "1727165280.0",
          "comment_id": "861651",
          "content": "Selected Answer: C\nTo reduce the latency of the Lambda function at all times of the day, the best solution is to configure provisioned concurrency on the Lambda function with a concurrency value of 1 and also configure AWS Application Auto Scaling on the Lambda function with provisioned concurrency values set to a minimum of 1 and a maximum of 100 (Option C).\n\nProvisioned concurrency will ensure that the Lambda function has a set number of instances always available, which will reduce the cold start time. By setting the provisioned concurrency values to a minimum of 1 and a maximum of 100, the Lambda function can handle sudden spikes in traffic and can scale down during low-traffic periods, thus minimizing costs.",
          "upvote_count": "9",
          "poster": "5aga"
        },
        {
          "poster": "9675557",
          "upvote_count": "1",
          "content": "Selected Answer: C\nGreat! Answer is C: Beacuase Provisioned Concurrency Keeps a pre-initialized number of instances ready to avoid cold starts.",
          "comment_id": "1589311",
          "timestamp": "1753188780.0"
        },
        {
          "upvote_count": "2",
          "content": "Selected Answer: C\nLambda integrates with Application Auto Scaling, allowing you to manage provisioned concurrency on a schedule or based on utilization.\nhttps://docs.aws.amazon.com/lambda/latest/dg/provisioned-concurrency.html\nhttps://docs.aws.amazon.com/autoscaling/application/userguide/services-that-can-integrate-lambda.html",
          "poster": "ele",
          "timestamp": "1727165340.0",
          "comment_id": "863664"
        },
        {
          "timestamp": "1727165340.0",
          "upvote_count": "2",
          "content": "Selected Answer: C\nBy implementing provisioned concurrency with auto-scaling and retaining the DynamoDB DAX cluster, the DevOps engineer can effectively reduce the latency of the Lambda function at all times of the day while ensuring that the application can handle varying request volumes.",
          "comment_id": "1209005",
          "poster": "c3518fc"
        },
        {
          "poster": "Gomer",
          "content": "Selected Answer: C\nAnswer is to is to use application autoscaling to create a Lambda scaling policy for Provisioned Concurrency based on a re-occuring schedule\nHere is reference that explains exactly how to do it CLI: https://aws.amazon.com/blogs/compute/scheduling-aws-lambda-provisioned-concurrency-for-recurring-peak-usage/\nHere are truncated command examples from the reference:\naws application-autoscaling register-scalable-target --service-namespace lambda [...] --min-capacity 1 --max-capacity 100 --scalable-dimension lambda:function:ProvisionedConcurrency\naws application-autoscaling put-scheduled-action --service-namespace lambda --scalable-dimension lambda:function:ProvisionedConcurrency --scalable-target-action MinCapacity=100 [...]",
          "comment_id": "1213138",
          "timestamp": "1727165280.0",
          "upvote_count": "1"
        },
        {
          "poster": "omankoman",
          "comment_id": "1269558",
          "upvote_count": "1",
          "timestamp": "1724165940.0",
          "content": "Selected Answer: C\nC is right answer."
        },
        {
          "poster": "NagaoShingo",
          "upvote_count": "1",
          "timestamp": "1723634760.0",
          "content": "Selected Answer: C\nC is right answer. Omamko.",
          "comment_id": "1265653"
        },
        {
          "poster": "thanhnv142",
          "comment_id": "1133255",
          "timestamp": "1706352900.0",
          "upvote_count": "1",
          "content": "C definitely"
        },
        {
          "timestamp": "1704636600.0",
          "poster": "yuliaqwerty",
          "upvote_count": "1",
          "content": "Agree answer C. Provisioned concurrency – This is the number of pre-initialized execution environments allocated to your function. These execution environments are ready to respond immediately to incoming function requests.",
          "comment_id": "1115895"
        },
        {
          "poster": "zijo",
          "content": "Auto Scaling makes it easy to dynamically adjust the provisioned concurrency based on metrics and hence option C is a good choice to dynamically adjust based on the changing demand levels of the lambda function throughout the day.",
          "timestamp": "1701189300.0",
          "comment_id": "1082735",
          "upvote_count": "1"
        },
        {
          "upvote_count": "2",
          "content": "C, This can help to optimize the number of instances available to serve requests, reducing the likelihood of cold starts and improving overall performance.",
          "comment_id": "921044",
          "timestamp": "1686529320.0",
          "poster": "SanChan"
        },
        {
          "upvote_count": "1",
          "content": "C it is",
          "poster": "alce2020",
          "comment_id": "871108",
          "timestamp": "1681576740.0"
        },
        {
          "upvote_count": "1",
          "timestamp": "1681492140.0",
          "poster": "jqso234",
          "comment_id": "870350",
          "content": "C is correct"
        },
        {
          "poster": "lqpO_Oqpl",
          "upvote_count": "1",
          "comment_id": "861597",
          "timestamp": "1680650940.0",
          "comments": [
            {
              "upvote_count": "1",
              "poster": "Gomer",
              "content": "Somewhat true, except (in researching this), I discovered it the setting can also be dynamically adjusted up or down by application auto-scaling based on a schedule. Look at my other references, and you'll understand.",
              "timestamp": "1716007980.0",
              "comment_id": "1213141"
            }
          ],
          "content": "D / C - Provisioned concurrency is a manually set fixed value."
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:12:34.663Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "stl1T1yWzqvSwXwrvI5g",
      "question_number": 113,
      "page": 23,
      "question_text": "A company's DevOps engineer uses AWS Systems Manager to perform maintenance tasks during maintenance windows. The company has a few Amazon EC2 instances that require a restart after notifications from AWS Health. The DevOps engineer needs to implement an automated solution to remediate these notifications. The DevOps engineer creates an Amazon EventBridge rule.\nHow should the DevOps engineer configure the EventBridge rule to meet these requirements?",
      "choices": {
        "C": "Configure an event source of AWS Health, a service of EC2, and an event type that indicates instance maintenance. Target a newly created AWS Lambda function that registers an automation task to restart the EC2 instance during a maintenance window.",
        "B": "Configure an event source of Systems Manager and an event type that indicates a maintenance window. Target a Systems Manager document to restart the EC2 instance.",
        "A": "Configure an event source of AWS Health, a service of EC2. and an event type that indicates instance maintenance. Target a Systems Manager document to restart the EC2 instance.",
        "D": "Configure an event source of EC2 and an event type that indicates instance maintenance. Target a newly created AWS Lambda function that registers an automation task to restart the EC2 instance during a maintenance window."
      },
      "correct_answer": "A",
      "answer_ET": "A",
      "answers_community": [
        "A (67%)",
        "C (29%)",
        "3%"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/105242-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-04-05 04:42:00",
      "unix_timestamp": 1680662520,
      "discussion_count": 45,
      "discussion": [
        {
          "comment_id": "938560",
          "poster": "MarDog",
          "upvote_count": "31",
          "content": "And AWS Training and Certification has A as the correct answer in the practice exam.",
          "timestamp": "1688070780.0"
        },
        {
          "comment_id": "984810",
          "upvote_count": "9",
          "content": "Selected Answer: A\nIt doesn't need to invoke Lambda.\nThere is a SSM document , RestartEC2Instance\nhttps://docs.aws.amazon.com/health/latest/ug/cloudwatch-events-health.html",
          "poster": "Seoyong",
          "timestamp": "1692397620.0"
        },
        {
          "poster": "VerRi",
          "comment_id": "1307165",
          "upvote_count": "3",
          "content": "Selected Answer: A\nBad wording. The SSM document here means the automation document (Runbook), not the Command document. EventBridge + SSM Automation (automation document aka Runbook) is a good practice",
          "timestamp": "1730770020.0"
        },
        {
          "upvote_count": "1",
          "timestamp": "1721890800.0",
          "poster": "jamesf",
          "content": "Selected Answer: A\nA\nUsing SSM document to restart EC2 Instance. Not require to invoke Lambda. \n\nhttps://docs.aws.amazon.com/health/latest/ug/cloudwatch-events-health.html#automating-instance-actions",
          "comment_id": "1254785"
        },
        {
          "poster": "trungtd",
          "upvote_count": "1",
          "comment_id": "1243751",
          "timestamp": "1720341240.0",
          "content": "Selected Answer: A\nNo need to invoke Lambda."
        },
        {
          "timestamp": "1719496740.0",
          "comments": [
            {
              "timestamp": "1723814340.0",
              "upvote_count": "1",
              "poster": "flaacko",
              "content": "From the question, the company is already using SSM so there is really no need to create a custom Lambda function. SSM is a valid target action for EventBridge events. You can trigger the running of the AWS-RestartEC2Instance automation document with an EventBridge event which means SSM documents are a valid target.",
              "comment_id": "1267135"
            }
          ],
          "comment_id": "1238215",
          "content": "Selected Answer: C\nI'm hesitant between A and C but I'm voting C\n1) SSM document is not a valid target, valid targets for SSM are: Automation, Run Command, OpsItem\n2) If company is already using maitenance windows devops engineer should use them instead of restarting instances immediately",
          "poster": "xdkonorek2",
          "upvote_count": "1"
        },
        {
          "upvote_count": "1",
          "comment_id": "1221489",
          "poster": "4bc91ae",
          "content": "Selected Answer: A\neasiest way to do this",
          "timestamp": "1717063020.0"
        },
        {
          "upvote_count": "1",
          "content": "Selected Answer: A\nSSM Runbook: AWS-RestartEC2Instance (restart one or more EC2 instances)",
          "poster": "Gomer",
          "comment_id": "1219964",
          "timestamp": "1716866100.0",
          "comments": [
            {
              "upvote_count": "1",
              "comment_id": "1219965",
              "timestamp": "1716866760.0",
              "poster": "Gomer",
              "content": "In reading through some of the responses I think \"maintenance windows\" (plural) doesn't imply scheduling through Lambda. A DevOps engineer can disable automation during production hours. The scenario is unclear if they want this running all the time, or just enabled to run ONLY in a maintenance window. What I'm sure of is they are wanting the SSM runbook as the answer. In the real world, if productin EC2 instance has a health issue, you might just very well want to reboot it automatically if that truly fixes the problem. Nuff said."
            }
          ]
        },
        {
          "poster": "bont",
          "comment_id": "1217288",
          "timestamp": "1716536940.0",
          "upvote_count": "1",
          "content": "The answer is C because A. This option is incorrect because AWS Health notifications do not trigger Systems Manager maintenance windows directly. Additionally, Systems Manager documents cannot restart EC2 instances directly; they need to be executed through other services like Systems Manager Automation or AWS Lambda."
        },
        {
          "timestamp": "1709142540.0",
          "comments": [
            {
              "upvote_count": "1",
              "comment_id": "1201060",
              "content": "A say Target a\"Systems Manager document\" not support by EB => need to use Lambda => Answer is C\nhttps://docs.aws.amazon.com/eventbridge/latest/userguide/eb-targets.html",
              "comments": [
                {
                  "content": "A Systems Manager document defines the actions that Systems Manager performs on your managed instances. An automation document is a type of Systems Manager document that's used to perform common maintenance and deployment tasks. This includes creating or updating an Amazon Machine Image (AMI). This topic outlines how to create, edit, publish, and delete automation documents with AWS Toolkit.\nsorry my mistake, ans is A",
                  "timestamp": "1716162660.0",
                  "upvote_count": "1",
                  "comment_id": "1214032",
                  "poster": "vn_thanhtung"
                }
              ],
              "timestamp": "1713917220.0",
              "poster": "vn_thanhtung"
            }
          ],
          "comment_id": "1161874",
          "upvote_count": "3",
          "poster": "zijo",
          "content": "Answer is A. You can create a maintenance window in AWS SSM and associate the EventBridge rule with the maintenance window. No need to customize the solution with lambda."
        },
        {
          "timestamp": "1708320900.0",
          "poster": "kyuhuck",
          "comment_id": "1153729",
          "content": "Selected Answer: C\nThus, Option C is the most accurate and effective solution for automating EC2 instance restarts in response to AWS Health notifications, leveraging the combined capabilities of AWS Health, Amazon EventBridge, AWS Lambda, and AWS Systems Manager.",
          "upvote_count": "1",
          "comments": [
            {
              "timestamp": "1715493180.0",
              "upvote_count": "2",
              "comment_id": "1210135",
              "content": "Why does Lambda have to be involved?",
              "poster": "01037"
            }
          ]
        },
        {
          "poster": "vortegon",
          "timestamp": "1706656680.0",
          "content": "Selected Answer: A\nhttps://docs.aws.amazon.com/health/latest/ug/cloudwatch-events-health.html#automating-instance-actions",
          "upvote_count": "4",
          "comment_id": "1136243"
        },
        {
          "upvote_count": "2",
          "comment_id": "1134190",
          "timestamp": "1706454060.0",
          "content": "A is correct:\nB: AWS health should be the event source, not system manager\nC and D: should not use lambda if already have System manager",
          "poster": "thanhnv142"
        },
        {
          "content": "The system is already using SSM to manage EC2 instances, why would you create another solution and use Lambda ? The maintenance window is added to confuse people. The event is from AWS health and need attention immediately. option A fits perfectly.",
          "upvote_count": "3",
          "comment_id": "1123398",
          "poster": "a54b16f",
          "timestamp": "1705324920.0"
        },
        {
          "timestamp": "1702736220.0",
          "comment_id": "1098224",
          "upvote_count": "2",
          "content": "https://docs.aws.amazon.com/health/latest/ug/cloudwatch-events-health.html#automating-instance-actions",
          "poster": "mehmetsungur"
        },
        {
          "upvote_count": "3",
          "poster": "koenigParas2324",
          "content": "Selected Answer: A\nOption A appears to be the most suitable:\n\nConfiguring AWS Health as the event source ensures notifications related to EC2 instances are captured.\nTargeting a Systems Manager document to restart the EC2 instance aligns with Systems Manager's capabilities for automated tasks like instance restarts.\n\nOption B focuses on Systems Manager events related to maintenance windows, which might not directly align with notifications triggered by AWS Health for EC2 instance maintenance.",
          "timestamp": "1700799960.0",
          "comment_id": "1079023"
        },
        {
          "content": "Selected Answer: A\nAnswer is A, lets breakdown the question. The first part is the DevOps uses system manager for maintenance windows (ok, normal approach) Second part of the question, some EC2 instances requires a restart after AWS Health notification (So, If there is a AWS Health notification the EC2 instance needs a restart), third part of the question, the DevOps should solve the part 2 problem automatically (but it doesn't say when, only a restart is needed), so .. the first part of the question is a catfish, you need to solve the problem automatically an the best way to do it is the A option,",
          "comment_id": "1073566",
          "upvote_count": "4",
          "timestamp": "1700248680.0",
          "poster": "zolthar_z"
        },
        {
          "content": "Selected Answer: C\nIt can't be A, as the AWS-RestartEC2Instance action is immediate. You need Lambda to schedule it to run during maintenance window.",
          "comment_id": "1066400",
          "upvote_count": "1",
          "timestamp": "1699532160.0",
          "poster": "bosmanx",
          "comments": [
            {
              "timestamp": "1702289880.0",
              "content": "system manager's automation document can also do it! the rule of thumb is all ec2 related things could directly be run via automation document but for complex task which is not by-default covered using automation document we use lambda.",
              "comment_id": "1093366",
              "upvote_count": "1",
              "poster": "Ffida2214"
            }
          ]
        },
        {
          "upvote_count": "3",
          "timestamp": "1699074060.0",
          "comment_id": "1061904",
          "content": "Selected Answer: A\nIf there is a AWS user guide / dg / FAQ with the solution trust that is the answer.\nhttps://docs.aws.amazon.com/health/latest/ug/cloudwatch-events-health.html",
          "poster": "2pk"
        },
        {
          "upvote_count": "1",
          "timestamp": "1698415440.0",
          "poster": "rlf",
          "comment_id": "1055635",
          "content": "Answer is A.\nType of scheduled event : instance stop, instance retirement, instance reboot, system maintenance(https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/monitoring-instances-status-check_sched.html)\nYou can automate actions that respond to scheduled events for your Amazon EC2 instances. When AWS Health sends an event to your AWS account, your EventBridge rule can then invoke targets, such as AWS Systems Manager Automation documents, to automate actions on your behalf(https://docs.aws.amazon.com/health/latest/ug/cloudwatch-events-health.html)"
        },
        {
          "content": "It is A:\nhttps://docs.aws.amazon.com/health/latest/ug/cloudwatch-events-health.html#automating-instance-actions",
          "comment_id": "1015766",
          "timestamp": "1695559200.0",
          "upvote_count": "1",
          "poster": "[Removed]"
        },
        {
          "upvote_count": "2",
          "timestamp": "1695019740.0",
          "comment_id": "1010333",
          "poster": "RVivek",
          "content": "Selected Answer: A\nBoth A and C has the same event rule configuration. A system system manager automation to restart and C uses lamda. I guess Lamda is not required since alreay system system manager automation is available"
        },
        {
          "poster": "daburahjail",
          "timestamp": "1694877540.0",
          "content": "Selected Answer: C\nC. Configure an event source of AWS Health, a service of EC2, and an event type that indicates instance maintenance. Target a newly created AWS Lambda function that registers an automation task to restart the EC2 instance during a maintenance window.\n\nThis option integrates AWS Health for triggering the need for maintenance, AWS Lambda for registering the task in the Systems Manager Maintenance Window, and Systems Manager for actually carrying out the restart during the designated maintenance window. This provides the least disruptive approach while still adhering to best practices.\nI don't think you have the option to immediately trigger the Systems Manager automation, yet still set it up to only execute at a specific maintenance window.",
          "comment_id": "1009170",
          "upvote_count": "1"
        },
        {
          "comment_id": "1004271",
          "content": "Selected Answer: C\nOption A would work if the update does not have to happen during a maintenance window. You can schedule the restart of an Amazon EC2 instance using AWS Systems Manager Maintenance Windows. Maintenance Windows lets you define a schedule for when to perform potentially disruptive actions on your instances, such as patch installations or restarts.\n\nIn the context of the question, if you want to automate the restart based on AWS Health notifications but only within a specific maintenance window, the situation gets a bit more complex.\n\nThat's why I am leaning towards C as it allows you to trigger a Lambda function when a specific AWS Health event occurs for an EC2 instance. The Lambda function can then check whether the current time falls within the maintenance window (which you would define in Systems Manager) before actually invoking the Systems Manager document to restart the instance.",
          "upvote_count": "4",
          "timestamp": "1694375340.0",
          "poster": "Jonfernz",
          "comments": [
            {
              "comment_id": "1006140",
              "upvote_count": "1",
              "timestamp": "1694573340.0",
              "poster": "aksliveswithaws",
              "content": "It is C. \nQuestion states that the restart should happen ONLY during maint. window.\nYou can register the AWS-RestartEC2Instance automation task to run during the maint. window using the Lambda ( say using Boto3 implementing what the CLI does in the link below. \nThis makes the end-to-end solution work based on requirements.\n1. Event Bridge rule intercepts the events from AWS Health.\n2. Event rule invokes a lambda that registers restart automation task \n3. During the main. window occurs, the EC2 gets restarted. \n\nhttps://docs.aws.amazon.com/systems-manager/latest/userguide/mw-cli-register-tasks-examples.html#:~:text=the%20following%20examples%20demonstrate%20how%20to%20register%20systems%20manager%20automation%20tasks%20with%20a%20maintenance%20window%20using%20the%20aws%20cli%3A"
            },
            {
              "content": "It has to be C. A is wrong because you are not able to ensure the reboot is within your maintenance window. There is no such \"scheduling\" parameter when the SSM automation is triggered by EventsBridge rule.",
              "comment_id": "1122613",
              "timestamp": "1705244160.0",
              "upvote_count": "1",
              "poster": "khchan123"
            }
          ]
        },
        {
          "comment_id": "997754",
          "upvote_count": "2",
          "timestamp": "1693756080.0",
          "content": "Selected Answer: A\nrestarting an ec2 with lambda is a long film",
          "poster": "BaburTurk"
        },
        {
          "timestamp": "1693527420.0",
          "comment_id": "995561",
          "content": "Selected Answer: A\nTo implement an automated solution for remediating notifications from AWS Health using Amazon EventBridge, the DevOps engineer should create an EventBridge rule with the following configuration:\n\nEvent Source: Select \"Event Source\" as \"AWS Health\".\n\nEvent Type: Choose the relevant event type(s) that correspond to notifications indicating that an Amazon EC2 instance requires a restart. For example, you might choose \"AWS Health Event\" with the specific event code that indicates an instance requires a restart.\n\nTargets: Add a target for the EventBridge rule to trigger the necessary action when the specified event type is detected. In this case, the target should be an AWS Systems Manager Automation document that performs the instance restart.\n\nTarget Type: Choose \"AWS Systems Manager Automation\".\n\nAutomation document: Select the appropriate AWS Systems Manager Automation document that defines the steps required to restart the EC2 instances. This document should include the necessary permissions to perform the required actions.",
          "upvote_count": "4",
          "poster": "WaiKit"
        },
        {
          "upvote_count": "1",
          "comment_id": "992680",
          "timestamp": "1693268640.0",
          "comments": [
            {
              "content": "Event type system maintenance will ensure the instances will be restarted only during maintenance",
              "poster": "RVivek",
              "upvote_count": "1",
              "comment_id": "1003716",
              "timestamp": "1694324280.0"
            }
          ],
          "poster": "cocegas",
          "content": "Selected Answer: C\nC - Must be in a maintenance window!!\nLetter A would immediately restart as soon the event happens, no necessary when the customer would like to restart it."
        },
        {
          "content": "Selected Answer: A\nhttps://docs.aws.amazon.com/health/latest/ug/cloudwatch-events-health.html",
          "poster": "n_d1",
          "comment_id": "985621",
          "upvote_count": "2",
          "timestamp": "1692517680.0"
        },
        {
          "upvote_count": "4",
          "comment_id": "975762",
          "content": "Selected Answer: C\nAWS Health provides information about the health of various AWS resources, including EC2 instances. By configuring AWS Health as the event source, you can capture notifications about instance maintenance events. Option A would not work because it targets a Systems Manager document, not an AWS Lambda function. Option B would not work because it does not specify an event type that indicates instance maintenance. Option D would not work because it targets an AWS Lambda function that is not configured to register an automation task to restart the EC2 instance during a maintenance window.",
          "poster": "jason7",
          "timestamp": "1691505120.0"
        },
        {
          "poster": "RVivek",
          "content": "Selected Answer: A\nAn event rule can target a system management document to restart EC2 instance . Creating a Lambda for that is not required",
          "comment_id": "970233",
          "upvote_count": "4",
          "timestamp": "1690983480.0"
        },
        {
          "poster": "aussiehoa",
          "timestamp": "1690699680.0",
          "upvote_count": "1",
          "comment_id": "966930",
          "content": "you can't target https://docs.aws.amazon.com/systems-manager-automation-runbooks/latest/userguide/automation-aws-restartec2instance.html with eventBridge so C",
          "comments": [
            {
              "upvote_count": "2",
              "poster": "beanxyz",
              "content": "Actually, You can choose the target as System Manager Automation and then the Document AWS-restartEC2Instance. The answer is A",
              "comment_id": "985673",
              "timestamp": "1692522420.0"
            }
          ]
        },
        {
          "timestamp": "1690215540.0",
          "comment_id": "961801",
          "upvote_count": "1",
          "poster": "haazybanj",
          "content": "Selected Answer: C\nThe DevOps engineer should configure the EventBridge rule as follows:\n\nC. Configure an event source of AWS Health, a service of EC2, and an event type that indicates instance maintenance. Target a newly created AWS Lambda function that registers an automation task to restart the EC2 instance during a maintenance window.\n\nThis configuration allows the EventBridge rule to capture the notifications from AWS Health indicating instance maintenance. The rule then targets a specific AWS Lambda function that is designed to register an automation task to restart the EC2 instance during the maintenance window. By using this approach, the DevOps engineer can ensure that the EC2 instances requiring restart after notifications from AWS Health are automatically remediated in a timely manner."
        },
        {
          "comment_id": "960794",
          "content": "Selected Answer: C\nGoing for C purely because it must be done during a maintenance window and you need Lambda for this. Otherwise A is correct.",
          "timestamp": "1690142580.0",
          "poster": "Certified101",
          "upvote_count": "2"
        },
        {
          "content": "I'm thinking this is A.\nhttps://docs.aws.amazon.com/systems-manager/latest/userguide/monitoring-systems-manager-targets.html",
          "comment_id": "938540",
          "timestamp": "1688069040.0",
          "poster": "MarDog",
          "upvote_count": "4"
        },
        {
          "content": "Selected Answer: A\nhttps://docs.aws.amazon.com/systems-manager-automation-runbooks/latest/userguide/automation-aws-restartec2instance.html",
          "upvote_count": "5",
          "comment_id": "933801",
          "timestamp": "1687716240.0",
          "poster": "pepecastr0"
        },
        {
          "upvote_count": "2",
          "comment_id": "933429",
          "content": "Selected Answer: C\nThe correct answer is option C.",
          "poster": "FunkyFresco",
          "timestamp": "1687686060.0"
        },
        {
          "upvote_count": "1",
          "content": "Selected Answer: B\nhttps://docs.aws.amazon.com/systems-manager/latest/userguide/monitoring-eventbridge-events.html\nThere is a Maintenance Windows event type for the Systems Manager service, with the following specific detail types:\nMaintenance Window State-change Notification\nMaintenance Window Target Registration Notification\nMaintenance Window Execution State-change Notification\nMaintenance Window Task Execution State-change Notification\nMaintenance Window Task Target Invocation State-change Notification\nMaintenance Window Task Registration Notification\n\nA is not correct, as AWS Health is related to maintenance activities that AWS performs, not ones performed by AWS customers.\nC and D are not correct, as there are no event types for EC2 that are related to Systems Manager Maintenance Windows.\n\nIt helps to actually go into the AWS console and look at the event types available for each AWS service.",
          "timestamp": "1687627320.0",
          "poster": "sb333",
          "comments": [
            {
              "content": "Read the question wrong. It's A.",
              "comment_id": "964698",
              "upvote_count": "1",
              "timestamp": "1690460820.0",
              "poster": "sb333"
            }
          ],
          "comment_id": "932807"
        },
        {
          "comment_id": "927890",
          "timestamp": "1687206000.0",
          "poster": "tartarus23",
          "content": "Selected Answer: C\nCorrect is C.\nExplanation: AWS Health provides real-time events and information related to your AWS infrastructure. It can be integrated with Amazon EventBridge to act upon the health events automatically. If the maintenance notification from AWS Health indicates that an EC2 instance requires a restart, you can set up an EventBridge rule to respond to such events. In this case, the target of this rule would be a Lambda function that would trigger a Systems Manager automation to restart the EC2 instance during a maintenance window.\n\nRemember, AWS Health is the source of the events (not EC2 or Systems Manager), and AWS Lambda can be used to execute complex remediation tasks, such as scheduling maintenance tasks via Systems Manager.",
          "upvote_count": "2"
        },
        {
          "timestamp": "1686554940.0",
          "poster": "ducluanxutrieu",
          "comment_id": "921207",
          "upvote_count": "3",
          "content": "Selected Answer: C\n1. Create an AWS Lambda function that registers an automation task to restart the EC2 instance during a maintenance window.\n2. Configure an Amazon EventBridge rule to trigger the Lambda function when an event is emitted from AWS Health indicating instance maintenance.\n3. Set the event source of the EventBridge rule to AWS Health.\n4. Set the event type of the EventBridge rule to indicate instance maintenance.\n5. Target the EventBridge rule to the newly created Lambda function."
        },
        {
          "timestamp": "1686243660.0",
          "upvote_count": "1",
          "comment_id": "918495",
          "content": "Selected Answer: D\nI think it is D. You can use run the SSM document as a target to an event but I didn't find a way to register document for a maintenance window as a target to an event. You need lambda to do this.",
          "poster": "madperro"
        },
        {
          "content": "Selected Answer: D\nI think it is D\n\nhttps://docs.aws.amazon.com/systems-manager/latest/userguide/scheduling-automations-maintenance-windows.html\n\nThe question says that it mut be done during a maintenance window out of hour. Via AWS health you receive the notification at any time of the day, but you have to execute the remediation during the next maintenance window. So you have to schedule the task, not run it immediately.",
          "poster": "bcx",
          "upvote_count": "1",
          "comment_id": "910350",
          "timestamp": "1685458320.0"
        },
        {
          "upvote_count": "1",
          "comment_id": "870463",
          "content": "A is the correct answer",
          "poster": "alce2020",
          "timestamp": "1681504560.0"
        },
        {
          "timestamp": "1680876060.0",
          "content": "Selected Answer: A\nsure A",
          "upvote_count": "2",
          "poster": "ele",
          "comment_id": "863960"
        },
        {
          "comment_id": "862449",
          "poster": "Dimidrol",
          "upvote_count": "2",
          "content": "Selected Answer: A\nA for me",
          "timestamp": "1680725460.0"
        },
        {
          "comment_id": "861695",
          "poster": "lqpO_Oqpl",
          "upvote_count": "1",
          "content": "A or B",
          "timestamp": "1680662520.0"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:12:34.663Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "2PhR6vqRkoOb92RHIUbd",
      "question_number": 114,
      "page": 23,
      "question_text": "A DevOps team uses AWS CodePipeline, AWS CodeBuild, and AWS CodeDeploy to deploy an application. The application is a REST API that uses AWS Lambda functions and Amazon API Gateway. Recent deployments have introduced errors that have affected many customers.\n\nThe DevOps team needs a solution that reverts to the most recent stable version of the application when an error is detected. The solution must affect the fewest customers possible.\n\nWhich solution will meet these requirements with the MOST operational efficiency?",
      "choices": {
        "A": "Set the deployment configuration in CodeDeploy to LambdaAllAtOnce. Configure automatic rollbacks on the deployment group. Create an Amazon CloudWatch alarm that detects HTTP Bad Gateway errors on API Gateway. Configure the deployment group to roll back when the number of alarms meets the alarm threshold.",
        "B": "Set the deployment configuration in CodeDeploy to LambdaCanary10Percent10Minutes. Configure automatic rollbacks on the deployment group. Create an Amazon CloudWatch alarm that detects HTTP Bad Gateway errors on API Gateway. Configure the deployment group to roll back when the number of alarms meets the alarm threshold.",
        "D": "Set the deployment configuration in CodeDeploy to LambdaCanary10Percent10Minutes. Configure manual rollbacks on the deployment group. Create a metric filter on an Amazon CloudWatch log group for API Gateway to monitor HTTP Bad Gateway errors. Configure the metric filter to invoke a new Lambda function that stops the current deployment and starts the most recent successful deployment.",
        "C": "Set the deployment configuration in CodeDeploy to LambdaAllAtOnce. Configure manual rollbacks on the deployment group. Create an Amazon Simple Notification Service (Amazon SNS) topic to send notifications every time a deployment fails. Configure the SNS topic to invoke a new Lambda function that stops the current deployment and starts the most recent successful deployment."
      },
      "correct_answer": "B",
      "answer_ET": "B",
      "answers_community": [
        "B (94%)",
        "6%"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/133298-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-02-07 19:03:00",
      "unix_timestamp": 1707328980,
      "discussion_count": 6,
      "discussion": [
        {
          "poster": "thanhnv142",
          "timestamp": "1723467960.0",
          "content": "Selected Answer: B\nB is correct:\nA and C: <CodeDeploy to LambdaAllAtOnce>: Replacing all at once would not allow roll back\nD: Metric filter cannot trigger lambda. Additionally, this options is a manual work",
          "upvote_count": "8",
          "comment_id": "1148244"
        },
        {
          "timestamp": "1729918980.0",
          "poster": "c3518fc",
          "upvote_count": "3",
          "content": "Selected Answer: B\nOption B provides the most operationally efficient solution by combining canary deployments, automatic rollbacks, and CloudWatch alarms to detect and respond to issues quickly while minimizing customer impact.",
          "comment_id": "1202388"
        },
        {
          "comment_id": "1195247",
          "upvote_count": "3",
          "timestamp": "1728874560.0",
          "content": "Selected Answer: B\nAnswer B",
          "poster": "dkp"
        },
        {
          "timestamp": "1726757040.0",
          "poster": "ogerber",
          "content": "Selected Answer: B\nIts B, 100%",
          "comment_id": "1177547",
          "upvote_count": "3"
        },
        {
          "timestamp": "1723563120.0",
          "content": "Selected Answer: C\noption C - LambdaAllAtOnce: Rolling back everything at once minimizes the window for potential customer impact compared to canary deployments.\nManual rollbacks: While automatic rollbacks may seem faster, they can be triggered by false positives, leading to unnecessary rollbacks and service disruptions. Manual rollbacks offer more control and allow the team to assess the situation before reverting.\nSNS notifications: Alerts about failing deployments are crucial for quick response.\nLambda function for rollback: Automating the rollback process with a Lambda function triggered by SNS notification streamlines the operation and reduces manual intervention.\nStarts the most recent successful deployment: This ensures reverting to a known-good state without manual selection, saving time and avoiding errors.",
          "comments": [
            {
              "poster": "Ramdi1",
              "timestamp": "1723563120.0",
              "comments": [
                {
                  "timestamp": "1729919040.0",
                  "upvote_count": "1",
                  "content": "don't say rubbish with confidence. B is the answer",
                  "comment_id": "1202389",
                  "poster": "c3518fc"
                }
              ],
              "content": "Here's how the other options fall short:\n \nOption A: While LambdaAllAtOnce is good, automatic rollbacks based on alarms might be triggered by transient issues, leading to unnecessary disruptions.\nOption B: Canary deployments are beneficial for testing, but rolling back 10% at a time might not be efficient if the error affects all users. Also, relying on automatic rollbacks has the same drawbacks as mentioned in A.\nOption D: While manual control is good, relying solely on CloudWatch logs and a separate Lambda function for stopping and starting deployments adds complexity and requires more manual intervention compared to the streamlined SNS-triggered rollback in option C.",
              "upvote_count": "1",
              "comment_id": "1149409"
            }
          ],
          "upvote_count": "1",
          "poster": "Ramdi1",
          "comment_id": "1149408"
        },
        {
          "timestamp": "1723046580.0",
          "content": "D. Rolling deployment with the option to stop once it detects any errors.",
          "poster": "Chelseajcole",
          "upvote_count": "1",
          "comment_id": "1143623"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:12:34.663Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "wy2suAQOGdqqHaimNKxN",
      "question_number": 115,
      "page": 23,
      "question_text": "A company recently deployed its web application on AWS. The company is preparing for a large-scale sales event and must ensure that the web application can scale to meet the demand.\n\nThe application's frontend infrastructure includes an Amazon CloudFront distribution that has an Amazon S3 bucket as an origin. The backend infrastructure includes an Amazon API Gateway API, several AWS Lambda functions, and an Amazon Aurora DB cluster.\n\nThe company's DevOps engineer conducts a load test and identifies that the Lambda functions can fulfil the peak number of requests. However, the DevOps engineer notices request latency during the initial burst of requests. Most of the requests to the Lambda functions produce queries to the database. A large portion of the invocation time is used to establish database connections.\n\nWhich combination of steps will provide the application with the required scalability? (Choose three.)",
      "choices": {
        "C": "Convert the DB cluster to an Aurora global database. Add additional Aurora Replicas in AWS Regions based on the locations of the company's customers.",
        "E": "Use Amazon RDS Proxy to create a proxy for the Aurora database. Update the Lambda functions to use the proxy endpoints for database connections.",
        "D": "Refactor the Lambda functions. Move the code blocks that initialize database connections into the function handlers.",
        "A": "Configure a higher reserved concurrency for the Lambda functions.",
        "B": "Configure a higher provisioned concurrency for the Lambda functions."
      },
      "correct_answer": "BCF",
      "answer_ET": "BCF",
      "answers_community": [
        "BCF (51%)",
        "BDF (33%)",
        "Other"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/133299-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-02-07 19:10:00",
      "unix_timestamp": 1707329400,
      "discussion_count": 19,
      "discussion": [
        {
          "timestamp": "1711835340.0",
          "upvote_count": "15",
          "comments": [
            {
              "content": "Glad I read this. I read D wrong the first time.",
              "timestamp": "1715539560.0",
              "upvote_count": "2",
              "poster": "Jay_2pt0_1",
              "comment_id": "1210338"
            },
            {
              "content": "Also, please notice that \"The company's DevOps engineer conducts a load test and identifies that the Lambda functions can fulfil the peak number of requests.\"",
              "poster": "WhyIronMan",
              "upvote_count": "5",
              "comment_id": "1186405",
              "timestamp": "1711835400.0"
            }
          ],
          "poster": "WhyIronMan",
          "comment_id": "1186404",
          "content": "Selected Answer: BCF\nA. this doesn't directly address the database connection issue and there will be moments were you will be not using it, so spending money\nB. correct, Configure a higher provisioned concurrency for the Lambda functions: This ensures that Lambda instances are ready to handle bursts of traffic, reducing cold start latency.\nC. Is correct, if they want to read only\nD. is wrong because it says \"... into the function handlers...\" while best practices say to do it OUTSIDE the function handlers. Starting NEW CONNECTIONS is bad thing.\nF. Is correct, it is a best practice"
        },
        {
          "comment_id": "1174881",
          "upvote_count": "8",
          "content": "Selected Answer: BDF\nA - Provisioned concurrency – This is the number of pre-initialized execution environments allocated to your function\nhttps://docs.aws.amazon.com/lambda/latest/dg/provisioned-concurrency.html\n\nD - Initialize SDK clients and database connections outside of the function handler\nhttps://docs.aws.amazon.com/lambda/latest/dg/best-practices.html\n\nF - RDS Proxy improves scalability by pooling and sharing database connections\nhttps://aws.amazon.com/rds/proxy/faqs/?nc=sn&loc=4",
          "timestamp": "1710581760.0",
          "poster": "DanShone"
        },
        {
          "poster": "Waak",
          "upvote_count": "1",
          "timestamp": "1738497660.0",
          "comment_id": "1350395",
          "content": "There should be another option E) Refactor the Lambda functions. Move the code blocks that initialize database connections outside the function handlers."
        },
        {
          "comment_id": "1324479",
          "content": "Selected Answer: BDF\nDanShone provided the right links to the answers",
          "timestamp": "1733824920.0",
          "poster": "teo2157",
          "upvote_count": "1"
        },
        {
          "upvote_count": "1",
          "content": "Selected Answer: BCF\nI think BCF",
          "timestamp": "1724500740.0",
          "comment_id": "1271654",
          "poster": "seetpt"
        },
        {
          "poster": "radhi2024",
          "content": "Selected Answer: BCF\nB C D i correct",
          "comment_id": "1268707",
          "timestamp": "1724073840.0",
          "upvote_count": "3"
        },
        {
          "comment_id": "1245780",
          "poster": "trungtd",
          "timestamp": "1720654680.0",
          "content": "Selected Answer: BCF\nThe person who chose D doesn't understand Lambda at all",
          "upvote_count": "4"
        },
        {
          "comment_id": "1243244",
          "timestamp": "1720250400.0",
          "content": "Selected Answer: BCF\nA. (NO) \"Lambda functions can fulfil the peak number of requests.\"\nB.(YES) \"The number of pre-initialized execution environments allocated to a function. These execution environments are ready to respond immediately to incoming function requests.\"\nC.(YES) Chosen in part by process of elimination because neither \"A\" or \"D\" is correct. \nD. (NO) Declaratons \"outside of the function's handler method remain initialized\" \"when the function is invoked again.\" \"if your Lambda function establishes a database connection\" \"the original connection is used in subsequent invocations.\"\nF.(YES) Chosen in part by process of elimination because neither \"A\" or \"D\" is correct.",
          "upvote_count": "3",
          "poster": "Gomer"
        },
        {
          "upvote_count": "3",
          "timestamp": "1712968260.0",
          "poster": "dkp",
          "comment_id": "1194589",
          "content": "BCF\nB.Configure a higher provisioned concurrency for the Lambda functions: This will help in maintaining a set number of initialized Lambda instances, reducing cold starts, and providing better scalability.\n\nC. Convert the DB cluster to an Aurora global database: This will help in reducing database connection latency for global users by replicating Aurora across multiple regions.\n\nF. Use Amazon RDS Proxy to create a proxy for the Aurora database: This will manage database connections efficiently with connection pooling, reducing the time to establish new connections and improving database interaction efficiency."
        },
        {
          "timestamp": "1712123460.0",
          "poster": "Ola2234",
          "comments": [
            {
              "poster": "c3518fc",
              "upvote_count": "1",
              "content": "but you choose option B",
              "comment_id": "1202578",
              "timestamp": "1714133880.0"
            }
          ],
          "content": "BDF\nOption A is a waste of resources\nOption D is not practicable",
          "upvote_count": "1",
          "comment_id": "1188488"
        },
        {
          "timestamp": "1710867780.0",
          "content": "ABF, 100%",
          "upvote_count": "1",
          "comments": [
            {
              "poster": "Ola2234",
              "comment_id": "1188489",
              "timestamp": "1712123580.0",
              "content": "Why would you want to configure both Reserved and Provisioned concurrency at the same time? would that not amount to a waste of resurces?",
              "upvote_count": "3"
            }
          ],
          "comment_id": "1177555",
          "poster": "ogerber"
        },
        {
          "upvote_count": "3",
          "poster": "Shasha1",
          "comment_id": "1172383",
          "timestamp": "1710319260.0",
          "content": "BCF \nreferance: https://repost.aws/knowledge-center/lambda-cold-start"
        },
        {
          "content": "Selected Answer: ABF\nD is bad pratice as mentioned here.\nhttps://docs.aws.amazon.com/lambda/latest/dg/best-practices.html#function-code:~:text=Initialize%20SDK%20clients%20and%20database%20connections%20outside%20of%20the%20function%20handler\nC - unsure if that helps, if the Lambda function is not replicated to other regions.",
          "comment_id": "1169516",
          "upvote_count": "2",
          "poster": "sejar",
          "timestamp": "1709991420.0"
        },
        {
          "content": "Selected Answer: ABF\nD is bad practice in this situation.\nConnecting DBs in global scope and using RDS Proxy can further improve performance.",
          "poster": "dzn",
          "upvote_count": "2",
          "comment_id": "1167695",
          "timestamp": "1709790720.0"
        },
        {
          "content": "Selected Answer: BDF\nConfiguring a higher reserved concurrency for the Lambda functions (Option A) ensures that a specific number of Lambda instances are available for your function, but it doesn't address the cold start issue as effectively as provisioned concurrency, nor does it directly address the database connection overhead.\n\nTherefore, the most effective combination of steps to provide the required scalability and address the identified issue would be Options B (Provisioned Concurrency), F (Amazon RDS Proxy), and a revised understanding of D that focuses on optimizing connection management for efficiency.",
          "comment_id": "1153687",
          "upvote_count": "4",
          "timestamp": "1708312920.0",
          "poster": "kyuhuck"
        },
        {
          "poster": "kyuhuck",
          "upvote_count": "3",
          "content": "Selected Answer: ABF\nB. Configure a higher provisioned concurrency for the Lambda functions: This ensures that Lambda instances are ready to handle bursts of traffic, reducing cold start latency.\nF. Use Amazon RDS Proxy to create a proxy for the Aurora database: This directly addresses the issue of database connection overhead, significantly reducing latency by pooling and reusing connections.\nA. Configure a higher reserved concurrency for the Lambda functions (optional based on specific needs): While this doesn't directly address the database connection issue, it ensures that enough Lambda instances are available to handle the application load, complementing the benefits of provisioned concurrency and RDS Proxy.",
          "comment_id": "1153330",
          "timestamp": "1708266780.0"
        },
        {
          "comment_id": "1149414",
          "upvote_count": "1",
          "comments": [
            {
              "comment_id": "1149415",
              "timestamp": "1707845880.0",
              "content": "While the other options have some merit:\nA & B: Increasing reserved/provisioned concurrency might help, but it has ongoing costs and might not be optimal for unpredictable surges.\nE: CloudFront primarily improves content delivery latency, not database-related delays.",
              "poster": "Ramdi1",
              "upvote_count": "1"
            }
          ],
          "poster": "Ramdi1",
          "content": "Selected Answer: CDF\nD: By moving connection initialization into the function handler, you avoid the cold start penalty encountered when a new Lambda instance is spun up. Each request can establish a fresh connection, reducing latency during the initial burst.\n \nF: RDS Proxy creates a connection pool, eliminating the need for each Lambda invocation to establish a new connection. Reusing connections significantly reduces request latency, especially for short-lived interactions.\n \nC: Aurora Global Database distributes data across multiple regions, improving performance for users in different locations. Adding replicas provides additional read capacity, increasing overall database scalability.",
          "timestamp": "1707845820.0"
        },
        {
          "upvote_count": "4",
          "timestamp": "1707750600.0",
          "poster": "thanhnv142",
          "content": "Selected Answer: BDF\nBDF are correct:\nA: <the Lambda functions can fulfil the peak number of requests> means we dont need to increase this \nB: correct\nC: irrelevant\nD and F: both correct, handling the connection issue",
          "comment_id": "1148247"
        },
        {
          "content": "BF. \n\nReserved concurrency – This represents the maximum number of concurrent instances allocated to your function. When a function has reserved concurrency, no other function can use that concurrency. Configuring reserved concurrency for a function incurs no additional charges.\n\nProvisioned concurrency – This is the number of pre-initialized execution environments allocated to your function. These execution environments are ready to respond immediately to incoming function requests. Configuring provisioned concurrency incurs additional charges to your AWS account.\n\nFor the query, it is the connection issue, so try to connect to a different endpoint",
          "comment_id": "1143630",
          "upvote_count": "2",
          "poster": "Chelseajcole",
          "timestamp": "1707329400.0"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:12:34.663Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "4ojnAz8rRIZPaxhOFGsg",
      "question_number": 116,
      "page": 24,
      "question_text": "A company runs a web application that extends across multiple Availability Zones. The company uses an Application Load Balancer (ALB) for routing, AWS Fargate for the application, and Amazon Aurora for the application data. The company uses AWS CloudFormation templates to deploy the application. The company stores all Docker images in an Amazon Elastic Container Registry (Amazon ECR) repository in the same AWS account and AWS Region.\n\nA DevOps engineer needs to establish a disaster recovery (DR) process in another Region. The solution must meet an RPO of 8 hours and an RTO of 2 hours. The company sometimes needs more than 2 hours to build the Docker images from the Dockerfile.\n\nWhich solution will meet the RTO and RPO requirements MOST cost-effectively?",
      "choices": {
        "A": "Copy the CloudFormation templates and the Dockerfile to an Amazon S3 bucket in the DR Region. Use AWS Backup to configure automated Aurora cross-Region hourly snapshots. In case of DR, build the most recent Docker image and upload the Docker image to an ECR repository in the DR Region. Use the CloudFormation template that has the most recent Aurora snapshot and the Docker image from the ECR repository to launch a new CloudFormation stack in the DR Region. Update the application DNS records to point to the new ALB.",
        "C": "Copy the CloudFormation templates to an Amazon S3 bucket in the DR Region. Use Amazon EventBridge to schedule an AWS Lambda function to take an hourly snapshot of the Aurora database and of the most recent Docker image in the ECR repository. Copy the snapshot and the Docker image to the DR Region. In case of DR, use the CloudFormation template with the most recent Aurora snapshot and the Docker image from the local ECR repository to launch a new CloudFormation stack in the DR Region.",
        "B": "Copy the CloudFormation templates to an Amazon S3 bucket in the DR Region. Configure Aurora automated backup Cross-Region Replication. Configure ECR Cross-Region Replication. In case of DR, use the CloudFormation template with the most recent Aurora snapshot and the Docker image from the local ECR repository to launch a new CloudFormation stack in the DR Region. Update the application DNS records to point to the new ALB.",
        "D": "Copy the CloudFormation templates to an Amazon S3 bucket in the DR Region. Deploy a second application CloudFormation stack in the DR Region. Reconfigure Aurora to be a global database. Update both CloudFormation stacks when a new application release in the current Region is needed. In case of DR, update the application DNS records to point to the new ALB."
      },
      "correct_answer": "B",
      "answer_ET": "B",
      "answers_community": [
        "B (95%)",
        "5%"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/133300-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-02-07 19:15:00",
      "unix_timestamp": 1707329700,
      "discussion_count": 9,
      "discussion": [
        {
          "upvote_count": "6",
          "poster": "thanhnv142",
          "timestamp": "1707750960.0",
          "comment_id": "1148257",
          "content": "Selected Answer: B\nB is correct:\nA: < build the most recent Docker image and upload the Docker image to an ECR repository in the DR Region>: This process might take longer than 2 hours, which does not meet the RTO \nB: correct\nC: <AWS Lambda function to take an hourly snapshot of the Aurora database>: lambda has a maximum running time of 15 minutes. Cannot do an hourly task\nD: No mention about the docker image for the ECS"
        },
        {
          "poster": "Srikantha",
          "timestamp": "1743339000.0",
          "upvote_count": "1",
          "comment_id": "1412479",
          "content": "Selected Answer: B\nThe best option in this case is Option B because it:\n\nEnsures both Aurora database and Docker images are replicated to the DR Region, meeting the RPO requirement of 8 hours.\nEliminates the need to rebuild Docker images in the DR Region, thus ensuring the RTO of 2 hours is met, which is crucial during the disaster recovery process.\nProvides a straightforward, cost-effective solution with minimal operational overhead by leveraging AWS native features like cross-Region replication for both Aurora and ECR."
        },
        {
          "poster": "spring21",
          "content": "Selected Answer: B\nYes, Amazon Aurora supports cross-Region automated backups:",
          "comment_id": "1325129",
          "upvote_count": "2",
          "timestamp": "1733938800.0"
        },
        {
          "content": "Selected Answer: C\nB is not correct as Amazon Aurora doesn´t support cross-Region automated backups: https://repost.aws/questions/QUPm9L8amQTTKkz1RXU7EqWA/unable-to-enable-cross-region-automated-backups-in-rds\nHave anyone that has responded B tried to enable cross-Region automated backups in an Aurora RDS?",
          "timestamp": "1733835900.0",
          "poster": "teo2157",
          "upvote_count": "1",
          "comment_id": "1324541"
        },
        {
          "content": "Selected Answer: B\nanswer is B",
          "poster": "dkp",
          "upvote_count": "3",
          "timestamp": "1712969100.0",
          "comment_id": "1194591"
        },
        {
          "comment_id": "1174880",
          "timestamp": "1710581460.0",
          "poster": "DanShone",
          "upvote_count": "4",
          "content": "Selected Answer: B\nB: Least operational overhead, lower cost and meets RPO and RTO"
        },
        {
          "timestamp": "1709238180.0",
          "poster": "fdoxxx",
          "content": "Selected Answer: B\nB provides a cost-effective solution that meets the RTO and RPO",
          "upvote_count": "4",
          "comment_id": "1162985"
        },
        {
          "content": "I also go with B",
          "poster": "LeoSantos121212121212121",
          "timestamp": "1707497460.0",
          "upvote_count": "2",
          "comment_id": "1145648"
        },
        {
          "poster": "Chelseajcole",
          "content": "B. Most cost effective",
          "comment_id": "1143639",
          "upvote_count": "2",
          "timestamp": "1707329700.0"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:12:45.126Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "nWInJWehjxbFrqFaLsIq",
      "question_number": 117,
      "page": 24,
      "question_text": "A company's application runs on Amazon EC2 instances. The application writes to a log file that records the username, date, time, and source IP address of the login. The log is published to a log group in Amazon CloudWatch Logs.\n\nThe company is performing a root cause analysis for an event that occurred on the previous day. The company needs to know the number of logins for a specific user from the past 7 days.\n\nWhich solution will provide this information?",
      "choices": {
        "C": "Create a CloudWatch Logs Insights query that uses an aggregation function to count the number of logins for the username over the past 7 days. Run the query against the log group.",
        "B": "Create a CloudWatch Logs subscription on the log group. Use a filter pattern that matches the username. Publish a CloudWatch metric that sums the number of logins over the past 7 days.",
        "D": "Create a CloudWatch dashboard. Add a number widget that has a filter pattern that counts the number of logins for the username over the past 7 days directly from the log group.",
        "A": "Create a CloudWatch Logs metric filter on the log group. Use a filter pattern that matches the username. Publish a CloudWatch metric that sums the number of logins over the past 7 days."
      },
      "correct_answer": "C",
      "answer_ET": "C",
      "answers_community": [
        "C (96%)",
        "4%"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/133301-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-02-07 19:25:00",
      "unix_timestamp": 1707330300,
      "discussion_count": 10,
      "discussion": [
        {
          "content": "Selected Answer: C\nC - CloudWatch Logs Insights: This service allows you to run ad-hoc queries against your log data without creating additional infrastructure like metrics or subscriptions.\nAggregation function: Functions like count() can specifically calculate the number of occurrences based on specific criteria.\nFiltering by username and timeframe: The query can be tailored to include the specific username and filter for entries within the past 7 days.",
          "upvote_count": "5",
          "timestamp": "1707846600.0",
          "comments": [
            {
              "content": "A. Metric filter: It can count occurrences, but requires additional metric creation and subscription, introducing complexity.\nB. Subscription: Similar to metric filter, requires creating an additional subscription and pushing data elsewhere.\nD. Dashboard widget: Limited in its capabilities, might not allow complex filtering and aggregation needed for this specific analysis.\n \nTherefore, CloudWatch Logs Insights offers the most direct and flexible solution for analyzing the desired login data within the given timeframe and user criteria.",
              "poster": "Ramdi1",
              "timestamp": "1707846600.0",
              "comment_id": "1149431",
              "upvote_count": "1"
            }
          ],
          "comment_id": "1149430",
          "poster": "Ramdi1"
        },
        {
          "content": "Selected Answer: C\nanswer C",
          "upvote_count": "5",
          "comment_id": "1194597",
          "poster": "dkp",
          "timestamp": "1712970240.0"
        },
        {
          "timestamp": "1727325960.0",
          "poster": "heff_bezos",
          "upvote_count": "2",
          "content": "Selected Answer: C\nThere is no cloudwatch metric that logs user logins",
          "comment_id": "1289285"
        },
        {
          "comment_id": "1263533",
          "poster": "itzrahulyadav",
          "timestamp": "1723298280.0",
          "content": "D\nFor A you would have to setup the cloudwatch metric filters beforehand as you won't be able to analyze past logs without the setup",
          "upvote_count": "1"
        },
        {
          "upvote_count": "4",
          "comment_id": "1190816",
          "timestamp": "1712473260.0",
          "content": "Selected Answer: C\nThe solution that will provide the number of logins for a specific user from the past 7 days is Option C: Create a CloudWatch Logs Insights query that uses an aggregation function to count the number of logins for the username over the past 7 days. Run the query against the log group.",
          "poster": "mumumu"
        },
        {
          "comment_id": "1172390",
          "poster": "Shasha1",
          "content": "C \nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/CWL_AnalyzeLogData_AggregationQuery.html",
          "upvote_count": "2",
          "timestamp": "1710320040.0"
        },
        {
          "upvote_count": "5",
          "poster": "fdoxxx",
          "comment_id": "1162995",
          "timestamp": "1709238960.0",
          "content": "Selected Answer: C\nC is the most suitable solution for obtaining the required information"
        },
        {
          "timestamp": "1708189440.0",
          "content": "Selected Answer: C\n, C. Create a CloudWatch Logs Insights query that uses an aggregation function to count the number of logins for the username over the past 7 days. Run the query against the log group. is the best solution. This method directly addresses the need to analyze log data for a specific pattern (user logins) and aggregate the counts over a specified period, which is exactly what's needed for a root cause analysis of the event.",
          "poster": "kyuhuck",
          "comment_id": "1152711",
          "upvote_count": "5"
        },
        {
          "poster": "thanhnv142",
          "comment_id": "1148261",
          "timestamp": "1707751320.0",
          "content": "Selected Answer: A\nA is correct: CloudWatch Logs metric filter can filter out relevant logs and count\nB: irrelevant, this is for sharing logs to other sources\nC: This can accomplish the task. However, loudWatch Logs metric filter offers us the same function with less cost. Using CloudWatch Logs Insights query incur more costs. This feature is primarily used for data analysis\nD: irrelevant",
          "upvote_count": "1"
        },
        {
          "content": "C. It must be CloudWatch Logs Insights query",
          "poster": "Chelseajcole",
          "upvote_count": "1",
          "timestamp": "1707330300.0",
          "comment_id": "1143650"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:12:45.126Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "VAofZNc1Kc8tZ7RVEhUf",
      "question_number": 118,
      "page": 24,
      "question_text": "A company has an AWS CodeDeploy application. The application has a deployment group that uses a single tag group to identify instances for the deployment of Application. The single tag group configuration identifies instances that have Environment=Production and Name=ApplicationA tags for the deployment of ApplicationA.\n\nThe company launches an additional Amazon EC2 instance with Department=Marketing, Environment=Production, and Name=ApplicationB tags. On the next CodeDeploy deployment of Application, the additional instance has ApplicationA installed on it. A DevOps engineer needs to configure the existing deployment group to prevent ApplicationA from being installed on the additional instance.\n\nWhich solution will meet these requirements?",
      "choices": {
        "D": "Change the current single tag group to include only the Environment=Production tag. Add another single tag group that includes only the Department=Marketing tag.",
        "B": "Change the current single tag group to include the Department=Marketing, Environment=production, and Name=ApplicationA tags.",
        "A": "Change the current single tag group to include only the Environment=Production tag. Add another single tag group that includes only the Name=ApplicationA tag.",
        "C": "Add another single tag group that includes only the Department=Marketing tag. Keep the Environment=Production and Name=ApplicationA tags with the current single tag group."
      },
      "correct_answer": "A",
      "answer_ET": "A",
      "answers_community": [
        "A (93%)",
        "7%"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/133302-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-02-07 19:28:00",
      "unix_timestamp": 1707330480,
      "discussion_count": 9,
      "discussion": [
        {
          "timestamp": "1707751920.0",
          "content": "Selected Answer: A\nA is correct: All tags in one tag group are OR operator. Tags are in multiple tag groups are AND operator",
          "poster": "thanhnv142",
          "upvote_count": "9",
          "comment_id": "1148268"
        },
        {
          "comments": [
            {
              "poster": "Gomer",
              "timestamp": "1720466640.0",
              "comment_id": "1244533",
              "content": "A:(YES) Block ApplicationA deployments on EC2B because it would now have to have both Production AND ApplicationA tags (AND Operator between tag groups)\nB: (NO) Allow ApplicationA deployments on EC2B because it has the Production tag (OR Operator within tag group)\nC: (NO) Allow ApplicationA deployments on EC2B because it has the Production and Marketing tags (OR Operator within tag group, AND Operator between tag groups)\nD: (NO) Allow ApplicationA deployments on EC2B because it has the Production and Marketing tags (OR Operator within tag group, AND Operator between tag groups)",
              "upvote_count": "2"
            }
          ],
          "upvote_count": "6",
          "timestamp": "1720466640.0",
          "content": "Selected Answer: A\nI Found this question largely incomprehensible untill I understood the following concepts and mapped out everything\n- Single tag group: Any instance identified by at least one tag in the group is included in the deployment group. (OR Operator within individual tag groups)\n- Multiple tag groups: Instances that are identified by at least one tag in each of the tag groups are included. (AND Operator between multiple tag groups)\nhttps://docs.aws.amazon.com/codedeploy/latest/userguide/instances-tagging.html",
          "poster": "Gomer",
          "comment_id": "1244532"
        },
        {
          "upvote_count": "1",
          "poster": "GripZA",
          "content": "Selected Answer: A\nwhen you understand how multiple tag groups deploy, it makes it easier to map out. the golden rule is that an instance must match AT LEAST ONE of the tags in each of the tag groups. so when adding another single tag group with Name=ApplicationA tag, the new instance needs to have Environment=Production AND Name=ApplicationA tag to be included in the existing deployment group",
          "comment_id": "1562437",
          "timestamp": "1745235000.0"
        },
        {
          "timestamp": "1743339300.0",
          "poster": "Srikantha",
          "upvote_count": "1",
          "content": "Selected Answer: B\nOption B is the most effective solution, as it specifies that the deployment group should only target instances that have all three tags (Department=Marketing, Environment=Production, and Name=ApplicationA). This ensures that only instances specifically meant for ApplicationA are selected for the deployment, preventing unwanted instances (like those with Name=ApplicationB) from receiving the application.",
          "comment_id": "1412480"
        },
        {
          "comment_id": "1331169",
          "poster": "youonebe",
          "content": "Selected Answer: A\nThis is a very tricky question, even ChatGPT can't get it right, a good one!\n\nMake sure to go through this AWS doc first and fully understand it.\nhttps://docs.aws.amazon.com/codedeploy/latest/userguide/instances-tagging.html\n\nFirst thing first, why ApplicationA was installed on the new EC2 instance?\nReason is <single tag group, multiple tags> was used. The <Environment> and <Name> tags are both defined within this tag group. So as long as the EC2 instance has one tag matched, ApplicationA would be deployed. In this case, the new EC2 instance has <Environment=Production> tag, so ApplicationA got deployed.",
          "timestamp": "1735054920.0",
          "upvote_count": "2",
          "comments": [
            {
              "timestamp": "1735054920.0",
              "poster": "youonebe",
              "content": "Now we look at the choices one by one:\nA. [Correct]This updates the current tag group, add a new tag group. Which means BOTH tags need to match before ApplicationA gets deployed. For the new EC2 instance, because Name=ApplicationB, hence preventing the mistake from happening. \n\nB. [Wrong] This will be very similar to the original configuration, as long as one of the 3 tags matches, ApplicationA will be deployed. This does not solve the problem.\n\nC. [Wrong] The company still needs to install ApplicationA to existing EC2 instances, adding\"Department=Marketing will break the current workflow.\n\nD. [Wrong] The company still needs to install ApplicationA to existing EC2 instances, adding Department=Marketing will break the current workflow.",
              "comment_id": "1331170",
              "upvote_count": "1"
            }
          ]
        },
        {
          "timestamp": "1710581160.0",
          "content": "Selected Answer: A\nA: A single tag group can only contain 1 tag, so multiple Single tag groups will be needed.\nhttps://docs.aws.amazon.com/codedeploy/latest/userguide/instances-tagging.html",
          "comment_id": "1174879",
          "upvote_count": "5",
          "poster": "DanShone"
        },
        {
          "timestamp": "1709238540.0",
          "poster": "fdoxxx",
          "upvote_count": "1",
          "content": "Selected Answer: B\nOption B is the most appropriate solution for meeting the requirements:\n\nChanging the current single tag group to include the specific tags (Department=Marketing, Environment=Production, and Name=ApplicationA) ensures that only instances with these specific tags are identified for the deployment of ApplicationA.\nThe other options are not suitable for achieving the desired outcome",
          "comment_id": "1162989"
        },
        {
          "comments": [
            {
              "timestamp": "1707846960.0",
              "comment_id": "1149439",
              "poster": "Ramdi1",
              "content": "Reasons why other options won't work:\n \nB: Including Department=Marketing in the current tag group wouldn't change anything - all instances in Production with ApplicationA tag would be targeted.\nC: Adding a Department=Marketing tag group alone still leaves the original tag group targeting the Marketing instance due to the Name=ApplicationA tag.\nD: Removing Name=ApplicationA from the initial tag group removes a necessary criterion, potentially deploying ApplicationA to all Production instances regardless of their Name tag.\n \nTherefore, solution A provides the most precise and effective way to exclude the Marketing instance from ApplicationA deployment while maintaining the desired targeting for other instances.",
              "upvote_count": "1"
            }
          ],
          "timestamp": "1707846960.0",
          "poster": "Ramdi1",
          "upvote_count": "5",
          "comment_id": "1149438",
          "content": "Selected Answer: A\nA - Original configuration: The single tag group with Environment=Production and Name=ApplicationA tags targets any instance with both tags, resulting in ApplicationA being deployed on the new Marketing instance despite its different Name tag.\nSolution A:\nChanging the current tag group to \"Environment=Production\" ensures only instances in the Production environment are considered.\nAdding a separate tag group with \"Name=ApplicationA\" specifically targets instances meant for that application.\nThe Marketing instance with Department=Marketing tag doesn't match the new criteria and avoids unintended ApplicationA installation."
        },
        {
          "content": "A. need to applicationA tag",
          "upvote_count": "1",
          "timestamp": "1707330480.0",
          "comment_id": "1143653",
          "poster": "Chelseajcole"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:12:45.126Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "J4KWryX6Ed7mZotP8rmi",
      "question_number": 119,
      "page": 24,
      "question_text": "A company is launching an application that stores raw data in an Amazon S3 bucket. Three applications need to access the data to generate reports. The data must be redacted differently for each application before the applications can access the data.\n\nWhich solution will meet these requirements?",
      "choices": {
        "D": "Create an S3 access point that uses the raw data’s S3 bucket as the destination. For each application, create an S3 Object Lambda access point that uses the S3 access point. Configure the AWS Lambda function for each S3 Object Lambda access point to redact data when objects are retrieved. Configure each application to consume data from its own S3 Object Lambda access point",
        "C": "For each application, create an S3 access point that uses the raw data's S3 bucket as the destination. Create an AWS Lambda function that is invoked by object creation events in the raw data's S3 bucket. Program the Lambda function to redact data for each application. Store the data in each application's S3 access point. Configure each application to consume data from its own S3 access point.",
        "A": "Create an S3 bucket for each application. Configure S3 Same-Region Replication (SRR) from the raw data's S3 bucket to each application's S3 bucket. Configure each application to consume data from its own S3 bucket.",
        "B": "Create an Amazon Kinesis data stream. Create an AWS Lambda function that is invoked by object creation events in the raw data’s S3 bucket. Program the Lambda function to redact data for each application. Publish the data on the Kinesis data stream. Configure each application to consume data from the Kinesis data stream."
      },
      "correct_answer": "D",
      "answer_ET": "D",
      "answers_community": [
        "D (92%)",
        "8%"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/133303-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-02-07 19:31:00",
      "unix_timestamp": 1707330660,
      "discussion_count": 9,
      "discussion": [
        {
          "content": "Selected Answer: D\nkeywords: S3 Object Lambda access point to redact data",
          "poster": "jamesf",
          "timestamp": "1722774000.0",
          "comment_id": "1260657",
          "upvote_count": "3"
        },
        {
          "poster": "c3518fc",
          "upvote_count": "3",
          "content": "Selected Answer: D\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/tutorial-s3-object-lambda-uppercase.html",
          "comment_id": "1202688",
          "timestamp": "1714147920.0"
        },
        {
          "timestamp": "1712972820.0",
          "content": "Selected Answer: D\nanswer D",
          "comment_id": "1194604",
          "poster": "dkp",
          "upvote_count": "3"
        },
        {
          "content": "Selected Answer: D\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/tutorial-s3-object-lambda-uppercase.html",
          "poster": "anj_k",
          "comment_id": "1175591",
          "upvote_count": "4",
          "timestamp": "1710651840.0"
        },
        {
          "poster": "dzn",
          "timestamp": "1709814600.0",
          "comment_id": "1167976",
          "content": "Selected Answer: C\nS3 Object Lambda access point is not suitable for generating reports. Generally, creating a report requires an aggregate process, which is expensive. Since reports are expected to be viewed multiple times, it is inefficient to pay for Lambda processing time and CPU costs each time they are viewed. \nTo adopt D, CloudFront should be added to the front.\nhttps://aws.amazon.com/jp/blogs/aws/new-use-amazon-s3-object-lambda-with-amazon-cloudfront-to-tailor-content-for-end-users/",
          "upvote_count": "2"
        },
        {
          "comment_id": "1163001",
          "content": "Selected Answer: D\nD, using S3 Object Lambda access points, is the most appropriate solution for the requirements:\nS3 Object Lambda allows you to add your own code to S3 GET requests to modify and process data at the time of retrieval. In this scenario, you can create an S3 access point that uses the raw data's S3 bucket as the destination. For each application, create a separate S3 Object Lambda access point that uses the S3 access point as the source. Configure the AWS Lambda function for each S3 Object Lambda access point to redact data when objects are retrieved.\nThis solution ensures that each application can access the data with its own redaction rules, and the redaction is applied dynamically at the time of retrieval.",
          "poster": "fdoxxx",
          "timestamp": "1709239200.0",
          "upvote_count": "4"
        },
        {
          "comments": [
            {
              "comment_id": "1149447",
              "poster": "Ramdi1",
              "timestamp": "1707847380.0",
              "upvote_count": "1",
              "content": "A: Duplicates data for each application, increasing storage costs and maintenance overhead.\nB: While Kinesis Data Streams can handle large data volumes, it adds an extra layer of complexity and latency compared to direct S3 access with redaction.\nC: Still requires upfront redaction for each application's specific needs, potentially duplicating redacted data across S3 access points."
            }
          ],
          "upvote_count": "3",
          "content": "Selected Answer: D\nSingle source of truth: This solution maintains a single copy of the raw data in the original S3 bucket, avoiding data duplication and associated costs.\nFine-grained redaction: Each application has its own S3 Object Lambda access point, allowing independent Lambda functions to redact data according to specific needs. This ensures targeted redaction without creating multiple S3 buckets with potentially inefficient data copies.\nEfficient access: Applications access the data through their respective S3 Object Lambda access points, incurring the redaction processing only when data is retrieved, improving cost-effectiveness compared to upfront redaction approaches.",
          "poster": "Ramdi1",
          "timestamp": "1707847380.0",
          "comment_id": "1149446"
        },
        {
          "content": "Selected Answer: D\nD is correct: S3 access point is actually S3 lambda access point, which is option D\nA and B: too expensive\nC: is not correct",
          "poster": "thanhnv142",
          "comment_id": "1148273",
          "upvote_count": "3",
          "timestamp": "1707752160.0"
        },
        {
          "content": "D. S3 Bucker endpoint plus Lambda",
          "upvote_count": "2",
          "timestamp": "1707330660.0",
          "comment_id": "1143656",
          "poster": "Chelseajcole"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:12:45.126Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "4yvDT2iTxPfFCHdaOtEw",
      "question_number": 120,
      "page": 24,
      "question_text": "A company uses AWS Control Tower and AWS CloudFormation to manage its AWS accounts and to create AWS resources. The company requires all Amazon S3 buckets to be encrypted with AWS Key Management Service (AWS KMS) when the S3 buckets are created in a CloudFormation stack.\n\nWhich solution will meet this requirement?",
      "choices": {
        "A": "Use AWS Organizations. Attach an SCP that denies the s3:PutObject permission if the request does not include an x-amz-server-side-encryption header that requests server-side encryption with AWS KMS keys (SSE-KMS).",
        "B": "Use AWS Control Tower with a multi-account environment. Configure and enable proactive AWS Control Tower controls on all OUs with CloudFormation hooks.",
        "D": "Use AWS Organizations. Create an AWS Config organizational rule to check whether a KMS encryption key is enabled for all S3 buckets. Deploy the rule. Create and apply an SCP to prevent users from stopping and deleting AWS Config across all AWS accounts,",
        "C": "Use AWS Control Tower with a multi-account environment. Configure and enable detective AWS Control Tower controls on all OUs with CloudFormation hooks."
      },
      "correct_answer": "B",
      "answer_ET": "B",
      "answers_community": [
        "B (97%)",
        "3%"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/133304-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-02-07 19:32:00",
      "unix_timestamp": 1707330720,
      "discussion_count": 10,
      "discussion": [
        {
          "timestamp": "1707847860.0",
          "poster": "Ramdi1",
          "comment_id": "1149452",
          "upvote_count": "5",
          "content": "Selected Answer: B\nProactive controls: Proactive controls are preventative measures that block actions violating defined policies before they occur. This ensures encryption gets applied automatically during S3 bucket creation within CloudFormation stacks.\nCloudFormation hooks: Hooks enable Control Tower to intercept and enforce policies on CloudFormation stack operations, making it ideal for enforcing encryption during resource creation.\nMulti-account environment: Since the requirement applies across all accounts, Control Tower's multi-account capabilities ensure consistent policy enforcement throughout the organization.",
          "comments": [
            {
              "poster": "Ramdi1",
              "timestamp": "1707847920.0",
              "content": "The other options have limitations:\n \n \nA: While SCPs enforce policies, they react to actions instead of proactively preventing them. Additionally, denying s3:PutObject might be too restrictive as it can impact other legitimate operations.\nC: Detective controls monitor and report on existing resources, not preventing non-compliant creations.\nD: Config and SCPs combined address encryption checks and user limitations, but lack the direct integration with CloudFormation stacks crucial for enforcing during creation.",
              "comment_id": "1149453",
              "upvote_count": "1"
            }
          ]
        },
        {
          "poster": "thanhnv142",
          "upvote_count": "5",
          "comment_id": "1148279",
          "content": "Selected Answer: B\nB is correct: <AWS Control Tower> means we need to use the proactive control \nA: SCP s3:PutObject permission only deny action related to put object to S3, not when creating it\nB: Detective controls used only for monitoring\nC: correct\nD: This option can achive the goal of the question. However, it is way more complicated than B.",
          "timestamp": "1707752460.0"
        },
        {
          "poster": "jamesf",
          "content": "Selected Answer: B\nkeywords: proactive",
          "upvote_count": "3",
          "timestamp": "1722829680.0",
          "comment_id": "1260661"
        },
        {
          "timestamp": "1720475040.0",
          "comment_id": "1244572",
          "poster": "Gomer",
          "upvote_count": "4",
          "content": "Selected Answer: B\nHere's the Control Tower proactive control:\n\"[CT.S3.PR.10] Require an Amazon S3 bucket to have server-side encryption configured using an AWS KMS key\"\nhttps://docs.aws.amazon.com/controltower/latest/controlreference/s3-rules.html#ct-s3-pr-10-description"
        },
        {
          "timestamp": "1718427120.0",
          "content": "Selected Answer: B\nClearly answer is B , here is article that explains the same.\nhttps://aws.amazon.com/blogs/mt/how-aws-control-tower-users-can-proactively-verify-compliance-in-aws-cloudformation-stacks/\n\nAnswer D with config rule also fits the bill (if no control tower), but since we have Control tower managing the accounts already its better to make use of the features that Control tower leverages",
          "comment_id": "1230771",
          "upvote_count": "2",
          "poster": "Venki_dev"
        },
        {
          "content": "Selected Answer: B\nAnswer B",
          "timestamp": "1712973600.0",
          "comment_id": "1194609",
          "upvote_count": "3",
          "poster": "dkp"
        },
        {
          "timestamp": "1712480700.0",
          "poster": "fdoxxx",
          "upvote_count": "3",
          "content": "Selected Answer: B\nB is better than D...",
          "comment_id": "1190869"
        },
        {
          "timestamp": "1710871380.0",
          "poster": "ogerber",
          "content": "Selected Answer: B\nB, 100%",
          "comment_id": "1177591",
          "upvote_count": "3"
        },
        {
          "content": "Selected Answer: D\nD provides a solution that leverages AWS Organizations and AWS Config to enforce the requirement for AWS KMS encryption on all S3 buckets created through CloudFormation:\nAWS Config Organizational Rule: Create an AWS Config organizational rule to check whether a KMS encryption key is enabled for all S3 buckets. This rule helps ensure that the encryption requirement is enforced.\nOptions A, B, and C do not directly address the requirement for AWS KMS encryption on S3 buckets created through CloudFormation:\nOption A mentions using an SCP but focuses on denying s3:PutObject without the required encryption header. However, this approach doesn't ensure that the encryption is enforced through AWS KMS.\nOptions B and C mention using AWS Control Tower with proactive or detective controls, but they don't specifically address the encryption requirement for S3 buckets.",
          "upvote_count": "1",
          "comment_id": "1164636",
          "timestamp": "1709457540.0",
          "comments": [],
          "poster": "fdoxxx"
        },
        {
          "timestamp": "1707330720.0",
          "upvote_count": "1",
          "comment_id": "1143657",
          "comments": [
            {
              "timestamp": "1707330840.0",
              "comment_id": "1143659",
              "content": "Because of AWS Config",
              "upvote_count": "1",
              "poster": "Chelseajcole"
            }
          ],
          "poster": "Chelseajcole",
          "content": "Maybe D"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:12:45.126Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "fOfVLmuxa0m1wYm9dsQn",
      "question_number": 121,
      "page": 25,
      "question_text": "A DevOps engineer has developed an AWS Lambda function. The Lambda function starts an AWS CloudFormation drift detection operation on all supported resources for a specific CloudFormation stack. The Lambda function then exits its invocation.\n\nThe DevOps engineer has created an Amazon EventBridge scheduled rule that invokes the Lambda function every hour. An Amazon Simple Notification Service (Amazon SNS) topic already exists in the AWS account. The DevOps engineer has subscribed to the SNS topic to receive notifications.\n\nThe DevOps engineer needs to receive a notification as soon as possible when drift is detected in this specific stack configuration.\n\nWhich solution will meet these requirements?",
      "choices": {
        "A": "Configure the existing EventBridge rule to also target the SNS topic. Configure an SNS subscription filter policy to match the CloudFormation stack. Attach the subscription filter policy to the SNS topic.",
        "D": "Configure AWS Config in the account. Use the cloudformation-stack-drift-detection-check managed rule. Create a second EventBridge rule that reacts to a compliance change event for the CloudFormation stack. Configure the SNS topic as a target of the second EventBridge rule.",
        "C": "Configure Amazon GuardDuty in the account with drift detection for all CloudFormation stacks. Create a second EventBridge rule that reacts to the GuardDuty drift detection event finding for the specific CloudFormation stack. Configure the SNS topic as a target of the second EventBridge rule.",
        "B": "Create a second Lambda function to query the CloudFormation API for the drift detection results for the stack. Configure the second Lambda function to publish a message to the SNS topic if drift is detected. Adjust the existing EventBridge rule to also target the second Lambda function."
      },
      "correct_answer": "D",
      "answer_ET": "D",
      "answers_community": [
        "D (73%)",
        "B (25%)",
        "3%"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/133084-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-02-06 19:27:00",
      "unix_timestamp": 1707244020,
      "discussion_count": 15,
      "discussion": [
        {
          "poster": "Nano803",
          "content": "Selected Answer: D\nI recommend checking out this blog which utilizes AWS Config and discusses Edenbridge. Here is the link: https://aws.amazon.com/blogs/mt/implementing-an-alarm-to-automatically-detect-drift-in-aws-cloudformation-stacks/\"",
          "timestamp": "1708664520.0",
          "comments": [
            {
              "upvote_count": "1",
              "timestamp": "1720483800.0",
              "content": "Info gleaned from following the link(++):\n\ncloudformation-stack-drift-detection-check\nAWS Config rule that checks if the actual configuration of a AWS CloudFormation (AWS CloudFormation) stack differs, or has drifted, from the expected configuration. \n\nMaximumExecutionFrequency\nThe maximum frequency with which AWS Config runs evaluations for a rule.\n\nExample stack to detect and notify on drift:\n[...]\nMaximumExecutionFrequency:\n Description: \"The maximum frequency with which drift in CloudFormation stacks need to be evaluated (default - One_Hour)\"\n Type: \"String\"\n Default: \"One_Hour\"\n AllowedValues: [\"One_Hour\",\"Three_Hours\",\"Six_Hours\",\"Twelve_Hours\",\"TwentyFour_Hours\"]\n[...]",
              "comment_id": "1244615",
              "poster": "Gomer"
            }
          ],
          "upvote_count": "7",
          "comment_id": "1156919"
        },
        {
          "comment_id": "1153772",
          "content": "Selected Answer: D\nGiven the options and the requirement for immediate notification upon drift detection, Option D is the most appropriate solution. It leverages AWS Config to continuously monitor and evaluate the configurations of AWS resources, including CloudFormation stacks. When AWS Config detects a drift from the desired configuration, it can trigger an EventBridge rule, which in turn can notify the interested parties via the SNS topic. This approach does not require additional custom logic to check for drift results, as AWS Config handles the evaluation and notification process based on configuration changes.",
          "upvote_count": "5",
          "poster": "kyuhuck",
          "timestamp": "1708330200.0"
        },
        {
          "timestamp": "1747403400.0",
          "comment_id": "1569402",
          "poster": "nickp84",
          "content": "Selected Answer: B\nD: AWS Config’s cloudformation-stack-drift-detection-check is a managed rule, but it runs on a periodic basis and may not provide immediate detection or notification. It also may not be as responsive as a custom Lambda-based solution.",
          "upvote_count": "1"
        },
        {
          "comment_id": "1261064",
          "comments": [
            {
              "timestamp": "1724996580.0",
              "content": "This approach introduces additional complexity by adding another Lambda function to query and check for drift manually. Who is going to trigger this Lambda function? Even if you do it on a interval, it defeats the purpose of getting notified immediately. \n\nAWS Config provides a more straightforward and managed way to detect and notify on drift with a managed rule, cloudformation-stack-drift-detection-check.\nhttps://docs.aws.amazon.com/config/latest/developerguide/cloudformation-stack-drift-detection-check.html",
              "upvote_count": "1",
              "comment_id": "1274937",
              "poster": "chinchin97"
            }
          ],
          "poster": "iulian0585",
          "timestamp": "1722860820.0",
          "content": "Selected Answer: B\nThe solution that will meet the requirements of receiving a notification as soon as possible when drift is detected in the specific CloudFormation stack configuration is: \n\nB. Create a second Lambda function to query the CloudFormation API for the drift detection results for the stack. Configure the second Lambda function to publish a message to the SNS topic if drift is detected. Adjust the existing EventBridge rule to also target the second Lambda function.\n\nOption D (Using AWS Config) would introduce additional complexity and potential delays, as AWS Config periodically evaluates resource configurations and may not provide immediate notifications upon drift detection.\n\nBy creating a separate Lambda function dedicated to monitoring drift detection results and publishing notifications to the existing SNS topic, you can ensure timely and reliable notifications while maintaining a modular and scalable architecture.",
          "upvote_count": "3"
        },
        {
          "poster": "dkp",
          "comment_id": "1194621",
          "timestamp": "1712976720.0",
          "upvote_count": "4",
          "content": "Selected Answer: D\nanswer D\nAWS Config Integration: AWS Config is specifically designed to monitor and detect configuration changes and drifts in AWS resources, including CloudFormation stacks. Using AWS Config's built-in cloudformation-stack-drift-detection-check managed rule ensures comprehensive and reliable drift detection for CloudFormation stacks.\n\nEvent-Driven Architecture: Creating an EventBridge rule that reacts to a compliance change event for the CloudFormation stack allows you to trigger an alert as soon as drift is detected. This event-driven approach ensures timely detection and alerting for CloudFormation stack drift.\n\nSNS Notification: By configuring the SNS topic as a target of the EventBridge rule, you can easily send notifications/alerts to various endpoints, including email, SMS, or other AWS services, ensuring immediate alerting when drift is detected."
        },
        {
          "content": "Selected Answer: D\nD, Use the cloudformation-stack-drift-detection-check managed rule\nB uses scheduled rule will not notify as soon as possible as it runs hourly",
          "timestamp": "1711799100.0",
          "poster": "WhyIronMan",
          "upvote_count": "4",
          "comment_id": "1186070"
        },
        {
          "timestamp": "1710580740.0",
          "comment_id": "1174874",
          "poster": "DanShone",
          "content": "Selected Answer: D\nD woudl be suitable - https://docs.aws.amazon.com/config/latest/developerguide/cloudformation-stack-drift-detection-check.html\nB would not work as it would still only be triggered once per hour as is using the same event bridge rule",
          "upvote_count": "5"
        },
        {
          "comment_id": "1173262",
          "content": "D\nrefer this: https://docs.aws.amazon.com/config/latest/developerguide/cloudformation-stack-drift-detection-check.html",
          "timestamp": "1710407940.0",
          "upvote_count": "2",
          "poster": "Shasha1"
        },
        {
          "upvote_count": "1",
          "comment_id": "1167069",
          "poster": "dzn",
          "comments": [
            {
              "poster": "sejar",
              "upvote_count": "1",
              "comment_id": "1169550",
              "content": "Any reference to 1 hour limit, ?",
              "timestamp": "1709994360.0"
            }
          ],
          "timestamp": "1709719620.0",
          "content": "Selected Answer: B\nThe minimum interval for the `cloudformation-stack-drift-detection-check` managed rule in AWS config is 1 hour and does not meet the following requirements.\n`as soon as possible when drift is detected`"
        },
        {
          "comment_id": "1164624",
          "upvote_count": "2",
          "content": "Selected Answer: B\nB is a suitable solution for meeting the requirements:\nThis solution provides a more direct and responsive approach.\nThe other options involve additional services like GuardDuty (Option C), which is not designed for CloudFormation drift detection, or AWS Config with managed rules (Option D), which may introduce unnecessary complexity for this specific scenario. Option A doesn't provide a straightforward way to react to drift detection events.",
          "timestamp": "1709457000.0",
          "poster": "fdoxxx"
        },
        {
          "comments": [
            {
              "timestamp": "1707848340.0",
              "poster": "Ramdi1",
              "upvote_count": "1",
              "content": "B: Introduces an additional Lambda function and complexity, and requires polling for drift status, possibly delaying notification compared to real-time detection.\nC: While GuardDuty offers centralized drift detection, setting up a separate EventBridge rule and relying on event findings adds extra steps and might not be as timely as direct notification from the Lambda function.\nD: Although Config's cloudformation-stack-drift-detection-check rule identifies drift, triggering an EventBridge rule on compliance changes adds another layer of complexity and might not offer real-time notification like option A.",
              "comment_id": "1149458"
            }
          ],
          "timestamp": "1707848280.0",
          "poster": "Ramdi1",
          "comment_id": "1149457",
          "upvote_count": "1",
          "content": "Selected Answer: A\nLeverages existing infrastructure: This approach utilizes the existing EventBridge rule and SNS topic, avoiding the need for additional resources or complex configurations.\nImmediate notification: Since the EventBridge rule already triggers the Lambda function every hour, adding the SNS topic as a target ensures drift detection results are published directly to the topic for immediate notification.\nFiltering for specific stack: Implementing an SNS subscription filter policy ensures you only receive notifications for the specific CloudFormation stack you're interested in, avoiding irrelevant noise."
        },
        {
          "comment_id": "1148286",
          "poster": "thanhnv142",
          "upvote_count": "1",
          "timestamp": "1707752700.0",
          "content": "Selected Answer: B\nB: is correct\nA: SNS topic would be trigger consistenly by the existing evenbridge, so this is incorrect\nC: Guarduty is for threat detection, not this\nD: irrelevant, the question requires using ACF drif detection, not AWS config for drift detection"
        },
        {
          "comment_id": "1143674",
          "content": "D. AWS Config",
          "poster": "Chelseajcole",
          "timestamp": "1707331320.0",
          "upvote_count": "2"
        },
        {
          "content": "Selected Answer: B\nB is the most appropriate solution for this scenario.\n\nA is incorrect because although it involves configuring the existing EventBridge rule to target the SNS topic and using an SNS subscription filter policy, it does not involve querying the CloudFormation API for drift detection results.\n\nC is incorrect because it involves using Amazon GuardDuty, which is not specifically designed for CloudFormation drift detection.\n\nD is incorrect because although it involves using AWS Config and EventBridge to react to compliance change events, it does not directly address CloudFormation drift detection.\n\nWith CloudWatch Events (now a part of EventBridge) https://aws.amazon.com/fr/blogs/mt/implement-automatic-drift-remediation-for-aws-cloudformation-using-amazon-cloudwatch-and-aws-lambda/",
          "comment_id": "1143503",
          "timestamp": "1707322680.0",
          "poster": "Arnaud92",
          "upvote_count": "2"
        },
        {
          "comment_id": "1142464",
          "poster": "Spavanko",
          "content": "Selected Answer: D\nB is wrong, you can not query the CloudFormation API",
          "timestamp": "1707244020.0",
          "upvote_count": "4"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:12:55.739Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "qBKf6LZK4e5uxjIaJOIr",
      "question_number": 122,
      "page": 25,
      "question_text": "A company has deployed a complex container-based workload on AWS. The workload uses Amazon Managed Service for Prometheus for monitoring. The workload runs in an Amazon\nElastic Kubernetes Service (Amazon EKS) cluster in an AWS account.\n\nThe company’s DevOps team wants to receive workload alerts by using the company’s Amazon Simple Notification Service (Amazon SNS) topic. The SNS topic is in the same AWS account as the EKS cluster.\n\nWhich combination of steps will meet these requirements? (Choose three.)",
      "choices": {
        "B": "Create an alerting rule that checks the availability of each of the workload’s containers.",
        "D": "Modify the access policy of the SNS topic. Grant the aps.amazonaws.com service principal the sns:Publish permission and the sns:GetTopicAttributes permission for the SNS topic.",
        "A": "Use the Amazon Managed Service for Prometheus remote write URL to send alerts to the SNS topic",
        "C": "Create an alert manager configuration for the SNS topic.",
        "E": "Modify the IAM role that Amazon Managed Service for Prometheus uses. Grant the role the sns:Publish permission and the sns:GetTopicAttributes permission for the SNS topic.",
        "F": "Create an OpenID Connect (OIDC) provider for the EKS cluster. Create a cluster service account. Grant the account the sns:Publish permission and the sns:GetTopicAttributes permission by using an IAM role."
      },
      "correct_answer": "BCD",
      "answer_ET": "BCD",
      "answers_community": [
        "BCD (72%)",
        "BCE (18%)",
        "4%"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/133085-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-02-06 19:31:00",
      "unix_timestamp": 1707244260,
      "discussion_count": 31,
      "discussion": [
        {
          "poster": "xdkonorek2",
          "timestamp": "1719068940.0",
          "content": "Selected Answer: BCD\nhttps://docs.aws.amazon.com/prometheus/latest/userguide/Troubleshooting-alerting-no-policy.html",
          "upvote_count": "5",
          "comment_id": "1235458"
        },
        {
          "poster": "Gomer",
          "content": "Selected Answer: BCD\nB:(YES) Steps towards \"configuring rules and the alert manager in Amazon Managed Service for Prometheus via the AWS management console.\"\n\"define an alerting rule that causes the Alert Manager to send a notification if a certain condition (defined in expr) holds true for a specified time period (for).\"\ncat << EOF > rules.yaml\ngroups:\n[...]\n rules:\n - alert: metric:alerting_rule\n expr: rate(adot_test_counter0[5m]) > 0.014\n for: 5m\nEOF\n\nC:(YES) Add \"SNS receiver to\" \"alert manager configuration\" using ARN of \"SNS topic\"(Q208.5)\n\nD:(YES) \"Give Amazon Managed Service for Prometheus permission to send messages to\" SNS\n\"Choose Access policy and add the following policy statement to the existing policy.\"\n[...]\n \"Principal\": {\n \"Service\": \"aps.amazonaws.com\"\n },\n \"Action\": [\n \"sns:Publish\",\n \"sns:GetTopicAttributes\"",
          "comment_id": "1244654",
          "upvote_count": "5",
          "timestamp": "1720493460.0",
          "comments": [
            {
              "timestamp": "1720493520.0",
              "upvote_count": "1",
              "poster": "Gomer",
              "comment_id": "1244655",
              "content": "https://docs.aws.amazon.com/prometheus/latest/userguide/AMP-alertmanager-receiver-config.html\nhttps://docs.aws.amazon.com/prometheus/latest/userguide/AMP-alertmanager-receiver-AMPpermission.html\nhttps://aws.amazon.com/blogs/mt/amazon-managed-service-for-prometheus-is-now-generally-available/"
            }
          ]
        },
        {
          "upvote_count": "2",
          "content": "Selected Answer: BCD\nhttps://docs.aws.amazon.com/prometheus/latest/userguide/AMP-alertmanager-receiver.html",
          "comment_id": "1324607",
          "timestamp": "1733839980.0",
          "poster": "teo2157"
        },
        {
          "comment_id": "1271658",
          "upvote_count": "1",
          "content": "I think BCD is true",
          "timestamp": "1724502000.0",
          "poster": "seetpt"
        },
        {
          "poster": "jamesf",
          "upvote_count": "4",
          "content": "Selected Answer: BCD\nBCD\nFor D as You must give Amazon Managed Service for Prometheus permission to send messages to your Amazon SNS topic. The following policy statement will give that permission. ...\n https://docs.aws.amazon.com/prometheus/latest/userguide/AMP-alertmanager-receiver-AMPpermission.html",
          "timestamp": "1722330960.0",
          "comment_id": "1258057"
        },
        {
          "upvote_count": "4",
          "comment_id": "1245848",
          "timestamp": "1720666740.0",
          "content": "Selected Answer: BCD\nAgree with BCD",
          "poster": "trungtd"
        },
        {
          "poster": "KaranNishad",
          "content": "Selected Answer: BCD\nBCD is answer",
          "upvote_count": "4",
          "timestamp": "1719672720.0",
          "comment_id": "1239332"
        },
        {
          "upvote_count": "5",
          "poster": "that1guy",
          "comments": [
            {
              "timestamp": "1716289500.0",
              "content": "https://docs.aws.amazon.com/prometheus/latest/userguide/AMP-alertmanager-receiver-AMPpermission.html Agree with BCD",
              "comment_id": "1214881",
              "poster": "vn_thanhtung",
              "upvote_count": "2"
            }
          ],
          "comment_id": "1211016",
          "content": "Selected Answer: BCD\nB, C, D, you need to grant the AMP Workspace access to the SQS queue via the SQS resource policy.",
          "timestamp": "1715623920.0"
        },
        {
          "upvote_count": "1",
          "poster": "seetpt",
          "content": "Selected Answer: BCE\nBCE for me",
          "comment_id": "1205546",
          "timestamp": "1714654560.0"
        },
        {
          "timestamp": "1714151100.0",
          "upvote_count": "2",
          "comment_id": "1202705",
          "comments": [
            {
              "comment_id": "1211014",
              "timestamp": "1715623800.0",
              "poster": "that1guy",
              "upvote_count": "1",
              "content": "This is incorrect. This is for users/tools to manage alerts, not to publish to SQS from AMP."
            }
          ],
          "content": "Selected Answer: BCE\nAmazon Managed Service for Prometheus uses an IAM role to assume permissions, not a service principal. https://docs.aws.amazon.com/prometheus/latest/userguide/AMP-alertmanager-IAM-permissions.html",
          "poster": "c3518fc"
        },
        {
          "content": "Selected Answer: BCD\nill go with bcd",
          "comment_id": "1194629",
          "poster": "dkp",
          "upvote_count": "5",
          "timestamp": "1712978880.0"
        },
        {
          "poster": "WhyIronMan",
          "timestamp": "1711798920.0",
          "upvote_count": "5",
          "comment_id": "1186068",
          "content": "Selected Answer: BCD\nB,C,D.\nThere is no way to exclude D, as it is really necessary as per all AWS documentations.\nYou can be in doubt of all the others, but not D"
        },
        {
          "content": "Selected Answer: BCE\nI'll go with BC & E. Im convinced that the Prometheus service role will need permissions added to push messages to SNS topic",
          "timestamp": "1711260180.0",
          "comment_id": "1181340",
          "upvote_count": "4",
          "poster": "CloudHandsOn"
        },
        {
          "timestamp": "1710872700.0",
          "poster": "ogerber",
          "upvote_count": "2",
          "comment_id": "1177614",
          "content": "Selected Answer: BCE\nhttps://docs.aws.amazon.com/prometheus/latest/userguide/AMP-alert-manager.html"
        },
        {
          "upvote_count": "3",
          "comment_id": "1176014",
          "content": "Selected Answer: BCD\nB: https://docs.aws.amazon.com/prometheus/latest/userguide/AMP-APIReference-CreateAlertManagerAlerts.html\nC: https://docs.aws.amazon.com/prometheus/latest/userguide/AMP-alertmanager-receiver-AMPpermission.html\nD: https://docs.aws.amazon.com/prometheus/latest/userguide/AMP-alertmanager-receiver-AMPpermission.html",
          "poster": "L1_",
          "timestamp": "1710700080.0"
        },
        {
          "comment_id": "1174869",
          "content": "Selected Answer: BCD\nBCD Makes the most sense from the docs \nhttps://docs.aws.amazon.com/prometheus/latest/userguide/AMP-alertmanager-receiver-AMPpermission.html",
          "timestamp": "1710579600.0",
          "upvote_count": "4",
          "poster": "DanShone"
        },
        {
          "timestamp": "1710409320.0",
          "content": "BCD\nAccording to this reference D is for sure : https://docs.aws.amazon.com/prometheus/latest/userguide/AMP-alertmanager-receiver-AMPpermission.html",
          "poster": "Shasha1",
          "upvote_count": "1",
          "comment_id": "1173291"
        },
        {
          "comment_id": "1164671",
          "timestamp": "1709459280.0",
          "content": "Selected Answer: ADE\nUse the Amazon Managed Service for Prometheus remote write URL: You can configure Prometheus to send alerts to an external service, such as an SNS topic, by using the remote write URL. This URL can be obtained from Amazon Managed Service for Prometheus.\nModify the access policy of the SNS topic: Grant the aps.amazonaws.com service principal the necessary permissions (sns:Publish and sns:GetTopicAttributes) to publish to the SNS topic and get its attributes. This allows Amazon Managed Service for Prometheus to interact with the SNS topic.\nModify the IAM role that Amazon Managed Service for Prometheus uses: Grant the IAM role associated with Amazon Managed Service for Prometheus the necessary permissions (sns:Publish and sns:GetTopicAttributes) to interact with the SNS topic.\nOptions B, C, and F are not directly related to integrating Amazon Managed Service for Prometheus with Amazon SNS",
          "upvote_count": "1",
          "poster": "fdoxxx"
        },
        {
          "content": "Selected Answer: BCF\nB. Create an alerting rule that checks the availability of each of the workload’s containers.\nC/F. Given the options and AWS services' functionalities, a direct integration approach as described is not straightforward. However, understanding the need for configuration and permission settings, a combination of creating alerting rules (B) and configuring permissions correctly either through IAM roles or service accounts (F) seems most relevant. Implementing a custom solution or using third-party tools that can act as an intermediary might be necessary.\nF. Create an OpenID Connect (OIDC) provider for the EKS cluster and configure permissions appropriately, which is a fundamental step in setting up Kubernetes services to interact with AWS services securely.",
          "timestamp": "1709280180.0",
          "upvote_count": "1",
          "comment_id": "1163318",
          "poster": "Jonalb"
        },
        {
          "timestamp": "1709280120.0",
          "content": "Answer:BCF",
          "poster": "Jonalb",
          "comment_id": "1163317",
          "upvote_count": "1"
        },
        {
          "timestamp": "1708987680.0",
          "upvote_count": "1",
          "content": "Selected Answer: BCD\nAnswer: BCD – Create Alert manager, create alert rule, modify access policy \nhttps://docs.aws.amazon.com/prometheus/latest/userguide/AMP-alertmanager-receiver-AMPpermission.html",
          "comment_id": "1160129",
          "poster": "Diego1414"
        },
        {
          "upvote_count": "2",
          "poster": "testhard",
          "comment_id": "1157336",
          "content": "Selected Answer: BCE\nIs related to Prometheus permission, not EKS, you need to give access to AMP send messages to SNS",
          "timestamp": "1708709400.0"
        },
        {
          "content": "Selected Answer: BCF\nAmazon Managed Service for Prometheus primarily deals with metrics collection and querying. Alerting is managed by the Alertmanager, which does not use the Prometheus service’s IAM role for sending alerts to SNS. The configuration for notifications is managed within Alertmanager itself and may require custom integration for SNS.\nTherefore, the correct steps are B, C, and F to set up alerting from Amazon Managed Service for Prometheus to an Amazon SNS topic, with proper monitoring, alert management configuration, and permissions setup for integration.",
          "timestamp": "1708268700.0",
          "comment_id": "1153358",
          "upvote_count": "1",
          "poster": "kyuhuck"
        },
        {
          "comment_id": "1152719",
          "poster": "kyuhuck",
          "upvote_count": "1",
          "timestamp": "1708190460.0",
          "content": "Selected Answer: BCF\nIn summary, the best approach involves defining alerting rules within Prometheus (B), potentially customizing Alertmanager or using an intermediary service for integration with SNS (C, but with the note that direct SNS configuration requires custom setup), and securely configuring permissions for Kubernetes workloads to interact with AWS services (F), acknowledging that some of the steps might involve additional customization or tools beyond the default capabilities of the mentioned AWS services."
        },
        {
          "content": "Selected Answer: CDE\nD & E: These steps are essential for enabling communication between Prometheus and SNS. Modifying the SNS topic policy allows the specific service principal and IAM role to publish and get attributes, respectively.\nC: An alert manager configuration is crucial for routing Prometheus alerts to the desired destination (SNS topic in this case). It defines how and where to send alerts based on specific criteria.",
          "timestamp": "1707852720.0",
          "comments": [
            {
              "upvote_count": "1",
              "content": "A: While Prometheus can utilize remote write URLs, it's not recommended for directly sending alerts to SNS due to limitations in message formatting and routing logic.\nB: Setting up an availability check rule is valuable for monitoring containers, but it's not directly related to sending alerts to SNS. It helps identify issues but doesn't trigger notifications.\nF: Using OIDC and service accounts can be part of a more complex setup for accessing resources securely, but it's not strictly necessary for the given scenario where both Prometheus and SNS reside in the same account.",
              "comment_id": "1149521",
              "poster": "Ramdi1",
              "timestamp": "1707852780.0"
            }
          ],
          "comment_id": "1149520",
          "upvote_count": "1",
          "poster": "Ramdi1"
        },
        {
          "timestamp": "1707753660.0",
          "comment_id": "1148311",
          "content": "Selected Answer: BCE\nBCE are correct:\nA: There is nothing called Amazon Managed Service for Prometheus remote write URL\nB: ok\nC: ok. Both B and C mention creating alert\nD: irrelevant\nE: Granting permissions for Prometheus\nF: irrelevant",
          "poster": "thanhnv142",
          "upvote_count": "2"
        },
        {
          "poster": "vortegon",
          "timestamp": "1707625980.0",
          "upvote_count": "4",
          "content": "Selected Answer: BCD\nOptions B (Create an alerting rule), C (Create an alert manager configuration for the SNS topic), and D (Modify the access policy of the SNS topic) are the correct choices. These steps ensure that alerting rules are in place, Alertmanager is configured to route alerts to SNS, and the necessary permissions are set for AMP to publish alerts to the SNS topic.",
          "comment_id": "1146983"
        },
        {
          "poster": "LeoSantos121212121212121",
          "comment_id": "1146357",
          "content": "I go with BDE.",
          "upvote_count": "1",
          "timestamp": "1707575580.0"
        },
        {
          "upvote_count": "1",
          "content": "BDE? Why need OIDC?",
          "poster": "Chelseajcole",
          "timestamp": "1707331620.0",
          "comment_id": "1143679"
        },
        {
          "poster": "Arnaud92",
          "comment_id": "1143511",
          "content": "Selected Answer: ADE\nOption A is necessary to configure Amazon Managed Service for Prometheus to send alerts to the specified destination, which in this case is the SNS topic.\nOption D is necessary to ensure that the SNS topic allows the Amazon Managed Service for Prometheus service principal to publish messages to the topic.\nOption E is necessary to grant the required permissions to the IAM role used by Amazon Managed Service for Prometheus to interact with the SNS topic.\nOptions B, C, and F are not directly related to integrating Amazon Managed Service for Prometheus alerts with Amazon SNS for the given scenario.",
          "timestamp": "1707323340.0",
          "upvote_count": "1"
        },
        {
          "timestamp": "1707244260.0",
          "poster": "Spavanko",
          "content": "Selected Answer: BDF\nI think different",
          "comment_id": "1142469",
          "upvote_count": "1"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:12:55.739Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "oMwm4RofK3J7yfB40sOm",
      "question_number": 123,
      "page": 25,
      "question_text": "A company's organization in AWS Organizations has a single OU. The company runs Amazon EC2 instances in the OU accounts. The company needs to limit the use of each EC2 instance’s credentials to the specific EC2 instance that the credential is assigned to. A DevOps engineer must configure security for the EC2 instances.\n\nWhich solution will meet these requirements?",
      "choices": {
        "D": "Create an SCP that checks whether the values of the aws:EC2InstanceSourceVPC and aws:VpcSourceIp condition keys are the same. Deny access if the values are not the same. In the same SCP check, check whether the values of the aws:EC2InstanceSourcePrivateIPv4 and aws:SourceVpc condition keys are the same. Deny access if the values are not the same. Apply the SCP to each account in the organization.",
        "B": "Create an SCP that checks whether the values of the aws:EC2InstanceSourceVPC and aws:SourceVpc condition keys are the same. Deny access if the values are not the same. In the same SCP check, check whether the values of the aws:EC2InstanceSourcePrivateIPv4 and aws:VpcSourceIp condition keys are the same. Deny access if the values are not the same. Apply the SCP to the OU.",
        "A": "Create an SCP that specifies the VPC CIDR block. Configure the SCP to check whether the value of the aws:VpcSourcelp condition key is in the specified block. In the same SCP check, check whether the values of the aws:EC2InstanceSourcePrivatelPv4 and aws:SourceVpc condition keys are the same. Deny access if either condition is false. Apply the SCP to the OU.",
        "C": "Create an SCP that includes a list of acceptable VPC values and checks whether the value of the aws:SourceVpc condition key is in the list. In the same SCP check, define a list of acceptable IP address values and check whether the value of the aws:VpcSourceIp condition key is in the list. Deny access if either condition is false. Apply the SCP to each account in the organization."
      },
      "correct_answer": "B",
      "answer_ET": "B",
      "answers_community": [
        "B (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/133293-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-02-07 17:39:00",
      "unix_timestamp": 1707323940,
      "discussion_count": 9,
      "discussion": [
        {
          "comment_id": "1195111",
          "content": "Selected Answer: B\nB obviously : https://aws.amazon.com/blogs/security/how-to-use-policies-to-restrict-where-ec2-instance-credentials-can-be-used-from/",
          "poster": "devakram",
          "upvote_count": "5",
          "timestamp": "1713036780.0"
        },
        {
          "poster": "RajAWSDevOps007",
          "content": "Answer is B here.\n\nHowever, pls note SCPs can be applied directly to member accounts as well- \nhttps://docs.aws.amazon.com › orgs_manage_policies_scps",
          "upvote_count": "1",
          "comment_id": "1327289",
          "timestamp": "1734346320.0"
        },
        {
          "upvote_count": "1",
          "comment_id": "1240185",
          "timestamp": "1719842760.0",
          "content": "NOT C,D:\n\"Apply the SCP to each account in the organization\" - SCPs apply to OUs, not accounts",
          "poster": "6ef9a08"
        },
        {
          "comment_id": "1164680",
          "content": "Selected Answer: B\nB is the most appropriate solution:\nOption A introduces unnecessary complexity with multiple conditions and may not provide the intended restriction.\nOption C suggests creating an SCP with lists of acceptable values, but it might be challenging to maintain and is less straightforward.\nOption D has the same issues as option A, introducing complexity with multiple conditions.",
          "timestamp": "1709459700.0",
          "poster": "fdoxxx",
          "upvote_count": "4"
        },
        {
          "upvote_count": "4",
          "content": "Selected Answer: B\nAnswer: B - aws:EC2InstanceSourceVPC = aws:SourceVpc and aws:EC2InstanceSourcePrivateIPv4 = aws:VpcSourceIp\nhttps://aws.amazon.com/blogs/security/how-to-use-policies-to-restrict-where-ec2-instance-credentials-can-be-used-from/",
          "poster": "Diego1414",
          "comment_id": "1160132",
          "timestamp": "1708988220.0"
        },
        {
          "poster": "thanhnv142",
          "upvote_count": "4",
          "timestamp": "1707753420.0",
          "comments": [
            {
              "poster": "thanhnv142",
              "upvote_count": "2",
              "timestamp": "1707753420.0",
              "comment_id": "1148305",
              "content": "Finally, I 've made it to the last one"
            }
          ],
          "comment_id": "1148303",
          "content": "Selected Answer: B\nB is correct: aws:EC2InstanceSourceVPC and aws:SourceVpc must be the same. Additionally, aws:EC2InstanceSourcePrivateIPv4 and aws:VpcSourceIp must be the same\nA: irrelevant\nC: <define a list of acceptable IP address values> is not correct\nD: <aws:EC2InstanceSourceVPC and aws:VpcSourceIp> is incorrect"
        },
        {
          "upvote_count": "2",
          "poster": "vortegon",
          "timestamp": "1707626400.0",
          "comment_id": "1146984",
          "content": "Selected Answer: B\nhttps://aws.amazon.com/fr/blogs/security/how-to-use-policies-to-restrict-where-ec2-instance-credentials-can-be-used-from/"
        },
        {
          "comment_id": "1143682",
          "content": "B. checks whether the values of the aws:EC2InstanceSourceVPC and aws:SourceVpc condition keys are the same and Apply the SCP to the OU.",
          "upvote_count": "1",
          "timestamp": "1707331740.0",
          "poster": "Chelseajcole"
        },
        {
          "poster": "Arnaud92",
          "upvote_count": "1",
          "comment_id": "1143525",
          "content": "Source: https://aws.amazon.com/fr/blogs/security/how-to-use-policies-to-restrict-where-ec2-instance-credentials-can-be-used-from/",
          "timestamp": "1707323940.0"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:12:55.739Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "SNPdjWrGX5m7YQ4NaHWQ",
      "question_number": 124,
      "page": 25,
      "question_text": "A company has containerized all of its in-house quality control applications. The company is running Jenkins on Amazon EC2 instances, which require patching and upgrading. The compliance officer has requested a DevOps engineer begin encrypting build artifacts since they contain company intellectual property.\nWhat should the DevOps engineer do to accomplish this in the MOST maintainable manner?",
      "choices": {
        "C": "Leverage AWS CodePipeline with a build action and encrypt the artifacts using AWS Secrets Manager.",
        "B": "Deploy Jenkins to an Amazon ECS cluster and copy build artifacts to an Amazon S3 bucket with default encryption enabled.",
        "A": "Automate patching and upgrading using AWS Systems Manager on EC2 instances and encrypt Amazon EBS volumes by default.",
        "D": "Use AWS CodeBuild with artifact encryption to replace the Jenkins instance running on EC2 instances."
      },
      "correct_answer": "D",
      "answer_ET": "D",
      "answers_community": [
        "D (82%)",
        "B (18%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/105524-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-04-07 17:07:00",
      "unix_timestamp": 1680880020,
      "discussion_count": 31,
      "discussion": [
        {
          "content": "Selected Answer: D\nThe question wants you to know which solution is the easiest to maintain. It's important not to get thrown by information provided about their current environment. Only the question they ask matters. The question asks which solution is the easiest to \"maintain\". The question did not ask whether it would be easy to transition from one solution to another or ask you to leverage containers like other parts of their environment.\n\nAs a managed service, AWS CodeBuild does not require patching and upgrading. AWS CodeBuild, using Amazon S3, provides automatic artifact encryption. So this solution is the easiest to maintain of all the solutions listed.\n\nhttps://docs.aws.amazon.com/codebuild/latest/userguide/welcome.html\nhttps://docs.aws.amazon.com/codebuild/latest/userguide/security-encryption.html",
          "comment_id": "932973",
          "timestamp": "1687647120.0",
          "upvote_count": "26",
          "poster": "sb333"
        },
        {
          "poster": "madperro",
          "content": "Selected Answer: D\nWhile B will require less changes to the build process I assume AWS is promoting managed services here and expects D answer.",
          "timestamp": "1686245640.0",
          "upvote_count": "12",
          "comment_id": "918525"
        },
        {
          "poster": "mungdol",
          "upvote_count": "1",
          "content": "Selected Answer: B\nwhy D???????????",
          "timestamp": "1754291160.0",
          "comment_id": "1594601"
        },
        {
          "timestamp": "1744766280.0",
          "content": "Selected Answer: B\nOption B is the best, provides a solution that improves the maintainability of the Jenkins infrastructure by migrating it to a managed service like ECS, while also addressing the build artifact encryption requirement by utilizing the secure and default-encrypted storage of Amazon S3.\n\nThinking in your deployment process in jenkins and change to codebuild, can be a difficult work to do. Change only the platform for ec2 to ecs you don't need to sustain patching and updates in ECS. And preserve your artifact on S3.",
          "poster": "MarcosSantos",
          "upvote_count": "1",
          "comment_id": "1561011"
        },
        {
          "timestamp": "1731858600.0",
          "content": "The answer is B\nContainerized Jenkins on ECS:\nBy deploying Jenkins on Amazon ECS (Elastic Container Service), you can leverage containerized environments to easily scale and manage Jenkins. This reduces the operational overhead of patching and upgrading EC2 instances running Jenkins.\nArtifact Storage with Encryption:Storing build artifacts in Amazon S3 with default encryption enabled ensures that all files in the bucket are automatically encrypted at rest using either SSE-S3 or SSE-KMS. This complies with the requirement to protect intellectual property by ensuring encryption of artifacts.\nThis approach ensures a fully managed and scalable solution for both Jenkins (containerized) and the artifact storage, aligning with best practices for security and compliance.",
          "comment_id": "1313635",
          "poster": "Ravi_Bulusu",
          "upvote_count": "2"
        },
        {
          "comment_id": "1287991",
          "upvote_count": "2",
          "timestamp": "1727068800.0",
          "poster": "newpotato",
          "content": "while option D could be easier for simple projects or when starting from scratch, it may not be the most maintainable solution for a company that already has a significant investment in Jenkins. Option B provides a balanced approach, leveraging Jenkins' capabilities while improving infrastructure management and security."
        },
        {
          "poster": "HarryLy",
          "content": "Selected Answer: D\nAWS codebuild use kms encryption key by default",
          "timestamp": "1717742580.0",
          "upvote_count": "1",
          "comment_id": "1225934"
        },
        {
          "comment_id": "1220588",
          "content": "Selected Answer: D\n\"D\" for me based on sb333's comments, etc.",
          "upvote_count": "1",
          "poster": "Gomer",
          "timestamp": "1716939480.0"
        },
        {
          "timestamp": "1715815620.0",
          "upvote_count": "2",
          "content": "Selected Answer: D\nD isn't cost effective, but most maintainable",
          "poster": "01037",
          "comment_id": "1212167"
        },
        {
          "upvote_count": "1",
          "content": "Answer is D\nAWS CodeBuild can be seamlessly integrated with containerized applications deployed on Amazon ECS.\nAWS CodeBuild utilizes multiple layers of encryption to safeguard your data at rest, in transit, and during execution.",
          "timestamp": "1709143620.0",
          "poster": "zijo",
          "comment_id": "1161891"
        },
        {
          "upvote_count": "1",
          "timestamp": "1709123460.0",
          "poster": "Vitalydt",
          "content": "Selected Answer: D\nD Seems the best option",
          "comment_id": "1161618"
        },
        {
          "content": "D is correct: codebuild has encryption by default -> easiest to maintain\nA: No mention of encrypting build artifacts\nB: Amazon S3 excryption only protect data at rest, not encrypting the data\nC: Using both AWS codepipline and AWS secret manager incurs more costs and makes maintenance much more difficult",
          "timestamp": "1706497500.0",
          "upvote_count": "3",
          "poster": "thanhnv142",
          "comment_id": "1134621"
        },
        {
          "comment_id": "1096565",
          "poster": "DucSiu",
          "content": "D is the right answer",
          "upvote_count": "1",
          "timestamp": "1702565220.0"
        },
        {
          "timestamp": "1701076140.0",
          "upvote_count": "1",
          "content": "Selected Answer: D\nD is the right answer",
          "comment_id": "1081334",
          "poster": "Sazeka"
        },
        {
          "upvote_count": "1",
          "poster": "2pk",
          "timestamp": "1699073700.0",
          "comment_id": "1061902",
          "content": "Selected Answer: B\nB is the answer . The ask is not to re engineer the whole solution it's just a simple task which needs encrypt the artifact. \nJenkins on Amazon ECS: Running Jenkins in an Amazon ECS cluster allows you to containerize your Jenkins setup, making it easier to manage and scale. ECS offers high availability, scalability, and easy maintenance.\n\nNormally Jenkin should run on ECS so it can handle multiple agents while use S3 as the default encryption."
        },
        {
          "poster": "RVivek",
          "upvote_count": "2",
          "timestamp": "1695021660.0",
          "content": "Selected Answer: D\nMOST maintainable manner is repacing jenkins with Codebuild a fully managed service\nIf the question had been with minimal chnage to the envornment then B would be best",
          "comment_id": "1010347"
        },
        {
          "comment_id": "1001112",
          "poster": "DaddyDee",
          "upvote_count": "1",
          "content": "Answer is D: MOST maintainable manner/managed service is the key word and there is no need to patch and upgrade. There is ECS with EC2 instances and ECS with fargate and the question is not explicit. Hence maintenance wise, a managed service is the way to go.\nhttps://jenkinshero.com/jenkins-vs-aws-codebuild-for-building-docker-images/",
          "timestamp": "1694053620.0"
        },
        {
          "poster": "habros",
          "timestamp": "1688786580.0",
          "content": "Selected Answer: D\nTechnically CodeBuild runs on a VM… albeit disposable. Switching on EC2 24/7 is not cost effective either.",
          "upvote_count": "1",
          "comment_id": "946119"
        },
        {
          "comment_id": "927891",
          "poster": "tartarus23",
          "upvote_count": "2",
          "content": "Selected Answer: D\nD. Use AWS CodeBuild with artifact encryption to replace the Jenkins instance running on EC2 instances.\n\nExplanation: AWS CodeBuild is a fully managed continuous integration service that compiles source code, runs tests, and produces software packages that are ready to deploy. With CodeBuild, you don’t need to provision, manage, and scale your own build servers. It also provides built-in support for artifact encryption, which would satisfy the compliance officer's requirements. This would eliminate the need for patching and upgrading Jenkins on EC2 instances, as well as the need to handle encryption at the storage level.",
          "timestamp": "1687206120.0"
        },
        {
          "timestamp": "1686929160.0",
          "upvote_count": "3",
          "content": "Selected Answer: B\nB is the right answer.. no one said to replace jenkins",
          "comment_id": "925325",
          "poster": "bakamon"
        },
        {
          "comments": [
            {
              "upvote_count": "1",
              "content": "Codebuild not codedeploy.",
              "timestamp": "1686561240.0",
              "comment_id": "921279",
              "poster": "Mail1964"
            }
          ],
          "content": "Selected Answer: D\nI think they are trying to take us down the code deploy route -The company is running Jenkins on Amazon EC2 instances, which require patching and upgrading. This suggest to me they want something that does not require this level of maintenance and is AWS.",
          "upvote_count": "2",
          "poster": "Mail1964",
          "timestamp": "1686561000.0",
          "comment_id": "921277"
        },
        {
          "comment_id": "912862",
          "content": "Replacing Jenkins with CodeBuild may require changes in all the deployment scripts. So, ECS is better way. Lift and Shift to ECS and the question mentions that company has already containerized most of its applications.\n\nCodeBuild is definitely more managed but then it means rewriting existing pipelines to work with CodeBuild.",
          "timestamp": "1685713380.0",
          "poster": "paali",
          "upvote_count": "4"
        },
        {
          "content": "Selected Answer: D\nthis is a managed service question... the answer is D. CodeBuild offers artifact encryption and is easily maintained. If you think managing jenkins on ECS is less overhead, you are certainly mistaken.",
          "comment_id": "911175",
          "poster": "rdoty",
          "timestamp": "1685533260.0",
          "upvote_count": "2"
        },
        {
          "content": "Selected Answer: D\nD for me. CodeBuild would encrypt the artifacts and require no maintenance. Any option of running jenkins requires maintenance. Jenkins is a high-maintenance tool. I use it every day. You have to maintain plugins updated every week and maintain the OS, deal with all the administrative stuff. We use it because we need it and for other reasons codebuild is not suitable for our project. But maintenance is a nightmare!",
          "upvote_count": "2",
          "timestamp": "1685458620.0",
          "comment_id": "910353",
          "poster": "bcx"
        },
        {
          "comment_id": "889075",
          "upvote_count": "2",
          "content": "Selected Answer: B\nThe most maintainable solution for encrypting build artifacts in this scenario would be to deploy Jenkins to an Amazon ECS cluster and copy build artifacts to an Amazon S3 bucket with default encryption enabled. This approach ensures that the build artifacts are automatically encrypted at rest without the need for additional configuration, reducing the risk of accidentally unencrypted data exposure.\n\nOn the other hand, for Option D, while using AWS CodeBuild with artifact encryption can also achieve the goal of encrypting build artifacts, it requires additional setup and may not be as easy to maintain as the ECS cluster and S3 bucket solution. Furthermore, this option involves replacing Jenkins with CodeBuild, which may require changes to existing CI/CD pipelines and scripts.",
          "poster": "5aga",
          "timestamp": "1683161760.0"
        },
        {
          "content": "Selected Answer: B\nThe MOST maintainable solution to encrypt build artifacts would be to deploy Jenkins to an Amazon ECS cluster and copy build artifacts to an Amazon S3 bucket with default encryption enabled. \n\nOption A is a good solution for patching and upgrading EC2 instances, but it does not address the artifact encryption requirement. Option C is not ideal because it involves using a build action, which may add complexity to the build process. Option D would require replacing Jenkins with AWS CodeBuild, which may not be feasible for the company's current environment.",
          "timestamp": "1682988180.0",
          "poster": "haazybanj",
          "comment_id": "886910",
          "upvote_count": "1"
        },
        {
          "content": "Selected Answer: B\nB ... not a great soln, but the best in this situation",
          "timestamp": "1682286900.0",
          "comment_id": "878828",
          "upvote_count": "1",
          "poster": "henryyvr"
        },
        {
          "poster": "alce2020",
          "content": "don't like Jenkins so for me the answer is D",
          "upvote_count": "1",
          "timestamp": "1681504920.0",
          "comment_id": "870468"
        },
        {
          "comment_id": "870365",
          "timestamp": "1681493700.0",
          "upvote_count": "5",
          "content": "The MOST maintainable solution for encrypting build artifacts from Jenkins running on EC2 instances is to deploy Jenkins to an Amazon ECS cluster and copy build artifacts to an Amazon S3 bucket with default encryption enabled. This solution provides high availability, scalability, and security for Jenkins and ensures that artifacts are encrypted when they are stored in S3. Using an ECS cluster allows for easy management and scaling of Jenkins instances, and S3 provides automatic encryption of objects at rest with default encryption enabled. Therefore, the correct answer is B. \nOption D is not the most maintainable solution since it requires replacing Jenkins with AWS CodeBuild and may require significant changes to the existing build process.",
          "poster": "jqso234"
        },
        {
          "timestamp": "1680953280.0",
          "poster": "Dimidrol",
          "comment_id": "864647",
          "content": "Selected Answer: D\nD for me. https://docs.aws.amazon.com/codebuild/latest/userguide/security-encryption.html",
          "upvote_count": "6"
        },
        {
          "content": "Selected Answer: B\nB makes sense.",
          "poster": "ele",
          "comment_id": "864004",
          "timestamp": "1680880020.0",
          "upvote_count": "3"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:12:55.739Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "cZbto7r7aJ8Cqt6ticA1",
      "question_number": 125,
      "page": 25,
      "question_text": "A company has a fleet of Amazon EC2 instances that run Linux in a single AWS account. The company is using an AWS Systems Manager Automation task across the EC2 instances.\n\nDuring the most recent patch cycle, several EC2 instances went into an error state because of insufficient available disk space. A DevOps engineer needs to ensure that the EC2 instances have sufficient available disk space during the patching process in the future.\n\nWhich combination of steps will meet these requirements? (Choose two.)",
      "choices": {
        "B": "Create a cron job that is installed on each EC2 instance to periodically delete temporary files.",
        "E": "Create an AWS Lambda function to periodically check for sufficient available disk space on all EC2 instances by evaluating each EC2 instance's respective Amazon CloudWatch log stream.",
        "A": "Ensure that the Amazon CloudWatch agent is installed on all EC2 instances.",
        "C": "Create an Amazon CloudWatch log group for the EC2 instances. Configure a cron job that is installed on each EC2 instance to write the available disk space to a CloudWatch log stream for the relevant EC2 instance.",
        "D": "Create an Amazon CloudWatch alarm to monitor available disk space on all EC2 instances. Add the alarm as a safety control to the Systems Manager Automation task."
      },
      "correct_answer": "AD",
      "answer_ET": "AD",
      "answers_community": [
        "AD (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/135847-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-03-13 06:21:00",
      "unix_timestamp": 1710307260,
      "discussion_count": 5,
      "discussion": [
        {
          "content": "Selected Answer: AD\nA. Ensure that the Amazon CloudWatch agent is installed on all EC2 instances.\n\nThe Amazon CloudWatch agent can collect system-level metrics, including disk space usage, and send them to Amazon CloudWatch. This will allow you to monitor the available disk space on each EC2 instance.\n\nD. Create an Amazon CloudWatch alarm to monitor available disk space on all EC2 instances. Add the alarm as a safety control to the Systems Manager Automation task.\n\nBy setting up a CloudWatch alarm to monitor the available disk space, you can trigger actions or notifications when the disk space falls below a certain threshold. Adding this alarm as a safety control to the Systems Manager Automation task ensures that the patching process will only proceed if there is sufficient available disk space.",
          "timestamp": "1720668000.0",
          "comment_id": "1245852",
          "poster": "trungtd",
          "upvote_count": "3"
        },
        {
          "upvote_count": "3",
          "timestamp": "1712980980.0",
          "comment_id": "1194638",
          "content": "Selected Answer: AD\nanswer A & D\nto configure disk usage, we can use custom metrics in the Cloudwatch agent configuration. don't need a cron job to pipe the disk usage.",
          "poster": "dkp"
        },
        {
          "comment_id": "1186061",
          "poster": "WhyIronMan",
          "content": "Selected Answer: AD\nA,D, Simple and accurate",
          "upvote_count": "4",
          "timestamp": "1711798140.0"
        },
        {
          "comment_id": "1172679",
          "content": "Selected Answer: AD\nThis article details the solution: \nhttps://aws.amazon.com/blogs/mt/avoid-patching-failures-due-to-low-disk-space-with-aws-systems-manager-automation-and-cloudwatch-alarms/",
          "upvote_count": "4",
          "poster": "Nano803",
          "timestamp": "1710343560.0"
        },
        {
          "content": "Selected Answer: AD\nA: Install AWS CloudWatch agent which will push disk information to a log group.\nB: Alarm depends on disk space.",
          "timestamp": "1710307260.0",
          "comment_id": "1172242",
          "poster": "Seoyong",
          "upvote_count": "4"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:12:55.739Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "hY7qlOnAUY8gWbJvgPbx",
      "question_number": 126,
      "page": 26,
      "question_text": "A DevOps engineer is building an application that uses an AWS Lambda function to query an Amazon Aurora MySQL DB cluster. The Lambda function performs only read queries. Amazon EventBridge events invoke the Lambda function.\n\nAs more events invoke the Lambda function each second, the database's latency increases and the database's throughput decreases. The DevOps engineer needs to improve the performance of the application.\n\nWhich combination of steps will meet these requirements? (Choose three.)",
      "choices": {
        "B": "Implement database connection pooling inside the Lambda code. Set a maximum number of connections on the database connection pool.",
        "F": "Connect to the Aurora cluster endpoint from the Lambda function.",
        "C": "Implement the database connection opening outside the Lambda event handler code.",
        "E": "Connect to the proxy endpoint from the Lambda function.",
        "D": "Implement the database connection opening and closing inside the Lambda event handler code.",
        "A": "Use Amazon RDS Proxy to create a proxy. Connect the proxy to the Aurora cluster reader endpoint. Set a maximum connections percentage on the proxy."
      },
      "correct_answer": "ACE",
      "answer_ET": "ACE",
      "answers_community": [
        "ACE (84%)",
        "ADE (16%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/135848-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-03-13 06:22:00",
      "unix_timestamp": 1710307320,
      "discussion_count": 8,
      "discussion": [
        {
          "content": "Selected Answer: ACE\nOpening and closing database connections outside the Lambda handler allows for efficient reuse of connections and implements connection pooling",
          "timestamp": "1712981700.0",
          "comment_id": "1194641",
          "poster": "dkp",
          "upvote_count": "6"
        },
        {
          "upvote_count": "6",
          "poster": "WhyIronMan",
          "content": "Selected Answer: ADE\nA, D, E\nFor those going with A,C,E, the same link already provided here https://aws.amazon.com/blogs/compute/using-amazon-rds-proxy-with-aws-lambda/\nin the nodejs/javascript code you can see that database connection opening (connection.connect) and closing (connection.end) are being handled INSIDE the handler function, which is correct because you want to open connections but it is a good practice to close connections",
          "comment_id": "1186059",
          "timestamp": "1711797840.0"
        },
        {
          "timestamp": "1723792680.0",
          "comment_id": "1266886",
          "poster": "GripZA",
          "content": "Selected Answer: ACE\n\"By opening the database connection outside the event handler, the connection can be reused across multiple invocations, which reduces the overhead of establishing new connections repeatedly.\"",
          "upvote_count": "3"
        },
        {
          "timestamp": "1722339540.0",
          "comment_id": "1258178",
          "poster": "jamesf",
          "content": "Selected Answer: ACE\nShould be ACE\nOpening database connections OUTSIDE the Lambda handler allows for efficient reuse of connections and implements connection pooling",
          "upvote_count": "4"
        },
        {
          "upvote_count": "4",
          "poster": "c3518fc",
          "timestamp": "1714154100.0",
          "content": "Selected Answer: ACE\nhttps://aws.amazon.com/blogs/compute/using-amazon-rds-proxy-with-aws-lambda/",
          "comment_id": "1202741"
        },
        {
          "timestamp": "1710701520.0",
          "poster": "L1_",
          "upvote_count": "5",
          "content": "Selected Answer: ACE\nACE: I also agree - https://aws.amazon.com/blogs/compute/using-amazon-rds-proxy-with-aws-lambda/",
          "comment_id": "1176027"
        },
        {
          "upvote_count": "5",
          "poster": "DanShone",
          "timestamp": "1710579060.0",
          "content": "Selected Answer: ACE\nI agree",
          "comment_id": "1174866"
        },
        {
          "timestamp": "1710307320.0",
          "upvote_count": "5",
          "content": "Selected Answer: ACE\nperfect",
          "poster": "Seoyong",
          "comment_id": "1172243"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:13:06.329Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "dMU2p0e7g85w6BSki7DZ",
      "question_number": 127,
      "page": 26,
      "question_text": "A company has an AWS CloudFormation stack that is deployed in a single AWS account. The company has configured the stack to send event notifications to an Amazon Simple Notification Service (Amazon SNS) topic.\n\nA DevOps engineer must implement an automated solution that applies a tag to the specific CloudFormation stack instance only after a successful stack update occurs. The DevOps engineer has created an AWS Lambda function that applies and updates this tag for the specific stack instance.\n\nWhich solution will meet these requirements?",
      "choices": {
        "B": "Create a custom AWS Config rule that produces a compliance change event if the CloudFormation stack has an UPDATE_COMPLETE instance status. Configure AWS Config to directly invoke the Lambda function to automatically remediate the change event.",
        "D": "Adjust the configuration of the CloudFormation stack to send notifications for only an UPDATE_COMPLETE instance status event to the SNS topic. Subscribe the Lambda function to the SNS topic.",
        "A": "Run the AWS-UpdateCloudFormationStack AWS Systems ManagerAutomation runbook when Systems Manager detects an UPDATE_COMPLETE event for the instance status of the CloudFormation stack. Configure the runbook to invoke the Lambda function.",
        "C": "Create an Amazon EventBridge rule that matches the UPDATE_COMPLETE event pattern for the instance status of the CloudFormation stack. Configure the rule to invoke the Lambda function."
      },
      "correct_answer": "C",
      "answer_ET": "C",
      "answers_community": [
        "C (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/137360-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-03-27 17:36:00",
      "unix_timestamp": 1711557360,
      "discussion_count": 4,
      "discussion": [
        {
          "content": "Selected Answer: C\nEventBridge is designed to detect specific events in AWS services, and it can be configured to match events such as UPDATE_COMPLETE from CloudFormation.\nThis allows you to automate the process of tagging the CloudFormation stack instancess whenever the UPDATE_COMPLETE event occurs.\nThe EventBridge rule will trigger the Lambda function, which will then apply the necessary tag to the stack.",
          "comment_id": "1266892",
          "upvote_count": "3",
          "timestamp": "1723793100.0",
          "poster": "GripZA"
        },
        {
          "upvote_count": "3",
          "poster": "dkp",
          "timestamp": "1712985540.0",
          "comment_id": "1194659",
          "content": "Selected Answer: C\noptions C and D are suitable for implementing the automated solution. However, using Option C with Amazon EventBridge is more direct and does not require additional SNS configuration"
        },
        {
          "content": "Selected Answer: C\nC,\nEventBridge + Lambda Function\nhttps://docs.aws.amazon.com/eventbridge/latest/userguide/eb-run-lambda-schedule.html",
          "comment_id": "1186054",
          "timestamp": "1711797000.0",
          "poster": "WhyIronMan",
          "upvote_count": "3"
        },
        {
          "upvote_count": "3",
          "poster": "ogerber",
          "content": "Selected Answer: C\nIts C, 100%",
          "timestamp": "1711557360.0",
          "comment_id": "1184229"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:13:06.329Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "7vRkw84WxZgjU4W1IiEn",
      "question_number": 128,
      "page": 26,
      "question_text": "A company deploys an application to two AWS Regions. The application creates and stores objects in an Amazon S3 bucket that is in the same Region as the application. Both deployments of the application need to have access to all the objects and their metadata from both Regions. The company has configured two-way replication between the S3 buckets and has enabled S3 Replication metrics on each S3 bucket.\n\nA DevOps engineer needs to implement a solution that retries the replication process if an object fails to replicate.\n\nWhich solution will meet these requirements?",
      "choices": {
        "D": "Create an AWS Lambda function that will use S3 batch operations to retry the replication on the existing object for a failed replication. Configure S3 event notifications to send failed replication notifications to the Lambda function.",
        "B": "Create an Amazon Simple Queue Service (Amazon SQS) queue. Configure S3 event notifications to send failed replication notifications to the SQS queue. Create an AWS Lambda function that downloads the failed replication object and then runs a PutObject command for the object to the destination bucket. Configure the Lambda function to poll the queue for notifications to process.",
        "C": "Create an Amazon EventBridge rule that listens to S3 event notifications for failed replications. Create an AWS Lambda function that downloads the failed replication object and then runs a PutObject command for the object to the destination bucket.",
        "A": "Create an Amazon EventBridge rule that listens to S3 event notifications for failed replication events. Create an AWS Lambda function that downloads the failed replication object and then runs a PutObject command for the object to the destination bucket. Configure the EventBridge rule to invoke the Lambda function to handle the object that failed to replicate."
      },
      "correct_answer": "D",
      "answer_ET": "D",
      "answers_community": [
        "D (85%)",
        "A (15%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/136239-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-03-16 19:54:00",
      "unix_timestamp": 1710615240,
      "discussion_count": 8,
      "discussion": [
        {
          "content": "Selected Answer: D\nS3 Batch Replication provides you a way to replicate objects that existed before a replication configuration was in place, objects that have previously been replicated, and objects that have failed replication.\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/s3-batch-replication-batch.html",
          "poster": "r4nd0m1z3r",
          "upvote_count": "6",
          "comment_id": "1178575",
          "timestamp": "1710957720.0"
        },
        {
          "upvote_count": "5",
          "poster": "L1_",
          "content": "Selected Answer: D\nThis post suggests D: https://docs.aws.amazon.com/AmazonS3/latest/userguide/s3-batch-replication-batch.html",
          "comment_id": "1176033",
          "timestamp": "1710702720.0"
        },
        {
          "upvote_count": "1",
          "content": "Selected Answer: A\nOption A provides the most efficient and streamlined solution by using EventBridge and Lambda to automatically retry replication failures",
          "timestamp": "1743340620.0",
          "poster": "Srikantha",
          "comment_id": "1412576"
        },
        {
          "content": "Selected Answer: D\nBy using S3 Batch Replication, you can replicate the following types of objects:\n\n Objects that existed before a replication configuration was in place\n\n Objects that have previously been replicated\n\n Objects that have failed replication",
          "upvote_count": "4",
          "timestamp": "1722862980.0",
          "poster": "iulian0585",
          "comment_id": "1261074"
        },
        {
          "comment_id": "1235526",
          "poster": "xdkonorek2",
          "content": "Selected Answer: D\n100% D \nS3 Batch Replication is one of actions provided by S3 batch operations https://docs.aws.amazon.com/AmazonS3/latest/userguide/batch-ops-operations.html\n\nbatch operations can be used for objects that failed replication\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/s3-batch-replication-batch.html#:~:text=Objects%20that%20have%20failed%20replication",
          "timestamp": "1719077700.0",
          "upvote_count": "2"
        },
        {
          "poster": "c3518fc",
          "comment_id": "1202751",
          "timestamp": "1714155120.0",
          "upvote_count": "3",
          "content": "Selected Answer: D\nS3 Batch Replication provides you a way to replicate objects that existed before a replication configuration was in place, objects that have previously been replicated, and objects that have failed replication. This is done through the use of a Batch Operations job. https://docs.aws.amazon.com/AmazonS3/latest/userguide/s3-batch-replication-batch.html"
        },
        {
          "upvote_count": "3",
          "poster": "WhyIronMan",
          "comment_id": "1186053",
          "content": "Selected Answer: D\nD,\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/s3-batch-replication-batch.html",
          "timestamp": "1711796520.0"
        },
        {
          "poster": "CloudHell",
          "timestamp": "1710615240.0",
          "upvote_count": "3",
          "content": "Selected Answer: A\nIt's A for me.",
          "comment_id": "1175201"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:13:06.329Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "yTM4OwarCslYpKqbkKXZ",
      "question_number": 129,
      "page": 26,
      "question_text": "A company needs to implement failover for its application. The application includes an Amazon CloudFront distribution and a public Application Load Balancer (ALB) in an AWS Region. The company has configured the ALB as the default origin for the distribution.\n\nAfter some recent application outages, the company wants a zero-second RTO. The company deploys the application to a secondary Region in a warm standby configuration. A DevOps engineer needs to automate the failover of the application to the secondary Region so that HTTP GET requests meet the desired RTO.\n\nWhich solution will meet these requirements?",
      "choices": {
        "C": "Create Amazon Route 53 alias records that have a failover policy and Evaluate Target Health set to Yes for both ALBs. Set the TTL of both records to 0. Update the distribution's origin to use the new record set.",
        "A": "Create a second CloudFront distribution that has the secondary ALB as the default origin. Create Amazon Route 53 alias records that have a failover policy and Evaluate Target Health set to Yes for both CloudFront distributions. Update the application to use the new record set.",
        "D": "Create a CloudFront function that detects HTTP 5xx status codes. Configure the function to return a 307 Temporary Redirect error response to the secondary ALB if the function detects 5xx status codes. Update the distribution's default behavior to send origin responses to the function.",
        "B": "Create a new origin on the distribution for the secondary ALCreate a new origin group. Set the original ALB as the primary origin. Configure the origin group to fail over for HTTP 5xx status codes. Update the default behavior to use the origin group."
      },
      "correct_answer": "B",
      "answer_ET": "B",
      "answers_community": [
        "B (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/136241-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-03-16 20:20:00",
      "unix_timestamp": 1710616800,
      "discussion_count": 4,
      "discussion": [
        {
          "comment_id": "1331214",
          "timestamp": "1735067700.0",
          "upvote_count": "2",
          "poster": "youonebe",
          "content": "Selected Answer: B\nvote for b"
        },
        {
          "comment_id": "1194682",
          "upvote_count": "3",
          "timestamp": "1728799860.0",
          "poster": "dkp",
          "content": "Selected Answer: B\nanswer B"
        },
        {
          "content": "Selected Answer: B\nB,\nmazon CloudFront offers origin failover, where if a given request to the primary endpoint fails, CloudFront routes the request to the secondary endpoint. Unlike the failover operations described previously, all subsequent requests still go to the primary endpoint, and failover is done per each request.\n\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/high_availability_origin_failover.html#concept_origin_groups.creating",
          "timestamp": "1727686800.0",
          "upvote_count": "4",
          "poster": "WhyIronMan",
          "comment_id": "1186052"
        },
        {
          "timestamp": "1726507200.0",
          "poster": "CloudHell",
          "comment_id": "1175215",
          "upvote_count": "4",
          "content": "Selected Answer: B\nIt's B for me.\n\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/high_availability_origin_failover.html"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:13:06.329Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "1FUmg256G6tyKhC67jGC",
      "question_number": 130,
      "page": 26,
      "question_text": "A cloud team uses AWS Organizations and AWS IAM Identity Center (AWS Single Sign-On) to manage a company's AWS accounts. The company recently established a research team. The research team requires the ability to fully manage the resources in its account. The research team must not be able to create IAM users.\n\nThe cloud team creates a Research Administrator permission set in IAM Identity Center for the research team. The permission set has the AdministratorAccess AWS managed policy attached. The cloud team must ensure that no one on the research team can create IAM users.\n\nWhich solution will meet these requirements?",
      "choices": {
        "A": "Create an IAM policy that denies the iam:CreateUser action. Attach the IAM policy to the Research Administrator permission set.",
        "C": "Create an SCP that denies the iam:CreateUser action. Attach the SCP to the research team's AWS account.",
        "D": "Create an AWS Lambda function that deletes IAM users. Create an Amazon EventBridge rule that detects the IAM CreateUser event. Configure the rule to invoke the Lambda function.",
        "B": "Create an IAM policy that allows all actions except the iam:CreateUser action. Use the IAM policy to set the permissions boundary for the Research Administrator permission set."
      },
      "correct_answer": "C",
      "answer_ET": "C",
      "answers_community": [
        "C (54%)",
        "A (46%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/136243-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-03-16 20:38:00",
      "unix_timestamp": 1710617880,
      "discussion_count": 17,
      "discussion": [
        {
          "poster": "CloudHell",
          "upvote_count": "8",
          "comment_id": "1175230",
          "content": "Selected Answer: C\nIt's C for me, here is a link with a similar scenario:\n\nhttps://asecure.cloud/a/scp_deny_iam_user_creation_w_exception/",
          "timestamp": "1710617880.0"
        },
        {
          "timestamp": "1712990820.0",
          "upvote_count": "7",
          "poster": "dkp",
          "content": "Selected Answer: C\nWhile IAM policies can deny actions, they are typically attached to individual users or roles. In this scenario, you want to restrict user creation across the entire research team's account, making an SCP the more appropriate choice.",
          "comment_id": "1194706"
        },
        {
          "timestamp": "1736778060.0",
          "content": "Selected Answer: A\nA as the restriction just needs to be applied to the research team but not the whole account users",
          "comment_id": "1339920",
          "poster": "teo2157",
          "upvote_count": "2"
        },
        {
          "comment_id": "1338052",
          "content": "Selected Answer: A\nThe wording is that only the research team should not be allowed to create users. This is A as the Permission Set will apply to just them. If you choose C it's an account wide deny so no other user or admins would be able to create a user which is outside the scope of the question.",
          "poster": "MrTizz",
          "upvote_count": "3",
          "timestamp": "1736361060.0"
        },
        {
          "upvote_count": "3",
          "content": "Selected Answer: A\nIAM Policy to Deny iam:CreateUser\nAn IAM policy is applied to individual IAM users, groups, or roles within an AWS account. Here's an example policy that denies the iam:CreateUser action:\n\nIAM Policy JSON\njson\nCopy code\n{\n \"Version\": \"2012-10-17\",\n \"Statement\": [\n {\n \"Effect\": \"Deny\",\n \"Action\": \"iam:CreateUser\",\n \"Resource\": \"*\"\n }\n ]\n}\nSteps to Attach IAM Policy to Research Administrator Permission Set:\nNavigate to AWS IAM Identity Center (SSO).\nSelect the Permission Sets section.\nChoose the Research Administrator permission set.\nAttach the custom policy above to the permission set by selecting Add permissions → Custom policy.",
          "comment_id": "1335051",
          "comments": [
            {
              "timestamp": "1735682880.0",
              "poster": "spring21",
              "upvote_count": "1",
              "content": "Comparison: IAM Policy vs SCP\nAspect IAM Policy SCP\nScope Affects only the user, group, or role it is attached to. Applies to all users, roles, and policies within the target account or OU.\nUse Case Granular control within an account. Broad guardrails across accounts or OUs.\nHierarchy Impact Does not affect parent accounts or organization. Enforces policies across all child accounts.\nEffect Denies specific actions only for targeted users or groups. Overrides any permissions granted at any level within the account.",
              "comment_id": "1335052"
            }
          ],
          "poster": "spring21",
          "timestamp": "1735682880.0"
        },
        {
          "poster": "Impromptu",
          "upvote_count": "3",
          "comment_id": "1315954",
          "timestamp": "1732213560.0",
          "content": "Selected Answer: A\nA meets the requirements.\nC would deny CreateUser for all the IAM entities in the account, not only the research team"
        },
        {
          "content": "Selected Answer: A\nFor those who selected C, why would you create ab SCP that will deny any IAM user from creating another IAM when the question clearly states only the research team shouldn't be able to create an IAM user? the deny policy will restrict only the Research Administrator permission set, which is what we want.",
          "timestamp": "1723794540.0",
          "comment_id": "1266899",
          "poster": "GripZA",
          "upvote_count": "2"
        },
        {
          "upvote_count": "3",
          "timestamp": "1722340980.0",
          "content": "Selected Answer: C\ni go for C\njust make sure no one can create account\nscp also can create with exception as mentioned by @CloudHell",
          "comment_id": "1258191",
          "poster": "jamesf"
        },
        {
          "poster": "tgv",
          "timestamp": "1721311860.0",
          "comment_id": "1250447",
          "content": "Selected Answer: A\nI'll go for A as the question says:\n\"The cloud team must ensure that no one on the research team can create IAM users.\"\n\nC will block everybody (not just the research team)",
          "upvote_count": "5",
          "comments": [
            {
              "timestamp": "1721312220.0",
              "poster": "tgv",
              "upvote_count": "1",
              "content": "even thoguh xdkonorek2 has a valid point.\njust flip a coin if you get this question in the exam",
              "comment_id": "1250458"
            }
          ]
        },
        {
          "content": "Selected Answer: C\nC,\nA is not enough due research team still could create iam role with that allows him to create iam user and e.g. invoke lambda that does it for him\nobviously unwanted implication is that no one in this account can create IAM users even admins, but still it fulfills the requirements",
          "upvote_count": "4",
          "poster": "xdkonorek2",
          "timestamp": "1720173540.0",
          "comment_id": "1242687"
        },
        {
          "upvote_count": "4",
          "content": "Selected Answer: A\nA, only the research team shouldn't be able to create IAM users.",
          "poster": "that1guy",
          "comment_id": "1212883",
          "timestamp": "1715948700.0"
        },
        {
          "content": "Selected Answer: C\nC for me",
          "upvote_count": "3",
          "poster": "seetpt",
          "comment_id": "1205549",
          "timestamp": "1714654800.0"
        },
        {
          "poster": "c3518fc",
          "upvote_count": "4",
          "content": "Selected Answer: C\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps_attach.html",
          "timestamp": "1714156260.0",
          "comment_id": "1202766"
        },
        {
          "timestamp": "1712194920.0",
          "upvote_count": "5",
          "comment_id": "1189021",
          "content": "Selected Answer: C\nC is the answer. IAM policy is not as scalable or centralized as using an SCP. \nYou can attach an SCP to the organization root, to an organizational unit (OU), or directly to an account\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps_attach.html",
          "poster": "tristan_07"
        },
        {
          "timestamp": "1711796160.0",
          "comments": [
            {
              "poster": "HayLLlHuK",
              "timestamp": "1712822700.0",
              "upvote_count": "5",
              "comment_id": "1193566",
              "content": "You can attach an SCP to the organization root, to an organizational unit (OU), or directly to an account.\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps_attach.html"
            }
          ],
          "poster": "WhyIronMan",
          "comment_id": "1186051",
          "upvote_count": "2",
          "content": "Selected Answer: A\nA is the correct option since you can not apply SCP directly to an AWS Account (need to be OU)"
        },
        {
          "timestamp": "1711613280.0",
          "poster": "rkddkwlrkwhgdk",
          "comment_id": "1184630",
          "comments": [],
          "content": "Selected Answer: A\nSCP can be applied to an OU. Therefore, the answer is A.",
          "upvote_count": "4"
        },
        {
          "timestamp": "1711558320.0",
          "poster": "ogerber",
          "comments": [],
          "content": "Selected Answer: A\nIts A, when you attach the SCP no one will be able to create new user not just the team",
          "upvote_count": "1",
          "comment_id": "1184234"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:13:06.329Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "iSllzlSn1qTwqZyPcWDk",
      "question_number": 131,
      "page": 27,
      "question_text": "A company releases a new application in a new AWS account. The application includes an AWS Lambda function that processes messages from an Amazon Simple Queue Service (Amazon SQS) standard queue. The Lambda function stores the results in an Amazon S3 bucket for further downstream processing. The Lambda function needs to process the messages within a specific period of time after the messages are published. The Lambda function has a batch size of 10 messages and takes a few seconds to process a batch of messages.\n\nAs load increases on the application's first day of service, messages in the queue accumulate at a greater rate than the Lambda function can process the messages. Some messages miss the required processing timelines. The logs show that many messages in the queue have data that is not valid. The company needs to meet the timeline requirements for messages that have valid data.\n\nWhich solution will meet these requirements?",
      "choices": {
        "C": "Increase the Lambda function's batch size. Configure S3 Transfer Acceleration on the S3 bucket. Configure an SQS dead-letter queue.",
        "A": "Increase the Lambda function's batch size. Change the SQS standard queue to an SQS FIFO queue. Request a Lambda concurrency increase in the AWS Region.",
        "B": "Reduce the Lambda function's batch size. Increase the SQS message throughput quota. Request a Lambda concurrency increase in the AWS Region.",
        "D": "Keep the Lambda function's batch size the same. Configure the Lambda function to report failed batch items. Configure an SQS dead-letter queue."
      },
      "correct_answer": "D",
      "answer_ET": "D",
      "answers_community": [
        "D (93%)",
        "7%"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/137361-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-03-27 17:58:00",
      "unix_timestamp": 1711558680,
      "discussion_count": 8,
      "discussion": [
        {
          "comment_id": "1202768",
          "timestamp": "1714156680.0",
          "poster": "c3518fc",
          "content": "Selected Answer: D\nConfigure a dead-letter queue to avoid creating a snowball anti-pattern in your serverless application’s architecture. For more information, see the Avoiding snowball anti-patterns section of this guide.\n\nConfigure your Lambda function event source mapping to make only the failed messages visible. To do this, you must include the value ReportBatchItemFailures in the FunctionResponseTypes list when configuring your event source mapping. https://docs.aws.amazon.com/prescriptive-guidance/latest/lambda-event-filtering-partial-batch-responses-for-sqs/best-practices-partial-batch-responses.html",
          "upvote_count": "5"
        },
        {
          "content": "Selected Answer: C\nWe have to increase the batch size to speed up the processing. \nD makes no sense since it will not speed up the processing in anyway.\nA cannot be right since it used FIFO queue which will reduce lambda concurrency.",
          "comment_id": "1331321",
          "upvote_count": "1",
          "poster": "hk0308",
          "timestamp": "1735092660.0"
        },
        {
          "upvote_count": "2",
          "timestamp": "1730944200.0",
          "comment_id": "1308202",
          "content": "Selected Answer: D\nA. 1 failure within a batch will cause all messages in that batch to fail, blocking other tasks and delaying overall processing",
          "poster": "VerRi"
        },
        {
          "comment_id": "1205550",
          "upvote_count": "2",
          "poster": "seetpt",
          "timestamp": "1714654800.0",
          "content": "Selected Answer: D\nD for me"
        },
        {
          "content": "answer D seems more approprite",
          "upvote_count": "2",
          "poster": "dkp",
          "comment_id": "1194719",
          "timestamp": "1712991960.0"
        },
        {
          "comment_id": "1190292",
          "content": "I am torn between option A or D",
          "upvote_count": "1",
          "poster": "Ola2234",
          "timestamp": "1712391960.0"
        },
        {
          "comment_id": "1186050",
          "upvote_count": "4",
          "content": "Selected Answer: D\nD,\nhttps://docs.aws.amazon.com/prescriptive-guidance/latest/lambda-event-filtering-partial-batch-responses-for-sqs/best-practices-partial-batch-responses.html",
          "timestamp": "1711795800.0",
          "poster": "WhyIronMan"
        },
        {
          "poster": "ogerber",
          "content": "its D, 100%",
          "comment_id": "1184235",
          "upvote_count": "2",
          "timestamp": "1711558680.0"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:13:16.760Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "dp08Y5j6jAc4BcaA5h8l",
      "question_number": 132,
      "page": 27,
      "question_text": "A company has an application that runs on AWS Lambda and sends logs to Amazon CloudWatch Logs. An Amazon Kinesis data stream is subscribed to the log groups in CloudWatch Logs. A single consumer Lambda function processes the logs from the data stream and stores the logs in an Amazon S3 bucket.\n\nThe company’s DevOps team has noticed high latency during the processing and ingestion of some logs.\n\nWhich combination of steps will reduce the latency? (Choose three.)",
      "choices": {
        "C": "Configure reserved concurrency for the Lambda function that processes the logs.",
        "A": "Create a data stream consumer with enhanced fan-out. Set the Lambda function that processes the logs as the consumer.",
        "E": "Turn off the ReportBatchItemFailures setting in the Lambda event source mapping.",
        "F": "Increase the number of shards in the Kinesis data stream.",
        "D": "Increase the batch size in the Kinesis data stream.",
        "B": "Increase the ParallelizationFactor setting in the Lambda event source mapping."
      },
      "correct_answer": "ABF",
      "answer_ET": "ABF",
      "answers_community": [
        "ABF (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/137340-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-03-27 09:38:00",
      "unix_timestamp": 1711528680,
      "discussion_count": 6,
      "discussion": [
        {
          "upvote_count": "2",
          "comment_id": "1267534",
          "poster": "GripZA",
          "timestamp": "1723879740.0",
          "content": "Selected Answer: ABF\nA: Kinesis Enhanced fan-out is an Amazon Kinesis Data Streams feature that enables consumers to receive records from a data stream with dedicated throughput of up to 2 MB of data per second per shard. A consumer that uses enhanced fan-out doesn't have to contend with other consumers that are receiving data from the stream.\n\nB: Reserved concurrency – This represents the maximum number of concurrent instances allocated to your function. When a function has reserved concurrency, no other function can use that concurrency. Reserved concurrency is useful for ensuring that your most critical functions always have enough concurrency to handle incoming requests.\n\nF: The data capacity of your stream is a function of the number of shards that you specify for the stream. The total capacity of the stream is the sum of the capacities of its shards."
        },
        {
          "timestamp": "1714654860.0",
          "content": "Selected Answer: ABF\nABF for me",
          "upvote_count": "2",
          "poster": "seetpt",
          "comment_id": "1205553"
        },
        {
          "content": "Selected Answer: ABF\nhttps://docs.aws.amazon.com/lambda/latest/dg/with-kinesis.html",
          "poster": "c3518fc",
          "timestamp": "1714157280.0",
          "comment_id": "1202774",
          "upvote_count": "4"
        },
        {
          "poster": "Ola2234",
          "comment_id": "1190319",
          "content": "ABF or ACF",
          "timestamp": "1712394480.0",
          "upvote_count": "1"
        },
        {
          "comment_id": "1186045",
          "content": "Selected Answer: ABF\nA,B,F,\nhttps://docs.aws.amazon.com/lambda/latest/dg/with-kinesis.html",
          "upvote_count": "3",
          "timestamp": "1711795500.0",
          "poster": "WhyIronMan"
        },
        {
          "comment_id": "1183941",
          "poster": "Seoyong",
          "timestamp": "1711528680.0",
          "upvote_count": "4",
          "content": "Selected Answer: ABF\nhttps://aws.amazon.com/about-aws/whats-new/2019/11/aws-lambda-supports-parallelization-factor-for-kinesis-and-dynamodb-event-sources/"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:13:16.760Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "f94p9FkeXnzxHXu2LeWC",
      "question_number": 133,
      "page": 27,
      "question_text": "A company operates sensitive workloads across the AWS accounts that are in the company's organization in AWS Organizations. The company uses an IP address range to delegate IP addresses for Amazon VPC CIDR blocks and all non-cloud hardware.\n\nThe company needs a solution that prevents principals that are outside the company’s IP address range from performing AWS actions in the organization's accounts.\n\nWhich solution will meet these requirements?",
      "choices": {
        "A": "Configure AWS Firewall Manager for the organization. Create an AWS Network Firewall policy that allows only source traffic from the company's IP address range. Set the policy scope to all accounts in the organization.",
        "D": "In Organizations, create an SCP that allows source IP addresses that are inside of the company’s IP address range. Attach the SCP to the organization's root.",
        "B": "In Organizations, create an SCP that denies source IP addresses that are outside of the company’s IP address range. Attach the SCP to the organization's root.",
        "C": "Configure Amazon GuardDuty for the organization. Create a GuardDuty trusted IP address list for the company's IP range. Activate the trusted IP list for the organization."
      },
      "correct_answer": "B",
      "answer_ET": "B",
      "answers_community": [
        "B (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/137362-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-03-27 18:09:00",
      "unix_timestamp": 1711559340,
      "discussion_count": 5,
      "discussion": [
        {
          "content": "Selected Answer: B\nB\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_examples_aws_deny-ip.html",
          "upvote_count": "5",
          "timestamp": "1727685540.0",
          "poster": "WhyIronMan",
          "comment_id": "1186043"
        },
        {
          "upvote_count": "3",
          "poster": "seetpt",
          "comment_id": "1205554",
          "content": "Selected Answer: B\nB 100%",
          "timestamp": "1730559660.0"
        },
        {
          "upvote_count": "4",
          "comment_id": "1202777",
          "poster": "c3518fc",
          "content": "Selected Answer: B\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_examples_aws_deny-ip.html",
          "timestamp": "1729968840.0"
        },
        {
          "content": "Selected Answer: B\nanswer b\nuses an SCP within AWS Organizations to deny source IP addresses that are outside of the company’s IP address range, providing a centralized and organization-wide control over AWS actions based on source IP addresses for all accounts and resources within the organization.",
          "upvote_count": "4",
          "timestamp": "1728804540.0",
          "comment_id": "1194744",
          "poster": "dkp"
        },
        {
          "content": "its B, 100%",
          "upvote_count": "2",
          "poster": "ogerber",
          "timestamp": "1727449740.0",
          "comment_id": "1184240"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:13:16.760Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "UBXZUHnHy4HPeR8WUaEJ",
      "question_number": 134,
      "page": 27,
      "question_text": "A company deploys an application in two AWS Regions. The application currently uses an Amazon S3 bucket in the primary Region to store data.\n\nA DevOps engineer needs to ensure that the application is highly available in both Regions. The DevOps engineer has created a new S3 bucket in the secondary Region. All existing and new objects must be in both S3 buckets. The application must fail over between the Regions with no data loss.\n\nWhich combination of steps will meet these requirements with the MOST operational efficiency? (Choose three.)",
      "choices": {
        "F": "Create an operation in S3 Batch Operations to replicate the contents of the source S3 bucket to the target S3 bucket. Configure the operation to use the IAM role for Amazon S3.",
        "E": "Create an AWS Batch job that has an AWS Fargate orchestration type. Configure the job to use the IAM role for AWS Batch. Specify a Bash command to use the AWS CLI to synchronize the contents of the source S3 bucket and the target S3 bucket",
        "B": "Create a new IAM role that allows the AWS Batch service principal to assume the role that has the necessary permissions for S3 replication.",
        "A": "Create a new IAM role that allows the Amazon S3 and S3 Batch Operations service principals to assume the role that has the necessary permissions for S3 replication.",
        "C": "Create an S3 Cross-Region Replication (CRR) rule on the source S3 bucket. Configure the rule to use the IAM role for Amazon S3 to replicate to the target S3 bucket.",
        "D": "Create a two-way replication rule on the source S3 bucket. Configure the rule to use the IAM role for Amazon S3 to replicate to the target S3 bucket."
      },
      "correct_answer": "ADF",
      "answer_ET": "ADF",
      "answers_community": [
        "ADF (69%)",
        "ACF (31%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/137363-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-03-27 18:14:00",
      "unix_timestamp": 1711559640,
      "discussion_count": 17,
      "discussion": [
        {
          "comment_id": "1212887",
          "timestamp": "1715949240.0",
          "upvote_count": "6",
          "content": "Selected Answer: ADF\nADF, \"All existing and new objects must be in BOTH S3 buckets.\" this requires two-way replication.",
          "poster": "that1guy"
        },
        {
          "poster": "d9iceguy",
          "upvote_count": "5",
          "comment_id": "1252830",
          "timestamp": "1721624100.0",
          "content": "Selected Answer: ADF\nNote: Application deployed to both regions, bi-directional replication will be required"
        },
        {
          "timestamp": "1747637100.0",
          "poster": "nickp84",
          "upvote_count": "1",
          "content": "Selected Answer: ACF\nD. Two-way replication rule: S3 does not support native two-way replication. You must configure two one-way CRR rules, one in each direction, if needed.",
          "comment_id": "1570083"
        },
        {
          "timestamp": "1730945940.0",
          "comment_id": "1308215",
          "content": "Selected Answer: ADF\nPoor wording. An active-active solution is recommended for HA, but bidirectional replication means CRR * 2. 'a two-way replication rule' is quite misleading",
          "poster": "VerRi",
          "upvote_count": "2"
        },
        {
          "comment_id": "1282491",
          "timestamp": "1726122360.0",
          "poster": "aws_god",
          "content": "Selected Answer: ACF\nNot D because it states creating the two-way replication on the source bucket and you need to configure it on both to work:\n\nWhen two-way replication is set up, a replication rule from the source bucket (DOC-EXAMPLE-BUCKET-1) to the bucket containing the replicas (DOC-EXAMPLE-BUCKET-2) is created. Then, a second replication rule from the bucket containing the replicas (DOC-EXAMPLE-BUCKET-2) to the source bucket (DOC-EXAMPLE-BUCKET-1) is created.",
          "upvote_count": "2"
        },
        {
          "upvote_count": "3",
          "content": "Selected Answer: ADF\nI think it's ADF check out this:\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/mrap-create-two-way-replication-rules.html",
          "poster": "Trex247",
          "comment_id": "1267343",
          "timestamp": "1723844820.0"
        },
        {
          "upvote_count": "2",
          "poster": "everydaysmile",
          "content": "Selected Answer: ACF\nTwo-way replication is possible using CRR.\n\n\"Replication is configured via rules. There is no rule for bi-directional replication. You will however setup a rule to replicate from the S3 bucket in the east AWS region to the west bucket, and you will setup a second rule to replicate going the opposite direction. These two rules will enable bi-directional replication across AWS regions.\"\n\n- https://catalog.workshops.aws/well-architected-reliability/en-US/4-failure-management/1-backup/20-bidirectional-replication-for-s3/2-configure-replication",
          "comment_id": "1265010",
          "timestamp": "1723528800.0"
        },
        {
          "content": "there is nothing called (Create a two-way replication rule on the source S3 bucket), the two-way replication is configured separately in each region per each bucket, that's why option D is incorrect.",
          "timestamp": "1722489120.0",
          "comments": [
            {
              "comment_id": "1330438",
              "upvote_count": "1",
              "content": "There is !\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/mrap-create-two-way-replication-rules.html#:~:text=page%2C%20choose%20the-,Replicate%20objects%20among%20all%20specified%20buckets,-template.%20The%20Replicate",
              "timestamp": "1734881880.0",
              "poster": "CHRIS12722222"
            }
          ],
          "poster": "hzaki",
          "comment_id": "1259201",
          "upvote_count": "1"
        },
        {
          "comment_id": "1258196",
          "upvote_count": "3",
          "poster": "jamesf",
          "content": "Selected Answer: ADF\nOption D: two-way replication required",
          "timestamp": "1722341880.0"
        },
        {
          "timestamp": "1722003660.0",
          "content": "Selected Answer: ADF\nD - for failover between regions. Any data stored on secondary bucket post failover operations needs to be replicated as well",
          "comment_id": "1255751",
          "poster": "auxwww",
          "upvote_count": "4"
        },
        {
          "timestamp": "1720173300.0",
          "poster": "xdkonorek2",
          "content": "Selected Answer: ADF\n\"The application must fail over between the Regions with no data loss.\"\nC is not enough, because if we failover to region B and then to A application couldn't access data that was created in region B in the meantime",
          "upvote_count": "4",
          "comment_id": "1242682"
        },
        {
          "content": "Two-Way Replication Rule is bidirectional, meaning objects are replicated from bucket A to bucket B and from bucket B to bucket A. This ensures that both buckets always contain the same data.\nS3 Cross-Region Replication (CRR) is unidirectional, meaning it replicates objects from a source bucket to a destination bucket. Changes made in the destination bucket do not propagate back to the source bucket.\nSo D, not C",
          "timestamp": "1719896760.0",
          "upvote_count": "2",
          "comment_id": "1240531",
          "poster": "6ef9a08"
        },
        {
          "timestamp": "1714654920.0",
          "content": "Selected Answer: ACF\nACF for me",
          "upvote_count": "1",
          "comment_id": "1205555",
          "poster": "seetpt"
        },
        {
          "timestamp": "1714525320.0",
          "comment_id": "1204783",
          "content": "ADF\nThe secondary also needs to replicate to the primary.",
          "upvote_count": "3",
          "comments": [
            {
              "content": "I agree. Does anyone have any reason why it wouldn't be B?",
              "upvote_count": "2",
              "comment_id": "1210763",
              "timestamp": "1715588280.0",
              "poster": "Jay_2pt0_1"
            }
          ],
          "poster": "MalonJay"
        },
        {
          "upvote_count": "2",
          "comment_id": "1194755",
          "poster": "dkp",
          "timestamp": "1712993880.0",
          "content": "Selected Answer: ACF\nanswer acf"
        },
        {
          "upvote_count": "2",
          "timestamp": "1712486280.0",
          "comment_id": "1190901",
          "content": "Selected Answer: ACF\nACF for sure. A - we need a replication role with principles for S3 Batch Operation, replicate job, and S3. C - will replicate all new objects, F - will replicate existing objects",
          "poster": "fdoxxx"
        },
        {
          "content": "Selected Answer: ACF\nits ACF for me",
          "timestamp": "1711559640.0",
          "poster": "ogerber",
          "upvote_count": "2",
          "comment_id": "1184242"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:13:16.760Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "tZJgvT6axyj5KUzJSDE1",
      "question_number": 135,
      "page": 27,
      "question_text": "An IT team has built an AWS CloudFormation template so others in the company can quickly and reliably deploy and terminate an application. The template creates an Amazon EC2 instance with a user data script to install the application and an Amazon S3 bucket that the application uses to serve static webpages while it is running.\nAll resources should be removed when the CloudFormation stack is deleted. However, the team observes that CloudFormation reports an error during stack deletion, and the S3 bucket created by the stack is not deleted.\nHow can the team resolve the error in the MOST efficient manner to ensure that all resources are deleted without errors?",
      "choices": {
        "B": "Add a custom resource with an AWS Lambda function with the DependsOn attribute specifying the S3 bucket, and an IAM role. Write the Lambda function to delete all objects from the bucket when RequestType is Delete.",
        "C": "Identify the resource that was not deleted. Manually empty the S3 bucket and then delete it.",
        "A": "Add a DelelionPolicy attribute to the S3 bucket resource, with the value Delete forcing the bucket to be removed when the stack is deleted.",
        "D": "Replace the EC2 and S3 bucket resources with a single AWS OpsWorks Stacks resource. Define a custom recipe for the stack to create and delete the EC2 instance and the S3 bucket."
      },
      "correct_answer": "B",
      "answer_ET": "B",
      "answers_community": [
        "B (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/105243-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-04-05 04:53:00",
      "unix_timestamp": 1680663180,
      "discussion_count": 9,
      "discussion": [
        {
          "upvote_count": "7",
          "comment_id": "1134636",
          "timestamp": "1722217020.0",
          "content": "B is correct: \n- Cant delete S3 so must check S3\n- There are several DeletionPolition option in ACF: delete, retain, snapshot. For S3, even if there is delete flag, S3 can only be deleted if all objects are removed\nA: wrong - add delete flag to deleteionpolicy cant forcing deletion of S3\nC: should not manually do the task\nD: should not swap to AWS opsworks",
          "poster": "thanhnv142"
        },
        {
          "content": "B. As per the AWS DeletionPolicy Options documentation it says, \"For Amazon S3 buckets, you must delete all objects in the bucket for deletion to succeed.\"\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html",
          "upvote_count": "7",
          "comment_id": "924549",
          "timestamp": "1702676940.0",
          "poster": "n_d1"
        },
        {
          "poster": "HarryLy",
          "content": "Selected Answer: B\nCloudformation does not have any behavior to force delete not empty bucket, need to invoke a custom lambda function to delete it",
          "timestamp": "1733561040.0",
          "upvote_count": "1",
          "comment_id": "1225936"
        },
        {
          "poster": "c3518fc",
          "upvote_count": "1",
          "timestamp": "1731319200.0",
          "comment_id": "1209651",
          "content": "Selected Answer: B\nKeyword \"Custom Resource\""
        },
        {
          "upvote_count": "1",
          "comment_id": "918531",
          "content": "Selected Answer: B\nB is a correct answer. A is wrong, you can't delete a bucket that has any objects.",
          "timestamp": "1702064160.0",
          "poster": "madperro"
        },
        {
          "poster": "haazybanj",
          "content": "Selected Answer: B\nB. Add a custom resource with an AWS Lambda function with the DependsOn attribute specifying the S3 bucket, and an IAM role. Write the Lambda function to delete all objects from the bucket when RequestType is Delete.",
          "upvote_count": "2",
          "timestamp": "1698893340.0",
          "comment_id": "886916"
        },
        {
          "content": "B is the correct answer",
          "upvote_count": "1",
          "poster": "alce2020",
          "timestamp": "1697316300.0",
          "comment_id": "870469"
        },
        {
          "upvote_count": "3",
          "comment_id": "864011",
          "timestamp": "1696691460.0",
          "content": "Selected Answer: B\nBecause it's B. CFN will not delete non-empty bucket. It must be emptied first. Custom resource will do it.",
          "poster": "ele"
        },
        {
          "timestamp": "1696474380.0",
          "content": "Why not A?",
          "poster": "lqpO_Oqpl",
          "comments": [
            {
              "comment_id": "879421",
              "poster": "tycho",
              "comments": [
                {
                  "timestamp": "1721203800.0",
                  "upvote_count": "2",
                  "comment_id": "1124877",
                  "poster": "tallmantim",
                  "content": "As per the linked article: \"For Amazon S3 buckets, you must delete all objects in the bucket for deletion to succeed.\""
                }
              ],
              "upvote_count": "1",
              "timestamp": "1698156720.0",
              "content": "https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html\ndeletion policy seems fine as well ..."
            }
          ],
          "upvote_count": "1",
          "comment_id": "861700"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:13:16.760Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "gduQIPaSnwQ5PmeLyyVM",
      "question_number": 136,
      "page": 28,
      "question_text": "A company uses an organization in AWS Organizations to manage multiple AWS accounts. The company needs an automated process across all AWS accounts to isolate any compromised Amazon EC2 instances when the instances receive a specific tag.\n\nWhich combination of steps will meet these requirements? (Choose two.)",
      "choices": {
        "B": "Create an SCP that has a Deny statement for the ec2:* action with a condition of \"aws:RequestTag/isolation\": false.",
        "C": "Attach the SCP to the root of the organization.",
        "E": "Create an AWS CloudFormation template that creates an EC2 instance role that has no IAM policies attached. Configure the template to have a security group that has no inbound rules or outbound rules. Use the CloudFormation template to create an AWS Lambda function that attaches the IAM role to instances. Configure the Lambda function to replace any existing security groups with the new security group. Set up an Amazon EventBridge rule to invoke the Lambda function when a specific tag is applied to a compromised EC2 instance.",
        "D": "Create an AWS CloudFormation template that creates an EC2 instance role that has no IAM policies attached. Configure the template to have a security group that has an explicit Deny rule on all traffic. Use the CloudFormation template to create an AWS Lambda function that attaches the IAM role to instances. Configure the Lambda function to add a network ACL. Set up an Amazon EventBridge rule to invoke the Lambda function when a specific tag is applied to a compromised EC2 instance.",
        "A": "Use AWS CloudFormation StackSets to deploy the CloudFormation stacks in all AWS accounts."
      },
      "correct_answer": "AE",
      "answer_ET": "AE",
      "answers_community": [
        "AE (84%)",
        "BC (16%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/137364-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-03-27 18:17:00",
      "unix_timestamp": 1711559820,
      "discussion_count": 10,
      "discussion": [
        {
          "poster": "Jay_2pt0_1",
          "content": "Selected Answer: AE\nWhat a weirdly worded question. I tend to agree with A & E. We need to isolate an EC2 that has a certain tag.",
          "comment_id": "1210765",
          "timestamp": "1715588580.0",
          "upvote_count": "6"
        },
        {
          "poster": "Jordarlu",
          "comment_id": "1295219",
          "upvote_count": "2",
          "timestamp": "1728490620.0",
          "content": "Selected Answer: AE\nThe B + C means no actions allowed on the tagged EC2 for all accounts in Organizations, but the asking was the needs of the isolation(implying the network isolation) on the tagged EC2; hence, A + E is a good option here.."
        },
        {
          "poster": "jamesf",
          "timestamp": "1722342240.0",
          "content": "Selected Answer: AE\nI go for AE\nisolating the instance should be mean block traffic",
          "comment_id": "1258201",
          "upvote_count": "3"
        },
        {
          "poster": "trungtd",
          "comment_id": "1246061",
          "upvote_count": "4",
          "content": "Selected Answer: AE\nThis CloudFormation template creates the necessary resources:\n\nAn EC2 instance role with no IAM policies, ensuring the instance cannot perform any actions.\nA security group with no inbound or outbound rules, effectively isolating the instance from all network traffic.\nA Lambda function that will be triggered by an EventBridge rule when a specific tag is applied to an EC2 instance. This function will attach the isolated security group to the compromised instance, ensuring it is isolated from any network communication.\nCombining these steps will provide an automated and consistent approach to isolate compromised EC2 instances across all AWS accounts in the organization.",
          "timestamp": "1720694820.0"
        },
        {
          "poster": "xdkonorek2",
          "upvote_count": "3",
          "comment_id": "1235567",
          "timestamp": "1719083640.0",
          "content": "Selected Answer: AE\nBD is wrong \nisolating the instance doesn't mean \"don't touch it\" with aws actions but to block traffic from and to it"
        },
        {
          "timestamp": "1714655160.0",
          "upvote_count": "1",
          "comment_id": "1205559",
          "content": "Selected Answer: BC\nBC for me",
          "comments": [
            {
              "comment_id": "1218145",
              "upvote_count": "3",
              "timestamp": "1716627300.0",
              "poster": "vn_thanhtung",
              "comments": [
                {
                  "content": "Answer is A, E",
                  "comment_id": "1218146",
                  "upvote_count": "2",
                  "timestamp": "1716627300.0",
                  "poster": "vn_thanhtung"
                }
              ],
              "content": "so funny, how to isolate incoming traffic. B,C means deny action with EC2"
            }
          ],
          "poster": "seetpt"
        },
        {
          "comment_id": "1194767",
          "upvote_count": "4",
          "timestamp": "1712994780.0",
          "content": "Selected Answer: AE\nill go with AE",
          "poster": "dkp"
        },
        {
          "comment_id": "1194154",
          "content": "CE for me.\nOption D is wrong because we can not use Security Group for an explicit deny rule. \nOption B is quite misleading with the resourceTagIsolation set to False instead of True.",
          "timestamp": "1712898780.0",
          "poster": "Ola2234",
          "upvote_count": "1"
        },
        {
          "comments": [
            {
              "comment_id": "1274389",
              "poster": "chinchin97",
              "timestamp": "1724914320.0",
              "upvote_count": "1",
              "content": "BC does not automate the isolation of instance. What it does is preventive measure that stops EC2 from perform action, but ultimately, it is still connected. \n\nYou will need security groups to cut off access to and from the compromised EC2 instance.\n\nTo have a complete solution, AE automates isolation based on tagging and deploy them to all EC2 instance, BC prevents any action by the EC2 using SCP. For this specific question, it is asking for the automation to isolate the EC2 instance so AE is the correct choice."
            }
          ],
          "comment_id": "1190905",
          "content": "Selected Answer: BC\nin my opinion it could not be AE because we would need a mechanism to apply this template to the right EC2 - I would vote for BC",
          "poster": "fdoxxx",
          "timestamp": "1712486640.0",
          "upvote_count": "3"
        },
        {
          "upvote_count": "4",
          "comment_id": "1184244",
          "poster": "ogerber",
          "content": "Selected Answer: AE\nA,E for me",
          "timestamp": "1711559820.0",
          "comments": [
            {
              "poster": "MalonJay",
              "timestamp": "1715270100.0",
              "upvote_count": "3",
              "comment_id": "1208979",
              "content": "AE\nThe question says isolate. What does isolate mean? Prevent outgoing and incoming traffic."
            }
          ]
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:13:27.218Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "7PEL8H4wtPlcBlOdSYDW",
      "question_number": 137,
      "page": 28,
      "question_text": "A company manages multiple AWS accounts by using AWS Organizations with OUs for the different business divisions. The company is updating their corporate network to use new IP address ranges. The company has 10 Amazon S3 buckets in different AWS accounts. The S3 buckets store reports for the different divisions. The S3 bucket configurations allow only private corporate network IP addresses to access the S3 buckets.\n\nA DevOps engineer needs to change the range of IP addresses that have permission to access the contents of the S3 buckets. The DevOps engineer also needs to revoke the permissions of two OUs in the company.\n\nWhich solution will meet these requirements?",
      "choices": {
        "B": "Create a new SCP that has a statement that allows only the new range of IP addresses to access the S3 buckets. Create another SCP that denies access to the S3 buckets. Attach the second SCP to the two OUs.",
        "C": "On all the S3 buckets, configure resource-based policies that allow only the new range of IP addresses to access the S3 buckets. Create a new SCP that denies access to the S3 buckets. Attach the SCP to the two OUs.",
        "D": "On all the S3 buckets, configure resource-based policies that allow only the new range of IP addresses to access the S3 buckets. Set a permissions boundary for the OrganizationAccountAccessRole role in the two OUs to deny access to the S3 buckets.",
        "A": "Create a new SCP that has two statements, one that allows access to the new range of IP addresses for all the S3 buckets and one that denies access to the old range of IP addresses for all the S3 buckets. Set a permissions boundary for the OrganizationAccountAccessRole role in the two OUs to deny access to the S3 buckets."
      },
      "correct_answer": "C",
      "answer_ET": "C",
      "answers_community": [
        "C (94%)",
        "6%"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/137342-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-03-27 10:10:00",
      "unix_timestamp": 1711530600,
      "discussion_count": 6,
      "discussion": [
        {
          "upvote_count": "1",
          "content": "Selected Answer: D\nOn all the S3 buckets, configure resource-based policies that allow only the new range of IP addresses to access the S3 buckets. Set a permissions boundary for the OrganizationAccountAccessRole role in the two OUs to deny access to the S3 buckets.\n\nThis solution meets the requirements most effectively:\n\nThe resource-based S3 bucket policies ensure that only the new IP address ranges are allowed access, effectively controlling access at the network level.\nBy setting a permissions boundary on the OrganizationAccountAccessRole role, the OUs' permissions to the S3 buckets can be explicitly controlled and revoked, ensuring that only the appropriate accounts have access.",
          "comment_id": "1412631",
          "timestamp": "1743341340.0",
          "poster": "Srikantha"
        },
        {
          "poster": "seetpt",
          "upvote_count": "3",
          "content": "Selected Answer: C\nC for me",
          "timestamp": "1730560020.0",
          "comment_id": "1205560"
        },
        {
          "timestamp": "1728808920.0",
          "upvote_count": "4",
          "comment_id": "1194807",
          "content": "Selected Answer: C\nanswer c",
          "poster": "dkp"
        },
        {
          "timestamp": "1728710820.0",
          "upvote_count": "1",
          "poster": "Ola2234",
          "comment_id": "1194161",
          "content": "C.\nUse bucket policy to allow or deny access to a range of IP addresses or VPC endpoints to an S3 resource. Restriction to OUs is best done using SCP."
        },
        {
          "comment_id": "1184249",
          "upvote_count": "4",
          "timestamp": "1727450760.0",
          "content": "Selected Answer: C\nC for me",
          "poster": "ogerber"
        },
        {
          "comment_id": "1183958",
          "upvote_count": "4",
          "timestamp": "1727421000.0",
          "poster": "Seoyong",
          "content": "Selected Answer: C\nrestrict access to S3 bucket using specific VPC endpoints or IP addresses:\nhttps://repost.aws/knowledge-center/block-s3-traffic-vpc-ip"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:13:27.218Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "RrYSuUUl6lOcCalqWtrV",
      "question_number": 138,
      "page": 28,
      "question_text": "A company has started using AWS across several teams. Each team has multiple accounts and unique security profiles. The company manages the accounts in an organization in AWS Organizations. Each account has its own configuration and security controls.\n\nThe company's DevOps team wants to use preventive and detective controls to govern all accounts. The DevOps team needs to ensure the security of accounts now and in the future as the company creates new accounts in the organization.\n\nWhich solution will meet these requirements?",
      "choices": {
        "B": "Create an AWS Control Tower landing zone. Configure OUs and appropriate controls in AWS Control Tower for the existing teams. Configure trusted access for AWS Control Tower. Enroll the existing accounts in the appropriate OUs that match the appropriate security policies for each team. Use AWS Control Tower to provision any new accounts.",
        "A": "Use Organizations to create OUs that have appropriate SCPs attached for each team. Place team accounts in the appropriate OUs to apply security controls. Create any new team accounts in the appropriate OUs.",
        "C": "Create AWS CloudFormation stack sets in the organization's management account. Configure a stack set that deploys AWS Config with configuration rules and remediation actions for all controls to each account in the organization. Update the stack sets to deploy to new accounts as the accounts are created.",
        "D": "Configure AWS Config to manage the AWS Config rules across all AWS accounts in the organization. Deploy conformance packs that provide AWS Config rules and remediation actions across the organization."
      },
      "correct_answer": "B",
      "answer_ET": "B",
      "answers_community": [
        "B (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/137341-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-03-27 10:04:00",
      "unix_timestamp": 1711530240,
      "discussion_count": 6,
      "discussion": [
        {
          "content": "Selected Answer: B\nA control is a high-level rule that provides ongoing governance for your overall AWS environment. It's expressed in plain language. AWS Control Tower implements preventive, detective, and proactive controls that help you govern your resources and monitor compliance across groups of AWS accounts. https://docs.aws.amazon.com/controltower/latest/controlreference/controls.html",
          "comment_id": "1202395",
          "timestamp": "1729920420.0",
          "upvote_count": "6",
          "poster": "c3518fc"
        },
        {
          "poster": "Seoyong",
          "upvote_count": "5",
          "timestamp": "1727420640.0",
          "comment_id": "1183955",
          "content": "Selected Answer: B\nAbout controls in AWS Control Tower:\nhttps://docs.aws.amazon.com/controltower/latest/userguide/controls.html"
        },
        {
          "content": "Selected Answer: B\nanswer B",
          "upvote_count": "3",
          "timestamp": "1728810240.0",
          "poster": "dkp",
          "comment_id": "1194820"
        },
        {
          "content": "Option B.\n\nKeyword: Preventive and detective controls to govern all accounts. This service is provided by guardrails as part of AWS Control Tower.",
          "upvote_count": "2",
          "comment_id": "1194165",
          "poster": "Ola2234",
          "timestamp": "1728711240.0"
        },
        {
          "upvote_count": "3",
          "comment_id": "1186013",
          "timestamp": "1727682540.0",
          "content": "Selected Answer: B\nhttps://docs.aws.amazon.com/controltower/latest/userguide/controls.html",
          "poster": "WhyIronMan"
        },
        {
          "content": "Selected Answer: B\nits B for me",
          "poster": "ogerber",
          "timestamp": "1727450940.0",
          "upvote_count": "2",
          "comment_id": "1184253"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:13:27.218Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "3GrsuLWQ6sSvYaRCc6vK",
      "question_number": 139,
      "page": 28,
      "question_text": "A company uses an AWS CodeCommit repository to store its source code and corresponding unit tests. The company has configured an AWS CodePipeline pipeline that includes an AWS CodeBuild project that runs when code is merged to the main branch of the repository.\n\nThe company wants the CodeBuild project to run the unit tests. If the unit tests pass, the CodeBuild project must tag the most recent commit.\n\nHow should the company configure the CodeBuild project to meet these requirements?",
      "choices": {
        "D": "Configure the CodeBuild project to use AWS CLI commands to copy the code from the CodeCommit repository. Configure the project to run the unit tests. Configure the project to use AWS CLI commands to create a new repository tag in the repository if the code passes the unit tests.",
        "C": "Configure the CodeBuild project to use AWS CLI commands to copy the code from the CodeCommit repository. Configure the project to run the unit tests. Configure the project to use AWS CLI commands to create a new Git tag in the repository if the code passes the unit tests.",
        "B": "Configure the CodeBuild projed to use native Git to done the CodeCommit repository. Configure the project to run the unit tests. Configure the project to use AWS CLI commands to create a new repository tag in the repository if the code passes the unit tests.",
        "A": "Configure the CodeBuild project to use native Git to done the CodeCommit repository. Configure the project to run the unit tests. Configure the project to use native Git to create a tag and to push the Git tag to the repository if the code passes the unit tests."
      },
      "correct_answer": "A",
      "answer_ET": "A",
      "answers_community": [
        "A (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/142973-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-06-27 10:57:00",
      "unix_timestamp": 1719478620,
      "discussion_count": 6,
      "discussion": [
        {
          "poster": "jamesf",
          "comment_id": "1258479",
          "timestamp": "1722388680.0",
          "content": "Selected Answer: A\nOption A effectively meets the requirements by using standard Git operations for cloning and tagging, ensuring an efficient and clear workflow for managing the repository in AWS CodePipeline.",
          "upvote_count": "2"
        },
        {
          "comment_id": "1248209",
          "poster": "tgv",
          "content": "---> A",
          "timestamp": "1721034900.0",
          "upvote_count": "2"
        },
        {
          "poster": "syh_rapha",
          "comment_id": "1247955",
          "upvote_count": "2",
          "timestamp": "1720987320.0",
          "content": "Selected Answer: A\nA\nhttps://docs.aws.amazon.com/codecommit/latest/userguide/getting-started.html#getting-started-create-commit"
        },
        {
          "timestamp": "1720706100.0",
          "poster": "trungtd",
          "upvote_count": "3",
          "content": "Selected Answer: A\nUsing Native Git: By configuring CodeBuild to use native Git, you ensure the repository is cloned in a way that supports all Git operations, including tagging and pushing changes.\n\nRunning Unit Tests: CodeBuild can be set up to run the unit tests using the build specification file (buildspec.yml), ensuring that the tests are executed before any further actions are taken.\n\nCreating and Pushing Tags: \npost_build:\n commands:\n - echo Unit tests passed, tagging commit...\n - git tag -a v1.0.0 -m \"Tagging commit after successful unit tests\" \n - git push origin v1.0.0",
          "comment_id": "1246152"
        },
        {
          "poster": "KaranNishad",
          "comment_id": "1238313",
          "upvote_count": "2",
          "content": "Selected Answer: A\nA is the Answer.",
          "timestamp": "1719510180.0"
        },
        {
          "upvote_count": "2",
          "comment_id": "1237994",
          "timestamp": "1719478620.0",
          "poster": "ihustle",
          "content": "A is the Answer."
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:13:27.218Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "nWvvZfyRE4LmnSszFgb8",
      "question_number": 140,
      "page": 28,
      "question_text": "A DevOps engineer manages a company's Amazon Elastic Container Service (Amazon ECS) cluster. The cluster runs on several Amazon EC2 instances that are in an Auto Scaling group. The DevOps engineer must implement a solution that logs and reviews all stopped tasks for errors.\n\nWhich solution will meet these requirements?",
      "choices": {
        "B": "Configure tasks to write log data in the embedded metric format. Store the logs in Amazon CloudWatch Logs. Monitor the ContainerInstanceCount metric for changes.",
        "D": "Configure an EC2 Auto Scaling lifecycle hook for the EC2_INSTANCE_TERMINATING scale-in event. Write the SystemEventLog file to Amazon S3. Use Amazon Athena to query the log file for errors.",
        "C": "Configure the EC2 instances to store logs in Amazon CloudWatch Logs. Create a CloudWatch Contributor Insights rule that uses the EC2 instance log data. Use the Contributor Insights rule to investigate stopped tasks.",
        "A": "Create an Amazon EventBridge rule to capture task state changes. Send the event to Amazon CloudWatch Logs. Use CloudWatch Logs Insights to investigate stopped tasks."
      },
      "correct_answer": "A",
      "answer_ET": "A",
      "answers_community": [
        "A (91%)",
        "9%"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/142990-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-06-27 19:46:00",
      "unix_timestamp": 1719510360,
      "discussion_count": 5,
      "discussion": [
        {
          "comment_id": "1240615",
          "upvote_count": "7",
          "content": "Selected Answer: A\nBy using Amazon EventBridge to capture ECS task state changes and sending these events to CloudWatch Logs, combined with the analytical capabilities of CloudWatch Logs Insights, option A provides a comprehensive and straightforward solution for logging and investigating stopped tasks for errors.",
          "poster": "6ef9a08",
          "timestamp": "1719905400.0"
        },
        {
          "content": "Selected Answer: A\nGoing for A as C is referring to configure the EC2 instances to store logs in Amazon CloudWatch Logs while the question is referring to ECS tasks",
          "comment_id": "1340200",
          "timestamp": "1736836680.0",
          "upvote_count": "1",
          "poster": "teo2157"
        },
        {
          "content": "---> A",
          "comment_id": "1248210",
          "upvote_count": "1",
          "timestamp": "1721034900.0",
          "poster": "tgv"
        },
        {
          "timestamp": "1720706400.0",
          "content": "Selected Answer: A\nJust A",
          "comment_id": "1246153",
          "upvote_count": "2",
          "poster": "trungtd"
        },
        {
          "poster": "KaranNishad",
          "upvote_count": "1",
          "content": "Selected Answer: C\nCloudWatch Contributor Insights",
          "timestamp": "1719510360.0",
          "comment_id": "1238320"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:13:27.218Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "uTedelrRFShcWK7yWnO4",
      "question_number": 141,
      "page": 29,
      "question_text": "A company wants to deploy a workload on several hundred Amazon EC2 instances. The company will provision the EC2 instances in an Auto Scaling group by using a launch template.\n\nThe workload will pull files from an Amazon S3 bucket, process the data, and put the results into a different S3 bucket. The EC2 instances must have least-privilege permissions and must use temporary security credentials.\n\nWhich combination of steps will meet these requirements? (Choose two.)",
      "choices": {
        "B": "Update the launch template to include the IAM instance profile.",
        "D": "Create a trust anchor and profile Attach the IAM role to the profile.",
        "A": "Create an IAM role that has the appropriate permissions for S3 buckets Add the IAM role to an instance profile.",
        "E": "Update the launch template Modify the user data to use the new secret key and token.",
        "C": "Create an IAM user that has the appropriate permissions for Amazon S3 Generate a secret key and token."
      },
      "correct_answer": "AB",
      "answer_ET": "AB",
      "answers_community": [
        "AB (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/143725-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-07-11 16:07:00",
      "unix_timestamp": 1720706820,
      "discussion_count": 2,
      "discussion": [
        {
          "upvote_count": "6",
          "timestamp": "1720706820.0",
          "content": "Selected Answer: AB\nA. This step ensures that the EC2 instances have the necessary permissions to access the S3 buckets. The IAM role should have policies attached that allow it to pull files from one S3 bucket and put results into another S3 bucket. By using an instance profile, the role can be associated with the EC2 instances.\nB. This step ensures that the EC2 instances launched by the Auto Scaling group will automatically use the instance profile (and thus the IAM role) with the appropriate permissions. \nC. This approach uses long-term credentials\nD. The term \"trust anchor\" is more relevant to AWS IAM Identity Center (formerly AWS Single Sign-On) or AWS Organizations. It is not directly applicable to setting up permissions for EC2 instances via Auto Scaling.\nE. Storing and using secret keys and tokens in user data scripts is insecure and not recommended.",
          "comment_id": "1246159",
          "poster": "trungtd"
        },
        {
          "timestamp": "1721034960.0",
          "upvote_count": "3",
          "comment_id": "1248211",
          "content": "---> AB",
          "poster": "tgv"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:13:37.650Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "YQCRP4uMxqOQyuAav8MA",
      "question_number": 142,
      "page": 29,
      "question_text": "A company is using AWS CodeDeploy to automate software deployment. The deployment must meet these requirements:\n\n• A number of instances must be available to serve traffic during the deployment. Traffic must be balanced across those instances, and the instances must automatically heal in the event of failure. • A new fleet of instances must be launched for deploying a new revision automatically, with no manual provisioning.\n• Traffic must be rerouted to the new environment to half of the new instances at a time. The deployment should succeed if traffic is rerouted to at least half of the instances: otherwise, it should fail.\n• Before routing traffic to the new fleet of instances, the temporary files generated during the deployment process must be deleted.\n• At the end of a successful deployment, the original instances in the deployment group must be deleted immediately to reduce costs.\n\nHow can a DevOps engineer meet these requirements?",
      "choices": {
        "A": "Use an Application Load Balancer and an in-place deployment. Associate the Auto Scaling group with the deployment group. Use the Automatically copy Auto Scaling group option, and use CodeDeployDefault.OneAtAtime as the deployment configuration. Instruct AWS CodeDeploy to terminate the original instances in the deployment group, and use the AllowTraffic hook within appspec.yml to delete the temporary files.",
        "C": "Use an Application Load Balancer and a blue/green deployment. Associate the Auto Scaling group and the Application Load Balancer target group with the deployment group. Use the Automatically copy Auto Scaling group option, and use CodeDeployDefault.HalfAtAtime as the deployment configuration. Instruct AWS CodeDeploy to terminate the original instances in the deployment group, and use the BeforeAllowTraffic hook within appspec.yml to delete the temporary files.",
        "B": "Use an Application Load Balancer and a blue/green deployment. Associate the Auto Scaling group and Application Load Balancer target group with the deployment group. Use the Automatically copy Auto Scaling group option, create a custom deployment configuration with minimum healthy hosts defined as 50%, and assign the configuration to the deployment group. Instruct AWS CodeDeploy to terminate the original instances in the deployment group, and use the BeforeBlockTraffic hook within appspec.yml to delete the temporary files.",
        "D": "Use an Application Load Balancer and an in-place deployment. Associate the Auto Scaling group and Application Load Balancer target group with the deployment group. Use the Automatically copy Auto Scaling group option, and use CodeDeployDefault.AllatOnce as a deployment configuration. Instruct AWS CodeDeploy to terminate the original instances in the deployment group, and use the BlockTraffic hook within appspec.yml to delete the temporary files."
      },
      "correct_answer": "C",
      "answer_ET": "C",
      "answers_community": [
        "C (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/142991-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-06-27 19:49:00",
      "unix_timestamp": 1719510540,
      "discussion_count": 4,
      "discussion": [
        {
          "content": "Selected Answer: C\nOption C is the best choice as it meets all the given requirements by utilizing a blue/green deployment strategy, correct traffic routing, proper temporary file cleanup timing, and automatic instance termination to manage costs.\n\nOption B is wrong due to: \n- Allows for custom deployment configuration, ensuring at least 50% of the instances must be healthy. However, \"reroute to half\" is somewhat ambiguous, as custom deployment with 50% only ensures half remain healthy but doesn't explicitly guarantee half reroute.\n- Uses BeforeBlockTraffic hook, which is not the correct timing for cleaning up temporary files because it should occur after instance provisioning but before allowing traffic.",
          "timestamp": "1722390000.0",
          "comment_id": "1258491",
          "poster": "jamesf",
          "upvote_count": "3"
        },
        {
          "upvote_count": "1",
          "timestamp": "1721035020.0",
          "content": "---> C",
          "comment_id": "1248212",
          "poster": "tgv"
        },
        {
          "comment_id": "1246377",
          "poster": "trungtd",
          "content": "Selected Answer: C\nAutomatically Copy Auto Scaling Group:\nThis option allows launching a new fleet of instances for each deployment automatically.\n\nCodeDeployDefault.HalfAtAtime Deployment Configuration:\nThis configuration meets the requirement of rerouting traffic to half of the new instances at a time. It ensures that the deployment succeeds if traffic is rerouted to at least half of the instances, otherwise, it fails.",
          "upvote_count": "2",
          "timestamp": "1720742160.0"
        },
        {
          "comment_id": "1240622",
          "upvote_count": "3",
          "poster": "6ef9a08",
          "timestamp": "1719906240.0",
          "content": "Selected Answer: C\nB: blue/green deployment with CodeDeployDefault.HalfAtAtime"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:13:37.650Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "a1zlLEbmmhcEPYiPusNy",
      "question_number": 143,
      "page": 29,
      "question_text": "A company needs to adopt a multi-account strategy to deploy its applications and the associated CI/CD infrastructure. The company has created an organization in AWS Organizations that has all features enabled. The company has configured AWS Control Tower and has set up a landing zone.\n\nThe company needs to use AWS Control Tower controls (guardrails) in all AWS accounts in the organization. The company must create the accounts for a multi-environment application and must ensure that all accounts are configured to an initial baseline.\n\nWhich solution will meet these requirements with the LEAST operational overhead?",
      "choices": {
        "C": "Use Organizations to provision a multi-environment AWS account and a CI/CD account. In the Organizations management account, create an AWS Lambda function that assumes the Organizations access role to apply the baseline configuration to the new accounts.",
        "B": "Use AWS Control Tower Account Factory to provision a dedicated AWS account for each environment and a CI/CD account. Use AWS CloudFormation StackSets to apply the baseline configuration to the new accounts.",
        "A": "Create an AWS Control Tower Account Factory Customization (AFC) blueprint that uses the baseline configuration. Use AWS Control Tower Account Factory to provision a dedicated AWS account for each environment and a CI/CD account by using the blueprint.",
        "D": "Use Organizations to provision a dedicated AWS account for each environment, an audit account, and a CI/CD account. Use AWS CloudFormation StackSets to apply the baseline configuration to the new accounts."
      },
      "correct_answer": "A",
      "answer_ET": "A",
      "answers_community": [
        "A (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/142992-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-06-27 19:53:00",
      "unix_timestamp": 1719510780,
      "discussion_count": 5,
      "discussion": [
        {
          "timestamp": "1724634000.0",
          "comment_id": "1272351",
          "upvote_count": "1",
          "content": "Answer is B\nUse AWS Control Tower Account Factory to provision dedicated AWS accounts for each environment and a CI/CD account. Then, use AWS CloudFormation StackSets to apply the baseline configuration to the new accounts.\nThis approach simplifies account provisioning and ensures consistency across environments while minimizing manual effort.",
          "poster": "limelight04"
        },
        {
          "upvote_count": "4",
          "comment_id": "1258494",
          "poster": "jamesf",
          "timestamp": "1722390240.0",
          "content": "Selected Answer: A\nkeywords: AWS Control Tower Account Factory Customization (AFC)\nOption A is the best choice as it meets all the requirements with the least operational overhead by leveraging AWS Control Tower’s Account Factory and Customization features. This option provides an automated, compliant, and consistent approach to account provisioning and baseline configuration."
        },
        {
          "upvote_count": "1",
          "comment_id": "1248214",
          "timestamp": "1721035020.0",
          "poster": "tgv",
          "content": "---> A"
        },
        {
          "upvote_count": "1",
          "poster": "trungtd",
          "content": "Selected Answer: A\nAll of these options are possible. But A is the LEAST operational overhead",
          "comment_id": "1246383",
          "timestamp": "1720743120.0"
        },
        {
          "timestamp": "1719510780.0",
          "poster": "KaranNishad",
          "content": "Selected Answer: A\nAWS Control Tower Account Factory Customization (AFC)",
          "comment_id": "1238332",
          "upvote_count": "2"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:13:37.650Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "JBogsMHn5St7QYhPsyR3",
      "question_number": 144,
      "page": 29,
      "question_text": "A DevOps team has created a Custom Lambda rule in AWS Config. The rule monitors Amazon Elastic Container Repository (Amazon ECR) policy statements for ecr:* actions. When a noncompliant repository is detected, Amazon EventBridge uses Amazon Simple Notification Service (Amazon SNS) to route the notification to a security team.\n\nWhen the custom AWS Config rule is evaluated, the AWS Lambda function fails to run.\n\nWhich solution will resolve the issue?",
      "choices": {
        "D": "Modify all the ECR repository policies to grant AWS Config access to the necessary ECR API actions.",
        "A": "Modify the Lambda function's resource policy to grant AWS Config permission to invoke the function.",
        "C": "Modify the Lambda function's execution role to include configuration changes for custom AWS Config rules.",
        "B": "Modify the SNS topic policy to include configuration changes for EventBridge to publish to the SNS topic."
      },
      "correct_answer": "A",
      "answer_ET": "A",
      "answers_community": [
        "A (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/143354-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-07-05 13:17:00",
      "unix_timestamp": 1720178220,
      "discussion_count": 6,
      "discussion": [
        {
          "poster": "youonebe",
          "timestamp": "1735229340.0",
          "upvote_count": "1",
          "comment_id": "1331984",
          "content": "Selected Answer: A\nBad wording \"fails to run\" which sounds like to \"fails to execute\", which here it actually means \"failed to invoke\""
        },
        {
          "timestamp": "1724923020.0",
          "comment_id": "1274434",
          "upvote_count": "2",
          "poster": "ThiagoCruzRJ",
          "content": "Selected Answer: A\nWhen you create a custom AWS Config rule that uses a Lambda function, AWS Config needs permission to invoke it. This is done by adding a resource-based policy to the Lambda function that explicitly permits AWS Config to invoke it. Without this permission, AWS Config cannot trigger the Lambda function, leading to the function failing to run."
        },
        {
          "upvote_count": "1",
          "comment_id": "1258496",
          "timestamp": "1722390420.0",
          "poster": "jamesf",
          "content": "Selected Answer: A\nOption A is the best choice to resolve the issue. By modifying the Lambda function's resource policy to grant AWS Config permission to invoke the function, we address the root cause of the invocation failure. This ensures that AWS Config can successfully execute the custom rule using the Lambda function."
        },
        {
          "timestamp": "1721629740.0",
          "content": "Selected Answer: A\nResource policy should allow Config invocation",
          "upvote_count": "2",
          "comment_id": "1252864",
          "poster": "d9iceguy"
        },
        {
          "poster": "amehim",
          "upvote_count": "2",
          "timestamp": "1721562300.0",
          "content": "A. Modify the Lambda function's resource policy to grant AWS Config permission to invoke the function. \n{\n \"Version\": \"2012-10-17\",\n \"Statement\": [\n {\n \"Effect\": \"Allow\",\n \"Principal\": {\n \"Service\": \"config.amazonaws.com\"\n },\n \"Action\": \"lambda:InvokeFunction\",\n \"Resource\": \"arn:aws:lambda:region:account-id:function:function-name\"\n }\n ]\n}",
          "comment_id": "1252410"
        },
        {
          "comment_id": "1248215",
          "poster": "tgv",
          "timestamp": "1721035020.0",
          "upvote_count": "4",
          "content": "---> A"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:13:37.650Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "Teo8ihOO7SKWitk3MCnj",
      "question_number": 145,
      "page": 29,
      "question_text": "A developer is creating a proof of concept for a new software as a service (SaaS) application. The application is in a shared development AWS account that is part of an organization in AWS Organizations.\n\nThe developer needs to create service-linked IAM roles for the AWS services that are being considered for the proof of concept. The solution needs to give the developer the ability to create and configure the service-linked roles only.\n\nWhich solution will meet these requirements?",
      "choices": {
        "D": "Create an IAM role that has the necessary IAM access to allow the developer to create policies and roles. Create and attach a permissions boundary to the role. Grant the developer access to assume the role.",
        "A": "Create an IAM user for the developer in the organization's management account. Configure a cross-account role in the development account for the developer to use. Limit the scope of the cross-account role to common services.",
        "B": "Add the developer to an IAM group. Attach the PowerUserAccess managed policy to the IAM group. Enforce multi-factor authentication (MFA) on the user account.",
        "C": "Add an SCP to the development account in Organizations. Configure the SCP with a Deny rule for iam:* to limit the developer's access."
      },
      "correct_answer": "D",
      "answer_ET": "D",
      "answers_community": [
        "D (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/143876-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-07-13 23:50:00",
      "unix_timestamp": 1720907400,
      "discussion_count": 3,
      "discussion": [
        {
          "poster": "trungtd",
          "content": "Selected Answer: D\nA. This approach involves creating a user in the management account and setting up cross-account roles, which adds unnecessary complexity and potential security risks. \nB. PowerUserAccess managed policy provides broad permissions that go beyond just creating and configuring service-linked roles. This approach does not meet the requirement to restrict the developer's capabilities specifically to service-linked role management.\nC. SCPs are used to set permission guardrails at the organizational or account level, but they do not grant permissions. They are used to restrict actions, and configuring an SCP with a deny rule for iam:* would likely prevent the developer from performing necessary actions\n\nD effectively meets the requirements",
          "upvote_count": "5",
          "timestamp": "1720907400.0",
          "comment_id": "1247504"
        },
        {
          "poster": "tgv",
          "timestamp": "1721035080.0",
          "comment_id": "1248216",
          "content": "---> D",
          "upvote_count": "2"
        },
        {
          "timestamp": "1720935660.0",
          "content": "Selected Answer: D\nD - is more granular since it provides the right balance of granting necessary permissions while maintaining security and following the principle of least privilege. It allows the developer to create and configure service-linked roles as needed for the proof of concept, while the permissions boundary ensures that they can't exceed their intended level of access.",
          "poster": "TEC1",
          "upvote_count": "4",
          "comment_id": "1247628"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:13:37.650Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "0jDlst47k2zzARV8lqEK",
      "question_number": 146,
      "page": 30,
      "question_text": "A company has an AWS CodePipeline pipeline that is configured with an Amazon S3 bucket in the eu-west-1 Region. The pipeline deploys an AWS Lambda application to the same Region. The pipeline consists of an AWS CodeBuild project build action and an AWS CloudFormation deploy action.\nThe CodeBuild project uses the aws cloudformation package AWS CLI command to build an artifact that contains the Lambda function code’s .zip file and the CloudFormation template. The CloudFormation deploy action references the CloudFormation template from the output artifact of the CodeBuild project’s build action.\nThe company wants to also deploy the Lambda application to the us-east-1 Region by using the pipeline in eu-west-1. A DevOps engineer has already updated the CodeBuild project to use the aws cloudformation package command to produce an additional output artifact for us-east-1.\nWhich combination of additional steps should the DevOps engineer take to meet these requirements? (Choose two.)",
      "choices": {
        "C": "Create an S3 bucket in us-east-1. Configure the S3 bucket policy to allow CodePipeline to have read and write access.",
        "B": "Create a new CloudFormation deploy action for us-east-1 in the pipeline. Configure the new deploy action to use the CloudFormation template from the us-east-1 output artifact.",
        "E": "Modify the pipeline to include the S3 bucket for us-east-1 as an artifact store. Create a new CloudFormation deploy action for us-east-1 in the pipeline. Configure the new deploy action to use the CloudFormation template from the us-east-1 output artifact.",
        "D": "Create an S3 bucket in us-east-1. Configure S3 Cross-Region Replication (CRR) from the S3 bucket in eu-west-1 to the S3 bucket in us-east-1.",
        "A": "Modify the CloudFormation template to include a parameter for the Lambda function code’s zip file location. Create a new CloudFormation deploy action for us-east-1 in the pipeline. Configure the new deploy action to pass in the us-east-1 artifact location as a parameter override."
      },
      "correct_answer": "CE",
      "answer_ET": "CE",
      "answers_community": [
        "CE (64%)",
        "AB (19%)",
        "Other"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/105247-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-04-05 05:25:00",
      "unix_timestamp": 1680665100,
      "discussion_count": 35,
      "discussion": [
        {
          "poster": "madperro",
          "comment_id": "919863",
          "upvote_count": "19",
          "content": "Selected Answer: CE\nAs below. You need S# bucket in the new region so C. You need to output artifacts to this new bucket so E.\nhttps://docs.aws.amazon.com/codepipeline/latest/userguide/actions-create-cross-region.html",
          "timestamp": "1686381600.0"
        },
        {
          "poster": "Jonalb",
          "comment_id": "1585227",
          "content": "Selected Answer: AB\nAlternativa A (correta):\nModificar o CloudFormation template para incluir um parâmetro para o zip da Lambda, e passar isso como parâmetro na nova ação para us-east-1.\n\nIsso resolve o problema de apontar para o código .zip armazenado em outro bucket (em outra região).\n\nVocê precisa disso quando o template não foi criado para aceitar esses valores como parâmetros.\n\nAltamente recomendado em casos multi-região.\n✅ CORRETA\n\n✅ Alternativa B (correta):\nCriar uma nova ação de deploy no pipeline para us-east-1 e usar o template do artefato gerado para us-east-1.\n\nIsso é necessário para qualquer deploy multirregional.\n\nCada região precisa de uma ação de deploy específica para ela.\n✅ CORRETA",
          "timestamp": "1752142380.0",
          "upvote_count": "1"
        },
        {
          "content": "Selected Answer: BC\nB and C are the right option",
          "timestamp": "1742764260.0",
          "upvote_count": "2",
          "comment_id": "1406304",
          "poster": "Priyank1912"
        },
        {
          "poster": "steli0",
          "timestamp": "1732479600.0",
          "comment_id": "1317206",
          "content": "Selected Answer: CE\nArtifact bucket is needed in the deployment region",
          "upvote_count": "1"
        },
        {
          "upvote_count": "1",
          "timestamp": "1731859140.0",
          "content": "The Answer is : AB\nTo deploy to a different region, the CloudFormation template should be flexible enough to accept a parameter for the Lambda function’s zip file location. This allows the template to be reused in both regions. The new CloudFormation action in us-east-1 should reference this parameter and pass in the appropriate location of the artifact for that region\nAfter the CodeBuild project outputs artifacts for both eu-west-1 and us-east-1, you need a separate CloudFormation deploy action in the pipeline that targets the us-east-1 region. This action should reference the CloudFormation template from the us-east-1 output artifact produced by the CodeBuild step.",
          "comment_id": "1313644",
          "poster": "Ravi_Bulusu"
        },
        {
          "comment_id": "1254808",
          "timestamp": "1721892720.0",
          "poster": "jamesf",
          "upvote_count": "1",
          "content": "Selected Answer: CE\nCE\nNot D because \"A DevOps engineer has already updated the CodeBuild project to use the aws cloudformation package command to produce an additional output artifact for us-east-1\"\n\nhttps://docs.aws.amazon.com/codepipeline/latest/userguide/actions-create-cross-region.html"
        },
        {
          "poster": "shammous",
          "upvote_count": "1",
          "timestamp": "1721890200.0",
          "content": "C and E are the right answers.",
          "comment_id": "1254779",
          "comments": [
            {
              "timestamp": "1721890260.0",
              "upvote_count": "2",
              "comment_id": "1254781",
              "content": "A: It suggests pointing directly to the Lambda function but we also need the Cloudformation template. So we can rule option A out.\nB: Here, we are missing the new region (us-east-1) artifact store where the new Cloudformation deploys action would store the artifacts upon completion.\nE: Includes the missing part in option B and is the right answer.\nD: \"A DevOps engineer has already updated the CodeBuild project to use the AWS CloudFormation package command to produce an additional output artifact for us-east-1.\" So as we are directly producing the artifact in the S3 bucket in us-east-1, I don't see the point of having a cross-replication.\nC: This option is mandatory as we should provide permissions to services (CodePipeline) to access resources (S3 bucket).",
              "poster": "shammous"
            }
          ]
        },
        {
          "comment_id": "1219712",
          "poster": "ihustle",
          "timestamp": "1716830100.0",
          "comments": [
            {
              "timestamp": "1716830160.0",
              "upvote_count": "1",
              "content": "Apologies, I meant C and E.",
              "poster": "ihustle",
              "comment_id": "1219715"
            }
          ],
          "upvote_count": "1",
          "content": "B and C are the answers. \nThe two important things to note here are the use of AWS CLI and artifacts from two different regions."
        },
        {
          "timestamp": "1708392600.0",
          "content": "Selected Answer: AB\na/b correct\na: the cloudformation template should be modified to incllude a parameter that indicates the location of the.zip file containing the lambda function's cod, this allows the cloudformation deploy action to use the correct artifact depending on the region, this is critical because lambda functions needto reference their code artifacts form the same region they are being deployed in, b. you tould also need to create a new cloudformation deploy action fro the us-east-1 region within the pipelinem this action should be confiured to use the cloudformation template from the artiface that was sepcifically created for us-eat-1",
          "upvote_count": "1",
          "comment_id": "1154405",
          "poster": "kyuhuck"
        },
        {
          "upvote_count": "4",
          "content": "C and E are correct: To achieve the goal. we need an empty S3 in the us-east-1 and create additional stage in the pipline\nA: No mention of S3 - incorrect\nB: no mention of S3 - incoorect\nD: we need an empty S3 to store artifact, Cross-Region Replicate incurs more unnecessary cost. Additionally, this way force the S3 in us-east-1 to be exactly like that of us-west-1, which is incorrect. Each S3 has a different set of artifacts (though they might be very similar)",
          "timestamp": "1706512380.0",
          "poster": "thanhnv142",
          "comment_id": "1134704"
        },
        {
          "timestamp": "1704580680.0",
          "comment_id": "1115481",
          "comments": [
            {
              "content": "\"A DevOps engineer has already updated the CodeBuild project to use the aws cloudformation package command to produce an additional output artifact for us-east-1\"",
              "comment_id": "1248557",
              "upvote_count": "3",
              "poster": "tgv",
              "timestamp": "1721074200.0"
            }
          ],
          "content": "Why C and not D?",
          "upvote_count": "1",
          "poster": "Bans"
        },
        {
          "poster": "DucSiu",
          "timestamp": "1702565940.0",
          "content": "https://docs.aws.amazon.com/codepipeline/latest/userguide/actions-create-cross-region.html#actions-create-cross-region-cfn\nCE is my answers",
          "upvote_count": "1",
          "comment_id": "1096577"
        },
        {
          "content": "For CloudFormation you need to add the Region parameter: https://docs.aws.amazon.com/codepipeline/latest/userguide/actions-create-cross-region.html#actions-create-cross-region-cfn",
          "comment_id": "1073743",
          "poster": "learnwithaniket",
          "upvote_count": "1",
          "timestamp": "1700269200.0"
        },
        {
          "comment_id": "1069254",
          "poster": "robertohyena",
          "content": "Selected Answer: CE\nAnswers: C E\nScenario:\n- We have Pipeline in RegionA (eu-west-1 Region)\n- We have Deploy action in RegionA (eu-west-1 Region)\nRequirements:\n- Need to have Deploy to RegionB (us-east-1 Region)\n- And still use RegionA pipeline above (eu-west-1 Region)",
          "timestamp": "1699875960.0",
          "upvote_count": "3",
          "comments": [
            {
              "comment_id": "1069255",
              "upvote_count": "2",
              "poster": "robertohyena",
              "timestamp": "1699876020.0",
              "content": "Which combination of additional steps to meet requirements:\n- Create bucket in RegionB (us-east-1 Region) [artifact store] This will be the OutputArtifact bucket for Deploy action in RegionB (us-east-1 Region)\n- Add a cross-Region action to a pipeline (CLI)\n-- Described in step #3 \"Modify the pipeline to include the S3 bucket for us-east-1 as an artifact store.\" \n-- Described in step #2 \"Create a new CloudFormation deploy action for us-east-1 in the pipeline.\" and \"Configure the new deploy action to use the CloudFormation template from the us-east-1 output artifact.\"\n\nREF: https://docs.aws.amazon.com/codepipeline/latest/userguide/actions-create-cross-region.html#actions-cross-region-cli\n\nINCORRECT\n- A B cannot be a \"combination of steps\". They both have \"Create a new CloudFormation deploy action for us-east-1 in the pipeline.\"\n- D - we do not need S3 Cross-Region Replication (CRR) in the solution."
            }
          ]
        },
        {
          "timestamp": "1699073160.0",
          "content": "A and B is the answer. Anything related to create S3 is wrong since Code Deploy have ability to automatically share artifacts to other regions",
          "comment_id": "1061900",
          "upvote_count": "1",
          "poster": "2pk",
          "comments": [
            {
              "comment_id": "1101275",
              "content": "we are not using codedeploy",
              "upvote_count": "3",
              "poster": "z_inderjot",
              "timestamp": "1703051280.0"
            }
          ]
        },
        {
          "comment_id": "1060870",
          "upvote_count": "3",
          "timestamp": "1698956880.0",
          "poster": "zain1258",
          "content": "Selected Answer: CE\nC & E are correct options"
        },
        {
          "comment_id": "1060482",
          "content": "Would go for CE",
          "timestamp": "1698922140.0",
          "upvote_count": "1",
          "poster": "denccc"
        },
        {
          "content": "It should be BC! In order to do cross-region deployment, we should create two s3 for each region for ArtifactStore. Then CodeBuild should bundle the sam template and upload to each bucket. Finally CodePipeline should have two separate action for Cloudformation deployment, and each deployment has a region attribute to be defined and needs to pull the template from the ArtifactStore respectively.",
          "poster": "DZ_Ben",
          "timestamp": "1698747180.0",
          "upvote_count": "1",
          "comment_id": "1058673"
        },
        {
          "timestamp": "1697639700.0",
          "upvote_count": "2",
          "comments": [
            {
              "upvote_count": "1",
              "timestamp": "1716231540.0",
              "poster": "Cappy46789",
              "content": "That link also says if you are using Cloudformation or CLI then you have to provide the buckets. So C and E",
              "comment_id": "1214565"
            }
          ],
          "content": "Selected Answer: AB\nC, D and E answers are wrong. CodePipeline automatically creates an S3 bucket in the cross-region for the artifacts. CodePipeline handles the copying of artifacts from one AWS Region to the other Regions when performing cross-region actions.\nhttps://docs.aws.amazon.com/codepipeline/latest/userguide/actions-create-cross-region.html",
          "comment_id": "1047000",
          "poster": "kacsabacsi78"
        },
        {
          "comment_id": "1027735",
          "content": "CE\nyou need to create s3 bucket to set it as output artifact for code build \ncode build can have one source and 5 output artifacts",
          "poster": "Mohammed_Elsabaa",
          "upvote_count": "1",
          "timestamp": "1696742580.0"
        },
        {
          "poster": "Skshitiz",
          "timestamp": "1693080060.0",
          "upvote_count": "2",
          "content": "Selected Answer: CE\nCE seems correct",
          "comment_id": "991020"
        },
        {
          "poster": "vherman",
          "content": "Selected Answer: CE\nCE \nWorks well",
          "comment_id": "968783",
          "upvote_count": "2",
          "timestamp": "1690871460.0"
        },
        {
          "comment_id": "933431",
          "content": "Selected Answer: AB\nThe correct answer is A and B.",
          "upvote_count": "1",
          "timestamp": "1687686240.0",
          "poster": "FunkyFresco"
        },
        {
          "timestamp": "1687589220.0",
          "upvote_count": "1",
          "comment_id": "932273",
          "poster": "Jeb",
          "content": "The correct answer is CE"
        },
        {
          "upvote_count": "1",
          "poster": "tartarus23",
          "timestamp": "1687206240.0",
          "comment_id": "927892",
          "content": "Selected Answer: AB\nExplanation:\n\nA. The CloudFormation template should be modified to include a parameter that indicates the location of the .zip file containing the Lambda function's code. This allows the CloudFormation deploy action to use the correct artifact depending on the region. This is critical because Lambda functions need to reference their code artifacts from the same region they are being deployed in.\n\nB. You would also need to create a new CloudFormation deploy action for the us-east-1 Region within the pipeline. This action should be configured to use the CloudFormation template from the artifact that was specifically created for us-east-1."
        },
        {
          "comments": [
            {
              "upvote_count": "1",
              "poster": "rdoty",
              "comment_id": "911189",
              "timestamp": "1685533860.0",
              "content": "https://docs.aws.amazon.com/codepipeline/latest/userguide/actions-create-cross-region.html#:~:text=You%20must%20have%20created%20the%20following%3A"
            }
          ],
          "poster": "rdoty",
          "upvote_count": "1",
          "timestamp": "1685533860.0",
          "content": "It is CE",
          "comment_id": "911188"
        },
        {
          "comment_id": "907045",
          "upvote_count": "1",
          "comments": [
            {
              "content": "This article specifically says \"When you create or edit a pipeline, you must have an artifact bucket in the pipeline Region and then you must have one artifact bucket per Region where you plan to execute an action. \" Which makes the Artifact S3 bucket in all deploying regions a must.",
              "upvote_count": "1",
              "comment_id": "990902",
              "poster": "Radeeka",
              "timestamp": "1693062780.0"
            }
          ],
          "timestamp": "1685076240.0",
          "content": "Selected Answer: AB\nhttps://docs.aws.amazon.com/codepipeline/latest/userguide/actions-create-cross-region.html\nC D E are not needed when you use the CodePipeline console",
          "poster": "qan1257"
        },
        {
          "upvote_count": "2",
          "comment_id": "896855",
          "content": "C E are correct options as per this AWS link- https://aws.amazon.com/blogs/devops/using-aws-codepipeline-to-perform-multi-region-deployments/\nIt shows that we need one S3 bucket in each region as an Artifact Store.\nThe pipeline should have one deploy action for every region and that will refer the bucket as artifact store for deployment.",
          "poster": "ipsingh",
          "timestamp": "1683992820.0"
        },
        {
          "poster": "ParagSanyashiv",
          "comment_id": "892002",
          "timestamp": "1683540840.0",
          "upvote_count": "1",
          "content": "Selected Answer: AC\nAC makes more sense in this scenario."
        },
        {
          "comment_id": "871149",
          "content": "To deploy the Lambda application to the us-east-1 Region by using the pipeline in eu-west-1, the DevOps engineer should take the following steps:\n\nCreate a new CloudFormation deploy action for us-east-1 in the pipeline. Configure the new deploy action to use the CloudFormation template from the us-east-1 output artifact. (Option B)\nModify the CloudFormation template to include a parameter for the Lambda function code’s zip file location. Create a new CloudFormation deploy action for us-east-1 in the pipeline. Configure the new deploy action to pass in the us-east-1 artifact location as a parameter override. (Option A)\nThese two steps will allow the company to deploy the Lambda application to the us-east-1 Region using the pipeline in eu-west-1",
          "timestamp": "1681578960.0",
          "poster": "alce2020",
          "upvote_count": "1"
        },
        {
          "comment_id": "870369",
          "timestamp": "1681493880.0",
          "upvote_count": "3",
          "content": "Selected Answer: AB\nTo deploy the Lambda application to the us-east-1 region using the existing pipeline in eu-west-1, the DevOps engineer needs to create a new CloudFormation deploy action for us-east-1 and configure it to use the CloudFormation template from the us-east-1 output artifact. Additionally, they need to modify the CloudFormation template to include a parameter for the Lambda function code’s zip file location, and configure the new deploy action to pass in the us-east-1 artifact location as a parameter override.\n\nTherefore, options A and B are both correct. Option C is not necessary because the S3 bucket already exists in the eu-west-1 region. Option D is not necessary since the pipeline only needs to deploy to us-east-1 and not replicate the S3 bucket. Option E is not necessary since the pipeline is already configured to use the S3 bucket in eu-west-1 as an artifact store.",
          "poster": "jqso234"
        },
        {
          "content": "Selected Answer: AC\nWill have to go with A C\nA- Need to update the CF code to point to the bucket dynamically something like this:\nResources:\n MyLambdaFunction:\n Type: AWS::Lambda::Function\n Properties:\n Code:\n S3Bucket: !GetAtt CodeZipFile.BucketName\n S3Key: !GetAtt CodeZipFile.ObjectKey\nC - Need to add access to codepipeline to retrieve artifacts from codebuild build action.\nD - for sure is wrong no need CRR here each execution has its own artifcats.\nB - Missing cloudformation template changes (CF will still point to eu-west-1)\nE - Will always point to us-east-1 it will replace eu-west-1 with us-east-1 (always).",
          "poster": "asfsdfsdf",
          "comment_id": "865706",
          "timestamp": "1681060920.0",
          "upvote_count": "3"
        },
        {
          "poster": "ele",
          "comments": [
            {
              "comment_id": "907035",
              "content": "D is incorrect.\nCodePipeline handles the copying of artifacts from one AWS Region to the other Regions when performing cross-region actions. So CRR is not need.\nhttps://docs.aws.amazon.com/codepipeline/latest/userguide/actions-create-cross-region.html",
              "upvote_count": "1",
              "poster": "qan1257",
              "timestamp": "1685075040.0"
            }
          ],
          "timestamp": "1680935340.0",
          "upvote_count": "3",
          "comment_id": "864467",
          "content": "Selected Answer: AD\nLambda requires the bucket to reside in the same AWS Region as the function, but creating a single template that references a single bucket confines your template to a single Region. Replication will provide the .zip in the us-east-1 region. CFN template will be one, and it must be region-agnostic."
        },
        {
          "comment_id": "863139",
          "upvote_count": "3",
          "poster": "Dimidrol",
          "content": "Selected Answer: CE\nC E for me",
          "timestamp": "1680798000.0"
        },
        {
          "timestamp": "1680665100.0",
          "poster": "lqpO_Oqpl",
          "content": "I think A, E",
          "comment_id": "861727",
          "upvote_count": "3"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:13:48.412Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "LJmJbR8CbaVQZlZQJZGT",
      "question_number": 147,
      "page": 30,
      "question_text": "A company uses AWS Organizations to manage its AWS accounts. The company wants its monitoring system to receive an alert when a root user logs in. The company also needs a dashboard to display any log activity that the root user generates.\n\nWhich combination of steps will meet these requirements? (Choose three.)",
      "choices": {
        "E": "Create an AWS CloudTrail organization trail. Configure the organization trail to send events to Amazon CloudWatch Logs.",
        "C": "Create an Amazon CloudWatch Logs metric filter to match root user login events. Configure a CloudWatch alarm and an Amazon Simple Notification Service (Amazon SNS) topic to send alerts to the company's monitoring system.",
        "A": "Enable AWS Config with a multi-account aggregator. Configure log forwarding to Amazon CloudWatch Logs.",
        "B": "Create an Amazon QuickSight dashboard that uses an Amazon CloudWatch Logs query.",
        "D": "Create an Amazon CloudWatch Logs subscription filter to match root user login events. Configure the filter to forward events to an Amazon Simple Notification Service (Amazon SNS) topic. Configure the SNS topic to send alerts to the company's monitoring system.",
        "F": "Create an Amazon CloudWatch dashboard that uses a CloudWatch Logs Insights query."
      },
      "correct_answer": "CEF",
      "answer_ET": "CEF",
      "answers_community": [
        "CEF (95%)",
        "5%"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/142993-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-06-27 20:00:00",
      "unix_timestamp": 1719511200,
      "discussion_count": 6,
      "discussion": [
        {
          "content": "Selected Answer: CEF\nCorrect answer.",
          "upvote_count": "6",
          "comment_id": "1238339",
          "poster": "KaranNishad",
          "timestamp": "1719511200.0"
        },
        {
          "comment_id": "1368543",
          "timestamp": "1741509720.0",
          "content": "Selected Answer: DEF\nI think “C” is better than “D”.\nThe filter pattern can identify the root user's login event and include the content of that event in the SNS.\nWith “C” notification, we only know that the root user logged in.",
          "poster": "asimohat",
          "upvote_count": "1"
        },
        {
          "timestamp": "1722390900.0",
          "upvote_count": "4",
          "poster": "jamesf",
          "comment_id": "1258500",
          "content": "Selected Answer: CEF\nOption E: Use CloudTrail to capture and forward root user activities.\nOption C: Set up metric filters and alarms to alert on root user login events.\nOption F: Create a CloudWatch dashboard for visualizing root user activities.\n\nAdditional Note:\nCloudWatch Logs Subscription Filter:\n- Real-time processing of log events, but typically used for streaming log data to other services like AWS Lambda or Elasticsearch.\n- Not necessary for the specific task of alerting on root user login events.\n\nAWS Config is not directly relevant to capturing and forwarding root user login events to CloudWatch Logs."
        },
        {
          "content": "---> CEF",
          "comment_id": "1248217",
          "timestamp": "1721035200.0",
          "upvote_count": "2",
          "poster": "tgv"
        },
        {
          "content": "Selected Answer: CEF\nE- AWS CloudTrail will log all activities, including root user logins, across all accounts in the organisation. Sending these logs to CloudWatch Logs enables further processing and analysis.\n\nC- Creating a metric filter to detect root user login events will allow you to trigger a CloudWatch alarm. The alarm can then send notifications via SNS to the company's monitoring system, ensuring real-time alerts for root user logins.\n\nF- Using CloudWatch Logs Insights, you can create queries to extract and visualise log data related to root user activity. This data can be displayed on a CloudWatch dashboard, providing a centralised view of root user actions.",
          "comment_id": "1247630",
          "upvote_count": "4",
          "timestamp": "1720936080.0",
          "poster": "TEC1"
        },
        {
          "poster": "trungtd",
          "comment_id": "1247506",
          "content": "Selected Answer: CEF\nE first, then C, and the last is F\n\nE ensures that all events, including root user login events, are captured across all accounts in the organization. By sending these events to CloudWatch Logs, you centralize the logging data, making it accessible for further processing.\nC creating a metric filter in CloudWatch Logs to detect specific patterns in the log data, such as root user login events. \nF creating a CloudWatch dashboard that utilizes CloudWatch Logs Insights to query and visualize the log data. This dashboard can be used to display detailed information about root user login activity and other relevant log events.",
          "upvote_count": "4",
          "timestamp": "1720907760.0"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:13:48.412Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "bL140Q8lkM2rlxiIBAGm",
      "question_number": 148,
      "page": 30,
      "question_text": "A company uses AWS Organizations to manage its AWS accounts. A DevOps engineer must ensure that all users who access the AWS Management Console are authenticated through the company’s corporate identity provider (IdP).\n\nWhich combination of steps will meet these requirements? (Choose two.)",
      "choices": {
        "D": "Create IAM groups in the Organizations management account to apply consistent permissions for all IAM users.",
        "E": "Create an SCP in Organizations to deny password creation for IAM users.",
        "A": "Use Amazon GuardDuty with a delegated administrator account Use GuardDuty to enforce denial of IAM user logins.",
        "C": "Create a permissions boundary in AWS IAM Identity Center to deny password logins for IAM users.",
        "B": "Use AWS IAM Identity Center to configure identity federation with SAML 2.0."
      },
      "correct_answer": "BE",
      "answer_ET": "BE",
      "answers_community": [
        "BE (83%)",
        "BC (17%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/142994-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-06-27 20:12:00",
      "unix_timestamp": 1719511920,
      "discussion_count": 5,
      "discussion": [
        {
          "comment_id": "1272357",
          "content": "Selected Answer: BC\nUse AWS IAM Identity Center to configure identity federation with SAML 2.0:\nConfigure SAML-based federation between your corporate IdP and AWS IAM.\nThis allows users to authenticate via your corporate identity provider when accessing the AWS Management Console.\n\nCreate a permissions boundary in AWS IAM Identity Center:\nSet up a permissions boundary to deny password logins for IAM users.\nThis ensures that users must authenticate through the corporate IdP rather than using IAM user credentials.",
          "upvote_count": "2",
          "timestamp": "1724635380.0",
          "poster": "limelight04"
        },
        {
          "upvote_count": "3",
          "comment_id": "1258502",
          "poster": "jamesf",
          "content": "Selected Answer: BE\nOption B: Configure identity federation with SAML 2.0 using AWS IAM Identity Center.\nOption E: Implement an SCP to deny password creation for IAM users, enforcing IdP authentication.\n\nIncorrect for C - Permissions Boundaries \n- Permissions boundaries in AWS IAM Identity Center define the maximum permissions an IAM entity can have but are not used to control login methods or deny password logins.\n- Permissions boundaries do not restrict authentication methods or enforce federation.\n- Permissions boundaries are not applicable for denying IAM user logins.",
          "timestamp": "1722391260.0"
        },
        {
          "timestamp": "1721035260.0",
          "poster": "tgv",
          "content": "---> BE",
          "upvote_count": "3",
          "comment_id": "1248218"
        },
        {
          "upvote_count": "3",
          "content": "Selected Answer: BE\nof course B.\nE enforce that users cannot log in directly with IAM credentials. Instead, they must use the SSO setup provided by AWS IAM Identity Center, ensuring compliance with the requirement to authenticate through the corporate IdP.",
          "comment_id": "1247509",
          "poster": "trungtd",
          "timestamp": "1720908060.0"
        },
        {
          "upvote_count": "4",
          "comment_id": "1238342",
          "content": "Selected Answer: BE\nBE is answer\n{\n \"Version\": \"2012-10-17\",\n \"Statement\": [\n {\n \"Effect\": \"Deny\",\n \"Action\": [\n \"iam:CreateLoginProfile\",\n \"iam:UpdateLoginProfile\"\n ],\n \"Resource\": \"*\"\n }\n ]\n}",
          "poster": "KaranNishad",
          "timestamp": "1719511920.0"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:13:48.412Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "mEtDR6H4HxEs8O5HWgNT",
      "question_number": 149,
      "page": 30,
      "question_text": "A company has deployed a new platform that runs on Amazon Elastic Kubernetes Service (Amazon EKS). The new platform hosts web applications that users frequently update. The application developers build the Docker images for the applications and deploy the Docker images manually to the platform.\n\nThe platform usage has increased to more than 500 users every day. Frequent updates, building the updated Docker images for the applications, and deploying the Docker images on the platform manually have all become difficult to manage.\n\nThe company needs to receive an Amazon Simple Notification Service (Amazon SNS) notification if Docker image scanning returns any HIGH or CRITICAL findings for operating system or programming language package vulnerabilities.\n\nWhich combination of steps will meet these requirements? (Choose two.)",
      "choices": {
        "D": "Create an AWS CodeBuild project that builds the Docker images and stores the Docker images in an Amazon Elastic Container Registry (Amazon ECR) repository. Turn on enhanced scanning for the ECR repository. Create an Amazon EventBridge rule that monitors ECR image scan events. Configure the EventBridge rule to send an event to an SNS topic when the finding-severity-counts parameter is more than 0 at a CRITICAL or HIGH level.",
        "A": "Create an AWS CodeCommit repository to store the Dockerfile and Kubernetes deployment files. Create a pipeline in AWS CodePipeline. Use an Amazon S3 event to invoke the pipeline when a newer version of the Dockerfile is committed. Add a step to the pipeline to initiate the AWS CodeBuild project.",
        "B": "Create an AWS CodeCommit repository to store the Dockerfile and Kubernetes deployment files. Create a pipeline in AWS CodePipeline. Use an Amazon EventBridge event to invoke the pipeline when a newer version of the Dockerfile is committed. Add a step to the pipeline to initiate the AWS CodeBuild project.",
        "C": "Create an AWS CodeBuild project that builds the Docker images and stores the Docker images in an Amazon Elastic Container Registry (Amazon ECR) repository. Turn on basic scanning for the ECR repository. Create an Amazon EventBridge rule that monitors Amazon GuardDuty events. Configure the EventBridge rule to send an event to an SNS topic when the finding-severity-counts parameter is more than 0 at a CRITICAL or HIGH level.",
        "E": "Create an AWS CodeBuild project that scans the Dockerfile. Configure the project to build the Docker images and store the Docker images in an Amazon Elastic Container Registry (Amazon ECR) repository if the scan is successful. Configure an SNS topic to provide notification if the scan returns any vulnerabilities."
      },
      "correct_answer": "BD",
      "answer_ET": "BD",
      "answers_community": [
        "BD (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/143368-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-07-05 16:53:00",
      "unix_timestamp": 1720191180,
      "discussion_count": 6,
      "discussion": [
        {
          "comment_id": "1247516",
          "poster": "trungtd",
          "upvote_count": "5",
          "timestamp": "1720908900.0",
          "content": "Selected Answer: BD\nB sets up a CI/CD pipeline with AWS CodePipeline triggered by changes in the AWS CodeCommit repository. Using Amazon EventBridge ensures that the pipeline is invoked whenever there is a new commit, automating the build and deployment process.\n\nD ensures that Docker images are built and pushed to ECR, where enhanced scanning is enabled. Enhanced scanning provides detailed vulnerability information. An EventBridge rule is configured to monitor scan events and trigger notifications via SNS when HIGH or CRITICAL vulnerabilities are found."
        },
        {
          "comment_id": "1272361",
          "poster": "limelight04",
          "upvote_count": "2",
          "timestamp": "1724635740.0",
          "content": "Selected Answer: BD\nThe answer is BD"
        },
        {
          "timestamp": "1722391680.0",
          "comment_id": "1258508",
          "upvote_count": "4",
          "content": "Selected Answer: BD\nOption B: \n- AWS CodeCommit repository to store the Dockerfile and Kubernetes deployment files. \n- Amazon EventBridge event to invoke the pipeline when a newer version of the Dockerfile is committed. \n\nOption D: \n- AWS CodeBuild project that builds the Docker images and stores the Docker images in an Amazon Elastic Container Registry (Amazon ECR) repository.\n- enhanced scanning for the ECR repository.",
          "poster": "jamesf"
        },
        {
          "timestamp": "1721035380.0",
          "upvote_count": "2",
          "content": "---> BD",
          "comment_id": "1248219",
          "poster": "tgv"
        },
        {
          "comment_id": "1244457",
          "poster": "inturist",
          "upvote_count": "3",
          "timestamp": "1720454760.0",
          "content": "Selected Answer: BD\nAgree with B,D"
        },
        {
          "poster": "tgv",
          "content": "--> B D",
          "comment_id": "1243954",
          "upvote_count": "2",
          "timestamp": "1720375800.0"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:13:48.412Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "VwHLddly4R22NFltHMMG",
      "question_number": 150,
      "page": 30,
      "question_text": "A company groups its AWS accounts in OUs in an organization in AWS Organizations. The company has deployed a set of Amazon API Gateway APIs in one of the Organizations accounts. The APIs are bound to the account's VPC and have no existing authentication mechanism. Only principals in a specific OU can have permissions to invoke the APIs.\n\nThe company applies the following policy to the API Gateway interface VPC endpoint:\n\n//IMG//\n\n\nThe company also updates the API Gateway resource policies to deny invocations that do not come through the interface VPC endpoint. After the updates, the following error message appears during attempts to use the interface VPC endpoint URL to invoke an API: \"User: anonymous is not authorized.\"\n\nWhich combination of steps will solve this problem? (Choose two.)",
      "choices": {
        "A": "Enable IAM authentication on all API methods by setting AWS JAM as the authorization method.",
        "B": "Create a token-based AWS Lambda authorizer that passes the caller's identity in a bearer token.",
        "E": "Verify the identity of the requester by using Signature Version 4 to sign client requests by using AWS credentials.",
        "C": "Create a request parameter-based AWS Lambda authorizer that passes the caller's identity in a combination of headers, query string parameters, stage variables, and $cortext variables.",
        "D": "Use Amazon Cognito user pools as the authorizer to control access to the API."
      },
      "correct_answer": "AE",
      "answer_ET": "AE",
      "answers_community": [
        "AE (95%)",
        "5%"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/143365-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [
        "https://img.examtopics.com/aws-certified-devops-engineer-professional-dop-c02/image21.png"
      ],
      "answer_images": [],
      "timestamp": "2024-07-05 16:23:00",
      "unix_timestamp": 1720189380,
      "discussion_count": 7,
      "discussion": [
        {
          "poster": "jamesf",
          "comment_id": "1258509",
          "content": "Selected Answer: AE\nHope is Typo for the Option A, AWS JAM = AWS IAM\n\nOption A. Enable IAM authentication on all API methods by setting AWS IAM as the authorization method.\n- This ensures that all requests to the API must be authenticated using IAM credentials, directly addressing the anonymous access issue.\n\nOption E. Verify the identity of the requester by using Signature Version 4 to sign client requests by using AWS credentials.\n- By using AWS Signature Version 4, requests are authenticated, ensuring they are authorized according to IAM policies linked to the specific Organizational Unit.",
          "upvote_count": "6",
          "timestamp": "1722392040.0"
        },
        {
          "upvote_count": "5",
          "poster": "d9iceguy",
          "timestamp": "1721647200.0",
          "content": "Selected Answer: AE\nJAM= IAM",
          "comment_id": "1253035"
        },
        {
          "content": "Selected Answer: AE\nThis is for requests from Interface VPC endpoints, which means all principals are internal and have aws identities. \nBCD are all for external request control in general.",
          "comment_id": "1331999",
          "poster": "youonebe",
          "upvote_count": "3",
          "timestamp": "1735230780.0"
        },
        {
          "poster": "limelight04",
          "upvote_count": "1",
          "comment_id": "1272369",
          "content": "Selected Answer: AB\nOption A\nEnable IAM authentication on all API methods:\nSet AWS IAM as the authorization method for all API methods.\nThis ensures that authentication is required for invoking the APIs1.\n\nOption B\nCreate a token-based AWS Lambda authorizer:\nImplement a custom Lambda authorizer that validates bearer tokens.\nPass the caller’s identity in the token to authorize API requests",
          "timestamp": "1724636700.0"
        },
        {
          "content": "Selected Answer: AE\nYou can enable IAM authorization for HTTP API routes. When IAM authorization is enabled, clients must use Signature Version 4 (SigV4) to sign their requests with AWS credentials. API Gateway invokes your API route only if the client has execute-api permission for the route.",
          "poster": "GripZA",
          "timestamp": "1724072280.0",
          "comment_id": "1268681",
          "upvote_count": "4"
        },
        {
          "poster": "tgv",
          "comment_id": "1248224",
          "content": "---> A E (assuming there's a typo in AWS JAM)\nIf there's no typo in AWS JAM, I'd go for B & E",
          "upvote_count": "3",
          "timestamp": "1721036280.0"
        },
        {
          "timestamp": "1720826040.0",
          "content": "Anser:B,E",
          "upvote_count": "1",
          "poster": "komorebi",
          "comment_id": "1247043"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:13:48.412Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "0yNkySuTtYCavqANhXiv",
      "question_number": 151,
      "page": 31,
      "question_text": "A company wants to decrease the time it takes to develop new features. The company uses AWS CodeBuild and AWS CodeDeploy to build and deploy its applications. The company uses AWS CodePipeline to deploy each microservice with its own CI/CD pipeline.\n\nThe company needs more visibility into the average time between the release of new features and the average time to recover after a failed deployment.\n\nWhich solution will provide this visibility with the LEAST configuration effort?",
      "choices": {
        "C": "Program an AWS Lambda function that writes information about successful runs and failed runs to Amazon DynamoDB. Create an Amazon EventBridge rule to invoke the Lambda function after every successful run and after every failed run. Build an Amazon QuickSight dashboard to show the information from DynamoDB.",
        "B": "Program an AWS Lambda function that creates Amazon CloudWatch custom metrics with information about successful runs and failed runs for each pipeline. Create an Amazon EventBridge rule to invoke the Lambda function after every successful run and after every failed run. Use the metrics to build a CloudWatch dashboard.",
        "D": "Program an AWS Lambda function that writes information about successful runs and failed runs to Amazon DynamoDB. Create an Amazon EventBridge rule to invoke the Lambda function every 5 minutes. Build an Amazon QuickSight dashboard to show the information from DynamoDB.",
        "A": "Program an AWS Lambda function that creates Amazon CloudWatch custom metrics with information about successful runs and failed runs for each pipeline. Create an Amazon EventBridge rule to invoke the Lambda function every 5 minutes. Use the metrics to build a CloudWatch dashboard."
      },
      "correct_answer": "B",
      "answer_ET": "B",
      "answers_community": [
        "B (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/143877-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-07-14 00:53:00",
      "unix_timestamp": 1720911180,
      "discussion_count": 4,
      "discussion": [
        {
          "content": "Selected Answer: B\nB is the correct answer",
          "poster": "limelight04",
          "timestamp": "1724636940.0",
          "comment_id": "1272370",
          "upvote_count": "3"
        },
        {
          "upvote_count": "3",
          "timestamp": "1722393660.0",
          "content": "Selected Answer: B\nB is most simple and direct with LEAST configuration effort.",
          "poster": "jamesf",
          "comment_id": "1258518"
        },
        {
          "content": "---> B",
          "upvote_count": "2",
          "timestamp": "1721036460.0",
          "poster": "tgv",
          "comment_id": "1248228"
        },
        {
          "timestamp": "1720911180.0",
          "poster": "trungtd",
          "content": "Selected Answer: B\nA. Invoking the Lambda function every 5 minutes is less efficient compared to event-driven invocation\nB. provides the needed visibility with minimal configuration effort\nC & D. Using DynamoDB and QuickSight involves more configuration",
          "upvote_count": "4",
          "comment_id": "1247536"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:13:58.848Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "nfaeQqUHF87erg32jZ4H",
      "question_number": 152,
      "page": 31,
      "question_text": "A company has developed a static website hosted on an Amazon S3 bucket. The website is deployed using AWS CloudFormation. The CloudFormation template defines an S3 bucket and a custom resource that copies content into the bucket from a source location.\n\nThe company has decided that it needs to move the website to a new location, so the existing CloudFormation stack must be deleted and re-created. However, CloudFormation reports that the stack could not be deleted cleanly.\n\nWhat is the MOST likely cause and how can the DevOps engineer mitigate this problem for this and future versions of the website?",
      "choices": {
        "C": "Deletion has failed because the custom resource does not define a deletion policy. Add a DeletionPolicy property to the custom resource definition with a value of RemoveOnDeletion.",
        "B": "Deletion has failed because the S3 bucket is not empty. Modify the custom resource's AWS Lambda function code to recursively empty the bucket when RequestType is Delete.",
        "D": "Deletion has failed because the S3 bucket is not empty. Modify the S3 bucket resource in the CloudFormation template to add a DeletionPolicy property with a value of Empty.",
        "A": "Deletion has failed because the S3 bucket has an active website configuration. Modify the CloudFormation template to remove the WebsiteConfiguration property from the S3 bucket resource."
      },
      "correct_answer": "B",
      "answer_ET": "B",
      "answers_community": [
        "B (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/143364-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-07-05 16:09:00",
      "unix_timestamp": 1720188540,
      "discussion_count": 6,
      "discussion": [
        {
          "timestamp": "1735231500.0",
          "upvote_count": "2",
          "comment_id": "1332005",
          "content": "Selected Answer: B\nYou can only delete empty buckets. Deletion fails for buckets that have contents.\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-s3-bucket.html",
          "poster": "youonebe"
        },
        {
          "poster": "GripZA",
          "content": "Selected Answer: B\nBy default cloudformation can't delete an S3 bucket that's not empty. If the bucket still contains objects when the stack deletion is attempted, the stack deletion will fail. \n\nAlthough the Q doesn't specify that the custom resource uses Lambda. I think it's safe to assume here that since a custom resource is responsible for copying content into the bucket, it can also be used to handle the cleanup process.",
          "comment_id": "1268692",
          "timestamp": "1724073060.0",
          "upvote_count": "3"
        },
        {
          "upvote_count": "2",
          "poster": "tgv",
          "content": "---> B",
          "comment_id": "1248230",
          "timestamp": "1721036580.0"
        },
        {
          "content": "Selected Answer: B\nof course B",
          "timestamp": "1720911300.0",
          "comment_id": "1247537",
          "poster": "trungtd",
          "upvote_count": "3"
        },
        {
          "timestamp": "1720778340.0",
          "poster": "inturist",
          "content": "Selected Answer: B\nAgree B",
          "upvote_count": "3",
          "comment_id": "1246631"
        },
        {
          "poster": "siheom",
          "timestamp": "1720747020.0",
          "upvote_count": "3",
          "comment_id": "1246408",
          "content": "Selected Answer: B\nDefinitely B"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:13:58.848Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "11Q0OqDQDCkWurAJhPhw",
      "question_number": 153,
      "page": 31,
      "question_text": "A company uses Amazon EC2 as its primary compute platform. A DevOps team wants to audit the company's EC2 instances to check whether any prohibited applications have been installed on the EC2 instances.\n\nWhich solution will meet these requirements with the MOST operational efficiency?",
      "choices": {
        "D": "Designate Amazon CloudWatch Logs as the log destination for all application instances. Run an automated script across all instances to create an inventory of installed applications. Configure the script to forward the results to CloudWatch Logs. Create a CloudWatch alarm that uses filter patterns to search log data to identify prohibited applications.",
        "C": "Configure AWS Systems Manager on each instance. Use Systems Manager Inventory. Filter a trail in AWS CloudTrail for Systems Manager Inventory events to identify prohibited applications.",
        "A": "Configure AWS Systems Manager on each instance. Use AWS Systems Manager Inventory. Use Systems Manager resource data sync to synchronize and store findings in an Amazon S3 bucket. Create an AWS Lambda function that runs when new objects are added to the S3 bucket. Configure the Lambda function to identify prohibited applications.",
        "B": "Configure AWS Systems Manager on each instance. Use Systems Manager Inventory Create AWS Config rules that monitor changes from Systems Manager Inventory to identify prohibited applications."
      },
      "correct_answer": "B",
      "answer_ET": "B",
      "answers_community": [
        "B (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/143475-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-07-06 22:32:00",
      "unix_timestamp": 1720297920,
      "discussion_count": 5,
      "discussion": [
        {
          "content": "Selected Answer: B\nkeywords: AWS Systems Manage, Systems Manager Inventory, AWS Config rule",
          "upvote_count": "5",
          "poster": "jamesf",
          "timestamp": "1722824100.0",
          "comment_id": "1260898"
        },
        {
          "content": "Selected Answer: B\nAWS Systems Manager Inventory integrates with AWS Config to record inventory data for historical views, change tracking, or auditing. When you use AWS Config recording for systems inventory data you can enable scenarios such as tracking newly installed or removed software applications, assessing security risks, troubleshooting, and tracking license usage. Additionally, you can create AWS Config Rules to define compliance rules based on inventory data (such as, detecting a blacklisted application) and take remediation action (such as sending email notifications or running an AWS Lambda function to uninstall the application) automatically.",
          "poster": "GripZA",
          "timestamp": "1745251140.0",
          "upvote_count": "1",
          "comment_id": "1562504"
        },
        {
          "timestamp": "1724637300.0",
          "content": "Selected Answer: B\nOption B: Configure AWS Systems Manager on each instance. Use Systems Manager Inventory and create AWS Config rules that monitor changes from Systems Manager Inventory to identify prohibited applications.\nThis approach leverages Systems Manager Inventory and AWS Config to efficiently track and identify prohibited applications while minimizing operational overhead.",
          "upvote_count": "3",
          "poster": "limelight04",
          "comment_id": "1272371"
        },
        {
          "timestamp": "1721036820.0",
          "comment_id": "1248232",
          "upvote_count": "2",
          "content": "---> B",
          "poster": "tgv"
        },
        {
          "poster": "getadroit",
          "comment_id": "1243548",
          "timestamp": "1720297920.0",
          "content": "B\nhttps://aws.amazon.com/blogs/mt/preventing-blacklisted-applications-with-aws-systems-manager-and-aws-config/",
          "upvote_count": "4"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:13:58.848Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "CbA17Ww4k2ecBIRHmPQm",
      "question_number": 154,
      "page": 31,
      "question_text": "A company has an event-driven JavaScript application. The application uses decoupled AWS managed services that publish, consume, and route events. During application testing, events are not delivered to the target that is specified by an Amazon EventBridge rule.\n\nA DevOps team must provide application testers with additional functionality to view, troubleshoot, and prevent the loss of events without redeployment of the application.\n\nWhich combination of steps should the DevOps team take to meet these requirements? (Choose three.)",
      "choices": {
        "F": "Update the application code base to use the AWS X-Ray SDK tracing feature to instrument the code with support for the X-Amzn-Trace-Id header.",
        "D": "Configure the EventBridge rule to use an Amazon Simple Queue Service (Amazon SQS) FIFO queue as a dead-letter queue.",
        "A": "Launch AWS Device Farm with a standard test environment and project to run a specific build of the application.",
        "C": "Configure the EventBridge rule to use an Amazon Simple Queue Service (Amazon SQS) standard queue as a dead-letter queue.",
        "E": "Create a log group in Amazon CloudWatch Logs Specify the log group as an additional target of the EventBridge rule.",
        "B": "Create an Amazon S3 bucket. Enable AWS CloudTrail. Create a CloudTrail trail that specifies the S3 bucket as the storage location."
      },
      "correct_answer": "BCE",
      "answer_ET": "BCE",
      "answers_community": [
        "BCE (74%)",
        "CEF (21%)",
        "5%"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/143363-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-07-05 16:06:00",
      "unix_timestamp": 1720188360,
      "discussion_count": 10,
      "discussion": [
        {
          "content": "Selected Answer: BCE\nF is incorrect because it invites an app redeployment with SDK being used to change code base! So BCE are the correct options",
          "comment_id": "1327249",
          "upvote_count": "3",
          "timestamp": "1734342060.0",
          "poster": "RajAWSDevOps007"
        },
        {
          "content": "Selected Answer: BCE\nKeywords: without redeployment of the application.\nOption B: Enabling CloudTrail will allow testers to track event activities and interactions with AWS services, aiding in troubleshooting.\nOption C: Using a standard SQS queue as a DLQ ensures that failed events are captured and can be analyzed or retried.\nOption E: Adding CloudWatch Logs as a target provides immediate logging of event processing, aiding in real-time monitoring and troubleshooting.",
          "upvote_count": "4",
          "timestamp": "1722401100.0",
          "poster": "jamesf",
          "comment_id": "1258565"
        },
        {
          "comment_id": "1253231",
          "timestamp": "1721674020.0",
          "upvote_count": "2",
          "poster": "TEC1",
          "content": "Selected Answer: CEF\nI will go with CEF"
        },
        {
          "content": "CEF, X-ray enables you to debug distributed applications to troubleshoot the root cause of performance issues and errors",
          "upvote_count": "1",
          "comment_id": "1250696",
          "poster": "dalieba",
          "timestamp": "1721338800.0"
        },
        {
          "timestamp": "1721196240.0",
          "comment_id": "1249414",
          "comments": [
            {
              "timestamp": "1730953320.0",
              "upvote_count": "1",
              "poster": "VerRi",
              "comment_id": "1308228",
              "content": "GJ bro"
            }
          ],
          "content": "Selected Answer: BCE\nwithout redeployment of the application.\n\nB,C,E",
          "poster": "noisonnoiton",
          "upvote_count": "4"
        },
        {
          "poster": "tgv",
          "content": "---> CEF\nI thought E its not possible, but --> https://repost.aws/knowledge-center/cloudwatch-log-group-eventbridge",
          "upvote_count": "1",
          "comments": [
            {
              "poster": "tgv",
              "comment_id": "1250305",
              "upvote_count": "2",
              "timestamp": "1721298720.0",
              "content": "good catch with \"without redeployment of the application.\"\nB, C, E"
            }
          ],
          "timestamp": "1721038560.0",
          "comment_id": "1248250"
        },
        {
          "comment_id": "1247616",
          "timestamp": "1720933020.0",
          "poster": "trungtd",
          "upvote_count": "1",
          "content": "Selected Answer: CEF\nC allows you to capture events that could not be delivered to the specified target\nE capture detailed logs of the events that are processed\nF end-to-end tracing capabilities\n\nB is wrong because while CloudTrail provides logging for AWS API calls, it is not specifically designed for capturing and troubleshooting event flows in EventBridge"
        },
        {
          "upvote_count": "3",
          "poster": "siheom",
          "content": "Selected Answer: BCE\nBCE...",
          "comment_id": "1246407",
          "timestamp": "1720746960.0"
        },
        {
          "timestamp": "1720188420.0",
          "upvote_count": "1",
          "content": "Selected Answer: CEF\nCEF\nC > D - because SQS FIFO is not supported by Dead Letter Queues in event bridge",
          "comment_id": "1242822",
          "poster": "xdkonorek2"
        },
        {
          "upvote_count": "1",
          "poster": "xdkonorek2",
          "comment_id": "1242820",
          "content": "Selected Answer: CDF\nCDF\nC > D - because SQS FIFO is not supported by Dead Letter Queues in event bridge",
          "timestamp": "1720188360.0"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:13:58.848Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "vorpcWGyBwXSFznevpGZ",
      "question_number": 155,
      "page": 31,
      "question_text": "A company is migrating its container-based workloads to an AWS Organizations multi-account environment. The environment consists of application workload accounts that the company uses to deploy and run the containerized workloads. The company has also provisioned a shared services account for shared workloads in the organization.\n\nThe company must follow strict compliance regulations. All container images must receive security scanning before they are deployed to any environment. Images can be consumed by downstream deployment mechanisms after the images pass a scan with no critical vulnerabilities. Pre-scan and post-scan images must be isolated from one another so that a deployment can never use pre-scan images.\n\nA DevOps engineer needs to create a strategy to centralize this process.\n\nWhich combination of steps will meet these requirements with the LEAST administrative overhead? (Choose two.)",
      "choices": {
        "E": "Create an AWS Lambda function. Create an Amazon EventBridge rule that reacts to image scanning completed events and invokes the Lambda function. Write function code that determines the image scanning status and pushes images without critical vulnerabilities to the post-scan repositories.",
        "C": "Configure image replication for each image from the image's pre-scan repository to the image's post-scan repository.",
        "A": "Create Amazon Elastic Container Registry (Amazon ECR) repositories in the shared services account: one repository for each pre-scan image and one repository for each post-scan image. Configure Amazon ECR image scanning to run on new image pushes to the pre-scan repositories. Use resource-based policies to grant the organization write access to the pre-scan repositories and read access to the post-scan repositories.",
        "D": "Create a pipeline in AWS CodePipeline for each pre-scan repository. Create a source stage that runs when new images are pushed to the pre-scan repositories. Create a stage that uses AWS CodeBuild as the action provider. Write a buildspec.yaml definition that determines the image scanning status and pushes images without critical vulnerabilities to the post-scan repositories.",
        "B": "Create pre-scan Amazon Elastic Container Registry (Amazon ECR) repositories in each account that publishes container images. Create repositories for post-scan images in the shared services account. Configure Amazon ECR image scanning to run on new image pushes to the pre-scan repositories. Use resource-based policies to grant the organization read access to the post-scan repositories."
      },
      "correct_answer": "AE",
      "answer_ET": "AE",
      "answers_community": [
        "AE (58%)",
        "AD (42%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/143361-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-07-05 15:47:00",
      "unix_timestamp": 1720187220,
      "discussion_count": 9,
      "discussion": [
        {
          "content": "Selected Answer: AE\nLEAST administrative overhead:\n=> Should create ECR repositories in the shared services account => A\nAnd should create only 1 Lambda function => E\n\nD wrong because it involves creating and managing multiple pipelines, which increases administrative overhead significantly",
          "poster": "trungtd",
          "upvote_count": "5",
          "timestamp": "1720940820.0",
          "comment_id": "1247658"
        },
        {
          "poster": "jojewi8143",
          "timestamp": "1738410780.0",
          "upvote_count": "1",
          "comment_id": "1349848",
          "content": "Selected Answer: AE\nAE because lambda"
        },
        {
          "poster": "aws_god",
          "content": "Selected Answer: AD\nLambda is not meant to work with Docker",
          "comment_id": "1283024",
          "timestamp": "1726205160.0",
          "upvote_count": "4"
        },
        {
          "timestamp": "1724638320.0",
          "poster": "limelight04",
          "upvote_count": "3",
          "comment_id": "1272375",
          "content": "Selected Answer: AD\nAD gives the least administrative overhead"
        },
        {
          "upvote_count": "2",
          "poster": "auxwww",
          "timestamp": "1723031700.0",
          "content": "Why E is not optimal - https://stackoverflow.com/questions/51158595/build-and-push-docker-image-to-aws-ecr-using-lambda",
          "comment_id": "1262082"
        },
        {
          "poster": "auxwww",
          "comment_id": "1262079",
          "content": "Selected Answer: AD\nWhy not E - To push images to the post-scan repo, you need a custom lambda container to run docker pull and push commands which is more complicated than Option D",
          "timestamp": "1723031520.0",
          "upvote_count": "3"
        },
        {
          "upvote_count": "4",
          "comment_id": "1258567",
          "timestamp": "1722401580.0",
          "poster": "jamesf",
          "content": "Selected Answer: AE\nkeywords: LEAST Administrative overhead\n\nOption A centralizes the repository management in the shared services account, simplifying access control and configuration management. Pre-scan and post-scan repositories are clearly separated, ensuring that only post-scan images are deployed.\n\nOption E uses event-driven automation to handle the scanning results and image promotion, reducing manual intervention and ensuring that only images that pass the security scan are moved to the post-scan repositories. This approach is efficient and minimizes administrative overhead compared to manually setting up pipelines or replication mechanisms."
        },
        {
          "content": "---> AE",
          "comment_id": "1248261",
          "upvote_count": "2",
          "timestamp": "1721039040.0",
          "poster": "tgv"
        },
        {
          "comment_id": "1242812",
          "upvote_count": "4",
          "timestamp": "1720187220.0",
          "poster": "xdkonorek2",
          "content": "Selected Answer: AE\nE > D for LEAST administrative overhead"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:13:58.848Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "VneCSZX5dM4y61ExEnCv",
      "question_number": 156,
      "page": 32,
      "question_text": "A company uses an Amazon Elastic Kubernetes Service (Amazon EKS) cluster to deploy its web applications on containers. The web applications contain confidential data that cannot be decrypted without specific credentials.\n\nA DevOps engineer has stored the credentials in AWS Secrets Manager. The secrets are encrypted by an AWS Key Management Service (AWS KMS) customer managed key. A Kubernetes service account for a third-party tool makes the secrets available to the applications. The service account assumes an IAM role that the company created to access the secrets.\n\nThe service account receives an Access Denied (403 Forbidden) error while trying to retrieve the secrets from Secrets Manager.\n\nWhat is the root cause of this issue?",
      "choices": {
        "A": "The IAM role that is attached to the EKS cluster does not have access to retrieve the secrets from Secrets Manager.",
        "D": "The IAM role that is assumed by the Kubernetes service account does not have permission to access the EKS cluster.",
        "C": "The key policy for the customer managed key does not allow the EKS cluster IAM role to use the key.",
        "B": "The key policy for the customer managed key does not allow the Kubernetes service account IAM role to use the key."
      },
      "correct_answer": "B",
      "answer_ET": "B",
      "answers_community": [
        "B (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/143360-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-07-05 15:45:00",
      "unix_timestamp": 1720187100,
      "discussion_count": 4,
      "discussion": [
        {
          "comment_id": "1349849",
          "timestamp": "1738410840.0",
          "poster": "jojewi8143",
          "content": "Selected Answer: B\nB seems correct to me",
          "upvote_count": "2"
        },
        {
          "timestamp": "1722401940.0",
          "comments": [
            {
              "poster": "jamesf",
              "upvote_count": "2",
              "timestamp": "1722401940.0",
              "content": "If the IAM role has the correct permissions to access Secrets Manager but still receives an \"Access Denied\" error, the issue is likely related to the KMS key policy. Specifically, the key policy needs to explicitly allow the IAM role to use the key for decrypting the secrets.\n\nSo, the error message indicates that the key policy for the customer-managed KMS key does not include the necessary permissions for the IAM role assumed by the Kubernetes service account. Adjusting the key policy to grant the required permissions should resolve the issue.",
              "comment_id": "1258571"
            }
          ],
          "poster": "jamesf",
          "comment_id": "1258570",
          "content": "Selected Answer: B\nWhen a service account in Amazon EKS tries to access secrets in AWS Secrets Manager, it does so by assuming an IAM role. The permissions required to access these secrets include:\n- Secrets Manager permissions: The IAM role must have the necessary permissions to retrieve the secrets from AWS Secrets Manager.\n- KMS key permissions: The IAM role must also have permissions to use the AWS KMS key that encrypts the secrets.",
          "upvote_count": "3"
        },
        {
          "timestamp": "1721040120.0",
          "comment_id": "1248272",
          "content": "---> B",
          "poster": "tgv",
          "upvote_count": "3"
        },
        {
          "content": "Selected Answer: B\nThe IAM role assumed by the Kubernetes service account, not the EKS cluster IAM role => C is wrong",
          "poster": "trungtd",
          "comment_id": "1247660",
          "timestamp": "1720941180.0",
          "upvote_count": "4"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:14:09.305Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "rGmjux9EnMbE3pbxQ3wA",
      "question_number": 157,
      "page": 32,
      "question_text": "A company runs an application on one Amazon EC2 instance. Application metadata is stored in Amazon S3 and must be retrieved if the instance is restarted. The instance must restart or relaunch automatically if the instance becomes unresponsive.\nWhich solution will meet these requirements?",
      "choices": {
        "B": "Configure AWS OpsWorks, and use the auto healing feature to stop and start the instance. Use a lifecycle event in OpsWorks to pull the metadata from Amazon S3 and update it on the instance.",
        "A": "Create an Amazon CloudWatch alarm for the StatusCheckFailed metric. Use the recover action to stop and start the instance. Use an S3 event notification to push the metadata to the instance when the instance is back up and running.",
        "C": "Use EC2 Auto Recovery to automatically stop and start the instance in case of a failure. Use an S3 event notification to push the metadata to the instance when the instance is back up and running.",
        "D": "Use AWS CloudFormation to create an EC2 instance that includes the UserData property for the EC2 resource. Add a command in UserData to retrieve the application metadata from Amazon S3."
      },
      "correct_answer": "B",
      "answer_ET": "B",
      "answers_community": [
        "B (97%)",
        "3%"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/105569-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-04-08 09:14:00",
      "unix_timestamp": 1680938040,
      "discussion_count": 18,
      "discussion": [
        {
          "upvote_count": "13",
          "comment_id": "1004281",
          "timestamp": "1694377680.0",
          "comments": [
            {
              "timestamp": "1723815480.0",
              "comment_id": "1267169",
              "content": "May I add that AWS Opswork offers lifecycle events which you can leverage to execute custom actions on the EC2 instance for example retrieving metadata from S3 as the question requested.",
              "poster": "flaacko",
              "upvote_count": "2"
            }
          ],
          "content": "Selected Answer: B\nBoth Amazon CloudWatch's recover action and EC2 Auto Recovery are designed to respond to system status check failures, not instance status check failures. System status check failures indicate issues with the underlying hardware, while instance status check failures are often related to issues within your instance (like an OS-level issue).\n\nIf the requirement is to handle unresponsiveness due to both system-level and instance-level issues, neither option A nor C would fully meet the requirement. In that case, AWS OpsWorks with auto healing (Option B) could be a better fit since OpsWorks allows you to configure more complex health checks and could recover from both system-level and instance-level issues.\n\nSo, if you want to handle both types of unresponsiveness, Option B would be the most comprehensive solution.",
          "poster": "Jonfernz"
        },
        {
          "content": "Selected Answer: B\nOpsWorks has now been retired, so don't expect to see this question. However, the answer appears to be B. \n\nA: doesn't make sense because S3 notifications only happen if the S3 objects are modified.\nC: same argument as A\nD: impossible.",
          "upvote_count": "3",
          "poster": "endian675",
          "comment_id": "1323794",
          "timestamp": "1733699700.0"
        },
        {
          "poster": "BrusingWayne",
          "content": "Every options are wrong at this moment. Opsworks reached EOL. Other options do not make any sense.",
          "comment_id": "1315230",
          "timestamp": "1732105200.0",
          "upvote_count": "1"
        },
        {
          "timestamp": "1731859560.0",
          "poster": "Ravi_Bulusu",
          "comment_id": "1313649",
          "content": "The best approach is C, using EC2 Auto Recovery to monitor and recover the instance if it becomes unresponsive, combined with S3 event notifications to ensure the application metadata is properly retrieved after the instance is back online.",
          "upvote_count": "2"
        },
        {
          "comment_id": "1225941",
          "poster": "HarryLy",
          "timestamp": "1717743120.0",
          "content": "Selected Answer: B\nB seem correct",
          "upvote_count": "1"
        },
        {
          "timestamp": "1717018320.0",
          "comment_id": "1221269",
          "upvote_count": "1",
          "content": "Identical with Question #: 102",
          "poster": "Gomer"
        },
        {
          "upvote_count": "1",
          "comment_id": "1193368",
          "timestamp": "1712798100.0",
          "content": "Selected Answer: B\nTo automatic restart, must pull artifact for proactive",
          "poster": "hoazgazh"
        },
        {
          "content": "B: is correct: AWS opsworks auto healing will monitor the healthiness of EC2. If there is failure, restart EC2 and pull data from S3 to EC2\nA: incorrect because no mention of method to trigger S3 and S3 will not trigger by itself\nC: incorrect because no mention of method to trigger S3 and S3 will not trigger by itself\nD: Cloud formation only for deploy, this task is about opswork",
          "upvote_count": "3",
          "poster": "thanhnv142",
          "comment_id": "1134718",
          "timestamp": "1706514000.0"
        },
        {
          "comment_id": "1102129",
          "poster": "z_inderjot",
          "upvote_count": "3",
          "content": "Selected Answer: B\nOpWorks is deprecated now , So will it be part of exam ? What is the point of learning of service that are not , going to use.",
          "timestamp": "1703127180.0"
        },
        {
          "content": "Selected Answer: B\nOpWorks is EOL now, however, I think this is the correct answer currenty.",
          "upvote_count": "3",
          "comments": [
            {
              "content": "yes indeed. its EOL",
              "upvote_count": "1",
              "poster": "harithzainudin",
              "comment_id": "1092336",
              "timestamp": "1702195860.0"
            }
          ],
          "timestamp": "1699296000.0",
          "poster": "TheAWSRhino",
          "comment_id": "1064159"
        },
        {
          "upvote_count": "4",
          "timestamp": "1693819800.0",
          "comment_id": "998424",
          "content": "Selected Answer: B\nA and C are wrong because S3 event notification destination is lambda, sqs and sns topic, you can't directly push metadata to EC2;\nD is wrong because although user data can retrieve s3 metadata, it can't restart automatically.",
          "poster": "beanxyz"
        },
        {
          "content": "Selected Answer: B\nB. It doesn't make sense for an S3 event notification to be triggered by an EC2 instance being restarted. The OpsWorks autohealing capability can detect failed instances and replace them. \nAfter the auto-healed instance is back online, OpsWorks triggers a Configure lifecycle event on the instance. The metadata from S3 could be retrieved by the lifecycle event with a recipe.\n\nhttps://docs.aws.amazon.com/opsworks/latest/userguide/workinginstances-autohealing.html\nhttps://docs.aws.amazon.com/opsworks/latest/userguide/workingcookbook-events.html\nhttps://github.com/awsdocs/aws-opsworks-user-guide/blob/master/doc_source/create-custom-configure.md",
          "timestamp": "1686859920.0",
          "comment_id": "924560",
          "poster": "n_d1",
          "upvote_count": "2"
        },
        {
          "poster": "madperro",
          "timestamp": "1686382260.0",
          "comment_id": "919866",
          "content": "Selected Answer: B\nB, not simplest one but the only that meets requirements.\nFor A - how can you push data from S3 to EC2? Data needs to be pulled from EC2.",
          "upvote_count": "2"
        },
        {
          "upvote_count": "1",
          "content": "Selected Answer: A\nA for me\n By creating a CloudWatch alarm for the StatusCheckFailed metric, the system can detect if the instance becomes unresponsive. The recover action can then be triggered to automatically stop and start the instance, ensuring it restarts or relaunches when necessary.\n\nAdditionally, an S3 event notification can be set up to push the metadata to the instance once it is back up and running. This ensures that the application metadata is retrieved and available after the restart",
          "poster": "Akaza",
          "comments": [
            {
              "poster": "bcx",
              "timestamp": "1685459640.0",
              "upvote_count": "3",
              "content": "The second part would not work. An S3 notification event occurs only when actions occur on the object. When you restart the instance, nobody is overwriting the object to trigger the notification. IMHO.",
              "comment_id": "910367"
            }
          ],
          "comment_id": "904673",
          "timestamp": "1684826220.0"
        },
        {
          "upvote_count": "2",
          "comment_id": "884066",
          "timestamp": "1682747940.0",
          "poster": "ParagSanyashiv",
          "content": "Selected Answer: B\nB seems to be more feasible in this case."
        },
        {
          "upvote_count": "1",
          "content": "https://aws.amazon.com/about-aws/whats-new/2022/03/amazon-ec2-default-automatic-recovery/",
          "comment_id": "882671",
          "timestamp": "1682601780.0",
          "poster": "Mail1964"
        },
        {
          "content": "I'd say the answer is A ..you can configure Amazon CloudWatch to monitor the EC2 instance and trigger an automatic restart or relaunch if it becomes unresponsive. You can set up a CloudWatch alarm to monitor the instance's CPU utilization, network traffic, or other metrics, and define an action to take if the alarm is triggered, such as rebooting the instance or terminating and relaunching it.",
          "upvote_count": "1",
          "comment_id": "871157",
          "comments": [
            {
              "upvote_count": "1",
              "poster": "aussiehoa",
              "comment_id": "966898",
              "timestamp": "1690696680.0",
              "content": "\"Use an S3 event notification to push the metadata to the instance when the instance is back up and running.\" makes no sense"
            }
          ],
          "timestamp": "1681579500.0",
          "poster": "alce2020"
        },
        {
          "content": "Selected Answer: B\nA and C both is wrong cause recover is only for system status check failure. So if it's instance status check fails, it will not respond.",
          "poster": "ele",
          "timestamp": "1680938040.0",
          "upvote_count": "1",
          "comment_id": "864482"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:14:09.305Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "2pCvUub9oS6F3IowK0gh",
      "question_number": 158,
      "page": 32,
      "question_text": "A company is migrating its product development teams from an on-premises data center to a hybrid environment. The new environment will add four AWS Regions and will give the developers the ability to use the Region that is geographically closest to them.\n\nAll the development teams use a shared set of Linux applications. The on-premises data center stores the applications on a NetApp ONTAP storage device. The storage volume is mounted read-only on the development on-premises VMs. The company updates the applications on the shared volume once a week.\n\nA DevOps engineer needs to replicate the data to all the new Regions. The DevOps engineer must ensure that the data is always up to date with deduplication. The data also must not be dependent on the availability of the on-premises storage device.\n\nWhich solution will meet these requirements?",
      "choices": {
        "C": "Create Multi-AZ Amazon FSx for NetApp ONTAP instances and volumes in each Region. Configure a scheduled SnapMirror relationship between the on-premises storage device and the FSx for ONTAP instances.",
        "A": "Create an Amazon S3 File Gateway in the on-premises data center. Create S3 buckets in each Region. Set up a cron job to copy the data from the storage device to the S3 File Gateway. Set up S3 Cross-Region Replication (CRR) to the S3 buckets in each Region.",
        "D": "Create an Amazon Elastic File System (Amazon EFS) file system in each Region. Deploy an AWS DataSync agent in the on-premises data center. Configure a schedule for DataSync to copy the data to Amazon EFS daily.",
        "B": "Create an Amazon FSx File Gateway in one Region. Create file servers in Amazon FSx for Windows File Server in each Region. Set up a cron job to copy the data from the storage device to the FSx File Gateway."
      },
      "correct_answer": "C",
      "answer_ET": "C",
      "answers_community": [
        "C (90%)",
        "10%"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/143359-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-07-05 15:27:00",
      "unix_timestamp": 1720186020,
      "discussion_count": 6,
      "discussion": [
        {
          "comment_id": "1272378",
          "upvote_count": "1",
          "timestamp": "1724638860.0",
          "poster": "limelight04",
          "content": "Selected Answer: D\nOption C does involve using Amazon FSx for NetApp ONTAP, it doesn’t address the deduplication requirement or the independence from the availability of the on-premises storage device. Additionally, SnapMirror relationships are typically used for data replication within the same storage system rather than across multiple Regions.\n\nFor the specific requirements of deduplication, independence, and multi-Region replication, Option D (using Amazon EFS with AWS DataSync) is a more suitable solution."
        },
        {
          "content": "Selected Answer: C\nAmazon FSx for NetApp ONTAP provides a managed NetApp ONTAP experience in the cloud. By creating Multi-AZ FSx for ONTAP instances in each Region, you can replicate data with high availability and redundancy.\n\nCheckout cheaper contributor access here: https://exammatter.net/\n\nSnapMirror is a replication technology provided by NetApp that allows for efficient and reliable data replication. Configuring SnapMirror relationships between your on-premises NetApp storage device and the FSx for ONTAP instances will ensure that your data is consistently replicated across all AWS Regions.",
          "upvote_count": "3",
          "timestamp": "1723099920.0",
          "comment_id": "1262355",
          "poster": "Duke315"
        },
        {
          "poster": "jamesf",
          "comment_id": "1258576",
          "timestamp": "1722402480.0",
          "upvote_count": "3",
          "content": "Selected Answer: C\nAmazon FSx for NetApp ONTAP provides a managed NetApp ONTAP experience in the cloud. By creating Multi-AZ FSx for ONTAP instances in each Region, you can replicate data with high availability and redundancy.\n\nSnapMirror is a replication technology provided by NetApp that allows for efficient and reliable data replication. Configuring SnapMirror relationships between your on-premises NetApp storage device and the FSx for ONTAP instances will ensure that your data is consistently replicated across all AWS Regions."
        },
        {
          "poster": "tgv",
          "content": "---> C",
          "timestamp": "1721040300.0",
          "comment_id": "1248275",
          "upvote_count": "2"
        },
        {
          "timestamp": "1720942140.0",
          "poster": "trungtd",
          "upvote_count": "3",
          "content": "Selected Answer: C\nC\nhttps://docs.aws.amazon.com/fsx/latest/ONTAPGuide/migrating-fsx-ontap-snapmirror.html",
          "comment_id": "1247665"
        },
        {
          "timestamp": "1720298460.0",
          "upvote_count": "2",
          "comment_id": "1243551",
          "content": "C\nhttps://aws.amazon.com/blogs/storage/cross-region-disaster-recovery-with-amazon-fsx-for-netapp-ontap/",
          "poster": "getadroit"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:14:09.305Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "nCdCC3xzo32ZksLdubnE",
      "question_number": 159,
      "page": 32,
      "question_text": "A company has an application that stores data that includes personally identifiable information (PII) in an Amazon S3 bucket. All data is encrypted with AWS Key Management Service (AWS KMS) customer managed keys. All AWS resources are deployed from an AWS CloudFormation template.\n\nA DevOps engineer needs to set up a development environment for the application in a different AWS account. The data in the development environment's S3 bucket needs to be updated once a week from the production environment's S3 bucket.\n\nThe company must not move PII from the production environment without anonymizing the PII first. The data in each environment must be encrypted with different KMS customer managed keys.\n\nWhich combination of steps should the DevOps engineer take to meet these requirements? (Choose two.)",
      "choices": {
        "C": "Set up an S3 Batch Operations job to copy files from the production S3 bucket to the development S3 bucket. In the development account, configure an AWS Lambda function to redact ail PII. Configure S3 Object Lambda to use the Lambda function for S3 GET requests. Give the Lambda function's IAM role encrypt and decrypt permissions on the KMS key in the development account.",
        "B": "Set up S3 replication between the production S3 bucket and the development S3 bucket. Activate Amazon Macie on the development S3 bucket. Create an AWS Step Functions state machine to initiate a discovery job and redact all PII as the files are copied to the development S3 bucket. Give the state machine tasks encrypt and decrypt permissions on the KMS key in the development account.",
        "A": "Activate Amazon Macie on the S3 bucket in the production account. Create an AWS Step Functions state machine to initiate a discovery job and redact all PII before copying files to the S3 bucket in the development account. Give the state machine tasks decrypt permissions on the KMS key in the production account. Give the state machine tasks encrypt permissions on the KMS key in the development account.",
        "D": "Create a development environment from the CloudFormation template in the development account. Schedule an Amazon EventBridge rule to start the AWS Step Functions state machine once a week.",
        "E": "Create a development environment from the CloudFormation template in the development account. Schedule a cron job on an Amazon EC2 instance to run once a week to start the S3 Batch Operations job."
      },
      "correct_answer": "AD",
      "answer_ET": "AD",
      "answers_community": [
        "AD (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/143375-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-07-05 18:43:00",
      "unix_timestamp": 1720197780,
      "discussion_count": 5,
      "discussion": [
        {
          "poster": "jamesf",
          "timestamp": "1722403140.0",
          "upvote_count": "4",
          "comment_id": "1258582",
          "content": "Selected Answer: AD\nOption A addresses the need to anonymize PII before moving data to the development environment. By using Amazon Macie, you can identify PII in the production S3 bucket. AWS Step Functions can orchestrate a workflow to redact this PII before transferring the data. This ensures compliance with data protection requirements. You need to provide the necessary KMS key permissions for decrypting and encrypting data as it moves between accounts.\n\nOption D ensures that the data update process is automated and scheduled. Using Amazon EventBridge to trigger the AWS Step Functions state machine on a weekly basis automates the data transfer and anonymization process."
        },
        {
          "content": "Selected Answer: AD\n---> A D",
          "comment_id": "1250881",
          "upvote_count": "3",
          "timestamp": "1721368680.0",
          "poster": "tgv"
        },
        {
          "upvote_count": "3",
          "content": "Selected Answer: AD\nA. Anonymizing PII in the Production Account\nD. Automating the Weekly Data Transfer \n\nB suggests replicating the data before redacting PII, which violates the requirement \nC does not ensure that the PII is redacted before the data is stored in the development environment\nE introduces additional infrastructure management and costs",
          "comment_id": "1247667",
          "poster": "trungtd",
          "timestamp": "1720942560.0"
        },
        {
          "content": "redact should be done before",
          "comment_id": "1243556",
          "poster": "getadroit",
          "upvote_count": "1",
          "timestamp": "1720298940.0"
        },
        {
          "comment_id": "1243555",
          "content": "A & D\nhttps://aws.amazon.com/blogs/security/how-to-use-amazon-macie-to-preview-sensitive-data-in-s3-buckets/",
          "upvote_count": "2",
          "timestamp": "1720298880.0",
          "poster": "getadroit"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:14:09.305Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "aasGVNYRIxkuMJy0WcX6",
      "question_number": 160,
      "page": 32,
      "question_text": "A company uses an Amazon Elastic Kubernetes Service (Amazon EKS) cluster to host its machine learning (ML) application. As the ML model and the container image size grow, the time that new pods take to start up has increased to several minutes.\n\nA DevOps engineer needs to reduce the startup time to seconds. The solution must also reduce the startup time to seconds when the pod runs on nodes that were recently added to the cluster.\n\nThe DevOps engineer creates an Amazon EventBridge rule that invokes an automation in AWS Systems Manager. The automation prefetches the container images from an Amazon Elastic Container Registry (Amazon ECR) repository when new images are pushed to the repository. The DevOps engineer also configures tags to be applied to the cluster and the node groups.\n\nWhat should the DevOps engineer do next to meet the requirements?",
      "choices": {
        "B": "Create an IAM role that has a policy that allows EventBridge to use Systems Manager to run commands in the EKS cluster's nodes. Create a Systems Manager State Manager association that uses the nodes' machine size to prefetch corresponding container images.",
        "C": "Create an IAM role that has a policy that allows EventBridge to use Systems Manager to run commands in the EKS cluster's nodes. Create a Systems Manager State Manager association that uses the nodes' tags to prefetch corresponding container images.",
        "D": "Create an IAM role that has a policy that allows EventBridge to use Systems Manager to run commands in the EKS cluster's control plane nodes. Create a Systems Manager State Manager association that uses the nodes' tags to prefetch corresponding container images.",
        "A": "Create an IAM role that has a policy that allows EventBridge to use Systems Manager to run commands in the EKS cluster's control plane nodes. Create a Systems Manager State Manager association that uses the control plane nodes' tags to prefetch corresponding container images."
      },
      "correct_answer": "C",
      "answer_ET": "C",
      "answers_community": [
        "C (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/143374-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-07-05 18:36:00",
      "unix_timestamp": 1720197360,
      "discussion_count": 4,
      "discussion": [
        {
          "comment_id": "1247669",
          "upvote_count": "7",
          "content": "Selected Answer: C\nThe control plane manages the Kubernetes cluster but does not run the application containers => A & D wrong\nMachine size is not a practical or flexible approach to determining where images should be prefetched. Should be Tag =>B Wrong",
          "timestamp": "1720943040.0",
          "poster": "trungtd"
        },
        {
          "poster": "youonebe",
          "timestamp": "1735236540.0",
          "comment_id": "1332054",
          "upvote_count": "2",
          "content": "Selected Answer: C\ncontrol plane is fully managed by aws."
        },
        {
          "upvote_count": "3",
          "poster": "tgv",
          "timestamp": "1721368740.0",
          "content": "Selected Answer: C\n---> C",
          "comment_id": "1250885"
        },
        {
          "content": "C\nhttps://aws.amazon.com/blogs/containers/start-pods-faster-by-prefetching-images/",
          "timestamp": "1720299360.0",
          "comment_id": "1243557",
          "upvote_count": "2",
          "poster": "getadroit"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:14:09.305Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "ObVX6FXX0mYlnbwkeYGM",
      "question_number": 161,
      "page": 33,
      "question_text": "A company's application has an API that retrieves workload metrics. The company needs to audit, analyze, and visualize these metrics from the application to detect issues at scale.\n\nWhich combination of steps will meet these requirements? (Choose three.)",
      "choices": {
        "D": "Connect an AWS Glue crawler to the Amazon DynamoDB stream to catalog the workload metric data. Create views in Amazon Athena for the cataloged data.",
        "A": "Configure an Amazon EventBridge schedule to invoke an AWS Lambda function that calls the API to retrieve workload metrics. Store the workload metric data in an Amazon S3 bucket.",
        "C": "Create an AWS Glue crawler to catalog the workload metric data in the Amazon S3 bucket. Create views in Amazon Athena for the cataloged data.",
        "B": "Configure an Amazon EventBridge schedule to invoke an AWS Lambda function that calls the API to retrieve workload metrics. Store the workload metric data in an Amazon DynamoDB table that has a DynamoDB stream enabled.",
        "F": "Create an Amazon CloudWatch dashboard that has custom widgets that invoke AWS Lambda functions. Configure the Lambda functions to query the workload metrics data from the Amazon Athena views.",
        "E": "Create Amazon QuickSight datasets from the Amazon Athena views. Create a QuickSight analysis to visualize the workload metric data as a dashboard."
      },
      "correct_answer": "ACE",
      "answer_ET": "ACE",
      "answers_community": [
        "ACE (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/143373-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-07-05 18:31:00",
      "unix_timestamp": 1720197060,
      "discussion_count": 4,
      "discussion": [
        {
          "comments": [
            {
              "poster": "jamesf",
              "timestamp": "1722403800.0",
              "upvote_count": "1",
              "comment_id": "1258584",
              "content": "Additional Note:\nNot Option B: Storing workload metric data in an Amazon DynamoDB table with DynamoDB Streams enabled is an option, but it’s not ideal for large-scale metrics and querying. DynamoDB is better suited for high-speed key-value access and doesn’t provide the same level of querying capabilities for large datasets compared to Amazon S3 with Athena.\n\nDynamoDB Streams captures a time-ordered sequence of item-level modifications in any DynamoDB table and stores this information in a log for up to 24 hours. Applications can access this log and view the data items as they appeared before and after they were modified, in near-real time."
            }
          ],
          "upvote_count": "4",
          "timestamp": "1722403560.0",
          "poster": "jamesf",
          "content": "Selected Answer: ACE\nOption A: Using Amazon EventBridge to schedule an AWS Lambda function that retrieves workload metrics from the API and stores the data in Amazon S3 provides a scalable and automated way to collect and store the data.\n\nOption C: AWS Glue can be used to catalog the data stored in Amazon S3, making it queryable using Amazon Athena. This step prepares the data for analysis by creating a schema and making it available for querying.\n\nOption E: Amazon QuickSight can be used to create datasets from the Athena views and then visualize the data in dashboards. This provides the capability to analyze and visualize workload metrics at scale.\n\nhttps://aws.amazon.com/blogs/mt/analyzing-amazon-cloudwatch-internet-monitor-measurement-logs-using-amazon-athena-amazon-quicksight/",
          "comment_id": "1258583"
        },
        {
          "content": "Selected Answer: ACE\n---> A C E",
          "upvote_count": "3",
          "timestamp": "1721368800.0",
          "poster": "tgv",
          "comment_id": "1250886"
        },
        {
          "comment_id": "1247671",
          "upvote_count": "4",
          "poster": "trungtd",
          "timestamp": "1720943460.0",
          "content": "Selected Answer: ACE\nData Collection and Storage: EventBridge Schedule + Lambda + S3\nData Cataloging and Querying: Glue Crawler + Athena\nData Visualization: QuickSight"
        },
        {
          "poster": "getadroit",
          "comment_id": "1243562",
          "content": "ACE\nhttps://aws.amazon.com/blogs/mt/analyzing-amazon-cloudwatch-internet-monitor-measurement-logs-using-amazon-athena-amazon-quicksight/",
          "upvote_count": "2",
          "timestamp": "1720299840.0"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:14:19.701Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "LY65IeITxnVDBodlKdms",
      "question_number": 162,
      "page": 33,
      "question_text": "A DevOps engineer is building the infrastructure for an application. The application needs to run on an Amazon Elastic Kubernetes Service (Amazon EKS) cluster that includes Amazon EC2 instances. The EC2 instances need to use an Amazon Elastic File System (Amazon EFS) file system as a storage backend. The Amazon EFS Container Storage Interface (CSI) driver is installed on the EKS cluster.\n\nWhen the DevOps engineer starts the application, the EC2 instances do not mount the EFS file system.\n\nWhich solutions will fix the problem? (Choose three.)",
      "choices": {
        "F": "Disable encryption or the EFS file system.",
        "C": "Create an IAM role that allows the Amazon EFS CSI driver to interact with the file system",
        "A": "Switch the EKS nodes from Amazon EC2 to AWS Fargate.",
        "B": "Add an inbound rule to the EFS file system’s security group to allow NFS traffic from the EKS cluster.",
        "D": "Set up AWS DataSync to configure file transfer between the EFS file system and the EKS nodes.",
        "E": "Create a mount target for the EFS file system in the subnet of the EKS nodes."
      },
      "correct_answer": "BCE",
      "answer_ET": "BCE",
      "answers_community": [
        "BCE (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/143068-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-06-29 14:48:00",
      "unix_timestamp": 1719665280,
      "discussion_count": 4,
      "discussion": [
        {
          "comment_id": "1258586",
          "poster": "jamesf",
          "comments": [
            {
              "upvote_count": "1",
              "comment_id": "1258587",
              "content": "Why Not for options below,\nNot A: Switching from EC2 to AWS Fargate would not directly address the issue with EFS mounting. AWS Fargate does not support mounting EFS file systems natively.\n\nNot D: AWS DataSync is used for data transfer tasks and is not required for mounting EFS file systems in EKS. It is not relevant to solving the problem of mounting EFS.\n\nNot F: Disabling encryption is not necessary and might compromise security. Encryption of EFS file systems should not interfere with mounting unless there is a configuration issue, which is unlikely to be resolved by disabling encryption.",
              "timestamp": "1722403980.0",
              "poster": "jamesf"
            }
          ],
          "content": "Selected Answer: BCE\nB: The EFS file system’s security group must allow inbound traffic on the NFS port (2049) from the EC2 instances in the EKS cluster. Without this rule, the EC2 instances won't be able to communicate with the EFS file system.\n\nC: The EFS CSI driver needs permissions to interact with the EFS file system. This involves creating an IAM role with the necessary permissions and associating it with the EFS CSI driver.\n\nE: EFS requires a mount target in each subnet where the EC2 instances reside. This mount target facilitates the network connectivity between the EFS file system and the EC2 instances.",
          "timestamp": "1722403920.0",
          "upvote_count": "3"
        },
        {
          "upvote_count": "2",
          "poster": "tgv",
          "content": "Selected Answer: BCE\n---> B C E",
          "comment_id": "1250888",
          "timestamp": "1721368920.0"
        },
        {
          "poster": "trungtd",
          "timestamp": "1720944000.0",
          "content": "Selected Answer: BCE\nB. EFS file system’s security group must allow inbound NFS traffic (typically on port 2049) from the security group or IP range of the EKS cluster nodes.\nC. Ensure that the EFS CSI driver has the necessary IAM permissions to interact with the EFS file system, such as \"elasticfilesystem:DescribeFileSystems\", \"elasticfilesystem:DescribeMountTargets\", and other relevant permissions.",
          "comment_id": "1247677",
          "upvote_count": "2"
        },
        {
          "poster": "KaranNishad",
          "timestamp": "1719665280.0",
          "upvote_count": "3",
          "content": "Selected Answer: BCE\nSo, the correct solutions are:\n\nB. Add an inbound rule to the EFS file system’s security group to allow NFS traffic from the EKS cluster.\nC. Create an IAM role that allows the Amazon EFS CSI driver to interact with the file system.\nE. Create a mount target for the EFS file system in the subnet of the EKS nodes.",
          "comment_id": "1239269"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:14:19.701Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "2ymH5M6eVQdyOZFPoiYB",
      "question_number": 163,
      "page": 33,
      "question_text": "A company deploys an application on on-premises devices in the company’s on-premises data center. The company uses an AWS Direct Connect connection between the data center and the company's AWS account. During initial setup of the on-premises devices and during application updates, the application needs to retrieve configuration files from an Amazon Elastic File System (Amazon EFS) file system.\n\nAll traffic from the on-premises devices to Amazon EFS must remain private and encrypted. The on-premises devices must follow the principle of least privilege for AWS access. The company's DevOps team needs the ability to revoke access from a single device without affecting the access of the other devices.\n\nWhich combination of steps will meet these requirements? (Choose two.)",
      "choices": {
        "D": "Use the amazon-efs-utils package to mount the EFS file system.",
        "C": "Create an IAM user that has an access key and a secret key for all devices. Attach the AmazonElasticFileSystemClientReadWriteAccess policy to the IAM user. Configure the AWS CLI on the on-premises devices to use the IAM user's access key and secret key.",
        "B": "Generate certificates for each on-premises device in AWS Private Certificate Authority. Create a trust anchor in IAM Roles Anywhere that references an AWS Private CA. Create an IAM role that trust IAM Roles Anywhere. Attach the AmazonElasticFileSystemClientReadWriteAccess to the role. Create an IAM Roles Anywhere profile for the IAM role. Configure the AWS CLI on the on-premises devices to use the aws_signing_helper command to obtain credentials.",
        "A": "Create an IAM user that has an access key and a secret key for each device. Attach the AmazonElasticFileSystemFullAccess policy to all IAM users. Configure the AWS CLI on the on-premises devices to use the IAM user's access key and secret key.",
        "E": "Use the native Linux NFS client to mount the EFS file system."
      },
      "correct_answer": "BD",
      "answer_ET": "BD",
      "answers_community": [
        "BD (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/143070-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-06-29 14:49:00",
      "unix_timestamp": 1719665340,
      "discussion_count": 4,
      "discussion": [
        {
          "content": "BD:\nhttps://aws.amazon.com/blogs/aws/amazon-efs-update-on-premises-access-via-direct-connect-vpc/",
          "upvote_count": "5",
          "timestamp": "1720300020.0",
          "comment_id": "1243563",
          "poster": "getadroit"
        },
        {
          "content": "Selected Answer: BD\nB. Generate certificates for each on-premises device in AWS Private Certificate Authority. Create a trust anchor in IAM Roles Anywhere that references an AWS Private CA. Create an IAM role that trusts IAM Roles Anywhere. Attach the AmazonElasticFileSystemClientReadWriteAccess policy to the role. Create an IAM Roles Anywhere profile for the IAM role. Configure the AWS CLI on the on-premises devices to use the aws_signing_helper command to obtain credentials.\nD. Use the amazon-efs-utils package to mount the EFS file system.",
          "comment_id": "1239271",
          "timestamp": "1719665340.0",
          "upvote_count": "5",
          "poster": "KaranNishad"
        },
        {
          "upvote_count": "2",
          "timestamp": "1721369100.0",
          "content": "Selected Answer: BD\n---> B D",
          "comment_id": "1250890",
          "poster": "tgv"
        },
        {
          "content": "Selected Answer: BD\nA. Creating individual IAM users with full access does not follow the principle of least privilege => Wrong\nC. Using a single IAM user for all devices does not allow the ability to revoke access from a single device without affecting others => Wrong\nE. Technically feasible, but it does not inherently provide encryption in transit",
          "poster": "trungtd",
          "timestamp": "1720944600.0",
          "upvote_count": "3",
          "comment_id": "1247679"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:14:19.701Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "o34n0BF0VgGCqtXgOwsx",
      "question_number": 164,
      "page": 33,
      "question_text": "A DevOps engineer is setting up an Amazon Elastic Container Service (Amazon ECS) blue/green deployment for an application by using AWS CodeDeploy and AWS CloudFormation. During the deployment window, the application must be highly available and CodeDeploy must shift 10% of traffic to a new version of the application every minute until all traffic is shifted.\n\nWhich configuration should the DevOps engineer add in the CloudFormation template to meet these requirements?",
      "choices": {
        "B": "Add the AWS::CodeDeployBlueGreen transform and the AWS::CodeDeploy::BlueGreen hook parameter with the CodeDeployDefault.ECSLinear10PercentEvery1Minutes deployment configuration.",
        "C": "Add an AppSpec file with the ECSCanary10Percent5Minutes deployment configuration.",
        "D": "Add the AWS::CodeDeployBlueGreen transform and the AWS::CodeDepioy::BlueGreen hook parameter with the ECSCanary10Percent5Minutes deployment configuration.",
        "A": "Add an AppSpec file with the CodeDeployDefault.ECSLinearl OPercentEveryl Minutes deployment configuration."
      },
      "correct_answer": "B",
      "answer_ET": "B",
      "answers_community": [
        "B (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/143372-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-07-05 18:14:00",
      "unix_timestamp": 1720196040,
      "discussion_count": 2,
      "discussion": [
        {
          "poster": "trungtd",
          "content": "Selected Answer: B\nobviously B",
          "upvote_count": "5",
          "comment_id": "1247680",
          "timestamp": "1720944660.0"
        },
        {
          "content": "--> BB",
          "upvote_count": "2",
          "comment_id": "1243967",
          "poster": "tgv",
          "timestamp": "1720377000.0"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:14:19.701Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "UOokebxfapez4BrouJUb",
      "question_number": 165,
      "page": 33,
      "question_text": "A company uses an organization in AWS Organizations to manage its AWS accounts. The company's DevOps team has developed an AWS Lambda function that calls the Organizations API to create new AWS accounts.\n\nThe Lambda function runs in the organization's management account. The DevOps team needs to move the Lambda function from the management account to a dedicated AWS account. The DevOps team must ensure that the Lambda function has the ability to create new AWS accounts only in Organizations before the team deploys the Lambda function to the new account.\n\nWhich solution will meet these requirements?",
      "choices": {
        "B": "In the management account, turn on delegated administration for Organizations. Create a new delegation policy that grants the new AWS account permission to create new AWS accounts in Organizations. Ensure that the Lambda execution role has the organizations:CreateAccount permission.",
        "A": "In the management account, create a new IAM role that has the necessary permission to create new accounts in Organizations. Allow the role to be assumed by the Lambda execution role in the new AWS account. Update the Lambda function code to assume the role when the Lambda function creates new AWS accounts. Update the Lambda execution role to ensure that it has permission to assume the new role.",
        "C": "In the management account, create a new IAM role that has the necessary permission to create new accounts in Organizations. Allow the role to be assumed by the Lambda service principal. Update the Lambda function code to assume the role when the Lambda function creates new AWS accounts. Update the Lambda execution role to ensure that it has permission to assume the new role.",
        "D": "In the management account, enable AWS Control Tower. Turn on delegated administration for AWS Control Tower. Create a resource policy that allows the new AWS account to create new AWS accounts in AWS Control Tower. Update the Lambda function code to use the AWS Control Tower API in the new AWS account. Ensure that the Lambda execution role has the controltower:CreateManagedAccount permission."
      },
      "correct_answer": "A",
      "answer_ET": "A",
      "answers_community": [
        "A (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/143371-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-07-05 18:13:00",
      "unix_timestamp": 1720195980,
      "discussion_count": 3,
      "discussion": [
        {
          "timestamp": "1722404880.0",
          "upvote_count": "5",
          "poster": "jamesf",
          "comment_id": "1258594",
          "comments": [
            {
              "poster": "jamesf",
              "timestamp": "1722404940.0",
              "upvote_count": "5",
              "comment_id": "1258595",
              "content": "Why not following options:\nB: Delegated administration in AWS Organizations typically refers to giving permissions to manage AWS Organizations itself, rather than delegating permissions to create new accounts. Creating new accounts via the Organizations API requires specific IAM permissions, not just a delegation policy.\n\nC: Allowing the Lambda service principal to assume an IAM role is not a valid approach for cross-account role assumption. Lambda functions assume roles that are explicitly allowed by their execution role, not service principals.\n\nD: AWS Control Tower manages accounts and governance but requires different permissions and APIs compared to AWS Organizations for creating new accounts. Control Tower also does not directly handle account creation in the way described; instead, it manages accounts and governance at a higher level."
            }
          ],
          "content": "Selected Answer: A\nCreate an IAM Role with Necessary Permissions: \n- In the management account, create an IAM role with permissions to call the AWS Organizations API for creating new accounts.\n\nAllow Role Assumption: - Configure this IAM role to be assumable by the Lambda execution role in the new AWS account. This way, the Lambda function in the new account can assume the role to gain the necessary permissions.\n\nUpdate Lambda Function and Execution Role: \n- Modify the Lambda function code in the new account to assume the role created in the management account when it needs to create new AWS accounts. Also, ensure the Lambda execution role in the new account has the permissions required to assume the role in the management account."
        },
        {
          "content": "Selected Answer: A\n- Create IAM Role in Management Account: include actions like \"organizations:CreateAccount\"\n- Allow Role Assumption: specifying the ARN of the Lambda execution role in the new account in the trust policy of the IAM role.\n- Using the AWS SDK to assume the role and get temporary credentials in Lambda's code\n- Ensure that the Lambda execution role in the new account has the necessary permissions to assume the IAM role created in the management account.",
          "upvote_count": "4",
          "timestamp": "1720945080.0",
          "comment_id": "1247682",
          "poster": "trungtd"
        },
        {
          "content": "---> A",
          "poster": "tgv",
          "upvote_count": "2",
          "timestamp": "1720377120.0",
          "comment_id": "1243968"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:14:19.701Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "OMorRTbYTrCDmXy1UToh",
      "question_number": 166,
      "page": 34,
      "question_text": "A company has deployed an application in a single AWS Region. The application backend uses Amazon DynamoDB tables and Amazon S3 buckets.\n\nThe company wants to deploy the application in a secondary Region. The company must ensure that the data in the DynamoDB tables and the S3 buckets persists across both Regions. The data must also immediately propagate across Regions.\n\nWhich solution will meet these requirements with the MOST operational efficiency?",
      "choices": {
        "C": "Implement two-way S3 bucket replication between the primary Region's S3 buckets and the secondary Region's S3 buckets. Enable DynamoDB streams on the DynamoDB tables in both Regions. In each Region, create an AWS Lambda function that subscribes to the DynamoDB streams. Configure the Lambda function to copy new records to the DynamoDB tables in the other Region.",
        "D": "Implement S3 Batch Operations copy jobs between the primary Region and the secondary Region for all S3 buckets. Enable DynamoDB streams on the DynamoDB tables in both Regions. In each Region, create an AWS Lambda function that subscribes to the DynamoDB streams. Configure the Lambda function to copy new records to the DynamoDB tables in the other Region.",
        "A": "Implement two-way S3 bucket replication between the primary Region's S3 buckets and the secondary Region’s S3 buckets. Convert the DynamoDB tables into global tables. Set the secondary Region as the additional Region.",
        "B": "Implement S3 Batch Operations copy jobs between the primary Region and the secondary Region for all S3 buckets. Convert the DynamoDB tables into global tables. Set the secondary Region as the additional Region."
      },
      "correct_answer": "A",
      "answer_ET": "A",
      "answers_community": [
        "A (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/143518-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-07-07 20:33:00",
      "unix_timestamp": 1720377180,
      "discussion_count": 3,
      "discussion": [
        {
          "poster": "jamesf",
          "upvote_count": "5",
          "timestamp": "1722405120.0",
          "comments": [
            {
              "comment_id": "1258598",
              "content": "why not following options:\nB: S3 Batch Operations are used for bulk operations and are not suitable for continuous synchronization. DynamoDB streams and Lambda functions are also not necessary when using DynamoDB global tables, as global tables automatically manage replication.\n\nC: Two-way S3 bucket replication is complex and typically unnecessary. Using DynamoDB streams and Lambda functions for replication can be operationally intensive and error-prone compared to using global tables.\n\nD: Similar to option C, using S3 Batch Operations and DynamoDB streams with Lambda functions involves more operational overhead and complexity compared to using DynamoDB global tables.",
              "timestamp": "1722405180.0",
              "upvote_count": "3",
              "poster": "jamesf"
            },
            {
              "comment_id": "1258599",
              "upvote_count": "2",
              "timestamp": "1722405240.0",
              "poster": "jamesf",
              "content": "keywords: \n- data must also immediately propagate across Regions.\n- MOST operational efficiency?"
            }
          ],
          "content": "Selected Answer: A\nTwo-Way S3 Bucket Replication: \n- While two-way replication is not typically needed for most scenarios (one-way replication is generally sufficient), for this requirement, if both regions need to have copies of data and keep them synchronized, you would implement replication rules to ensure data consistency across S3 buckets in different regions.\n\nGlobal DynamoDB Tables: \n- DynamoDB global tables are designed specifically for multi-Region, fully replicated tables. When you convert your DynamoDB tables into global tables and add the secondary Region, DynamoDB handles the replication of data across Regions automatically and immediately. This provides efficient and consistent data replication without requiring custom solutions.",
          "comment_id": "1258597"
        },
        {
          "upvote_count": "5",
          "content": "Selected Answer: A\nB & D. S3 Batch Operations copy jobs are not immediate and are typically used for bulk copying of data. They do not provide the immediacy required for data propagation across regions.\nC. DynamoDB streams with AWS Lambda functions to replicate data introduces additional complexity and operational overhead.",
          "timestamp": "1720945260.0",
          "poster": "trungtd",
          "comment_id": "1247685"
        },
        {
          "comment_id": "1243969",
          "timestamp": "1720377180.0",
          "poster": "tgv",
          "content": "---> A",
          "upvote_count": "2"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:14:30.195Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "DXno7VUrC8NuDltWSEqM",
      "question_number": 167,
      "page": 34,
      "question_text": "A company has configured Amazon RDS storage autoscaling for its RDS DB instances. A DevOps team needs to visualize the autoscaling events on an Amazon CloudWatch dashboard.\n\nWhich solution will meet this requirement?",
      "choices": {
        "D": "Create a trail by using AWS CloudTrail with data events configured. Configure the trail to send the data events to Amazon CloudWatch Logs. Create a metric filter in CloudWatch Logs to match the RDS storage autoscaling events. Visualize the metric filter by using the CloudWatch dashboard.",
        "A": "Create an Amazon EventBridge rule that reacts to RDS storage autoscaling events from RDS events. Create an AWS Lambda function that publishes a CloudWatch custom metric. Configure the EventBridge rule to invoke the Lambda function. Visualize the custom metric by using the CloudWatch dashboard.",
        "B": "Create a trail by using AWS CloudTrail with management events configured. Configure the trail to send the management events to Amazon CloudWatch Logs. Create a metric filter in CloudWatch Logs to match the RDS storage autoscaling events. Visualize the metric filter by using the CloudWatch dashboard.",
        "C": "Create an Amazon EventBridge rule that reacts to RDS storage autoscaling events from the RDS events. Create a CloudWatch alarm. Configure the EventBridge rule to change the status of the CloudWatch alarm. Visualize the alarm status by using the CloudWatch dashboard."
      },
      "correct_answer": "A",
      "answer_ET": "A",
      "answers_community": [
        "A (82%)",
        "B (18%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/143519-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-07-07 20:35:00",
      "unix_timestamp": 1720377300,
      "discussion_count": 6,
      "discussion": [
        {
          "poster": "spring21",
          "upvote_count": "2",
          "timestamp": "1735780260.0",
          "comment_id": "1335330",
          "content": "Selected Answer: A\nAWS CloudTrail does log autoscaling operations by capturing API calls made to the Auto Scaling service, meaning all actions related to scaling your EC2 instances, including scaling up, down, or adjusting scaling policies, will be recorded in your CloudTrail logs."
        },
        {
          "timestamp": "1729206420.0",
          "comment_id": "1299477",
          "poster": "auxwww",
          "upvote_count": "4",
          "content": "Selected Answer: A\nB - Incorrect \n\"Autoscaling operations aren't logged by AWS CloudTrail. For more information on CloudTrail, see Monitoring Amazon RDS API calls in AWS CloudTrail.\""
        },
        {
          "timestamp": "1726753560.0",
          "upvote_count": "3",
          "poster": "Shenannigan",
          "content": "Selected Answer: B\nThis question is tricky as both A and B are correct\nIf it said in near real time then I would have chosen A, but it didn't so I am saying B as it is less complex and doesn't require writing a lambda function to use custom metrics.\n\nHonestly on this question flip a coin and choose one or the other",
          "comment_id": "1286354"
        },
        {
          "poster": "jamesf",
          "comment_id": "1258601",
          "upvote_count": "4",
          "timestamp": "1722405420.0",
          "content": "Selected Answer: A\nkeywords: \n- Amazon EventBridge rule \n- CloudWatch custom metric"
        },
        {
          "timestamp": "1720945500.0",
          "upvote_count": "4",
          "content": "Selected Answer: A\nWhile CloudTrail can capture RDS events and send them to CloudWatch Logs, creating a metric filter in CloudWatch Logs is more complex and indirect compared to using EventBridge and a Lambda function to publish custom metrics directly to CloudWatch.",
          "comment_id": "1247689",
          "poster": "trungtd"
        },
        {
          "comment_id": "1243970",
          "upvote_count": "2",
          "poster": "tgv",
          "content": "---> A",
          "timestamp": "1720377300.0"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:14:30.195Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "tmIgVQNBJm2Nwoz2Gavd",
      "question_number": 168,
      "page": 34,
      "question_text": "A company has multiple AWS accounts. The company uses AWS IAM Identity Center (AWS Single Sign-On) that is integrated with AWS Toolkit for Microsoft Azure DevOps. The attributes for access control feature is enabled in IAM Identity Center.\nThe attribute mapping list contains two entries. The department key is mapped to ${path:enterprise.department}. The costCenter key is mapped to ${path:enterprise.costCenter}.\nAll existing Amazon EC2 instances have a department tag that corresponds to three company departments (d1, d2, d3). A DevOps engineer must create policies based on the matching attributes. The policies must minimize administrative effort and must grant each Azure AD user access to only the EC2 instances that are tagged with the user’s respective department name.\nWhich condition key should the DevOps engineer include in the custom permissions policies to meet these requirements?",
      "choices": {
        "B": "",
        "A": "",
        "D": "",
        "C": ""
      },
      "correct_answer": "C",
      "answer_ET": "C",
      "answers_community": [
        "C (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/105570-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-04-08 09:25:00",
      "unix_timestamp": 1680938700,
      "discussion_count": 5,
      "discussion": [
        {
          "poster": "thanhnv142",
          "upvote_count": "6",
          "comment_id": "1134795",
          "timestamp": "1706521260.0",
          "content": "C is correct: check the EC2's department tag, if it is the same as user(principaltag)'s department tag, allow access.\nA: wrong synxtax, should be StringEquals only\nB: we checking the tag of Ec2, not aws. \nD: if config like this, every cases will match and everyone can access every EC2, regardless of department"
        },
        {
          "content": "Selected Answer: C\nC, related with ABAC.",
          "poster": "jamesf",
          "upvote_count": "1",
          "timestamp": "1721893620.0",
          "comment_id": "1254819"
        },
        {
          "comment_id": "919869",
          "content": "Selected Answer: C\nC, see an example at\nhttps://docs.aws.amazon.com/singlesignon/latest/userguide/configure-abac.html",
          "timestamp": "1686382500.0",
          "poster": "madperro",
          "upvote_count": "4"
        },
        {
          "timestamp": "1681506180.0",
          "comment_id": "870475",
          "upvote_count": "2",
          "content": "C is the correct answer",
          "poster": "alce2020"
        },
        {
          "comment_id": "864487",
          "timestamp": "1680938700.0",
          "poster": "ele",
          "content": "Selected Answer: C\nhttps://aws.amazon.com/blogs/aws/new-attributes-based-access-control-with-aws-single-sign-on/",
          "upvote_count": "4"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:14:30.195Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "UA5NOrgKvoAtVyWqfvV5",
      "question_number": 169,
      "page": 34,
      "question_text": "A company uses containers for its applications. The company learns that some container images are missing required security configurations.\n\nA DevOps engineer needs to implement a solution to create a standard base image. The solution must publish the base image weekly to the us-west-2 Region, us-east-2 Region, and eu-central-1 Region.\n\nWhich solution will meet these requirements?",
      "choices": {
        "D": "Create an AWS CodePipeline pipeline that uses an AWS CodeBuild project to build the image. Use AWS CodeDeploy to publish the image to Amazon Elastic Container Registry (Amazon ECR) repositories in all three Regions. Configure the pipeline to run weekly.",
        "A": "Create an EC2 Image Builder pipeline that uses a container recipe to build the image. Configure the pipeline to distribute the image to an Amazon Elastic Container Registry (Amazon ECR) repository in us-west-2. Configure ECR replication from us-west-2 to us-east-2 and from us-east-2 to eu-central-1. Configure the pipeline to run weekly.",
        "C": "Create an EC2 Image Builder pipeline that uses a container recipe to build the image. Configure the pipeline to distribute the image to Amazon Elastic Container Registry (Amazon ECR) repositories in all three Regions. Configure the pipeline to run weekly.",
        "B": "Create an AWS CodePipeline pipeline that uses an AWS CodeBuild project to build the image. Use AWS CodeDeploy to publish the image to an Amazon Elastic Container Registry (Amazon ECR) repository in us-west-2. Configure ECR replication from us-west-2 to us-east-2 and from us-east-2 to eu-central-1. Configure the pipeline to run weekly."
      },
      "correct_answer": "C",
      "answer_ET": "C",
      "answers_community": [
        "C (77%)",
        "A (15%)",
        "8%"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/143370-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-07-05 18:08:00",
      "unix_timestamp": 1720195680,
      "discussion_count": 8,
      "discussion": [
        {
          "comment_id": "1258603",
          "upvote_count": "5",
          "comments": [
            {
              "upvote_count": "1",
              "content": "Why not following Options:\nOptions A and B involve using ECR replication, which adds extra complexity and is not as streamlined as direct distribution through EC2 Image Builder. \nOption D suggests using AWS CodePipeline and AWS CodeDeploy, which is less specialized for container image building and distribution compared to EC2 Image Builder.",
              "poster": "jamesf",
              "comment_id": "1258605",
              "timestamp": "1722405720.0"
            }
          ],
          "timestamp": "1722405660.0",
          "poster": "jamesf",
          "content": "Selected Answer: C\nEC2 Image Builder: \n- This service is designed to automate the creation and distribution of container images. It supports defining container recipes and automating the build process.\nDirect Distribution: \n- EC2 Image Builder can be configured to distribute the images directly to multiple ECR repositories across different regions. This aligns with the requirement of publishing the base image to the us-west-2, us-east-2, and eu-central-1 regions.\nWeekly Schedule: \n- EC2 Image Builder can be scheduled to run on a weekly basis, meeting the requirement for regular updates."
        },
        {
          "upvote_count": "5",
          "poster": "noisonnoiton",
          "comment_id": "1246410",
          "timestamp": "1720747260.0",
          "content": "Selected Answer: C\nhttps://docs.aws.amazon.com/ko_kr/imagebuilder/latest/userguide/manage-distribution-settings.html"
        },
        {
          "timestamp": "1734173820.0",
          "comment_id": "1326436",
          "content": "Selected Answer: A\nA is correct. It offers a more operationally efficient and AWS-native solution for distributing container images across multiple regions.",
          "upvote_count": "1",
          "poster": "sinanci"
        },
        {
          "content": "Selected Answer: D\nEC2 Image Builder is for AMIs not container images that go to ECR...\n\n\"A replication action only occurs once per image push. For example, if you configured cross-Region replication from us-west-2 to us-east-1 and from us-east-1 to us-east-2, an image pushed to us-west-2 replicates to only us-east-1, it doesn't replicate again to us-east-2. This behavior applies to both cross-Region and cross-account replication.\"\n\nhttps://docs.aws.amazon.com/AmazonECR/latest/userguide/replication.html",
          "comments": [
            {
              "upvote_count": "1",
              "poster": "heff_bezos",
              "timestamp": "1727425080.0",
              "content": "Ehh nvm it might be C",
              "comment_id": "1289876"
            }
          ],
          "upvote_count": "1",
          "poster": "heff_bezos",
          "timestamp": "1727424840.0",
          "comment_id": "1289873"
        },
        {
          "poster": "tgv",
          "timestamp": "1721046120.0",
          "comment_id": "1248324",
          "upvote_count": "4",
          "content": "---> C\nYou can distributes container image to Amazon ECR repository in multiple regions.\n---\nhttps://docs.aws.amazon.com/imagebuilder/latest/userguide/cr-upd-container-distribution-settings.html"
        },
        {
          "content": "Selected Answer: A\nVOTE A",
          "comment_id": "1246412",
          "upvote_count": "1",
          "poster": "siheom",
          "timestamp": "1720747560.0"
        },
        {
          "comment_id": "1245592",
          "timestamp": "1720629120.0",
          "poster": "inturist",
          "upvote_count": "2",
          "content": "A replication action only occurs once per image push. For example, if you configured cross-Region replication from us-west-2 to us-east-1 and from us-east-1 to us-east-2, an image pushed to us-west-2 replicates to only us-east-1, it doesn't replicate again to us-east-2. This behavior applies to both cross-Region and cross-account replication."
        },
        {
          "timestamp": "1720300440.0",
          "comment_id": "1243564",
          "upvote_count": "1",
          "poster": "getadroit",
          "content": "B\nhttps://aws.amazon.com/blogs/containers/cross-region-replication-in-amazon-ecr-has-landed/"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:14:30.195Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "YRLjv5DI8F9sdhJOYf4s",
      "question_number": 170,
      "page": 34,
      "question_text": "A DevOps engineer needs to implement a solution to install antivirus software on all the Amazon EC2 instances in an AWS account. The EC2 instances run the most recent version of Amazon Linux.\n\nThe solution must detect all instances and must use an AWS Systems Manager document to install the software if the software is not present.\n\nWhich solution will meet these requirements?",
      "choices": {
        "B": "Set up AWS Config to record all the resources in the account. Create an AWS Config custom rule to determine if the software is installed on all the EC2 instances. Configure an automatic remediation action that uses the Systems Manager document for noncompliant EC2 instances.",
        "C": "Activate Amazon EC2 scanning on Amazon Inspector to determine if the software is installed on all the EC2 instances. Associate the findings with the Systems Manager document.",
        "D": "Create an Amazon EventBridge rule that uses AWS CloudTrail to detect the Runinstances API call. Configure inventory collection in Systems Manager Inventory to determine if the software is installed on the EC2 instances. Associate the Systems Manager inventory with the Systems Manager document.",
        "A": "Create an association in Systems Manager State Manager. Target all the managed nodes. Include the software in the association. Configure the association to use the Systems Manager document."
      },
      "correct_answer": "A",
      "answer_ET": "A",
      "answers_community": [
        "A (80%)",
        "B (20%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/143395-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-07-06 07:29:00",
      "unix_timestamp": 1720243740,
      "discussion_count": 6,
      "discussion": [
        {
          "comment_id": "1326634",
          "upvote_count": "1",
          "poster": "spring21",
          "timestamp": "1734218820.0",
          "content": "Selected Answer: A\nhttps://docs.aws.amazon.com/systems-manager/latest/userguide/state-manager-associations-creating.html"
        },
        {
          "content": "Selected Answer: B\nGiven the requirement to detect instances and use an SSM document for installation, Option B seems most appropriate. It combines AWS Config for detection and Systems Manager for remediation.",
          "comment_id": "1273045",
          "timestamp": "1724712000.0",
          "poster": "limelight04",
          "upvote_count": "1"
        },
        {
          "content": "Selected Answer: A\nAWS Systems Manager State Manager:\nAutomatic Detection: \n- State Manager allows you to manage the desired state of your AWS resources, including EC2 instances. By targeting all managed nodes, you ensure that every EC2 instance under Systems Manager's management is included in the scope.\nSoftware Installation: \n- You can specify a Systems Manager document (SSM document) to define the steps required to install the antivirus software. The association will ensure that the software is installed on any instances where it is missing.\nContinuous Compliance: \n- State Manager can continuously enforce the desired state, which means it will periodically check for the presence of the software and reapply the document if necessary.",
          "comments": [
            {
              "poster": "jamesf",
              "upvote_count": "1",
              "comment_id": "1258626",
              "timestamp": "1722407760.0",
              "content": "Use Case Alignment:\nManaged Nodes Targeting: \n- This allows for broad application across all instances, ensuring that no instances are missed, as long as they are configured as managed instances.\nEase of Configuration: \n- Setting up an association in State Manager is straightforward and integrates well with the existing AWS Systems Manager services, making it a robust choice for managing configurations across instances."
            }
          ],
          "upvote_count": "1",
          "poster": "jamesf",
          "timestamp": "1722407760.0",
          "comment_id": "1258625"
        },
        {
          "content": "State Manager associations\nA State Manager association is a configuration that you assign to your AWS resources. The configuration defines the state that you want to maintain on your resources. For example, an association can specify that antivirus software must be installed and running on a managed node, or that certain ports must be closed.\n\nAn association specifies a schedule for when to apply the configuration and the targets for the association. For example, an association for antivirus software might run once a day on all managed nodes in an AWS account. If the software isn't installed on a node, then the association could instruct State Manager to install it. If the software is installed, but the service isn't running, then the association could instruct State Manager to start the service.",
          "timestamp": "1721807400.0",
          "poster": "d0229a2",
          "upvote_count": "1",
          "comment_id": "1254219"
        },
        {
          "poster": "trungtd",
          "content": "Selected Answer: A\nBy creating an association, you can ensure that all instances have the antivirus software installed and kept up-to-date.",
          "timestamp": "1721219340.0",
          "comment_id": "1249583",
          "upvote_count": "2"
        },
        {
          "content": "---> I'm between A & D\nNot 100% sure about this but here are my 2 cents about DETECTING the instances that don't have the software installed:\n\nA - it's a bit tricky because it states that it targets all managed nodes - but what if there are other nodes that are not managed? It just assumes that all instances are managed by AWS Systems Manager \n\nB - How can Config determine if the software is installed?\n\nC - Amazon Inspector is focused on security assessments and compliance checks, not on ensuring software is installed. It would require additional setup and is not designed for direct software installation.\n\nD - it ensures that all instances are detected. It ensures that the installed software is tracked by using the AWS Systems Manager Inventory (which is designed for this kind of things). I'm not 100% sure about the phrase \"Associate the Systems Manager inventory with the Systems Manager document.\" which I don't believe its technically possible",
          "timestamp": "1721194920.0",
          "upvote_count": "1",
          "poster": "tgv",
          "comment_id": "1249400"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:14:30.195Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "OV4VFNfBlHoEE4VnOJxC",
      "question_number": 171,
      "page": 35,
      "question_text": "A company needs to increase the security of the container images that run in its production environment. The company wants to integrate operating system scanning and programming language package vulnerability scanning for the containers in its CI/CD pipeline. The CI/CD pipeline is an AWS CodePipeline pipeline that includes an AWS CodeBuild build project, AWS CodeDeploy actions, and an Amazon Elastic Container Registry (Amazon ECR) repository.\n\nA DevOps engineer needs to add an image scan to the CI/CD pipeline. The CI/CD pipeline must deploy only images without CRITICAL and HIGH findings into production.\n\nWhich combination of steps will meet these requirements? (Choose two.)",
      "choices": {
        "C": "Configure Amazon ECR to submit a Rejected status to the CI/CD pipeline when the image scan returns CRITICAL or HIGH findings.",
        "A": "Use Amazon ECR basic scanning.",
        "B": "Use Amazon ECR enhanced scanning.",
        "E": "Configure an Amazon EventBridge rule to invoke an AWS Lambda function when the image scan is completed. Configure the Lambda function to consume the Clair scan status and to submit an Approved or Rejected status to the CI/CD pipeline.",
        "D": "Configure an Amazon EventBridge rule to invoke an AWS Lambda function when the image scan is completed. Configure the Lambda function to consume the Amazon Inspector scan status and to submit an Approved or Rejected status to the CI/CD pipeline."
      },
      "correct_answer": "BD",
      "answer_ET": "BD",
      "answers_community": [
        "BD (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/143880-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-07-14 10:45:00",
      "unix_timestamp": 1720946700,
      "discussion_count": 3,
      "discussion": [
        {
          "upvote_count": "2",
          "timestamp": "1722408240.0",
          "comment_id": "1258635",
          "content": "Selected Answer: BD\nB. Use Amazon ECR Enhanced Scanning\n- Comprehensive Vulnerability Checks: Amazon ECR enhanced scanning is integrated with Amazon Inspector, providing thorough security checks on container images. It scans for both operating system vulnerabilities and application-level vulnerabilities in programming language packages, which basic scanning does not support.\n- Integration with Amazon Inspector: Enhanced scanning leverages Amazon Inspector for deeper vulnerability analysis, ensuring the images are secure before deployment.\n- CRITICAL and HIGH Severity Detection: The enhanced scanning option specifically identifies CRITICAL and HIGH vulnerabilities, aligning with the requirement to only deploy images that do not have these issues.",
          "poster": "jamesf"
        },
        {
          "timestamp": "1721807040.0",
          "upvote_count": "1",
          "content": "All images pushed to Amazon ECR after enhanced scanning is turned on are continually scanned for the configured duration.",
          "comment_id": "1254212",
          "poster": "d0229a2"
        },
        {
          "upvote_count": "3",
          "timestamp": "1721386800.0",
          "content": "Selected Answer: BD\n---> B D\n\nAs per documentation, basic scanning use CVEs from the open-source Clair project. Enhanced scanning is an integration with Amazon Inspector. This suggests both options use different database/scanners.\n\nhttps://docs.aws.amazon.com/AmazonECR/latest/userguide/image-scanning-enhanced.html\n\nhttps://docs.aws.amazon.com/AmazonECR/latest/userguide/image-scanning-basic.html",
          "poster": "tgv",
          "comment_id": "1251133"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:14:40.640Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "7ZZZ1mlyHNQDz3f8J9kV",
      "question_number": 172,
      "page": 35,
      "question_text": "A company's DevOps team manages a set of AWS accounts that are in an organization in AWS Organizations.\n\nThe company needs a solution that ensures that all Amazon EC2 instances use approved AM Is that the DevOps team manages. The solution also must remediate the usage of AMIs that are not approved. The individual account administrators must not be able to remove the restriction to use approved AMIs.\n\nWhich solution will meet these requirements?",
      "choices": {
        "D": "Enable AWS Config across the organization. Create a conformance pack that uses the approved-amis-by-id AWS Config managed rule with the list of approved AMIs. Deploy the conformance pack across the organization. Configure the rule to run the AWS-StopEC2lnstance AWS Systems Manager Automation runbook for the noncompliant EC2 instances.",
        "A": "Use AWS CloudFormation StackSets to deploy an Amazon EventBridge rule to each account. Configure the rule to react to AWS CloudTrail events for Amazon EC2 and to send a notification to an Amazon Simple Notification Service (Amazon SNS) topic. Subscribe the DevOps team to the SNS topic.",
        "C": "Create an AWS Lambda function that processes AWS CloudTrail events for Amazon EC2. Configure the Lambda function to send a notification to an Amazon Simple Notification Service (Amazon SNS) topic. Subscribe the DevOps team to the SNS topic. Deploy the Lambda function in each account in the organization. Create an Amazon EventBridge rule in each account. Configure the EventBridge rules to react to AWS CloudTrail events for Amazon EC2 and to invoke the Lambda function.",
        "B": "Use AWS CloudFormation StackSets to deploy the approved-amis-by-id AWS Config managed rule to each account. Configure the rule with the list of approved AMIs. Configure the rule to run the AWS-StopEC2Instance AWS Systems Manager Automation runbook for the noncompliant EC2 instances."
      },
      "correct_answer": "D",
      "answer_ET": "D",
      "answers_community": [
        "D (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/143394-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-07-06 07:19:00",
      "unix_timestamp": 1720243140,
      "discussion_count": 2,
      "discussion": [
        {
          "poster": "trungtd",
          "upvote_count": "5",
          "content": "Selected Answer: D\nA & C. only alert, not automatically remediate noncompliant instances\nB. deploy via CloudFormation StackSets to individual accounts can still allow account administrators to modify or remove the rules.",
          "timestamp": "1720946940.0",
          "comment_id": "1247703"
        },
        {
          "upvote_count": "4",
          "poster": "tgv",
          "comment_id": "1251152",
          "timestamp": "1721388420.0",
          "content": "Selected Answer: D\n---> D"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:14:40.640Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "wBrtJBnfIJT7nt9jR0Cq",
      "question_number": 173,
      "page": 35,
      "question_text": "A company gives its employees limited rights to AWS. DevOps engineers have the ability to assume an administrator role. For tracking purposes, the security team wants to receive a near-real-time notification when the administrator role is assumed.\n\nHow should this be accomplished?",
      "choices": {
        "A": "Configure AWS Config to publish logs to an Amazon S3 bucket. Use Amazon Athena to query the logs and send a notification to the security team when the administrator role is assumed.",
        "D": "Create an Amazon EventBridge events rule using an AWS API call that uses an AWS CloudTrail event pattern to invoke an AWS Lambda function that publishes a message to an Amazon SNS topic if the administrator role is assumed.",
        "B": "Configure Amazon GuardDuty to monitor when the administrator role is assumed and send a notification to the security team.",
        "C": "Create an Amazon EventBridge event rule using an AWS Management Console sign-in events event pattern that publishes a message to an Amazon SNS topic if the administrator role is assumed."
      },
      "correct_answer": "D",
      "answer_ET": "D",
      "answers_community": [
        "D (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/143393-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-07-06 07:17:00",
      "unix_timestamp": 1720243020,
      "discussion_count": 4,
      "discussion": [
        {
          "comment_id": "1258646",
          "upvote_count": "5",
          "timestamp": "1722408840.0",
          "content": "Selected Answer: D\nOption D provides a robust and effective approach to tracking and alerting on the assumption of the administrator role by leveraging the power of AWS CloudTrail, Amazon EventBridge, AWS Lambda, and Amazon SNS.\n\nNot Option C as Incorrect Event Pattern: This option specifies monitoring AWS Management Console sign-in events, which are unrelated to the AssumeRole API call used when assuming a role programmatically. It wouldn't detect role assumptions made through CLI or SDKs.",
          "poster": "jamesf"
        },
        {
          "upvote_count": "2",
          "comment_id": "1340846",
          "content": "Selected Answer: D\nI select D because C is refering just to Console sign-in events but why a lambda function is required when an EventBridge rule can publish directly to an SNS topic?",
          "timestamp": "1736939700.0",
          "poster": "teo2157"
        },
        {
          "timestamp": "1722056700.0",
          "comment_id": "1256040",
          "upvote_count": "4",
          "poster": "ericphl",
          "content": "Selected Answer: D\nVote D. \nA is not near-real-time solution. \nB. GuardDuty is designed for threat detection. not for monitoring role assuming. \nC. while C use the EventBridge, it monitoring console sign-in event only. rather than API call for assuming roles."
        },
        {
          "poster": "tgv",
          "content": "Selected Answer: D\n---> D",
          "upvote_count": "2",
          "comment_id": "1248514",
          "timestamp": "1721067300.0"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:14:40.640Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "Yo5TY2WvGUPOzFBIZl9L",
      "question_number": 174,
      "page": 35,
      "question_text": "A company needs a strategy for failover and disaster recovery of its data and application. The application uses a MySQL database and Amazon EC2 instances. The company requires a maximum RPO of 2 hours and a maximum RTO of 10 minutes for its data and application at all times.\n\nWhich combination of deployment strategies will meet these requirements? (Choose two.)",
      "choices": {
        "B": "Create an Amazon Aurora global database in two AWS Regions as the data store. In the event of a failure, promote the secondary Region to the primary for the application. Update the application to use the Aurora cluster endpoint in the secondary Region.",
        "E": "Set up the application in two AWS Regions. Configure AWS Global Accelerator to point to Application Load Balancers (ALBs) in both Regions. Add both ALBs to a single endpoint group. Use health checks and Auto Scaling groups in each Region.",
        "D": "Set up the application in two AWS Regions. Use Amazon Route 53 failover routing that points to Application Load Balancers in both Regions. Use health checks and Auto Scaling groups in each Region.",
        "A": "Create an Amazon Aurora Single-AZ cluster in multiple AWS Regions as the data store. Use Aurora's automatic recovery capabilities in the event of a disaster.",
        "C": "Create an Amazon Aurora cluster in multiple AWS Regions as the data store. Use a Network Load Balancer to balance the database traffic in different Regions."
      },
      "correct_answer": "BD",
      "answer_ET": "BD",
      "answers_community": [
        "BD (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/143392-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-07-06 07:13:00",
      "unix_timestamp": 1720242780,
      "discussion_count": 5,
      "discussion": [
        {
          "upvote_count": "1",
          "comment_id": "1562543",
          "poster": "GripZA",
          "timestamp": "1745261640.0",
          "content": "Selected Answer: BD\nB - With the Amazon Aurora Global Database feature, you set up multiple Aurora DB clusters that span multiple AWS Regions. Aurora automatically synchronizes all changes made in the primary DB cluster to one or more secondary clusters. An Aurora global database has a primary DB cluster in one Region, and up to five secondary DB clusters in different Regions. This multi-Region configuration provides fast recovery from the rare outage that might affect an entire AWS Region. Having a full copy of all your data in multiple geographic locations also enables low-latency read operations for applications that connect from widely separated locations around the world.\n\nD - Failover routing lets you route traffic to a resource when the resource is healthy or to a different resource when the first resource is unhealthy. The primary and secondary records can route traffic to anything from an Amazon S3 bucket that is configured as a website to a complex tree of records."
        },
        {
          "poster": "VerRi",
          "content": "Selected Answer: BD\nE. Global Accelerator could provide faster failover speed but not necessary in this case.",
          "comment_id": "1308253",
          "upvote_count": "2",
          "timestamp": "1730959140.0"
        },
        {
          "content": "Selected Answer: BD\nkeywords: Amazon Aurora global database , Route 53",
          "timestamp": "1722408960.0",
          "comment_id": "1258649",
          "upvote_count": "2",
          "poster": "jamesf"
        },
        {
          "content": "Selected Answer: BD\n---> BD",
          "poster": "tgv",
          "comment_id": "1248513",
          "timestamp": "1721067120.0",
          "upvote_count": "2"
        },
        {
          "poster": "trungtd",
          "upvote_count": "2",
          "timestamp": "1720947240.0",
          "content": "Selected Answer: BD\nOnly BD meets the requirements",
          "comment_id": "1247711"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:14:40.640Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "Cs1lfAAMQf1PX37Tu11U",
      "question_number": 175,
      "page": 35,
      "question_text": "A developer is using the AWS Serverless Application Model (AWS SAM) to create a prototype for an AWS Lambda function. The AWS SAM template contains an AWS::Serverless::Function resource that has the CodeUri property that points to an Amazon S3 location. The developer wants to identify the correct commands for deployment before creating a CI/CD pipeline.\n\nThe developer creates an archive of the Lambda function code named package.zip. The developer uploads the .zip file archive to the S3 location specified in the CodeUri property. The developer runs the sam deploy command and deploys the Lambda function. The developer updates the Lambda function code and uses the same steps to deploy the new version of the Lambda function. The sam deploy command fails and returns an error of no changes to deploy.\n\nWhich solutions will deploy the new version? (Choose two.)",
      "choices": {
        "B": "Use the aws cloudformation update-stack-instances command instead of the sam deploy command.",
        "A": "Use the aws cloudformation update-stack command instead of the sam deploy command.",
        "D": "Update the CodeUri property to reference the local application code folder. Use the aws cloudformation create-change-set command and the aws cloudformation execute-change-set command.",
        "E": "Update the CodeUri property to reference the local application code folder. Use the aws cloudformation package command and the aws cloudformation deploy command.",
        "C": "Update the CodeUri property to reference the local application code folder. Use the sam deploy command."
      },
      "correct_answer": "CE",
      "answer_ET": "CE",
      "answers_community": [
        "CE (85%)",
        "AC (15%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/143391-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-07-06 07:03:00",
      "unix_timestamp": 1720242180,
      "discussion_count": 4,
      "discussion": [
        {
          "timestamp": "1720947720.0",
          "content": "Selected Answer: CE\nC. Update the CodeUri property to reference the local application code folder, AWS SAM will handle packaging and uploading the code to S3 during the \"sam deploy\" command execution.\nE.\n- \"aws cloudformation package\" command packages the local artifacts (such as Lambda function code) and uploads them to an S3 bucket. It then generates a CloudFormation template that references these artifacts.\n- \"aws cloudformation deploy\" command deploys the generated CloudFormation template.\n\nA. \"aws cloudformation update-stack\": without the packaging step, it won't recognize changes in the Lambda function code\nB. used for stack set instances\nD. without proper packaging of the local code, it may not detect changes correctly.",
          "poster": "trungtd",
          "comment_id": "1247714",
          "upvote_count": "6"
        },
        {
          "content": "Selected Answer: CE\nC: When you set the CodeUri in your AWS SAM template to point to a local directory (e.g., CodeUri: ./src/), the sam deploy command automatically packages your application:\n\nIt uploads the local code to an Amazon S3 bucket\nIt updates the cfn template with the new S3 URI\nIt deploys the updated application\n\nE: this approach is similar to C but uses AWS cfn commands directly:\n\naws cloudformation package uploads your local code to S3 and generates a new template with the updated CodeUri\naws cloudformation deploy uses the updated template to deploy your application.\n\nthis is more manual compared to using sam deploy but achieves the same result",
          "comment_id": "1562546",
          "poster": "GripZA",
          "timestamp": "1745262420.0",
          "upvote_count": "1"
        },
        {
          "content": "Selected Answer: CE\nBoth Option C and Option E provide efficient and reliable methods to deploy updated Lambda function code using AWS SAM and CloudFormation. They address the deployment issue by ensuring that changes are recognized and appropriately handled, facilitating successful code updates in a CI/CD context.\n\nBy updating the CodeUri to reference the local folder, both approaches ensure that SAM or CloudFormation acknowledges code changes, effectively resolving the \"no changes to deploy\" error and enabling seamless deployments.",
          "comment_id": "1258673",
          "comments": [
            {
              "timestamp": "1722410280.0",
              "comment_id": "1258678",
              "upvote_count": "1",
              "poster": "jamesf",
              "content": "Not A as Misfit for Code Changes:\n- aws cloudformation update-stack is primarily used for updating CloudFormation stack configurations, not for detecting code changes in deployment packages.\n- It relies on an already packaged and uploaded S3 file specified in the template. Since no template changes are detected, the command would not recognize code updates.\n\nNot B as Incorrect Command Context:\n- aws cloudformation update-stack-instances is designed for AWS CloudFormation StackSets, which are used to manage resources across multiple AWS accounts and regions, not for deploying Lambda functions or single stack updates.\n- This command is irrelevant to the deployment of a single Lambda function and won't address the issue at hand."
            }
          ],
          "upvote_count": "4",
          "poster": "jamesf",
          "timestamp": "1722410160.0"
        },
        {
          "content": "Selected Answer: AC\nThe two correct solutions to deploy the new version of the Lambda function code when sam deploy reports no changes are:\n\nA. Use the aws cloudformation update-stack command instead of the sam deploy command.\nC. Update the CodeUri property to reference the local application code folder. Use the sam deploy command.\nThese approaches ensure that changes to your Lambda function code are correctly identified and deployed without encountering the \"no changes to deploy\" error.",
          "comment_id": "1253624",
          "poster": "awsaz",
          "timestamp": "1721733360.0",
          "upvote_count": "2"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:14:40.640Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "I0UZrIMvG5Cb8D6M4In6",
      "question_number": 176,
      "page": 36,
      "question_text": "A company runs its container workloads in AWS App Runner. A DevOps engineer manages the company's container repository in Amazon Elastic Container Registry (Amazon ECR).\n\nThe DevOps engineer must implement a solution that continuously monitors the container repository. The solution must create a new container image when the solution detects an operating system vulnerability or language package vulnerability.\n\nWhich solution will meet these requirements?",
      "choices": {
        "D": "Create an AWS CodeBuild project to create a container image. Use Amazon ECR as the target repository. Configure AWS Systems Manager Compliance to scan all managed nodes. Create an Amazon EventBridge rule to capture a configuration compliance state change event. Use the event to invoke the CodeBuild project.",
        "C": "Create an AWS CodeBuild project to create a container image. Use Amazon ECR as the target repository. Turn on basic scanning on the repository. Create an Amazon EventBridge rule to capture an ECR image action event. Use the event to invoke the CodeBuild project. Re-upload the container to the repository.",
        "B": "Use EC2 Image Builder to create a container image pipeline. Use Amazon ECR as the target repository. Enable Amazon GuardDuty Malware Protection on the container workload. Create an Amazon EventBridge rule to capture a GuardDuty finding event. Use the event to invoke the image pipeline.",
        "A": "Use EC2 Image Builder to create a container image pipeline. Use Amazon ECR as the target repository. Turn on enhanced scanning on the ECR repository. Create an Amazon EventBridge rule to capture an Inspector? finding event. Use the event to invoke the image pipeline. Re-upload the container to the repository."
      },
      "correct_answer": "A",
      "answer_ET": "A",
      "answers_community": [
        "A (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/143792-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-07-12 14:28:00",
      "unix_timestamp": 1720787280,
      "discussion_count": 4,
      "discussion": [
        {
          "comments": [
            {
              "upvote_count": "1",
              "comment_id": "1246739",
              "content": "https://docs.aws.amazon.com/inspector/latest/user/scanning-ecr.html#:~:text=To%20configure%20your%20enhanced%20scanning%20settings&text=Open%20the%20Amazon%20ECR%20console,registry%2C%20and%20then%20choose%20Settings.",
              "poster": "TEC1",
              "timestamp": "1720787340.0"
            }
          ],
          "content": "Selected Answer: A\nTurn on enhanced scanning in the Amazon ECR repository settings. This enables Amazon Inspector to scan images for vulnerabilities.",
          "comment_id": "1246734",
          "poster": "TEC1",
          "upvote_count": "6",
          "timestamp": "1720787280.0"
        },
        {
          "content": "Selected Answer: A\nKeywords: Enhanced scanning, Amazon ECR, Amazon Inspector, vulnerabilities",
          "comment_id": "1258681",
          "timestamp": "1722410400.0",
          "upvote_count": "5",
          "poster": "jamesf"
        },
        {
          "timestamp": "1721067060.0",
          "content": "Selected Answer: A\n---> A",
          "upvote_count": "4",
          "poster": "tgv",
          "comment_id": "1248509"
        },
        {
          "timestamp": "1720947900.0",
          "poster": "trungtd",
          "comment_id": "1247715",
          "upvote_count": "4",
          "content": "Selected Answer: A\nEnhanced scanning provides deep and comprehensive scanning for vulnerabilities in container images using Amazon Inspector."
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:14:51.088Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "jj9zyHp6FEQhVNllkIdv",
      "question_number": 177,
      "page": 36,
      "question_text": "A company wants to use AWS Systems Manager documents to bootstrap physical laptops for developers. The bootstrap code is stored in GitHub. A DevOps engineer has already created a Systems Manager activation, installed the Systems Manager agent with the registration code, and installed an activation ID on all the laptops.\n\nWhich set of steps should be taken next?",
      "choices": {
        "A": "Configure the Systems Manager document to use the AWS-RunShellScript command to copy the files from GitHub to Amazon S3, then use the aws-downloadContent plugin with a sourceType of S3.",
        "D": "Configure the Systems Manager document to use the aws:softwareInventory plugin and run the script from the Git repository.",
        "C": "Configure the Systems Manager document to use the aws-downloadContent plugin with a sourceType of GitHub and sourceInfo with the repository details.",
        "B": "Configure the Systems Manager document to use the aws-configurePackage plugin with an install action and point to the Git repository."
      },
      "correct_answer": "C",
      "answer_ET": "C",
      "answers_community": [
        "C (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/143389-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-07-06 06:50:00",
      "unix_timestamp": 1720241400,
      "discussion_count": 4,
      "discussion": [
        {
          "timestamp": "1722410640.0",
          "content": "Selected Answer: C\naws:downloadContent\n(Schema version 2.0 or later) Download SSM documents and scripts from remote locations. GitHub Enterprise repositories are not supported. This plugin is supported on Linux and Windows Server operating systems.",
          "poster": "jamesf",
          "comment_id": "1258686",
          "upvote_count": "4"
        },
        {
          "poster": "tgv",
          "comment_id": "1249403",
          "upvote_count": "3",
          "content": "Selected Answer: C\n---> C",
          "timestamp": "1721195580.0"
        },
        {
          "content": "Selected Answer: C\nThe aws-downloadContent plugin is specifically designed to download content from various sources, including GitHub.\nSetting the sourceType to GitHub and providing the repository details in sourceInfo to directly download the bootstrap code from GitHub to the laptops.",
          "comment_id": "1247815",
          "upvote_count": "4",
          "timestamp": "1720968720.0",
          "poster": "trungtd"
        },
        {
          "comment_id": "1243570",
          "upvote_count": "1",
          "poster": "getadroit",
          "timestamp": "1720301760.0",
          "content": "c\nhttps://docs.aws.amazon.com/systems-manager/latest/userguide/documents-running-remote-github-s3.html"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:14:51.088Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "3hp0diW8ujhGV97jPKiU",
      "question_number": 178,
      "page": 36,
      "question_text": "A company's development team uses AWS CloudFormation to deploy its application resources. The team must use CloudFormation for all changes to the environment. The team cannot use the AWS Management Console or the AWS CLI to make manual changes directly.\n\nThe team uses a developer IAM role to access the environment. The role is configured with the AdministratorAccess managed IAM policy. The company has created a new CloudFormationDeployment IAM role that has the following policy attached:\n\n//IMG//\n\n\nThe company wants to ensure that only CloudFormation can use the new role. The development team cannot make any manual changes to the deployed resources.\n\nWhich combination of steps will meet these requirements? (Choose three.)",
      "choices": {
        "D": "Update the trust policy of the CloudFormationDeployment role to allow the cloudformation.amazonaws.com AWS principal to perform the iam:AssumeRole action.",
        "A": "Remove the AdministratorAccess policy. Assign the ReadOnlyAccess managed IAM policy to the developer role. Instruct the developers to use the CloudFormationDeployment role as a CloudFormation service role when the developers deploy new stacks.",
        "E": "Remove the AdministratorAccess policy. Assign the ReadOnlyAccess managed IAM policy to the developer role. Instruct the developers to assume the CloudFormationDeployment role when the developers deploy new stacks.",
        "F": "Add an IAM policy to the CloudFormationDeployment role to allow cloudformation:* on all resources. Add a policy that allows the iam:PassRole action for the ARN of the CloudFormationDeployment role if iam:PassedToService equals cloudformation.amazonaws.com.",
        "B": "Update the trust policy of the CloudFormationDeployment role to allow the developer IAM role to assume the CloudFormationDeployment role.",
        "C": "Configure the developer IAM role to be able to get and pass the CloudFormationDeployment role if iam:PassedToService equals . Configure the CloudFormationDeployment role to allow all cloudformation actions for all resources."
      },
      "correct_answer": "ADF",
      "answer_ET": "ADF",
      "answers_community": [
        "ADF (88%)",
        "13%"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/143378-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [
        "https://img.examtopics.com/aws-certified-devops-engineer-professional-dop-c02/image22.png"
      ],
      "answer_images": [],
      "timestamp": "2024-07-05 20:28:00",
      "unix_timestamp": 1720204080,
      "discussion_count": 4,
      "discussion": [
        {
          "content": "Selected Answer: ABD\nWhile Option F seems reasonable at first glance, it has a potential issue. By allowing cloudformation:* on all resources, you grant broad permissions to CloudFormation actions, which may not align with the goal of restricting manual changes. It’s essential to strike a balance between security and functionality.\n\nThe combination of Options A, B, and D ensures that only CloudFormation can assume the CloudFormationDeployment role, and the ReadOnlyAccess policy for developers prevents unintended modifications. This approach maintains a more controlled and secure environment.",
          "timestamp": "1724717280.0",
          "upvote_count": "1",
          "poster": "limelight04",
          "comment_id": "1273059"
        },
        {
          "comments": [
            {
              "comment_id": "1258690",
              "content": "Not Option E: Developer Assumption of CloudFormationDeployment Role\n- Manual Role Assumption: Similar to option B, this option would allow developers to directly assume the CloudFormationDeployment role. It introduces the risk of developers bypassing CloudFormation for changes, which violates the requirement to prevent manual modifications.",
              "timestamp": "1722411120.0",
              "upvote_count": "3",
              "poster": "jamesf"
            }
          ],
          "comment_id": "1258688",
          "timestamp": "1722411060.0",
          "content": "Selected Answer: ADF\nA. Remove the AdministratorAccess policy. Assign the ReadOnlyAccess managed IAM policy to the developer role. Instruct the developers to use the CloudFormationDeployment role as a CloudFormation service role when the developers deploy new stacks.\n\nD. Update the trust policy of the CloudFormationDeployment role to allow the cloudformation.amazonaws.com AWS principal to perform the iam:AssumeRole action.\n\nF. Add an IAM policy to the CloudFormationDeployment role to allow cloudformation:* on all resources. Add a policy that allows the iam:PassRole action for the ARN of the CloudFormationDeployment role if iam:PassedToService equals cloudformation.amazonaws.com.",
          "poster": "jamesf",
          "upvote_count": "3"
        },
        {
          "poster": "tgv",
          "timestamp": "1721195880.0",
          "content": "Selected Answer: ADF\n---> A D F",
          "upvote_count": "1",
          "comment_id": "1249407"
        },
        {
          "content": "Selected Answer: ADF\nA. ensures that developers cannot make manual changes to the environment.\nD. ensures that only CloudFormation can assume this role.\nF. ensures that the role can only be passed to CloudFormation, not to any other service or user.",
          "upvote_count": "3",
          "timestamp": "1720969260.0",
          "comment_id": "1247818",
          "poster": "trungtd"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:14:51.088Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "9VWe1E0Iu3aOlZqjEuhm",
      "question_number": 179,
      "page": 36,
      "question_text": "A company hosts a security auditing application in an AWS account. The auditing application uses an IAM role to access other AWS accounts. All the accounts are in the same organization in AWS Organizations.\nA recent security audit revealed that users in the audited AWS accounts could modify or delete the auditing application's IAM role. The company needs to prevent any modification to the auditing application's IAM role by any entity other than a trusted administrator IAM role.\nWhich solution will meet these requirements?",
      "choices": {
        "A": "Create an SCP that includes a Deny statement for changes to the auditing application's IAM role. Include a condition that allows the trusted administrator IAM role to make changes. Attach the SCP to the root of the organization.",
        "D": "Create an IAM permissions boundary that includes a Deny statement for changes to the auditing application’s IAM role. Include a condition that allows the trusted administrator IAM role to make changes. Attach the permissions boundary to the auditing application's IAM role in the AWS accounts.",
        "C": "Create an IAM permissions boundary that includes a Deny statement for changes to the auditing application's IAM role. Include a condition that allows the trusted administrator IAM role to make changes. Attach the permissions boundary to the audited AWS accounts.",
        "B": "Create an SCP that includes an Allow statement for changes to the auditing application's IAM role by the trusted administrator IAM role. Include a Deny statement for changes by all other IAM principals. Attach the SCP to the IAM service in each AWS account where the auditing application has an IAM role."
      },
      "correct_answer": "A",
      "answer_ET": "A",
      "answers_community": [
        "A (89%)",
        "8%"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/105335-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-04-05 22:17:00",
      "unix_timestamp": 1680725820,
      "discussion_count": 19,
      "discussion": [
        {
          "upvote_count": "20",
          "poster": "jqso234",
          "timestamp": "1697305260.0",
          "comment_id": "870371",
          "content": "Selected Answer: A\nSCPs (Service Control Policies) are the best way to restrict permissions at the organizational level, which in this case would be used to restrict modifications to the IAM role used by the auditing application, while still allowing trusted administrators to make changes to it. Options C and D are not as effective because IAM permission boundaries are applied to IAM entities (users, groups, and roles), not the account itself, and must be applied to all IAM entities in the account."
        },
        {
          "upvote_count": "1",
          "comments": [
            {
              "content": "I'm sorry Folks!\nI'm wrong the right option is A and here is the solution:\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps_examples_general.html#example-scp-restricts-with-exception\n\nI thought it was suggesting to add two statements one allow and another deny, but in fact, option A is suggesting to add one Deny with the condition parameter.",
              "timestamp": "1743073860.0",
              "poster": "Serial_X25",
              "comment_id": "1410854",
              "upvote_count": "2"
            }
          ],
          "comment_id": "1409659",
          "timestamp": "1742818200.0",
          "poster": "Serial_X25",
          "content": "Selected Answer: D\nA and B are wrong because \"SCP never grants permissions\", as stated at https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html?icmpid=docs_orgs_console#scp-effects-on-permissions.\nC is wrong because you can't attach the permission boundary to an AWS account, only to IAM entities, https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_boundaries.html#access_policies_boundaries-eval-logic.\nD is the correct option."
        },
        {
          "poster": "4555894",
          "upvote_count": "1",
          "content": "Selected Answer: A\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html?icmpid=docs_orgs_console",
          "timestamp": "1725793500.0",
          "comment_id": "1168798"
        },
        {
          "poster": "zijo",
          "content": "Service Control Policies (SCPs) in AWS Organizations can be used to enforce maximum permissions for member accounts. They don't directly grant permissions or create permission boundaries. So C & D can be ruled out.",
          "comment_id": "1162959",
          "upvote_count": "1",
          "timestamp": "1724953140.0"
        },
        {
          "timestamp": "1724025480.0",
          "poster": "dzn",
          "content": "Selected Answer: A\nSCPs are applied at the account or OU level and affect all IAM entities within that organization. IAM Permission boundaries are applied individually to specific IAM roles or users.",
          "comment_id": "1153657",
          "upvote_count": "3"
        },
        {
          "upvote_count": "1",
          "comments": [
            {
              "content": "B is not correct because can only attach scp to AWS org",
              "poster": "thanhnv142",
              "upvote_count": "1",
              "comment_id": "1142351",
              "comments": [
                {
                  "upvote_count": "1",
                  "comment_id": "1210195",
                  "poster": "vn_thanhtung",
                  "content": "B wrong because SCP not support principals",
                  "timestamp": "1731411780.0"
                }
              ],
              "timestamp": "1722953220.0"
            }
          ],
          "timestamp": "1722855720.0",
          "poster": "thanhnv142",
          "content": "Selected Answer: A\nA is correct: < prevent any modification to the auditing application's IAM role> means scp\nA: <Include a condition that allows the trusted administrator IAM role> this is not the same as allow statement. So this option still valid\nB: SCP does not have allow statement\nC and D: These options make modification to permission boundary of the auditing application's IAM role, which is irrelavant. Other accounts may or may not assume this role.",
          "comment_id": "1141103"
        },
        {
          "comment_id": "1018783",
          "timestamp": "1711549140.0",
          "content": "AWS supports permissions boundaries for IAM entities (users or roles)",
          "poster": "flameme",
          "upvote_count": "1"
        },
        {
          "comment_id": "966879",
          "upvote_count": "2",
          "comments": [
            {
              "poster": "nlw",
              "timestamp": "1714987680.0",
              "content": "I think its because its not two policies. Its only one policy which applies when condition is account not equal security admin account. So A should work",
              "upvote_count": "4",
              "comment_id": "1063780"
            }
          ],
          "poster": "aussiehoa",
          "content": "in option A, shouldn't the first half override the second half. Explicitly deny everybody( will not matter if later it says Allow Admin ).",
          "timestamp": "1706599620.0"
        },
        {
          "content": "A seems ok. For C its wrong as you don't use permission boundary to deny permission. You use it to specify what and what can be done and not what cannot be done.",
          "upvote_count": "1",
          "timestamp": "1705581660.0",
          "poster": "ogwu2000",
          "comment_id": "955245"
        },
        {
          "poster": "madperro",
          "comment_id": "919889",
          "content": "Selected Answer: A\nFor AWS Organizations the SCP is the way to go. So A.",
          "timestamp": "1702201680.0",
          "upvote_count": "1"
        },
        {
          "comment_id": "910377",
          "content": "Selected Answer: A\nAn SCP would accomplish efficiently the task for all the accounts from a single place. A permission boundary is not for that, it would have to be configured in each account and for all the users IMHO.",
          "poster": "bcx",
          "timestamp": "1701364800.0",
          "upvote_count": "2"
        },
        {
          "comment_id": "911213",
          "content": "Selected Answer: A\nIt is A without a question. SCP is far more efficient.",
          "upvote_count": "1",
          "poster": "rdoty",
          "timestamp": "1701353220.0"
        },
        {
          "comment_id": "907079",
          "content": "Selected Answer: A\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html?icmpid=docs_orgs_console",
          "upvote_count": "1",
          "poster": "qan1257",
          "timestamp": "1700983800.0"
        },
        {
          "upvote_count": "1",
          "content": "Selected Answer: C\nC is more suitable option here to restrict permission.",
          "timestamp": "1699446420.0",
          "poster": "ParagSanyashiv",
          "comment_id": "892010"
        },
        {
          "upvote_count": "1",
          "poster": "5aga",
          "comment_id": "881012",
          "timestamp": "1698284220.0",
          "content": "Selected Answer: C\nA permissions boundary is designed to restrict permissions on IAM principals, such as roles, such that permissions don’t exceed what was originally intended. The permissions boundary uses an AWS or customer managed policy to restrict access, and it’s similar to other IAM policies you’re familiar with because it has resource, action, and effect statements. A permissions boundary alone doesn’t grant access to anything. Rather, it enforces a boundary that can’t be exceeded, even if broader permissions are granted by some other policy attached to the role.\n\nhttps://aws.amazon.com/blogs/security/when-and-where-to-use-iam-permissions-boundaries/"
        },
        {
          "content": "mi vote is for C as the right answer",
          "comment_id": "870478",
          "poster": "alce2020",
          "upvote_count": "1",
          "timestamp": "1697317920.0"
        },
        {
          "timestamp": "1696873620.0",
          "poster": "asfsdfsdf",
          "comment_id": "865719",
          "content": "Selected Answer: A\nOnly valid solution is A, for C or D you need to attach boundaries on all IAM roles/users not the account or the role itself.",
          "upvote_count": "1"
        },
        {
          "poster": "ele",
          "content": "Selected Answer: C\nBetween A and C, A looks good, but \"SCPs affect only member accounts in the organization. They have no effect on users or roles in the management account.\" \nC would do the work for all accounts in the Organization.",
          "timestamp": "1696750560.0",
          "comments": [
            {
              "comment_id": "907078",
              "timestamp": "1700983740.0",
              "content": "All the accounts are in the same organization in AWS Organizations.",
              "poster": "qan1257",
              "upvote_count": "3"
            }
          ],
          "comment_id": "864503",
          "upvote_count": "1"
        },
        {
          "content": "Selected Answer: A\nA seems good",
          "poster": "Dimidrol",
          "upvote_count": "1",
          "timestamp": "1696537020.0",
          "comment_id": "862451"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:14:51.088Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "BqffUMqKzL5ICHMoLRf1",
      "question_number": 180,
      "page": 36,
      "question_text": "A company is developing a web application's infrastructure using AWS CloudFormation. The database engineering team maintains the database resources in a CloudFormation template, and the software development team maintains the web application resources in a separate CloudFormation template. As the scope of the application grows, the software development team needs to use resources maintained by the database engineering team. However, both teams have their own review and lifecycle management processes that they want to keep. Both teams also require resource-level change-set reviews. The software development team would like to deploy changes to this template using their CI/CD pipeline.\n\nWhich solution will meet these requirements?",
      "choices": {
        "A": "Create a stack export from the database CloudFormation template and import those references into the web application CloudFormation template.",
        "B": "Create a CloudFormation nested stack to make cross-stack resource references and parameters available in both stacks.",
        "C": "Create a CloudFormation stack set to make cross-stack resource references and parameters available in both stacks.",
        "D": "Create input parameters in the web application CloudFormation template and pass resource names and IDs from the database stack."
      },
      "correct_answer": "A",
      "answer_ET": "A",
      "answers_community": [
        "A (93%)",
        "7%"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/143377-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-07-05 20:09:00",
      "unix_timestamp": 1720202940,
      "discussion_count": 5,
      "discussion": [
        {
          "comment_id": "1247830",
          "timestamp": "1720970940.0",
          "content": "Selected Answer: A\nB. Nested stacks combine all stacks into a single stack, which can complicate the independent lifecycle and review processes for each team.\nC. Stack sets are typically used for deploying stacks across multiple AWS accounts and Regions\nD. could work but require manual intervention to update the parameter values whenever there is a change in the database stack resources.",
          "poster": "trungtd",
          "upvote_count": "5"
        },
        {
          "comment_id": "1570319",
          "content": "Selected Answer: A\noption D is wrong:\nD. Pass input parameters manually: Feasible, but error-prone and lacks validation, automation, and traceability. Requires manual coordination between teams, which violates the goal of independent lifecycle management.",
          "poster": "nickp84",
          "upvote_count": "1",
          "timestamp": "1747676160.0"
        },
        {
          "content": "Selected Answer: D\nOption D is the most suitable solution because it allows separation of management processes, minimizes hard dependencies between templates, and is easily integrated into the development team's CI/CD pipeline.",
          "upvote_count": "1",
          "poster": "tdlAws",
          "comment_id": "1343887",
          "timestamp": "1737409380.0"
        },
        {
          "timestamp": "1722411540.0",
          "content": "Selected Answer: A\nA. Create a stack export from the database CloudFormation template and import those references into the web application CloudFormation template.\n- Decoupled Management: Each team retains its management process, aligning with their specific workflows.\n- Cross-Stack Referencing: Utilizes CloudFormation's Exports and Fn::ImportValue to reference resources between stacks efficiently.\n- Resource-Level Change Sets: Supports detailed change-set reviews, enabling teams to preview changes before deployment.\n- CI/CD Pipeline Compatibility: Works seamlessly with CI/CD pipelines by allowing modular updates to stacks without direct dependencies.",
          "poster": "jamesf",
          "upvote_count": "4",
          "comment_id": "1258694"
        },
        {
          "poster": "xdkonorek2",
          "comment_id": "1242947",
          "content": "Selected Answer: A\nA. Create a stack export from the database CloudFormation template and import those references into the web application CloudFormation template.",
          "timestamp": "1720202940.0",
          "upvote_count": "4"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:14:51.088Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "pkiHBbbQG2v4xLuKTULt",
      "question_number": 181,
      "page": 37,
      "question_text": "A company has an organization in AWS Organizations. A DevOps engineer needs to maintain multiple AWS accounts that belong to different OUs in the organization. All resources, including IAM policies and Amazon S3 policies within an account, are deployed through AWS CloudFormation. All templates and code are maintained in an AWS CodeCommit repository. Recently, some developers have not been able to access an S3 bucket from some accounts in the organization.\n\nThe following policy is attached to the S3 bucket:\n\n//IMG//\n\n\nWhat should the DevOps engineer do to resolve this access issue?",
      "choices": {
        "B": "Verify that no IAM permissions boundaries are denying developers access to the S3 bucket. Make the necessary changes to IAM permissions boundaries. Use an AWS Config recorder in the individual developer accounts that are experiencing the issue to revert any changes that are blocking access. Commit the fix back into the CodeCommit repository. Invoke deployment through CloudFormation to apply the changes.",
        "A": "Modify the S3 bucket policy. Turn off the S3 Block Public Access setting on the S3 bucket. In the S3 policy, add the aws:SourceAccount condition. Add the AWS account IDs of all developers who are experiencing the issue.",
        "C": "Configure an SCP that stops anyone from modifying IAM resources in developer OUs. In the S3 policy, add the aws:SourceAccount condition. Add the AWS account IDs of all developers who are experiencing the issue. Commit the fix back into the CodeCommit repository. Invoke deployment through CloudFormation to apply the changes.",
        "D": "Ensure that no SCP is blocking access for developers to the S3 bucket. Ensure that no IAM policy permissions boundaries are denying access to developer IAM users. Make the necessary changes to the SCP and IAM policy permissions boundaries in the CodeCommit repository. Invoke deployment through CloudFormation to apply the changes."
      },
      "correct_answer": "D",
      "answer_ET": "D",
      "answers_community": [
        "D (93%)",
        "7%"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/143893-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [
        "https://img.examtopics.com/aws-certified-devops-engineer-professional-dop-c02/image23.png"
      ],
      "answer_images": [],
      "timestamp": "2024-07-14 17:37:00",
      "unix_timestamp": 1720971420,
      "discussion_count": 4,
      "discussion": [
        {
          "comment_id": "1247833",
          "content": "Selected Answer: D\nOption D is the most comprehensive and aligns with the requirements:\n- It ensures that both SCPs and IAM policies are correctly configured.\n- It adheres to the use of CloudFormation for all changes.\n- It addresses the immediate issue while providing a scalable and manageable approach.",
          "upvote_count": "5",
          "timestamp": "1720971420.0",
          "poster": "trungtd"
        },
        {
          "timestamp": "1736951220.0",
          "comment_id": "1341073",
          "poster": "teo2157",
          "upvote_count": "1",
          "content": "Selected Answer: B\nGoing with B as if there was an SCP in place, it affects all developers and not some of them. Furthermore, with the config recorded you can trace the changes done in the iam policies for the users that are not able to access the S3 bucket and fix it."
        },
        {
          "comment_id": "1258699",
          "content": "Selected Answer: D\n- Comprehensive approach: Reviews both SCPs and IAM permissions boundaries that could block access.\n- Changes are committed to CodeCommit and deployed through CloudFormation, maintaining the required deployment pipeline.\n- By checking both SCPs and permissions boundaries, this solution covers potential organizational and account-level restrictions that could impact access.",
          "poster": "jamesf",
          "timestamp": "1722411900.0",
          "upvote_count": "4"
        },
        {
          "upvote_count": "4",
          "comment_id": "1248505",
          "poster": "tgv",
          "timestamp": "1721066880.0",
          "content": "Selected Answer: D\n---> D"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:15:01.522Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "w7SFfq9CrjvJeucEcSxM",
      "question_number": 182,
      "page": 37,
      "question_text": "A company has an organization in AWS Organizations for its multi-account environment. A DevOps engineer is developing an AWS CodeArtifact based strategy for application package management across the organization. Each application team at the company has its own account in the organization. Each application team also has limited access to a centralized shared services account.\n\nEach application team needs full access to download, publish, and grant access to its own packages. Some common library packages that the application teams use must also be shared with the entire organization.\n\n\nWhich combination of steps will meet these requirements with the LEAST administrative overhead? (Choose three.)",
      "choices": {
        "B": "Create a domain in the shared services account. Grant the organization read access and CreateRepository access.",
        "C": "Create a repository in each application team’s account. Grant each application team’s account full read access and write access to its own repository.",
        "E": "For teams that require shared packages, create resource-based policies that allow read access to the repository from other application teams' accounts.",
        "A": "Create a domain in each application team's account. Grant each application team's account full read access and write access to the application team's domain.",
        "D": "Create a repository in the shared services account. Grant the organization read access to the repository in the shared services account Set the repository as the upstream repository in each application team's repository.",
        "F": "Set the other application teams' repositories as upstream repositories."
      },
      "correct_answer": "BCD",
      "answer_ET": "BCD",
      "answers_community": [
        "BCD (81%)",
        "Other"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/143894-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-07-14 17:53:00",
      "unix_timestamp": 1720972380,
      "discussion_count": 5,
      "discussion": [
        {
          "comment_id": "1258701",
          "content": "Selected Answer: BCD\nB: Establish a centralized domain in the shared services account and provide organizational access to common libraries.\nD: Create a repository for common libraries in the shared services account, allow organization-wide read access, and configure upstream repositories.\nC: Create individual repositories in each team’s account and grant full access to manage their own packages.",
          "comments": [
            {
              "poster": "jamesf",
              "comment_id": "1258702",
              "upvote_count": "1",
              "content": "Keywords: LEAST Administrative overhead\nWhy not following options: \nA: (Optional) Creating a domain in each team’s account if they need isolated domains (generally, a centralized domain in the shared services account might be more efficient).\nE: (Optional) Configure resource-based policies for cross-account access if specific repositories need access by other teams' accounts beyond the shared services domain.\nF: (Optional) Set other repositories as upstream if required, though this may be redundant if the shared services account repository is already upstream.",
              "timestamp": "1722412200.0"
            }
          ],
          "poster": "jamesf",
          "upvote_count": "5",
          "timestamp": "1722412140.0"
        },
        {
          "poster": "limelight04",
          "upvote_count": "1",
          "content": "Selected Answer: BCE\nWhile Option D is a valid approach, it introduces additional complexity by requiring each application team to set up their repositories with the shared services account as an upstream repository. This can lead to more administrative overhead and potential misconfigurations.\n\nIn contrast, Options B, C, and E provide a simpler and more direct way to achieve the desired outcome. By centralizing the domain in the shared services account, granting organization-wide access, and allowing resource-based policies for shared packages, you can efficiently manage package distribution without relying on individual repository configurations.",
          "comments": [
            {
              "timestamp": "1735067280.0",
              "comment_id": "1331212",
              "upvote_count": "1",
              "content": "\"Some common library packages that the application teams use must also be shared with the entire organization.\"\n\nlooks like it is option D",
              "poster": "CHRIS12722222"
            }
          ],
          "comment_id": "1273072",
          "timestamp": "1724718420.0"
        },
        {
          "timestamp": "1721197680.0",
          "upvote_count": "4",
          "poster": "tgv",
          "comment_id": "1249434",
          "content": "Selected Answer: BCD\n---> BCD"
        },
        {
          "poster": "TEC1",
          "timestamp": "1721156760.0",
          "comment_id": "1249173",
          "content": "Selected Answer: BDE\nI will go with BDE",
          "upvote_count": "2"
        },
        {
          "content": "Selected Answer: BCD\nB allows for centralized control and management of common packages, and the organization can easily access and create repositories within this domain.\nC ensures that each team has full control over their packages\nD allows all teams to access common packages",
          "comment_id": "1247839",
          "upvote_count": "4",
          "poster": "trungtd",
          "timestamp": "1720972380.0"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:15:01.522Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "VJrB5LxwCcEp95iGnD9D",
      "question_number": 183,
      "page": 37,
      "question_text": "A company deploys an application to Amazon EC2 instances. The application runs Amazon Linux 2 and uses AWS CodeDeploy. The application has the following file structure for its code repository:\n\n//IMG//\n\n\nThe appspec.yml file has the following contents in the files section:\n\n//IMG//\n\n\nWhat will the result be for the deployment of the config.txt file?",
      "choices": {
        "B": "The config.txt file will be deployed to /usr/local/src/config.txt and to /var/www/html/config/config.txt.",
        "A": "The config.txt file will be deployed to only /var/www/html/config/config.txt.",
        "C": "The config.txt file will be deployed to only /usr/local/src/config.txt.",
        "D": "The config.txt file will be deployed to /usr/local/src/config.txt and to /var/www/html/application/web/config.txt."
      },
      "correct_answer": "B",
      "answer_ET": "B",
      "answers_community": [
        "B (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/143406-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [
        "https://img.examtopics.com/aws-certified-devops-engineer-professional-dop-c02/image24.png",
        "https://img.examtopics.com/aws-certified-devops-engineer-professional-dop-c02/image25.png"
      ],
      "answer_images": [],
      "timestamp": "2024-07-06 11:30:00",
      "unix_timestamp": 1720258200,
      "discussion_count": 3,
      "discussion": [
        {
          "comment_id": "1251787",
          "timestamp": "1721485320.0",
          "content": "Selected Answer: B\nFirst Entry:\n\nsource: config/config.txt\ndestination: /usr/local/src/config.txt\nThis rule specifically copies the config/config.txt file from the source repository to /usr/local/src/config.txt on the EC2 instance.\n\nSecond Entry:\n\nsource: /\ndestination: /var/www/html\nThis rule copies the entire contents of the root directory of the repository to /var/www/html on the EC2 instance. This includes the config/config.txt file, so it will be copied to /var/www/html/config/config.txt.\n\nGiven these two rules:\n\nThe config/config.txt file will be copied to /usr/local/src/config.txt by the first rule.\nThe same file will also be copied to /var/www/html/config/config.txt due to the second rule.\nSo, the correct answer is indeed B. The config.txt file will be deployed to /usr/local/src/config.txt and to /var/www/html/config/config.txt.",
          "poster": "Moumita",
          "upvote_count": "5"
        },
        {
          "poster": "jamesf",
          "upvote_count": "4",
          "content": "Selected Answer: B\nThe config/config.txt file will be copied to /usr/local/src/config.txt based on the first mapping.\nSince the entire source directory (/) is also mapped to /var/www/html, config/config.txt will also be copied to /var/www/html/config/config.txt.",
          "comment_id": "1258708",
          "timestamp": "1722412860.0"
        },
        {
          "timestamp": "1721299140.0",
          "comment_id": "1250308",
          "content": "Selected Answer: B\n---> B",
          "poster": "tgv",
          "upvote_count": "4"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:15:01.522Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "QgULUTkyGGKgayjgxjH5",
      "question_number": 184,
      "page": 37,
      "question_text": "A company has set up AWS CodeArtifact repositories with public upstream repositories. The company's development team consumes open source dependencies from the repositories in the company's internal network.\n\nThe company's security team recently discovered a critical vulnerability in the most recent version of a package that the development team consumes. The security team has produced a patched version to fix the vulnerability. The company needs to prevent the vulnerable version from being downloaded. The company also needs to allow the security team to publish the patched version.\n\nWhich combination of steps will meet these requirements? (Choose two.)",
      "choices": {
        "A": "Update the status of the affected CodeArtifact package version to unlisted.",
        "D": "Update the CodeArtifact package origin control settings to allow direct publishing and to block upstream operations.",
        "C": "Update the status of the affected CodeArtifact package version to archived.",
        "E": "Update the CodeArtifact package origin control settings to block direct publishing and to allow upstream operations.",
        "B": "Update the status of the affected CodeArtifact package version to deleted."
      },
      "correct_answer": "CD",
      "answer_ET": "CD",
      "answers_community": [
        "CD (70%)",
        "BD (30%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/143405-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-07-06 11:26:00",
      "unix_timestamp": 1720257960,
      "discussion_count": 12,
      "discussion": [
        {
          "comment_id": "1257717",
          "content": "Selected Answer: CD\nA - unlisted does not prevent download\nB - deleted is not a valid code artifact package version status\nC- archived will prevent download\n\nhttps://docs.aws.amazon.com/codeartifact/latest/ug/packages-overview.html#package-version-status",
          "upvote_count": "5",
          "timestamp": "1722290760.0",
          "poster": "BvGVAXeAMP"
        },
        {
          "upvote_count": "5",
          "poster": "Weninka",
          "content": "Selected Answer: CD\nI had this question in my exam and checking what was the correct option for the package version led me here. C - archived seems to be the right one.\nA - unlisted will only remove the package version from the list of versions returned to package managers, but it WILL NOT prevent the download.\nB - deleted - it's not a valid package version status (https://docs.aws.amazon.com/codeartifact/latest/ug/packages-overview.html#package-version-status)\nC - archived - will block the package version download.\nD - Allow direct publishing will give the internal team permissions to upload the new version of the package \nE - block direct publishing means the package version are updated from external (public) repos \nMore on the packages origin control settings here: https://docs.aws.amazon.com/codeartifact/latest/ug/package-origin-controls.html",
          "comment_id": "1251088",
          "timestamp": "1721381940.0"
        },
        {
          "poster": "luisfsm_111",
          "timestamp": "1734522960.0",
          "content": "Selected Answer: BD\nIf there's a critical vulnerability, there's no reason to archive instead of deleting\n\nhttps://docs.aws.amazon.com/codeartifact/latest/ug/delete-package.html",
          "upvote_count": "1",
          "comment_id": "1328476"
        },
        {
          "poster": "aws_god",
          "content": "Selected Answer: CD\nThere is no delete version status - https://docs.aws.amazon.com/codeartifact/latest/ug/packages-overview.html#package-version-status",
          "comment_id": "1283490",
          "timestamp": "1726290060.0",
          "upvote_count": "3"
        },
        {
          "timestamp": "1724940540.0",
          "comment_id": "1274548",
          "poster": "ApacheKafkaAWS",
          "content": "Selected Answer: BD\nyou have to delete it not archive it",
          "upvote_count": "1"
        },
        {
          "poster": "limelight04",
          "upvote_count": "1",
          "content": "Selected Answer: BD\nOption B: Update the status of the affected CodeArtifact package version to deleted. This action will prevent the vulnerable version from being accessible.\nOption D: Update the CodeArtifact package origin control settings to allow direct publishing and block upstream operations. This ensures that only the security team can publish the patched version directly.",
          "timestamp": "1724719680.0",
          "comment_id": "1273082"
        },
        {
          "upvote_count": "3",
          "poster": "jamesf",
          "timestamp": "1722413760.0",
          "comments": [
            {
              "timestamp": "1722413820.0",
              "content": "Why not B as \"deleted\" is not a valid code artifact package version status\nhttps://docs.aws.amazon.com/codeartifact/latest/ug/packages-overview.html#package-version-status",
              "poster": "jamesf",
              "upvote_count": "1",
              "comment_id": "1258715"
            }
          ],
          "content": "Selected Answer: CD\nC. Update the status of the affected CodeArtifact package version to archived.\n- Reason: Setting the package version status to Archived will prevent it from being downloaded while still retaining its metadata. This ensures that the vulnerable version cannot be accessed or used but allows you to track or potentially restore it later if needed.\n\nD. Update the CodeArtifact package origin control settings to allow direct publishing and to block upstream operations.\n- Reason: Allowing direct publishing and blocking upstream operations will enable the security team to publish the patched version directly to your repository without being blocked by upstream restrictions. This ensures that the patched version can be made available while preventing any interference from upstream repositories.",
          "comment_id": "1258714"
        },
        {
          "poster": "tgv",
          "comment_id": "1248368",
          "content": "Selected Answer: BD\n---> BD",
          "upvote_count": "1",
          "timestamp": "1721049120.0"
        },
        {
          "comment_id": "1247849",
          "content": "Selected Answer: BD\nBy allowing direct publishing, the security team can publish the patched version directly to the CodeArtifact repository. Blocking upstream operations ensures that only the patched version is available and prevents the vulnerable version from being pulled from the upstream repository.",
          "poster": "trungtd",
          "upvote_count": "1",
          "timestamp": "1720973940.0"
        },
        {
          "content": "Selected Answer: BD\n-----> B,D",
          "upvote_count": "1",
          "timestamp": "1720895580.0",
          "comment_id": "1247429",
          "poster": "inturist"
        },
        {
          "timestamp": "1720753740.0",
          "poster": "siheom",
          "comment_id": "1246440",
          "upvote_count": "1",
          "content": "Selected Answer: BD\nVOTE B,D"
        },
        {
          "poster": "getadroit",
          "upvote_count": "1",
          "comment_id": "1243572",
          "timestamp": "1720302720.0",
          "content": "BE\nhttps://aws.amazon.com/blogs/devops/tighten-your-package-security-with-codeartifact-package-origin-control-toolkit/"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:15:01.522Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "Rn7RpiiMkhzI60PuAsvi",
      "question_number": 185,
      "page": 37,
      "question_text": "A company is running a custom-built application that processes records. All the components run on Amazon EC2 instances that run in an Auto Scaling group. Each record's processing is a multistep sequential action that is compute-intensive. Each step is always completed in 5 minutes or less.\n\nA limitation of the current system is that if any steps fail, the application has to reprocess the record from the beginning. The company wants to update the architecture so that the application must reprocess only the failed steps.\n\nWhat is the MOST operationally efficient solution that meets these requirements?",
      "choices": {
        "A": "Create a web application to write records to Amazon S3. Use S3 Event Notifications to publish to an Amazon Simple Notification Service (Amazon SNS) topic. Use an EC2 instance to poll Amazon SNS and start processing. Save intermediate results to Amazon S3 to pass on to the next step.",
        "C": "Create a web application to pass records to an Amazon Kinesis data stream. Decouple the processing by using the Kinesis data stream and AWS Lambda functions.",
        "D": "Create a web application to pass records to AWS Step Functions. Decouple the processing into Step Functions tasks and AWS Lambda functions.",
        "B": "Perform the processing steps by using logic in the application. Convert the application code to run in a container. Use AWS Fargate to manage the container instances. Configure the container to invoke itself to pass the state from one step to the next."
      },
      "correct_answer": "D",
      "answer_ET": "D",
      "answers_community": [
        "D (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/143912-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-07-15 03:45:00",
      "unix_timestamp": 1721007900,
      "discussion_count": 3,
      "discussion": [
        {
          "comments": [
            {
              "content": "State Management and Fault Tolerance:\n- AWS Step Functions is a service that allows you to coordinate multiple AWS services into serverless workflows. It manages the state and execution of your tasks, ensuring that each step is completed in sequence.\nIf a step fails, Step Functions can retry the failed step or handle errors based on the defined retry policies or error catchers. This means that only the failed steps will be reprocessed, rather than starting from the beginning.\n\nDecoupling and Scalability:\n- Using AWS Lambda functions in combination with Step Functions allows you to run each step of the process as a separate Lambda function. This provides scalability and makes it easier to handle compute-intensive tasks as they can be distributed across multiple Lambda invocations.",
              "upvote_count": "1",
              "timestamp": "1722414480.0",
              "comment_id": "1258723",
              "poster": "jamesf"
            }
          ],
          "poster": "jamesf",
          "timestamp": "1722414420.0",
          "upvote_count": "2",
          "content": "Selected Answer: D\nhttps://docs.aws.amazon.com/step-functions/latest/dg/welcome.html",
          "comment_id": "1258721"
        },
        {
          "timestamp": "1721391480.0",
          "poster": "tgv",
          "comment_id": "1251183",
          "content": "Selected Answer: D\n---> D",
          "upvote_count": "2"
        },
        {
          "upvote_count": "3",
          "timestamp": "1721007900.0",
          "comment_id": "1248016",
          "poster": "trungtd",
          "content": "Selected Answer: D\nStep Functions and Lambda:\n- Decoupling Tasks\n- Error Handling and Retry Logic\n- State Management"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:15:01.522Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "VQ3034dY1lJ4hsUwrkWY",
      "question_number": 186,
      "page": 38,
      "question_text": "A company is migrating its on-premises Windows applications and Linux applications to AWS. The company will use automation to launch Amazon EC2 instances to mirror the on-premises configurations. The migrated applications require access to shared storage that uses SMB for Windows and NFS for Linux.\n\nThe company is also creating a pilot light disaster recovery (DR) environment in another AWS Region. The company will use automation to launch and configure the EC2 instances in the DR Region. The company needs to replicate the storage to the DR Region.\n\nWhich storage solution will meet these requirements?",
      "choices": {
        "D": "Use Amazon FSx for NetApp ONTAP for the application storage. Create an FSx for ONTAP instance in the DR Region. Configure NetApp SnapMirror replication from the primary Region to the DR Region.",
        "C": "Use a Volume Gateway in AWS Storage Gateway for the application storage. Configure Cross-Region Replication (CRR) of the Volume Gateway from the primary Region to the DR Region.",
        "A": "Use Amazon S3 for the application storage. Create an S3 bucket in the primary Region and an S3 bucket in the DR Region. Configure S3 Cross-Region Replication (CRR) from the primary Region to the DR Region.",
        "B": "Use Amazon Elastic Block Store (Amazon EBS) for the application storage. Create a backup plan in AWS Backup that creates snapshots of the EBS volumes that are in the primary Region and replicates the snapshots to the DR Region."
      },
      "correct_answer": "D",
      "answer_ET": "D",
      "answers_community": [
        "D (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/143404-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-07-06 11:06:00",
      "unix_timestamp": 1720256760,
      "discussion_count": 4,
      "discussion": [
        {
          "upvote_count": "4",
          "comment_id": "1258728",
          "timestamp": "1722414900.0",
          "content": "Selected Answer: D\nAmazon FSx for NetApp ONTAP support Multi-protocol access to data using the Network File System (NFS), Server Message Block (SMB), Internet Small Computer Systems Interface (iSCSI), and Non-Volatile Memory Express (NVMe) protocols\nhttps://docs.aws.amazon.com/fsx/latest/ONTAPGuide/what-is-fsx-ontap.html#features-overview",
          "poster": "jamesf"
        },
        {
          "poster": "tgv",
          "timestamp": "1721391480.0",
          "content": "Selected Answer: D\n---> D",
          "comment_id": "1251182",
          "upvote_count": "3"
        },
        {
          "comment_id": "1248017",
          "timestamp": "1721008200.0",
          "poster": "trungtd",
          "content": "Selected Answer: D\nNetApp ONTAP:\nMulti-Protocol Support:\n- SMB for Windows: Fully supports the SMB protocol required by your Windows applications.\n- NFS for Linux: Fully supports the NFS protocol required by your Linux applications.\n\nCross-Region Replication:\nNetApp SnapMirror: Provides efficient and reliable replication of data between ONTAP instances in different regions",
          "upvote_count": "4"
        },
        {
          "content": "Selected Answer: D\nAmazon FSx for NetApp ONTAP supports both SMB and NFS protocols, making it suitable for both Windows and Linux applications",
          "upvote_count": "4",
          "timestamp": "1720643160.0",
          "comment_id": "1245734",
          "poster": "TEC1"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:15:11.904Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "9wZtKQxrCvdR1pno14dR",
      "question_number": 187,
      "page": 38,
      "question_text": "A company's application uses a fleet of Amazon EC2 On-Demand Instances to analyze and process data. The EC2 instances are in an Auto Scaling group. The Auto Scaling group is a target group for an Application Load Balancer (ALB). The application analyzes critical data that cannot tolerate interruption. The application also analyzes noncritical data that can withstand interruption.\n\nThe critical data analysis requires quick scalability in response to real-time application demand. The noncritical data analysis involves memory consumption. A DevOps engineer must implement a solution that reduces scale-out latency for the critical data. The solution also must process the noncritical data.\n\nWhich combination of steps will meet these requirements? (Choose two.)",
      "choices": {
        "C": "For the critical data, modify the existing Auto Scaling group. Create a lifecycle hook to ensure that bootstrap scripts are completed successfully. Ensure that the application on the instances is ready to accept traffic before the instances are registered. Create a new version of the launch template that has detailed monitoring enabled.",
        "B": "For the critical data, modify the existing Auto Scaling group. Create a warm pool instance in the stopped state. Define the warm pool size. Create a new version of the launch template that has detailed monitoring enabled. Use On-Demand Instances.",
        "D": "For the noncritical data, create a second Auto Scaling group that uses a launch template. Configure the launch template to install the unified Amazon CloudWatch agent and to configure the CloudWatch agent with a custom memory utilization metric. Use Spot Instances. Add the new Auto Scaling group as the target group for the ALB. Modify the application to use two target groups for critical data and noncritical data.",
        "E": "For the noncritical data, create a second Auto Scaling group. Choose the predefined memory utilization metric type for the target tracking scaling policy. Use Spot Instances. Add the new Auto Scaling group as the target group for the ALB. Modify the application to use two target groups for critical data and noncritical data.",
        "A": "For the critical data, modify the existing Auto Scaling group. Create a warm pool instance in the stopped state. Define the warm pool size. Create a new version of the launch template that has detailed monitoring enabled. Use Spot Instances."
      },
      "correct_answer": "BD",
      "answer_ET": "BD",
      "answers_community": [
        "BD (87%)",
        "7%"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/143403-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-07-06 10:41:00",
      "unix_timestamp": 1720255260,
      "discussion_count": 5,
      "discussion": [
        {
          "timestamp": "1721008800.0",
          "content": "Selected Answer: BD\nAWS Auto Scaling does not provide a predefined memory utilization metric type",
          "comment_id": "1248020",
          "upvote_count": "5",
          "poster": "trungtd"
        },
        {
          "content": "Selected Answer: CD\nThis combination ensures quick scaling and uninterrupted processing for critical data and cost-efficient, memory-optimized scaling for noncritical data.",
          "upvote_count": "1",
          "poster": "Rs123x",
          "comment_id": "1563430",
          "timestamp": "1745523480.0"
        },
        {
          "content": "Selected Answer: BD\nOption B (For critical data): Creates a warm pool and ensures quick scaling with On-Demand Instances, addressing the need for low latency in scaling.\nOption D (For noncritical data): Uses Spot Instances with memory-based scaling policies to handle noncritical data efficiently.",
          "upvote_count": "4",
          "poster": "jamesf",
          "timestamp": "1722415260.0",
          "comment_id": "1258731",
          "comments": [
            {
              "timestamp": "1722839100.0",
              "comment_id": "1260951",
              "upvote_count": "1",
              "content": "For D, Spot Instance, Using Cloudwatch with Custom Memory Utilization Metric\nhttps://aws.amazon.com/blogs/mt/create-amazon-ec2-auto-scaling-policy-memory-utilization-metric-linux/\n\nNot E as Auto Scaling does not provide predefined memory utilization.",
              "poster": "jamesf"
            }
          ]
        },
        {
          "poster": "tgv",
          "timestamp": "1721391540.0",
          "content": "Selected Answer: BD\n---> B D",
          "upvote_count": "4",
          "comment_id": "1251184"
        },
        {
          "comment_id": "1245738",
          "poster": "TEC1",
          "timestamp": "1720643820.0",
          "content": "Selected Answer: BE\nOn-Demand Instances: For critical data that cannot tolerate interruption, On-Demand Instances are reliable and provide the required stability without the risk of termination\n\nSpot Instances: Utilising Spot Instances for noncritical data processing can significantly reduce costs since these workloads can tolerate interruptions.\n\nThis combination ensures that the critical data analysis benefits from reduced scale-out latency and reliability, while noncritical data processing leverages cost-effective Spot Instances and is scaled based on memory usage.",
          "upvote_count": "1"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:15:11.904Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "VefAVaTbvziEW3qobbJd",
      "question_number": 188,
      "page": 38,
      "question_text": "A company recently migrated its application to an Amazon Elastic Kubernetes Service (Amazon EKS) cluster that uses Amazon EC2 instances. The company configured the application to automatically scale based on CPU utilization.\n\nThe application produces memory errors when it experiences heavy loads. The application also does not scale out enough to handle the increased load. The company needs to collect and analyze memory metrics for the application over time.\n\nWhich combination of steps will meet these requirements? (Choose three.)",
      "choices": {
        "D": "Collect performance logs by deploying the AWS Distro for OpenTelemetry collector as a DaemonSet.",
        "B": "Attach the CloudWatchAgentServerPolicy managed IAM policy to a service account role for the cluster.",
        "E": "Analyze the pod_memory_utilization Amazon CloudWatch metric in the ContainerInsights namespace by using the Service dimension.",
        "A": "Attach the CloudWatchAgentServerPolicy managed IAM policy to the IAM instance profile that the cluster uses.",
        "F": "Analyze the node_memory_utilization Amazon CloudWatch metric in the ContainerInsights namespace by using the ClusterName dimension.",
        "C": "Collect performance metrics by deploying the unified Amazon CloudWatch agent to the existing EC2 instances in the cluster. Add the agent to the AMI for any new EC2 instances that are added to the cluster."
      },
      "correct_answer": "ACE",
      "answer_ET": "ACE",
      "answers_community": [
        "ACE (82%)",
        "Other"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/143402-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-07-06 10:38:00",
      "unix_timestamp": 1720255080,
      "discussion_count": 6,
      "discussion": [
        {
          "comment_id": "1248023",
          "upvote_count": "9",
          "content": "Selected Answer: ACE\nA. This policy grants the necessary permissions for the Amazon CloudWatch agent to collect and publish metrics from the EC2 instances.\nC. The unified Amazon CloudWatch agent can collect both CPU and memory utilization metrics. Deploying it ensures you capture memory metrics across all EC2 instances in the EKS cluster.\nE. pod_memory_utilization metric provides detailed insights into memory usage at the pod level\n\nB. service account role is more relevant for applications running within Kubernetes pods needing AWS permissions.\nD irrelevant\nF Node-level metrics do not provide the granularity needed to diagnose pod-level memory issues effectively",
          "poster": "trungtd",
          "timestamp": "1721009520.0"
        },
        {
          "timestamp": "1747941900.0",
          "poster": "nickp84",
          "content": "Selected Answer: ACE\nD. Collect performance logs by deploying the AWS Distro for OpenTelemetry collector as a DaemonSet. \nThe AWS Distro for OpenTelemetry (ADOT) collector is typically used for collecting traces, metrics, and logs from applications, often at the application or pod level, and is deployed as a Kubernetes workload (e.g., DaemonSet). While ADOT can collect certain metrics, it is primarily focused on application-level telemetry (e.g., custom metrics or traces) rather than system-level memory metrics for EC2 instances. The requirement emphasizes memory metrics, which are more effectively collected by the CloudWatch agent at the node level for EC2-based EKS clusters. \n\nNot Relevant: ADOT is less suited for collecting system-level memory metrics compared to the CloudWatch agent.",
          "comment_id": "1571393",
          "upvote_count": "1"
        },
        {
          "comment_id": "1258740",
          "poster": "jamesf",
          "upvote_count": "4",
          "timestamp": "1722416460.0",
          "comments": [
            {
              "upvote_count": "1",
              "comment_id": "1261903",
              "content": "it seen BCE better as option B \n- attached the policy to EKS service account role for better Granular Control, Scalability and Management\n- this policy targets permissions at the Kubernetes level, granting specific pods or services within the cluster the ability to collect and send metrics.",
              "timestamp": "1722995220.0",
              "poster": "jamesf"
            }
          ],
          "content": "Selected Answer: ACE\nAfter check, feel Option A better\n- provides necessary permissions at the EC2 instance level, which is where the CloudWatch agent runs.\n- is directly suitable for metrics collection because it ensures that EC2 instances can send metrics to CloudWatch. The CloudWatch agent on the EC2 instances needs the IAM policy to push metrics and logs to CloudWatch.\n\nHope someone can explain further if choose option B instead of A."
        },
        {
          "comment_id": "1250051",
          "poster": "noisonnoiton",
          "timestamp": "1721265480.0",
          "content": "Selected Answer: BCE\nB - control permission with service account\nC - cloudwatch agent on k8s worker nodes\nE - monitoring with k8s service (pods)",
          "upvote_count": "1"
        },
        {
          "timestamp": "1721159400.0",
          "poster": "TEC1",
          "upvote_count": "1",
          "comments": [
            {
              "upvote_count": "1",
              "timestamp": "1721159580.0",
              "poster": "TEC1",
              "comment_id": "1249198",
              "content": "B- Necessary permissions\nC- Cloud watch agent installed\nE - understanding performance and scaling of the application within Kubernetes Enviro"
            }
          ],
          "comment_id": "1249194",
          "content": "Selected Answer: BCE\nI will go with B C E"
        },
        {
          "timestamp": "1720826040.0",
          "content": "Selected Answer: CEF\nAnswer : C E F",
          "comment_id": "1247042",
          "poster": "komorebi",
          "upvote_count": "1"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:15:11.904Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "qRerM7bw3u3zZ0QilOS9",
      "question_number": 189,
      "page": 38,
      "question_text": "A company's video streaming platform usage has increased from 10,000 users each day to 50,000 users each day in multiple countries. The company deploys the streaming platform on Amazon Elastic Kubernetes Service (Amazon EKS). The EKS workload scales up to thousands of nodes during peak viewing time.\n\nThe company's users report occurrences of unauthorized logins. Users also report sudden interruptions and logouts from the platform.\n\nThe company wants additional security measures for the entire platform. The company also needs a summarized view of the resource behaviors and interactions across the company's entire AWS environment. The summarized view must show login attempts, API calls, and network traffic. The solution must permit network traffic analysis while minimizing the overhead of managing logs. The solution must also quickly investigate any potential malicious behavior that is associated with the EKS workload.\n\nWhich solution will meet these requirements?",
      "choices": {
        "A": "Enable Amazon GuardDuty for EKS Audit Log Monitoring. Enable AWS CloudTrail logs. Store the EKS audit logs and CloudTrail log files in an Amazon S3 bucket. Use Amazon Athena to create an external table. Use Amazon QuickSight to create a dashboard.",
        "D": "Enable Amazon GuardDuty for EKS Audit Log Monitoring. Enable Amazon CloudWatch Container Insights and VPC Flow Logs. Enable AWS CloudTrail logs.",
        "C": "Enable Amazon CloudWatch Container Insights. Enable AWS CloudTrail logs. Store the EKS audit logs and CloudTrail log files in an Amazon S3 bucket. Use Amazon Athena to create an external table. Use Amazon QuickSight to create a dashboard.",
        "B": "Enable Amazon GuardDuty for EKS Audit Log Monitoring. Enable Amazon Detective in the company's AWS account. Enable EKS audit logs from optional source packages in Detective."
      },
      "correct_answer": "B",
      "answer_ET": "B",
      "answers_community": [
        "B (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/143401-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-07-06 10:15:00",
      "unix_timestamp": 1720253700,
      "discussion_count": 5,
      "discussion": [
        {
          "content": "Selected Answer: B\nAmazon Detective helps you quickly analyze and investigate security events across one or more AWS accounts by generating data visualizations that represent the ways your resources behave and interact over time. Detective creates visualizations of GuardDuty findings.\nhttps://docs.aws.amazon.com/guardduty/latest/ug/detective-integration.html\n\nAmazon EKS audit logs is an optional data source package that can be added to your Detective behavior graph.\nhttps://docs.aws.amazon.com/detective/latest/userguide/source-data-types-EKS.html",
          "comment_id": "1258742",
          "timestamp": "1722416700.0",
          "poster": "jamesf",
          "upvote_count": "2"
        },
        {
          "comment_id": "1249202",
          "content": "Selected Answer: B\nB- Guardduty any potential malicious behavior and Amazon Detective summarised view must show login attempts, API calls, and network traffic",
          "poster": "TEC1",
          "upvote_count": "2",
          "timestamp": "1721160120.0"
        },
        {
          "poster": "trungtd",
          "timestamp": "1721010840.0",
          "comment_id": "1248030",
          "content": "Selected Answer: B\nhttps://docs.aws.amazon.com/guardduty/latest/ug/detective-integration.html\nhttps://docs.aws.amazon.com/detective/latest/userguide/source-data-types-EKS.html",
          "upvote_count": "3"
        },
        {
          "comment_id": "1246435",
          "content": "Selected Answer: B\nvote B",
          "poster": "siheom",
          "upvote_count": "2",
          "timestamp": "1720753140.0"
        },
        {
          "comment_id": "1243575",
          "content": "D\nhttps://aws.amazon.com/blogs/security/how-to-use-new-amazon-guardduty-eks-protection-findings/",
          "poster": "getadroit",
          "timestamp": "1720303440.0",
          "upvote_count": "3"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:15:11.904Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "2NgY2TStKs4b4mLZs0pP",
      "question_number": 190,
      "page": 38,
      "question_text": "A company has an on-premises application that is written in Go. A DevOps engineer must move the application to AWS. The company's development team wants to enable blue/green deployments and perform A/B testing.\nWhich solution will meet these requirements?",
      "choices": {
        "C": "Use AWS CodeArtifact to store the application code. Use AWS CodeDeploy to deploy the application to a fleet of Amazon EC2 instances. Use Elastic Load Balancing to distribute the traffic to the EC2 instances. When making changes to the application, upload a new version to CodeArtifact and create a new CodeDeploy deployment.",
        "D": "Use AWS Elastic Beanstalk to host the application. Store a zipped version of the application in Amazon S3. Use that location to deploy new versions of the application. Use Elastic Beanstalk to manage the deployment options.",
        "A": "Deploy the application on an Amazon EC2 instance, and create an AMI of the instance. Use the AMI to create an automatic scaling launch configuration that is used in an Auto Scaling group. Use Elastic Load Balancing to distribute traffic. When changes are made to the application, a new AMI will be created, which will initiate an EC2 instance refresh.",
        "B": "Use Amazon Lightsail to deploy the application. Store the application in a zipped format in an Amazon S3 bucket. Use this zipped version to deploy new versions of the application to Lightsail. Use Lightsail deployment options to manage the deployment."
      },
      "correct_answer": "D",
      "answer_ET": "D",
      "answers_community": [
        "D (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/105572-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-04-08 09:51:00",
      "unix_timestamp": 1680940260,
      "discussion_count": 10,
      "discussion": [
        {
          "upvote_count": "5",
          "timestamp": "1718935680.0",
          "poster": "z_inderjot",
          "content": "Selected Answer: D\nD is undoubtedly is most correct one . But they shoud mention that , we are going to deploy application in different environment . Since deployming to the same envirnmont just overide the previous deployment . In order to meed the requirement of Blue / Green deployment we need two separate enviormnent . Then we have two separate version running simulateously and we can do DNS swapping to quicky shilf traffic .",
          "comment_id": "1102172"
        },
        {
          "comment_id": "1163031",
          "poster": "zijo",
          "content": "AWS Elastic Beanstalk deploy action can be used to deploy the application artifact from the S3 bucket to the green environment, which is the AWS cloud environment here.",
          "upvote_count": "3",
          "timestamp": "1724959620.0"
        },
        {
          "comment_id": "1153694",
          "poster": "dzn",
          "content": "Selected Answer: D\nLightsail does not have built-in Blue/Green deployment capabilities like Elastic Beanstalk.",
          "timestamp": "1724031840.0",
          "upvote_count": "2"
        },
        {
          "timestamp": "1705013820.0",
          "upvote_count": "3",
          "content": "Selected Answer: D\nI was about to discard D because I was unsure if beanstalk supported GO (yes it does )\nhttps://docs.aws.amazon.com/elasticbeanstalk/latest/dg/concepts.platforms.html\n\nSo D is undoubtedly the best option to quickly move to cloud and to do blue green with an testing",
          "comment_id": "949308",
          "poster": "Kiroo"
        },
        {
          "content": "Selected Answer: D\nI guess D is easiest option to orchestrate blue/green deployments and A/B testing in this case.",
          "timestamp": "1702202220.0",
          "comment_id": "919898",
          "poster": "madperro",
          "upvote_count": "2"
        },
        {
          "poster": "rdoty",
          "comment_id": "911217",
          "upvote_count": "1",
          "timestamp": "1701353280.0",
          "content": "Selected Answer: D\nD elastic beanstalk due to deployment options"
        },
        {
          "comment_id": "898436",
          "upvote_count": "1",
          "poster": "mgonblan",
          "timestamp": "1700068800.0",
          "content": "Maybe D, but it looks like an old approach. we need to use codebuild and codepipelines and Elastic beanstalk, but elastic beanstalk could be changed by AWS cloudformation."
        },
        {
          "poster": "haazybanj",
          "upvote_count": "4",
          "comment_id": "886935",
          "content": "Selected Answer: D\nD. Use AWS Elastic Beanstalk to host the application. Store a zipped version of the application in Amazon S3. Use that location to deploy new versions of the application. Use Elastic Beanstalk to manage the deployment options.\n\nAWS Elastic Beanstalk provides a platform for deploying web applications, which is well-suited for use cases that require blue/green deployments and A/B testing. Elastic Beanstalk can deploy applications written in a variety of programming languages and frameworks, including Go. Elastic Beanstalk supports blue/green deployments, which allow you to deploy a new version of your application to a separate environment before switching traffic to it. This enables you to perform A/B testing before fully rolling out a new version of your application. Elastic Beanstalk also allows you to manage the deployment options, including the deployment strategy, instance types, and autoscaling options.",
          "timestamp": "1698894480.0"
        },
        {
          "comment_id": "870479",
          "poster": "alce2020",
          "timestamp": "1697317980.0",
          "upvote_count": "1",
          "content": "D it is"
        },
        {
          "poster": "ele",
          "comment_id": "864531",
          "timestamp": "1696751460.0",
          "upvote_count": "1",
          "content": "Selected Answer: D\nElastic Beanstalk"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:15:11.904Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "vNfS3TS9AybrbprlsaIH",
      "question_number": 191,
      "page": 39,
      "question_text": "A company uses AWS Organizations to manage hundreds of AWS accounts. The company has a team that is responsible for AWS Identity and Access Management (IAM).\n\nThe IAM team wants to implement AWS IAM Identity Center (AWS Single Sign-On). The IAM team must have only the minimum needed permissions to manage IAM Identity Center. The IAM team must not be able to gain unneeded access to the Organizations management account. The IAM team must be able to provision new IAM Identity Center permission sets and assignments for existing and new member accounts.\n\nWhich combination of steps will meet these requirements? (Choose three.)",
      "choices": {
        "D": "In IAM Identity Center, create users and a group for the IAM team. Add the users to the group. Create a new permission set. Attach the AWSSSOMemberAccountAdministrator managed IAM policy to the group.",
        "C": "In IAM Identity Center, create users and a group for the IAM team. Add the users to the group. Create a new permission set. Attach the AWSSSODirectoryAdministrator managed IAM policy to the group.",
        "E": "Assign the permission set to the Organizations management account. Allow the IAM team group to use the permission set.",
        "F": "Assign the permission set to the new AWS account. Allow the IAM team group to use the permission set.",
        "A": "Create a new AWS account for the IAM team. In the new account, enable IAM Identity Center. In the Organizations management account, register the new account as a delegated administrator for IAM Identity Center.",
        "B": "Create a new AWS account for the IAM team. In the Organizations management account, enable IAM Identity Center. In the Organizations management account, register the new account as a delegated administrator for IAM Identity Center."
      },
      "correct_answer": "BDF",
      "answer_ET": "BDF",
      "answers_community": [
        "BDF (41%)",
        "ADF (39%)",
        "BCF (17%)",
        "2%"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/143399-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-07-06 09:46:00",
      "unix_timestamp": 1720251960,
      "discussion_count": 13,
      "discussion": [
        {
          "timestamp": "1726171440.0",
          "comment_id": "1282822",
          "content": "Selected Answer: BDF\nFor B see this: https://aws.amazon.com/blogs/security/getting-started-with-aws-sso-delegated-administration/\nFor D: compare the two policies AWSSSODirectoryAdministrator does not grant managment of permission sets\nF: makes sure that the IAM team has the necessary permissions within their designated account",
          "upvote_count": "8",
          "poster": "Shenannigan"
        },
        {
          "comment_id": "1248032",
          "timestamp": "1721011440.0",
          "content": "Selected Answer: ADF\nA ensures that the IAM team operates within their own account, isolating their permissions and activities from the Organizations management account.\nD provides the IAM team with the necessary permissions to manage IAM Identity Center across member accounts, without granting broader access.\n*Note that AWSSSODirectoryAdministrator policy grants broader permissions than necessary\nF. ensures that the IAM team has the necessary permissions within their designated account\n\nB. \"The IAM team must not be able to gain unneeded access to the Organizations management account\" => So B is wrong\nC contradicting the principle of least privilege.\nE should be avoided to prevent the IAM team from gaining unneeded access.",
          "upvote_count": "7",
          "poster": "trungtd"
        },
        {
          "poster": "AWSLoverLoverLoverLoverLover",
          "content": "Selected Answer: BDF\nSelected Answer: BDF",
          "upvote_count": "1",
          "comment_id": "1571837",
          "timestamp": "1748075760.0"
        },
        {
          "comment_id": "1571278",
          "timestamp": "1747910400.0",
          "poster": "AWSLoverLoverLoverLoverLover",
          "content": "Selected Answer: BDF\nBDF are correct",
          "upvote_count": "1"
        },
        {
          "timestamp": "1734523500.0",
          "comment_id": "1328485",
          "content": "Selected Answer: ADF\nReally helpful\n\nhttps://www.youtube.com/watch?v=aXqRKlvK160",
          "comments": [
            {
              "poster": "CHRIS12722222",
              "upvote_count": "1",
              "timestamp": "1735082640.0",
              "content": "The video suggests it is B, as identity centre was already enabled before delegating to the new account",
              "comment_id": "1331269"
            }
          ],
          "poster": "luisfsm_111",
          "upvote_count": "2"
        },
        {
          "poster": "teo2157",
          "content": "Selected Answer: BDF\nB. It's required to enable AWS IAM Identity Center in the Management Account and later delegate administration of AWS IAM Identity Center to a specific member account where the IAM team operates.\nD. To meet the requirements of the IAM team needing to provision new IAM Identity Center permission sets and assignments for existing and new member accounts, while ensuring they do not gain unneeded access to the Organizations management account, you should use the `AWSSSOMemberAccountAdministrator` policy. This policy provides the necessary permissions to manage AWS SSO settings and assignments within member accounts without granting full administrative access to the AWS SSO directory.\nF. The IAM roles or users must be created in the delegated member account for the IAM team to prevent the IAM team from gaining unneeded access.",
          "upvote_count": "5",
          "comment_id": "1326064",
          "timestamp": "1734080160.0"
        },
        {
          "content": "Selected Answer: BDF\nB D F is correct.\nYou enable the Identity Center in the management account first, not in the new account.\nFor assignment, you need AWSSSOMemberAccountAdministrator, not AWSSSODirectoryAdministrator.",
          "upvote_count": "4",
          "poster": "tinyshare",
          "comment_id": "1312365",
          "timestamp": "1731636240.0",
          "comments": [
            {
              "content": "For B: https://aws.amazon.com/blogs/security/getting-started-with-aws-sso-delegated-administration/\nFor D: https://docs.aws.amazon.com/singlesignon/latest/userguide/security-iam-awsmanpol.html",
              "upvote_count": "1",
              "comment_id": "1312367",
              "poster": "tinyshare",
              "timestamp": "1731636480.0"
            }
          ]
        },
        {
          "content": "Selected Answer: C\nC for sure.\n\nThe AWSSSOMemberAccountAdministrator policy provides required administrative actions to principals. The policy is intended for principals who perform the job role of an IAM Identity Center administrator.\n\nhttps://docs.aws.amazon.com/singlesignon/latest/userguide/security-iam-awsmanpol.html#security-iam-awsmanpol-AWSSSOMemberAccountAdministrator",
          "poster": "heff_bezos",
          "timestamp": "1727454480.0",
          "comments": [
            {
              "poster": "heff_bezos",
              "upvote_count": "1",
              "content": "This policy grants administrative permissions over IAM Identity Center users and groups. Principals with this policy attached can make any updates to IAM Identity Center users and groups.",
              "comment_id": "1290142",
              "comments": [
                {
                  "poster": "heff_bezos",
                  "content": "AWSSSODirectoryAdministrator is the policy described above",
                  "timestamp": "1727454600.0",
                  "upvote_count": "1",
                  "comment_id": "1290143"
                }
              ],
              "timestamp": "1727454540.0"
            }
          ],
          "comment_id": "1290141",
          "upvote_count": "1"
        },
        {
          "comment_id": "1273088",
          "timestamp": "1724721600.0",
          "content": "Selected Answer: BCF\nOption B: Create a new AWS account for the IAM team. In the Organizations management account, enable IAM Identity Center. Register the new account as a delegated administrator for IAM Identity Center. This ensures that the IAM team can manage IAM Identity Center without gaining access to the Organizations management account.\nOption C: In IAM Identity Center, create users and a group for the IAM team. Add the users to the group. Create a new permission set and attach the AWSSSODirectoryAdministrator managed IAM policy to the group. This allows the IAM team to provision new permission sets and assignments for member accounts.\nOption F: Assign the permission set to the new AWS account. Allow the IAM team group to use the permission set. This ensures that the IAM team can effectively manage IAM Identity Center in their dedicated account.",
          "poster": "limelight04",
          "upvote_count": "3"
        },
        {
          "comments": [
            {
              "poster": "jamesf",
              "timestamp": "1722993000.0",
              "comment_id": "1261889",
              "content": "keywords: \n- minimum needed permissions to manage IAM Identity Center. \n- unneeded access to the Organizations management account.",
              "upvote_count": "2"
            }
          ],
          "poster": "jamesf",
          "timestamp": "1722417420.0",
          "upvote_count": "4",
          "comment_id": "1258746",
          "content": "Selected Answer: ADF\nI go for ADF\nOption A: This option creates a new account for IAM Identity Center management, separating it from the Organizations management account. This helps in maintaining the principle of least privilege and ensures that IAM Identity Center management is handled without direct access to broader organizational settings.\n\nOption D: The AWSSSOMemberAccountAdministrator policy provides comprehensive IAM Identity Center permissions needed for provisioning new permission sets and assignments across member accounts. This policy aligns well with the requirement to manage IAM Identity Center with full administrative capabilities. (Require to manage new and all member accounts)"
        },
        {
          "upvote_count": "2",
          "poster": "d9iceguy",
          "timestamp": "1721750820.0",
          "comment_id": "1253756",
          "content": "Selected Answer: BCF\nAWSSSODirectoryAdministrator policy better\nAWSSSOMasterAccountAdministrator gives too much permissions\nhttps://docs.aws.amazon.com/singlesignon/latest/userguide/security-iam-awsmanpol.html\n\nB also because you want to enabled AWS IAM IDC in the management account and delegate administration to the IAM account"
        },
        {
          "upvote_count": "5",
          "timestamp": "1721048520.0",
          "comment_id": "1248355",
          "content": "Selected Answer: ADF\n---> ADF",
          "poster": "tgv"
        },
        {
          "timestamp": "1720778940.0",
          "poster": "TEC1",
          "comment_id": "1246634",
          "content": "Selected Answer: BCF\nB - This step is correct because it enables IAM Identity Center in the management account (which is necessary) and then delegates administration to a separate account for the IAM team. This approach follows the principle of least privilege by not giving the IAM team unnecessary access to the management account.\n\nC - This step is correct because it sets up the necessary users and groups in IAM Identity Center and assigns the appropriate permissions. The AWSSSODirectoryAdministrator policy provides the necessary permissions to manage IAM Identity Center without granting excessive privileges.\n\nF - This step completes the setup by assigning the permission set to the new account created for the IAM team, allowing them to perform their duties within that account rather than in the management account.",
          "upvote_count": "3"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:15:22.318Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "fQJ1VYGhHUA6iACqZrRc",
      "question_number": 192,
      "page": 39,
      "question_text": "A company uses an organization in AWS Organizations that has all features enabled. The company uses AWS Backup in a primary account and uses an AWS Key Management Service (AWS KMS) key to encrypt the backups.\n\nThe company needs to automate a cross-account backup of the resources that AWS Backup backs up in the primary account. The company configures cross-account backup in the Organizations management account. The company creates a new AWS account in the organization and configures an AWS Backup backup vault in the new account. The company creates a KMS key in the new account to encrypt the backups. Finally, the company configures a new backup plan in the primary account. The destination for the new backup plan is the backup vault in the new account.\n\nWhen the AWS Backup job in the primary account is invoked, the job creates backups in the primary account. However, the backups are not copied to the new account's backup vault.\n\nWhich combination of steps must the company take so that backups can be copied to the new account's backup vault? (Choose two.)",
      "choices": {
        "B": "Edit the backup vault access policy in the primary account to allow access to the new account.",
        "D": "Edit the key policy of the KMS key in the primary account to share the key with the new account.",
        "A": "Edit the backup vault access policy in the new account to allow access to the primary account.",
        "E": "Edit the key policy of the KMS key in the new account to share the key with the primary account.",
        "C": "Edit the backup vault access policy in the primary account to allow access to the KMS key in the new account."
      },
      "correct_answer": "AD",
      "answer_ET": "AD",
      "answers_community": [
        "AD (72%)",
        "AE (28%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/143435-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-07-06 18:16:00",
      "unix_timestamp": 1720282560,
      "discussion_count": 11,
      "discussion": [
        {
          "upvote_count": "8",
          "comment_id": "1258992",
          "poster": "auxwww",
          "content": "Selected Answer: AD\nhttps://docs.aws.amazon.com/aws-backup/latest/devguide/create-cross-account-backup.html\n\nIn your destination account, you must create a backup vault. Then, you assign a customer managed key to encrypt backups in the destination account, and a resource-based access policy to allow AWS Backup to access the resources you would like to copy. In the source account, if your resources are encrypted with a customer managed key, you must share this customer managed key with the destination account. You can then create a backup plan and choose a destination account that is part of your organizational unit in AWS Organizations.",
          "timestamp": "1722444960.0"
        },
        {
          "poster": "xdkonorek2",
          "upvote_count": "5",
          "timestamp": "1720282560.0",
          "content": "Selected Answer: AD\nbackup a backup using aws backup to backup account :) \nAD\nsecond paragraph: https://docs.aws.amazon.com/aws-backup/latest/devguide/create-cross-account-backup.html",
          "comment_id": "1243444"
        },
        {
          "poster": "teo2157",
          "upvote_count": "2",
          "comment_id": "1341501",
          "content": "Selected Answer: AD\n@auxwww expplanation is perfect",
          "timestamp": "1737010740.0"
        },
        {
          "timestamp": "1733746020.0",
          "upvote_count": "1",
          "poster": "luisfsm_111",
          "content": "Selected Answer: AE\nIn my view, D is not needed because the backups in the new account will use the KMS key in the new account, not the primary account’s key.",
          "comment_id": "1324026"
        },
        {
          "timestamp": "1729209780.0",
          "content": "Selected Answer: AD\nA,D - Correct\n\n\"n your destination account, you must create a backup vault. Then, you assign a customer managed key to encrypt backups in the destination account, and a resource-based access policy to allow AWS Backup to access the resources you would like to copy. In the source account, if your resources are encrypted with a customer managed key, you must share this customer managed key with the destination account. You can then create a backup plan and choose a destination account that is part of your organizational unit in AWS Organizations.\"",
          "poster": "auxwww",
          "comment_id": "1299491",
          "upvote_count": "3"
        },
        {
          "comment_id": "1273701",
          "upvote_count": "2",
          "content": "Selected Answer: AE\nOption A: Edit the backup vault access policy in the new account to allow access to the primary account. This step ensures that the primary account has the necessary permissions to copy backups into the new account’s backup vault.\n\nOption E: Edit the key policy of the KMS key in the new account to share the key with the primary account. This step allows the primary account to use the KMS key in the new account for encryption during the backup copy process",
          "poster": "limelight04",
          "timestamp": "1724797500.0"
        },
        {
          "comments": [
            {
              "timestamp": "1722419100.0",
              "content": "A. Edit the backup vault access policy in the new account to allow access from the primary account.\nE. Edit the key policy of the KMS key in the new account to share the key with the primary account.\n\nBackup Plan and Resource located in Management Account.\nBackup Vault and KMS Key located in new account. \n\nBased on URLs below, still confusing as the KMS key in new account (Destination) already\nhttps://docs.aws.amazon.com/aws-backup/latest/devguide/create-cross-account-backup.html\nhttps://repost.aws/knowledge-center/backup-troubleshoot-cross-account-copy\n\nHope someone choose option D can explain further why option D but not E.",
              "poster": "jamesf",
              "comment_id": "1258756",
              "upvote_count": "1"
            },
            {
              "poster": "jamesf",
              "comment_id": "1260967",
              "timestamp": "1722843180.0",
              "upvote_count": "1",
              "content": "Seen like D correct \n- For the resources that aren't fully managed by AWS Backup, the backups use the same KMS key as the source resource.\n- For the resources that are fully managed by AWS Backup, the backups are encrypted with encryption key of the backup vault.\nhttps://repost.aws/knowledge-center/backup-troubleshoot-cross-account-copy"
            }
          ],
          "comment_id": "1258749",
          "poster": "jamesf",
          "timestamp": "1722418500.0",
          "upvote_count": "2",
          "content": "Selected Answer: AE\nI prefer AE as\n1. the company need cross-account backup but not cross-account copy. \n2. And the KMS key created in new account for backup encryption. \nhighlighted keys: \n- The company configures cross-account backup in the Organizations management account. \n- The company creates a new AWS account in the organization and configures an AWS Backup backup vault in the new account. \n- The company creates a KMS key in the new account to encrypt the backups. \n- Finally, the company configures a new backup plan in the primary account. \n- The destination for the new backup plan is the backup vault in the new account."
        },
        {
          "timestamp": "1721752440.0",
          "upvote_count": "4",
          "comment_id": "1253775",
          "content": "Selected Answer: AD\nD - https://docs.aws.amazon.com/aws-backup/latest/devguide/create-cross-account-backup.html#backup-cab-encryption\nDuring a cross-account copy, the source account KMS key policy must allow the destination account on the KMS key policy.",
          "poster": "d9iceguy"
        },
        {
          "comment_id": "1250414",
          "timestamp": "1721307900.0",
          "upvote_count": "4",
          "poster": "inturist",
          "content": "Selected Answer: AD\nA, D \nhttps://docs.aws.amazon.com/aws-backup/latest/devguide/create-cross-account-backup.html"
        },
        {
          "timestamp": "1721013060.0",
          "upvote_count": "4",
          "content": "Selected Answer: AE\nA: Ensures the primary account can access the backup vault in the new account.\nE: Ensures the primary account can use the KMS key in the new account for encryption.",
          "comment_id": "1248045",
          "poster": "trungtd"
        },
        {
          "upvote_count": "1",
          "comment_id": "1246447",
          "timestamp": "1720754460.0",
          "content": "Selected Answer: AE\nVOTE AE",
          "poster": "siheom"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:15:22.318Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "r4CluOxck2UpNuJCZMMC",
      "question_number": 193,
      "page": 39,
      "question_text": "A company runs an application that uses an Amazon S3 bucket to store images. A DevOps engineer needs to implement a multi-Region strategy for the objects that are stored in the S3 bucket. The company needs to be able to fail over to an S3 bucket in another AWS Region. When an image is added to either S3 bucket, the image must be replicated to the other S3 bucket within 15 minutes.\n\nThe DevOps engineer enables two-way replication between the S3 buckets.\n\nWhich combination of steps should the DevOps engineer take next to meet the requirements? (Choose three.)",
      "choices": {
        "E": "Configure a routing control in Amazon Route 53 Recovery Controller. Add the S3 buckets in an active-passive configuration.",
        "B": "Create an S3 Multi-Region Access Point in an active-passive configuration.",
        "A": "Enable S3 Replication Time Control (S3 RTC) on each replication rule.",
        "D": "Enable S3 Transfer Acceleration on both S3 buckets.",
        "F": "Call the UpdateRoutingControlStates operation in the AWS API when the company needs to fail over to the S3 bucket in the other Region.",
        "C": "Call the SubmitMultiRegionAccessPointRoutes operation in the AWS API when the company needs to fail over to the S3 bucket in the other Region."
      },
      "correct_answer": "ABC",
      "answer_ET": "ABC",
      "answers_community": [
        "ABC (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/143418-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-07-06 14:37:00",
      "unix_timestamp": 1720269420,
      "discussion_count": 5,
      "discussion": [
        {
          "upvote_count": "1",
          "content": "Selected Answer: ABC\naws s3control submit-multi-region-access-point-routes \\\n --account-id <account-id> \\\n --mrap <multi-region-access-point-arn> \\\n --route-updates '[{\"Bucket\":\"<bucket-name>\",\"Region\":\"<region-name>\",\"TrafficDialPercentage\":100},{\"Bucket\":\"<bucket-name>\",\"Region\":\"<region-name>\",\"TrafficDialPercentage\":0}]'",
          "poster": "DKM",
          "timestamp": "1742320980.0",
          "comment_id": "1400229"
        },
        {
          "content": "Selected Answer: ABC\nhttps://aws.amazon.com/getting-started/hands-on/getting-started-with-amazon-s3-multi-region-access-points/",
          "poster": "jamesf",
          "timestamp": "1722429120.0",
          "upvote_count": "2",
          "comment_id": "1258851"
        },
        {
          "timestamp": "1721013780.0",
          "upvote_count": "3",
          "comment_id": "1248047",
          "poster": "trungtd",
          "content": "Selected Answer: ABC\nUpdateRoutingControlStates is for Route53"
        },
        {
          "comment_id": "1245721",
          "poster": "TEC1",
          "content": "Selected Answer: ABC\nA. Enable S3 Replication Time Control (S3 RTC) on each replication rule. This ensures that 99.99% of objects are replicated within 15 minutes.\n\nB. Create an S3 Multi-Region Access Point in an active-passive configuration. This is crucial for managing access to data across multiple S3 buckets in different AWS Regions and facilitating failover.\n\nC. Call the SubmitMultiRegionAccessPointRoutes operation in the AWS API when the company needs to fail over to the S3 bucket in the other Region. This operation updates the routes to direct traffic to the designated S3 bucket, enabling failover.",
          "timestamp": "1720641720.0",
          "upvote_count": "4"
        },
        {
          "upvote_count": "4",
          "comment_id": "1243581",
          "content": "ABC\nhttps://aws.amazon.com/getting-started/hands-on/getting-started-with-amazon-s3-multi-region-access-points/?ref=docs_gateway/amazons3/MultiRegionAccessPoints.html",
          "timestamp": "1720304520.0",
          "poster": "getadroit"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:15:22.318Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "hHDpuqk4ABOSJBdCM7hq",
      "question_number": 194,
      "page": 39,
      "question_text": "A company uses the AWS Cloud Development Kit (AWS CDK) to define its application. The company uses a pipeline that consists of AWS CodePipeline and AWS CodeBuild to deploy the CDK application.\n\nThe company wants to introduce unit tests to the pipeline to test various infrastructure components. The company wants to ensure that a deployment proceeds if no unit tests result in a failure.\n\nWhich combination of steps will enforce the testing requirement in the pipeline? (Choose two.)",
      "choices": {
        "A": "Update the CodeBuild build phase commands to run the tests then to deploy the application. Set the OnFailure phase property to ABORT.",
        "D": "Create a test that uses the AWS CDK assertions module. Use the template.hasResourceProperties assertion to test that resources have the expected properties.",
        "E": "Create a test that uses the cdk diff command. Configure the test to fail if any resources have changed.",
        "C": "Update the CodeBuild build phase commands to run the tests then to deploy the application. Add the --require-approval any-change flag to the cdk deploy command.",
        "B": "Update the CodeBuild build phase commands to run the tests then to deploy the application. Add the --rollback true flag to the cdk deploy command."
      },
      "correct_answer": "AD",
      "answer_ET": "AD",
      "answers_community": [
        "AD (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/143414-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-07-06 13:32:00",
      "unix_timestamp": 1720265520,
      "discussion_count": 3,
      "discussion": [
        {
          "content": "Selected Answer: AD\nOption A: <OnFailure phase>\n- This option involves configuring the build phase in CodeBuild to run tests before deployment. Setting the OnFailure phase property to ABORT ensures that if any test fails, the build process is stopped, and the deployment doesn't proceed.\n- This ensures that the build and deployment process only proceeds if the tests are successful, which is crucial for maintaining application integrity.\n\nOption D: <CDK assertions>\n- This option involves writing unit tests using the AWS CDK assertions module. The template.hasResourceProperties assertion checks if CloudFormation templates have resources with expected properties, ensuring the infrastructure's logical correctness.\n- This enforces unit testing by checking that the CloudFormation templates have the expected properties, ensuring the application’s infrastructure is as intended.",
          "timestamp": "1722997080.0",
          "upvote_count": "3",
          "poster": "jamesf",
          "comment_id": "1261906"
        },
        {
          "poster": "trungtd",
          "timestamp": "1721014260.0",
          "upvote_count": "3",
          "content": "Selected Answer: AD\nI'm not sure but A D seems right",
          "comment_id": "1248051"
        },
        {
          "upvote_count": "3",
          "comment_id": "1245723",
          "content": "Selected Answer: AD\nA. This step is crucial because:\n\nIt integrates the unit tests into the build phase of CodeBuild.\nBy setting the OnFailure property to ABORT, it ensures that the pipeline stops if any tests fail, preventing deployment of potentially faulty infrastructure.\nIf all tests pass, the deployment will proceed as normal.\n\nD. This step is important because:\n\nIt utilizes the AWS CDK assertions module, which is specifically designed for testing CDK applications.\nThe template.hasResourceProperties assertion allows you to verify that the resources defined in your CDK code have the expected properties.\nThis type of test can catch issues in your infrastructure definition before deployment.",
          "timestamp": "1720641960.0",
          "poster": "TEC1"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:15:22.318Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "m43y3BZZLHAo4EQABEl3",
      "question_number": 195,
      "page": 39,
      "question_text": "A company has an application that runs on Amazon EC2 instances behind an Application Load Balancer (ALB). The EC2 instances are in multiple Availability Zones. The application was misconfigured in a single Availability Zone, which caused a partial outage of the application.\n\nA DevOps engineer made changes to ensure that the unhealthy EC2 instances in one Availability Zone do not affect the healthy EC2 instances in the other Availability Zones. The DevOps engineer needs to test the application's failover and shift where the ALB sends traffic. During failover, the ALB must avoid sending traffic to the Availability Zone where the failure has occurred.\n\nWhich solution will meet these requirements?",
      "choices": {
        "A": "Turn off cross-zone load balancing on the ALB. Use Amazon Route 53 Application Recovery Controller to start a zonal shift away from the Availability Zone.",
        "D": "Create an Amazon Route 53 Application Recovery Controller resource set that uses the ARN of the ALB’s target group. Create a readiness check that uses the ElbV2TargetGroupsCanServeTraffic rule.",
        "C": "Create an Amazon Route 53 Application Recovery Controller resource set that uses the DNS hostname of the ALB. Start a zonal shift for the resource set away from the Availability Zone.",
        "B": "Turn off cross-zone load balancing on the ALB’s target group. Use Amazon Route 53 Application Recovery Controller to start a zonal shift away from the Availability Zone."
      },
      "correct_answer": "A",
      "answer_ET": "A",
      "answers_community": [
        "A (50%)",
        "B (38%)",
        "12%"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/143413-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-07-06 13:19:00",
      "unix_timestamp": 1720264740,
      "discussion_count": 11,
      "discussion": [
        {
          "poster": "inturist",
          "comments": [
            {
              "comment_id": "1256843",
              "upvote_count": "1",
              "poster": "Exto1124",
              "content": "\"With Application Load Balancers, cross-zone load balancing is always turned on at the load balancer level, and cannot be turned off. For target groups, the default is to use the load balancer setting, but you can override the default by explicitly turning cross-zone load balancing off at the target group level.\"\nhttps://docs.aws.amazon.com/elasticloadbalancing/latest/application/disable-cross-zone.html#:~:text=The%20nodes%20for%20your%20load,in%20all%20registered%20Availability%20Zones.",
              "timestamp": "1722177300.0"
            }
          ],
          "content": "Selected Answer: A\nFor me the correct answer is A:\"Note that the Elastic Load Balancing resources must have cross-zone load balancing turned off to use this capability.\"\nhttps://docs.aws.amazon.com/r53recovery/latest/dg/arc-zonal-shift.html",
          "upvote_count": "6",
          "timestamp": "1721130660.0",
          "comment_id": "1248861"
        },
        {
          "comment_id": "1570328",
          "timestamp": "1747679700.0",
          "poster": "nickp84",
          "upvote_count": "1",
          "content": "Selected Answer: A\nThe statement is incorrect because cross-zone load balancing for an Application Load Balancer (ALB) is configured at the ALB level, not the target group level. To meet the requirements from the previous question:\nDisable cross-zone load balancing on the ALB using the load_balancing.cross_zone.enabled=false attribute to isolate traffic to each AZ, ensuring unhealthy instances in one AZ do not receive traffic (assuming health checks mark them unhealthy).\n\nUse Amazon Route 53 Application Recovery Controller to start a zonal shift to redirect ALB traffic away from the affected AZ during failover testing.\n\nThe correct solution remains Option A"
        },
        {
          "upvote_count": "1",
          "comment_id": "1563432",
          "content": "Selected Answer: C\nThis solution leverages Amazon Route 53 Application Recovery Controller to initiate a zonal shift and ensures that traffic is redirected away from the failing Availability Zone while keeping the ALB in operation.",
          "poster": "Rs123x",
          "timestamp": "1745524800.0"
        },
        {
          "upvote_count": "1",
          "poster": "jojewi8143",
          "comment_id": "1350358",
          "content": "Selected Answer: A\ni choose a",
          "timestamp": "1738494300.0"
        },
        {
          "content": "Selected Answer: B\nCant turn off cross zone load balancing at ALB level",
          "timestamp": "1735124640.0",
          "poster": "CHRIS12722222",
          "comment_id": "1331524",
          "upvote_count": "3"
        },
        {
          "upvote_count": "2",
          "poster": "spring21",
          "content": "Selected Answer: B\nTo turn off cross-zone load balancing on an Application Load Balancer (ALB), you cannot directly disable it at the ALB level; instead, you need to modify the target group settings and explicitly disable cross-zone load balancing within the target group attributes",
          "comment_id": "1327560",
          "timestamp": "1734372180.0"
        },
        {
          "comment_id": "1283518",
          "timestamp": "1726296600.0",
          "content": "Selected Answer: B\nAnswer is B\nYou can not turn of cross-zone load balancing on an ALB: https://docs.aws.amazon.com/elasticloadbalancing/latest/application/application-load-balancers.html#cross-zone-load-balancing\n\nYou can turn it off at the Target Group: https://aws.amazon.com/about-aws/whats-new/2022/11/application-load-balancers-turning-off-cross-zone-load-balancing-per-target-group/\n\nzonal shift:\nhttps://docs.aws.amazon.com/elasticloadbalancing/latest/userguide/how-elastic-load-balancing-works.html#cross-zone-load-balancing#:~:text=zonal%20shift",
          "upvote_count": "4",
          "poster": "Shenannigan"
        },
        {
          "comment_id": "1273703",
          "poster": "limelight04",
          "upvote_count": "2",
          "content": "Selected Answer: A\nA is the most appropriate answer due to the reasons below;\n\nOption A: Turning off cross-zone load balancing on the ALB and using Amazon Route 53 Application Recovery Controller to start a zonal shift away from the affected Availability Zone ensures that traffic is not sent to the unhealthy instances in the problematic zone. This directly addresses the need to avoid sending traffic to the Availability Zone where the failure has occurred.\n\nOption D: While creating a resource set and readiness check with Amazon Route 53 Application Recovery Controller is useful for monitoring and ensuring traffic is routed to healthy instances, it doesn’t explicitly mention turning off cross-zone load balancing, which is crucial for isolating the affected Availability Zone.",
          "timestamp": "1724798700.0"
        },
        {
          "timestamp": "1724657100.0",
          "content": "Selected Answer: B\nTurning off cross-zone load balancing at the target group level ensures that each target group handles traffic independently for its specific Availability Zone. Why specifically target group is because cross-zone load balancing is always turned on and cannot be turned off for a ALB. But, if using target group, the default is to use the load balancer setting, which you can override the default by explicitly turning cross-zone load balancing off at the target group level.\n\nThe next part us to move away from the affected region by using Amazon Route 53 Application Recovery Controller. \n\nOption A works well for a ELB, but for ALB, you can't turn off cross-zone load balancing. \n\nOption C is good to handle DNS failover, but doesn't stop traffic from going to the affected zone. \n\nOption D doesn't handle moving of traffic away from the affected zone and add extra complexity by introducing a readiness check.",
          "poster": "chinchin97",
          "comment_id": "1272505",
          "upvote_count": "1"
        },
        {
          "content": "Selected Answer: A\n- Turning off cross-zone load balancing on the ALB ensures that each Availability Zone only handles traffic directed to its own healthy instances, allowing for granular control over traffic distribution. This prevents the ALB from sending traffic to the unhealthy instances across all zones, thereby isolating the problem to the affected zone.\n\n- Amazon Route 53 Application Recovery Controller's Zonal Shifts is used to direct traffic away from a specific AZ that experiences a failure, allowing the ALB to reroute requests to healthy AZs automatically.\n\nAmazon Route 53 Application Recovery Controller currently supports the following resources for zonal shift and zonal autoshift:\n- Network Load Balancers with cross-zone load balancing disabled\n- Application Load Balancers with cross-zone load balancing disabled\nhttps://docs.aws.amazon.com/r53recovery/latest/dg/arc-zonal-shift.resource-types.html",
          "comment_id": "1258869",
          "upvote_count": "3",
          "comments": [
            {
              "upvote_count": "2",
              "poster": "jamesf",
              "comment_id": "1258871",
              "content": "B Incorrect: This option mentions turning off cross-zone load balancing on the ALB's target group rather than the ALB itself. This does not apply because cross-zone load balancing is a property of the ALB, not the target group. This makes the solution nonviable because the configuration settings described do not exist on target groups.",
              "timestamp": "1722430860.0"
            }
          ],
          "poster": "jamesf",
          "timestamp": "1722430800.0"
        },
        {
          "poster": "trungtd",
          "timestamp": "1721014680.0",
          "content": "Selected Answer: C\nA&B Turn off cross-zone load balancing is a bad idea\nD involves creating a readiness check rule (ElbV2TargetGroupsCanServeTraffic) which checks the ability of the ALB’s target groups to serve traffic. However, this does not directly control traffic routing based on Availability Zone health.\n\nC Route 53 Application Recovery Controller: It directly manages traffic based on health checks and allows for zonal shifts\nZonal Shift: Specifically addresses the requirement to avoid sending traffic to an Availability Zone experiencing issues",
          "comment_id": "1248054",
          "upvote_count": "2"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:15:22.318Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "3ahssDznYpZ0TRTiST8w",
      "question_number": 196,
      "page": 40,
      "question_text": "A company sends its AWS Network Firewall flow logs to an Amazon S3 bucket. The company then analyzes the flow logs by using Amazon Athena.\n\nThe company needs to transform the flow logs and add additional data before the flow logs are delivered to the existing S3 bucket.\n\nWhich solution will meet these requirements?",
      "choices": {
        "C": "Create an Amazon EventBridge rule that is associated with the default EventBridge event bus. Configure the rule to react to all object create events for the existing S3 bucket. Define a new S3 bucket as the target for the rule. Create an EventBridge input transformation to customize the event before passing the event to the rule target.",
        "A": "Create an AWS Lambda function to transform the data and to write a new object to the existing S3 bucket. Configure the Lambda function with an S3 trigger for the existing S3 bucket. Specify all object create events for the event type. Acknowledge the recursive invocation.",
        "D": "Create an Amazon Kinesis Data Firehose delivery stream that is configured with an AWS Lambda transformer. Specify the existing S3 bucket as the destination. Change the Network Firewall logging destination from Amazon S3 to Kinesis Data Firehose.",
        "B": "Enable Amazon EventBridge notifications on the existing S3 bucket. Create a custom EventBridge event bus. Create an EventBridge rule that is associated with the custom event bus. Configure the rule to react to all object create events for the existing S3 bucket and to invoke an AWS Step Functions workflow. Configure a Step Functions task to transform the data and to write the data into a new S3 bucket."
      },
      "correct_answer": "D",
      "answer_ET": "D",
      "answers_community": [
        "D (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/143412-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-07-06 13:08:00",
      "unix_timestamp": 1720264080,
      "discussion_count": 4,
      "discussion": [
        {
          "content": "Selected Answer: D\nAmazon Kinesis Data Firehose:\nKinesis Data Firehose is designed for real-time streaming data delivery and transformation. It can ingest data, process it with a Lambda function, and deliver the transformed data to destinations like Amazon S3, Redshift, or Elasticsearch.\nhttps://aws.amazon.com/firehose/faqs/\n\nAWS Lambda Transformer:\nBy configuring a Lambda function as a transformer within Kinesis Data Firehose, you can implement custom logic to transform the flow logs and add any additional data required before the logs are written to the existing S3 bucket.",
          "poster": "jamesf",
          "comment_id": "1258875",
          "timestamp": "1722431160.0",
          "comments": [
            {
              "comment_id": "1258877",
              "upvote_count": "3",
              "poster": "jamesf",
              "content": "keywords: transform, flow logs",
              "timestamp": "1722431220.0"
            }
          ],
          "upvote_count": "4"
        },
        {
          "upvote_count": "3",
          "comment_id": "1253803",
          "content": "Selected Answer: D\nD for me",
          "timestamp": "1721754660.0",
          "poster": "d9iceguy"
        },
        {
          "comment_id": "1248060",
          "content": "Selected Answer: D\nD for me",
          "upvote_count": "4",
          "poster": "trungtd",
          "timestamp": "1721015340.0"
        },
        {
          "comment_id": "1243586",
          "timestamp": "1720305060.0",
          "content": "Dhttps://aws.amazon.com/firehose/faqs/",
          "poster": "getadroit",
          "upvote_count": "3"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:15:32.765Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "1KDptaJe9ZSS8be4nYe5",
      "question_number": 197,
      "page": 40,
      "question_text": "A DevOps engineer needs to implement integration tests into an existing AWS CodePipeline CI/CD workflow for an Amazon Elastic Container Service (Amazon ECS) service. The CI/CD workflow retrieves new application code from an AWS CodeCommit repository and builds a container image. The Cl/CD workflow then uploads the container image to Amazon Elastic Container Registry (Amazon ECR) with a new image tag version.\n\nThe integration tests must ensure that new versions of the service endpoint are reachable and that various API methods return successful response data. The DevOps engineer has already created an ECS cluster to test the service.\n\nWhich combination of steps will meet these requirements with the LEAST management overhead? (Choose three.)",
      "choices": {
        "E": "Create an AWS Lambda function that runs connectivity checks and API calls against the service. Integrate the Lambda function with CodePipeline by using a Lambda action stage.",
        "B": "Add a deploy stage to the pipeline. Configure AWS CodeDeploy as the action provider.",
        "D": "Update the image build pipeline stage to output an imagedefinitions.json file that references the new image tag.",
        "A": "Add a deploy stage to the pipeline. Configure Amazon ECS as the action provider.",
        "C": "Add an appspec.yml file to the CodeCommit repository.",
        "F": "Write a script that runs integration tests against the service. Upload the script to an Amazon S3 bucket. Integrate the script in the S3 bucket with CodePipeline by using an S3 action stage."
      },
      "correct_answer": "ADE",
      "answer_ET": "ADE",
      "answers_community": [
        "ADE (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/143411-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-07-06 13:04:00",
      "unix_timestamp": 1720263840,
      "discussion_count": 3,
      "discussion": [
        {
          "poster": "xdkonorek2",
          "content": "Selected Answer: ADE\nADE\nhttps://docs.aws.amazon.com/codepipeline/latest/userguide/ecs-cd-pipeline.html",
          "timestamp": "1720263840.0",
          "upvote_count": "7",
          "comment_id": "1243336"
        },
        {
          "content": "Selected Answer: ADE\nADE for sure",
          "upvote_count": "2",
          "comment_id": "1332199",
          "timestamp": "1735269540.0",
          "poster": "youonebe"
        },
        {
          "poster": "jamesf",
          "content": "Selected Answer: ADE\nkeywords: LEAST management overhead\n\nA. Add a deploy stage to the pipeline. Configure Amazon ECS as the action provider.\n- Directly deploys the container image to ECS, ensuring the service is updated with the latest code without unnecessary complexity.\n\nD. Update the image build pipeline stage to output an image definitions.json file that references the new image tag.\n- Necessary for ECS to recognize and deploy the new image version, facilitating automated updates.\n\nE. Create an AWS Lambda function that runs connectivity checks and API calls against the service. Integrate the Lambda function with CodePipeline by using a Lambda action stage.\n- Provides a low-management solution for running integration tests, leveraging AWS Lambda's serverless capabilities.",
          "upvote_count": "3",
          "comment_id": "1261907",
          "timestamp": "1722997500.0"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:15:32.765Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "NG4qTrS2lMlE2KDLMKvG",
      "question_number": 198,
      "page": 40,
      "question_text": "A company runs applications on Windows and Linux Amazon EC2 instances. The instances run across multiple Availability Zones in an AWS Region. The company uses Auto Scaling groups for each application.\n\nThe company needs a durable storage solution for the instances. The solution must use SMB for Windows and must use NFS for Linux. The solution must also have sub-millisecond latencies. All instances will read and write the data.\n\nWhich combination of steps will meet these requirements? (Choose three.)",
      "choices": {
        "C": "Create a General Purpose SSD (gp3) Amazon Elastic Block Store (Amazon EBS) volume to use for shared storage.",
        "A": "Create an Amazon Elastic File System (Amazon EFS) file system that has targets in multiple Availability Zones.",
        "B": "Create an Amazon FSx for NetApp ONTAP Multi-AZ file system.",
        "F": "Update the EC2 instances for each application to mount the file system when new instances are launched.",
        "E": "Perform an instance refresh on each Auto Scaling group.",
        "D": "Update the user data for each application’s launch template to mount the file system."
      },
      "correct_answer": "BDE",
      "answer_ET": "BDE",
      "answers_community": [
        "BDE (81%)",
        "BDF (19%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/143410-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-07-06 12:59:00",
      "unix_timestamp": 1720263540,
      "discussion_count": 9,
      "discussion": [
        {
          "poster": "raycomh",
          "upvote_count": "5",
          "comment_id": "1247603",
          "content": "Selected Answer: BDE\nTo meet the requirements of the scenario, the company should take the following steps:\n\nCreate an Amazon FSx for NetApp ONTAP Multi-AZ file system (Option B): Amazon FSx for NetApp ONTAP supports both SMB (for Windows) and NFS (for Linux), and it provides sub-millisecond latencies. It also supports Multi-AZ configurations for high availability and durability.\nUpdate the user data for each application’s launch template to mount the file system (Option D): This ensures that every new instance launched by the Auto Scaling group will have the file system mounted.\nPerform an instance refresh on each Auto Scaling group (Option E): This will update the existing instances with the new launch template configuration, ensuring that they have the file system mounted.",
          "timestamp": "1720930740.0"
        },
        {
          "content": "Selected Answer: BDE\nD has updated UserData, new EC2 should be auto-mounted, why F?",
          "upvote_count": "2",
          "comment_id": "1308392",
          "poster": "VerRi",
          "timestamp": "1730989560.0"
        },
        {
          "comment_id": "1273704",
          "poster": "limelight04",
          "content": "Selected Answer: BDF\nOption B: Create an Amazon FSx for NetApp ONTAP Multi-AZ file system. This provides high-performance storage with support for both SMB and NFS protocols.\n\nOption D: Update the user data for each application’s launch template to mount the file system. This ensures that the file system is automatically mounted when new instances are launched.\n\nOption F: Update the EC2 instances for each application to mount the file system when new instances are launched. This ensures that all instances can read and write data to the file system",
          "timestamp": "1724800020.0",
          "upvote_count": "1"
        },
        {
          "poster": "jamesf",
          "timestamp": "1722433320.0",
          "comment_id": "1258907",
          "content": "Selected Answer: BDE\nafter review and check, BDE will be better\nOption E focuses on updating existing instances with the new configurations by replacing them with new instances based on the updated launch template. This is particularly useful when you want all instances, including those currently running, to immediately adhere to new configurations.\n\nB. Amazon FSx for NetApp ONTAP Multi-AZ file system.\n- SMB and NFS Support\n- Sub-Millisecond Latency\n- Multi-AZ Availability: ensures HA and fault tolerance, as the data is replicated across multiple Availability Zones, which aligns with the requirement for a durable storage solution.\n\nD. Update the user data for each application’s launch template to mount the file system.\n- Automated Mounting the FSx file system at startup\n- Protocol-Specific Commands: For Windows instances, mount the SMB share, while for Linux instances, mount the NFS share",
          "comments": [
            {
              "upvote_count": "1",
              "poster": "jamesf",
              "timestamp": "1722433380.0",
              "comment_id": "1258909",
              "content": "Not A. Amazon EFS as supports only NFS, not SMB support for Windows.\n\nNot C. Amazon EBS volume not natively support SMB or NFS, and EBS is block storage devices that attach to individual EC2 instances and do not support being shared directly across multiple instances."
            },
            {
              "comment_id": "1258914",
              "timestamp": "1722433620.0",
              "poster": "jamesf",
              "content": "Not Option F \"Update the EC2 instances for each application to mount the file system when new instances are launched.\"\n- because this is more about ensuring that future instances have the correct configuration, without immediately affecting running instances. It sets the stage for consistency moving forward but does not address existing instances.\n- and future instances also take care by Option D",
              "upvote_count": "1"
            }
          ],
          "upvote_count": "4"
        },
        {
          "comment_id": "1258898",
          "timestamp": "1722432420.0",
          "comments": [
            {
              "upvote_count": "1",
              "comments": [
                {
                  "timestamp": "1722433440.0",
                  "comment_id": "1258910",
                  "poster": "jamesf",
                  "content": "Hi Moderator, please delete this comment, thank you.",
                  "upvote_count": "1"
                }
              ],
              "comment_id": "1258900",
              "poster": "jamesf",
              "timestamp": "1722432600.0",
              "content": "Not A. Amazon EFS as supports only NFS, not SMB support for Windows. \n\nNot C. Amazon EBS volume not natively support SMB or NFS, and EBS is block storage devices that attach to individual EC2 instances and do not support being shared directly across multiple instances. \n\nNot E, Perform an instance refresh on each Auto Scaling group.\n- Not Necessary for Mounting: An instance refresh primarily updates instances in the Auto Scaling group to use new configurations or launch templates. While useful in other scenarios, it doesn't directly relate to mounting the file system or affect the requirement for shared storage.\n- Redundant with Updated Launch Templates: If launch templates and user data scripts are updated correctly, new instances will automatically mount the file system, making a manual refresh unnecessary unless other updates are needed."
            }
          ],
          "content": "Selected Answer: BDF\nB. Amazon FSx for NetApp ONTAP Multi-AZ file system.\n- SMB and NFS Support\n- Sub-Millisecond Latency\n- Multi-AZ Availability: ensures HA and fault tolerance, as the data is replicated across multiple Availability Zones, which aligns with the requirement for a durable storage solution.\n\nD. Update the user data for each application’s launch template to mount the file system.\n- Automated Mounting the FSx file system at startup\n- Protocol-Specific Commands: For Windows instances, mount the SMB share, while for Linux instances, mount the NFS share\n\nF. Update the EC2 instances for each application to mount the file system when new instances are launched.\n- Configuration Consistency: ensures that every new instance launched as part of the Auto Scaling group auto mounts the FSx file system.\n- Ease of Management: By automating the mounting process, you reduce the administrative overhead and potential for errors, ensuring a consistent and reliable setup.",
          "upvote_count": "1",
          "poster": "jamesf"
        },
        {
          "comment_id": "1248326",
          "upvote_count": "2",
          "content": "---> BDE",
          "poster": "tgv",
          "timestamp": "1721046180.0"
        },
        {
          "poster": "trungtd",
          "timestamp": "1721016360.0",
          "content": "Selected Answer: BDE\nBDE for me",
          "upvote_count": "3",
          "comment_id": "1248069"
        },
        {
          "comment_id": "1246443",
          "timestamp": "1720753920.0",
          "poster": "siheom",
          "upvote_count": "2",
          "content": "Selected Answer: BDF\nVOTE BDF"
        },
        {
          "poster": "xdkonorek2",
          "content": "Selected Answer: BDE\nBDE\nNetApp ONTAP for SMB and NFS at the same time",
          "upvote_count": "3",
          "comment_id": "1243335",
          "comments": [
            {
              "upvote_count": "1",
              "content": "why not BDF?",
              "timestamp": "1720721880.0",
              "comment_id": "1246265",
              "poster": "pepedaruiz999"
            }
          ],
          "timestamp": "1720263540.0"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:15:32.765Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "nn3tZx1r51aok4eGUGPs",
      "question_number": 199,
      "page": 40,
      "question_text": "A company uses an organization in AWS Organizations that a security team and a DevOps team manage. Both teams access the accounts by using AWS IAM Identity Center.\n\nA dedicated group has been created for each team. The DevOps team's group has been assigned a permission set named DevOps. The permission set has the AdministratorAccess managed IAM policy attached. The permission set has been applied to all accounts in the organization.\n\nThe security team wants to ensure that the DevOps team does not have access to IAM Identity Center in the organization's management account. The security team has attached the following SCP to the organization root:\n\n//IMG//\n\n\nAfter implementing the policy, the security team discovers that the DevOps team can still access IAM Identity Center.\n\nWhich solution will fix the problem?",
      "choices": {
        "D": "In IAM Identity Center, update the DevOps permission set. Ensure that the assigned policy has full access but explicitly denies permission for the sso:* action and the sso-directory:* action. In the Deny statement, add a StringEquals condition that compares the aws:SourceAccount global condition context key with the organization's management account IDelete the SCP.",
        "A": "In the organization's management account, create a new OU. Move the organization's management account to the new OU. Detach the SCP from the organization root. Attach the SCP to the new OU.",
        "C": "In IAM Identity Center, create a new permission set. Ensure that the assigned policy has full access but explicitly denies permission for the sso:* action and the sso-directory:* action. Update the assigned permission set for the DevOps team's group role in the organization's management account. Delete the SCP.",
        "B": "In the organization's management account, update the SCP condition reference to the ARN of the DevOps team's group role to include the AWS account ID of the organization's management account."
      },
      "correct_answer": "D",
      "answer_ET": "D",
      "answers_community": [
        "D (64%)",
        "B (29%)",
        "7%"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/146254-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [
        "https://img.examtopics.com/aws-certified-devops-engineer-professional-dop-c02/image26.png"
      ],
      "answer_images": [],
      "timestamp": "2024-08-21 15:20:00",
      "unix_timestamp": 1724246400,
      "discussion_count": 7,
      "discussion": [
        {
          "timestamp": "1732278480.0",
          "comment_id": "1316301",
          "content": "Selected Answer: D\nA and B do not work. SCPs do not apply to the management account. \nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_best-practices_mgmt-acct.html\nBest practice is to limit access to the management account, and/or to delegate to Identity Centro to another member account. But since that's not an option given these answers, limiting the permission set is the next best thing.",
          "upvote_count": "3",
          "poster": "Impromptu"
        },
        {
          "poster": "tinyshare",
          "content": "Selected Answer: D\nD is correct. Identity Center uses permission sets.",
          "timestamp": "1731643920.0",
          "comment_id": "1312400",
          "upvote_count": "2"
        },
        {
          "content": "Selected Answer: B\nD might work, but SCPs are used to limit permissions and permission sets are used to grant permissions.",
          "comment_id": "1308414",
          "upvote_count": "2",
          "poster": "VerRi",
          "timestamp": "1730991420.0"
        },
        {
          "upvote_count": "2",
          "timestamp": "1724937600.0",
          "poster": "ApacheKafkaAWS",
          "comment_id": "1274503",
          "content": "Selected Answer: B\nIt's B according to chatGPT"
        },
        {
          "timestamp": "1724800800.0",
          "comment_id": "1273707",
          "content": "Selected Answer: D\nOption D \n \nIn IAM Identity Center, update the DevOps permission set. Ensure that the assigned policy has full access but explicitly denies permission for the sso:* action and the sso-directory:* action. In the Deny statement, add a StringEquals condition that compares the aws:SourceAccount global condition context key with the organization’s management account. Delete the SCP.\n\nThis approach ensures that the DevOps team retains necessary permissions while explicitly denying access to IAM Identity Center actions in the management account. Adding the StringEquals condition ensures that the policy is applied specifically to the management account, effectively preventing access.",
          "upvote_count": "2",
          "poster": "limelight04"
        },
        {
          "comment_id": "1272407",
          "poster": "siheom",
          "timestamp": "1724647260.0",
          "upvote_count": "2",
          "content": "Selected Answer: D\nvote D"
        },
        {
          "comment_id": "1270141",
          "upvote_count": "1",
          "timestamp": "1724246400.0",
          "comments": [
            {
              "upvote_count": "1",
              "content": "Sorry, It's D",
              "comment_id": "1276495",
              "timestamp": "1725259260.0",
              "poster": "hzaki"
            }
          ],
          "content": "Selected Answer: A\nThe right answer is A",
          "poster": "hzaki"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:15:32.765Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "hQT5tOIAbfWrsapNHvtq",
      "question_number": 200,
      "page": 40,
      "question_text": "An Amazon EC2 Auto Scaling group manages EC2 instances that were created from an AMI. The AMI has the AWS Systems Manager Agent installed. When an EC2 instance is launched into the Auto Scaling group, tags are applied to the EC2 instance.\n\nEC2 instances that are launched by the Auto Scaling group must have the correct operating system configuration.\n\nWhich solution will meet these requirements?",
      "choices": {
        "A": "Create a Systems Manager Run Command document that configures the desired instance configuration. Set up Systems Manager Compliance to invoke the Run Command document when the EC2 instances are not in compliance with the most recent patches.",
        "D": "Create a Systems Manager Patch Manager patch baseline and a patch group that use the same tags that the Auto Scaling group applies. Register the patch group with the patch baseline. Define a Systems Manager command document to patch the instances Invoke the document by using Systems Manager Run Command.",
        "C": "Create a Systems Manager Run Command task that specifies the desired instance configuration. Create a maintenance window in Systems Manager Maintenance Windows that runs daily. Register the Run Command task against the maintenance window. Designate the targets.",
        "B": "Create a Systems Manager State Manager association that links to the Systems Manager command document. Create a tag query that runs immediately."
      },
      "correct_answer": "B",
      "answer_ET": "B",
      "answers_community": [
        "B (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/146391-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-08-24 16:19:00",
      "unix_timestamp": 1724509140,
      "discussion_count": 3,
      "discussion": [
        {
          "comment_id": "1273710",
          "timestamp": "1724801160.0",
          "poster": "limelight04",
          "content": "Selected Answer: B\nOption B: Create a Systems Manager State Manager association that links to the Systems Manager command document. Create a tag query that runs immediately.\n\nHere’s why:\n\nState Manager allows you to define and maintain consistent configuration of your instances. By creating an association that links to the command document, you can ensure that the desired configuration is applied as soon as the instances are launched.\nUsing a tag query ensures that the configuration is applied to the correct instances based on their tags, which are applied by the Auto Scaling group.",
          "upvote_count": "2"
        },
        {
          "content": "Selected Answer: B\nState Manager ensures that all instances launched by the Auto Scaling group automatically receive the desired configuration which immediately applies when instances are launched.\n\nWhile the primary focus of Option D is on patch management (applying updates and patches) rather than configuring the overall state of the operating system. Using Run Command is very AD-hoc which means you need to time the launching of the EC2 instance with the command and might not provide the continuous assurance that the configuration is maintained.",
          "comment_id": "1272452",
          "poster": "chinchin97",
          "timestamp": "1724650980.0",
          "upvote_count": "2"
        },
        {
          "content": "Selected Answer: B\nB. Create a Systems Manager State Manager association that links to the Systems Manager command document. Create a tag query that runs immediately.",
          "comment_id": "1271694",
          "upvote_count": "2",
          "poster": "Kushab94",
          "timestamp": "1724509140.0"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:15:32.765Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "ptiVFH3B6yv0OqSYjb6j",
      "question_number": 201,
      "page": 41,
      "question_text": "A developer is maintaining a fleet of 50 Amazon EC2 Linux servers. The servers are part of an Amazon EC2 Auto Scaling group, and also use Elastic Load Balancing for load balancing.\nOccasionally, some application servers are being terminated after failing ELB HTTP health checks. The developer would like to perform a root cause analysis on the issue, but before being able to access application logs, the server is terminated.\nHow can log collection be automated?",
      "choices": {
        "C": "Use Auto Scaling lifecycle hooks to put instances in a Terminating:Wait state. Create an Amazon CloudWatch subscription filter for EC2 Instance Terminate Successful and trigger a CloudWatch agent that invokes a script to collect logs, push them to Amazon S3, and complete the lifecycle action once logs are collected.",
        "A": "Use Auto Scaling lifecycle hooks to put instances in a Pending:Wait state. Create an Amazon CloudWatch alarm for EC2 Instance Terminate Successful and trigger an AWS Lambda function that invokes an SSM Run Command script to collect logs, push them to Amazon S3, and complete the lifecycle action once logs are collected.",
        "D": "Use Auto Scaling lifecycle hooks to put instances in a Terminating:Wait state. Create an Amazon EventBridge rule for EC2 Instance-terminate Lifecycle Action and trigger an AWS Lambda function that invokes an SSM Run Command script to collect logs, push them to Amazon S3, and complete the lifecycle action once logs are collected.",
        "B": "Use Auto Scaling lifecycle hooks to put instances in a Terminating:Wait state. Create an AWS Config rule for EC2 Instance-terminate Lifecycle Action and trigger a step function that invokes a script to collect logs, push them to Amazon S3, and complete the lifecycle action once logs are collected."
      },
      "correct_answer": "D",
      "answer_ET": "D",
      "answers_community": [
        "D (87%)",
        "13%"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/106206-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-04-14 23:25:00",
      "unix_timestamp": 1681507500,
      "discussion_count": 18,
      "discussion": [
        {
          "upvote_count": "11",
          "comment_id": "919900",
          "content": "Selected Answer: D\nD is the easiest solution.",
          "timestamp": "1686383940.0",
          "poster": "madperro"
        },
        {
          "comment_id": "1319172",
          "content": "Selected Answer: C\nwhy C is not correct？",
          "comments": [
            {
              "timestamp": "1738638360.0",
              "poster": "hayjaykay",
              "upvote_count": "1",
              "content": "Think fleet of EC2s, think SSM (systems manager).",
              "comment_id": "1351180"
            }
          ],
          "poster": "wikn",
          "timestamp": "1732789860.0",
          "upvote_count": "1"
        },
        {
          "upvote_count": "1",
          "poster": "Ravi_Bulusu",
          "content": "Option A is the most efficient and straightforward approach to automate log collection and prevent premature termination of EC2 instances by using Auto Scaling lifecycle hooks, CloudWatch alarms, Lambda functions, and SSM to gather and store logs in Amazon S3 before the instance is terminated.",
          "comment_id": "1313653",
          "timestamp": "1731860640.0"
        },
        {
          "content": "it is D not C because CloudWatch agent can not invokes a script",
          "comment_id": "1310739",
          "timestamp": "1731429900.0",
          "poster": "Saudis",
          "upvote_count": "1"
        },
        {
          "comment_id": "1259239",
          "content": "Selected Answer: D\nD as the EC2 is Terminating and Cloudwatch Agent should be not running and cannot collect the logs",
          "upvote_count": "2",
          "poster": "jamesf",
          "timestamp": "1722496200.0"
        },
        {
          "comment_id": "1234191",
          "upvote_count": "1",
          "poster": "Rahul369",
          "content": "Selected Answer: C\nIt must be 'C' as CloudWatch Agent will push the logs to a particular CloudWatch log group.",
          "timestamp": "1718944320.0"
        },
        {
          "upvote_count": "3",
          "comments": [
            {
              "timestamp": "1712799180.0",
              "poster": "hoazgazh",
              "comment_id": "1193373",
              "upvote_count": "1",
              "content": "why not C bro"
            }
          ],
          "timestamp": "1708328640.0",
          "content": "Selected Answer: D\nTerminating:Wait refers to a state in which an instance is determined to be terminated by the Auto Scaling group as part of the termination process and is temporarily put on hold before it is actually terminated. This state pauses the termination process and provides an opportunity to perform custom actions (logging, graceful shutdown, data backup, etc).",
          "poster": "dzn",
          "comment_id": "1153760"
        },
        {
          "poster": "thanhnv142",
          "content": "D is correct: Using Eventbridge in combination with lambda is a common practice.\nA: Cloudwatch alarm only alert, no action so it cannot trigger lambda (when this question came out, it could not)\nB: AWS config rule cannot triger a script. \nC: cloudwatch agent itself does not have any direct action on the host but collecting logs",
          "upvote_count": "4",
          "timestamp": "1706536260.0",
          "comment_id": "1135013"
        },
        {
          "comments": [
            {
              "comment_id": "1115094",
              "content": "I think we can't select C because it says that it invokes the cloudwatch agent after the EC2 instance is terminated. It can't collect the logs from terminated EC2 Instance.",
              "upvote_count": "2",
              "timestamp": "1704533760.0",
              "poster": "davdan99"
            }
          ],
          "comment_id": "1112686",
          "timestamp": "1704279240.0",
          "content": "C is also a good choice in this question. Why? you need to have a CW agent installed on the hosts to be able to collect logs from the servers before termination.",
          "poster": "Jaguaroooo",
          "upvote_count": "1"
        },
        {
          "comment_id": "910380",
          "content": "Selected Answer: D\nD is the correct one IMHO.\n\nASG actions are not logged to cloudwatch logs to use a filter, and if so it would be complicated to extract the data. The canonical way is to rely in an EventBridge event.",
          "timestamp": "1685460300.0",
          "upvote_count": "3",
          "poster": "bcx"
        },
        {
          "content": "D\n\n\"When a scale-in event occurs, a lifecycle hook pauses the instance before it is terminated and sends you a notification using Amazon EventBridge. While the instance is in the wait state, you can invoke an AWS Lambda function or connect to the instance to download logs or other data before the instance is fully terminated. \"\n\nhttps://aws.amazon.com/blogs/infrastructure-and-automation/run-code-before-terminating-an-ec2-auto-scaling-instance/",
          "upvote_count": "4",
          "comment_id": "904663",
          "poster": "levster",
          "timestamp": "1684825380.0"
        },
        {
          "content": "Selected Answer: D\nD for sure 100%",
          "upvote_count": "1",
          "poster": "vherman",
          "comment_id": "888631",
          "timestamp": "1683125160.0"
        },
        {
          "timestamp": "1682989860.0",
          "content": "Selected Answer: D\nD. Use Auto Scaling lifecycle hooks to put instances in a Terminating:Wait state. Create an Amazon EventBridge rule for EC2 Instance-terminate Lifecycle Action and trigger an AWS Lambda function that invokes an SSM Run Command script to collect logs, push them to Amazon S3, and complete the lifecycle action once logs are collected.\n\nWith this solution, you can use an Auto Scaling lifecycle hook to put instances in a wait state before termination. This provides an opportunity to collect logs before the instance is terminated. The solution can use an Amazon EventBridge rule for EC2 Instance-terminate Lifecycle Action to trigger an AWS Lambda function that will execute an SSM Run Command script. The script can collect logs and push them to Amazon S3 before completing the lifecycle action and allowing the instance to terminate. This solution provides a way to collect logs before instances are terminated, allowing for root cause analysis of issues.",
          "comment_id": "886936",
          "upvote_count": "4",
          "poster": "haazybanj"
        },
        {
          "content": "Selected Answer: D\nD seems to be more relevant for this scenario",
          "upvote_count": "1",
          "timestamp": "1682748660.0",
          "poster": "ParagSanyashiv",
          "comment_id": "884070"
        },
        {
          "content": "Note that there is a similar question on Tutorial Dojo and the answer is to \"trigger cloudwatch agent\"",
          "comments": [
            {
              "comment_id": "896874",
              "poster": "ipsingh",
              "timestamp": "1683994140.0",
              "content": "read this link and you will understand that C is wrong option- https://aws.amazon.com/blogs/infrastructure-and-automation/run-code-before-terminating-an-ec2-auto-scaling-instance/",
              "upvote_count": "1"
            }
          ],
          "comment_id": "878885",
          "poster": "henryyvr",
          "timestamp": "1682295420.0",
          "upvote_count": "1"
        },
        {
          "comments": [
            {
              "poster": "beanxyz",
              "content": "No way. Cloudwatch subscription filter is normally used to send cloudwatch log to kinesis firehose stream so that it can be consumed by other tools such as Splunk. If you need to invoke a lambda, the easiest way is to use event rule.",
              "timestamp": "1693820520.0",
              "upvote_count": "2",
              "comment_id": "998437"
            }
          ],
          "upvote_count": "2",
          "poster": "henryyvr",
          "timestamp": "1682294700.0",
          "comment_id": "878873",
          "content": "Selected Answer: C\nshould be C"
        },
        {
          "timestamp": "1681550880.0",
          "poster": "ele",
          "content": "Selected Answer: D\nD sure",
          "comment_id": "870805",
          "upvote_count": "1"
        },
        {
          "upvote_count": "1",
          "comment_id": "870483",
          "content": "Selected Answer: D\nI think is D",
          "timestamp": "1681507500.0",
          "poster": "alce2020"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:15:43.322Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "ZgI2mofgcuP5E2e0gaRE",
      "question_number": 202,
      "page": 41,
      "question_text": "A company uses AWS Organizations to manage its AWS accounts. The organization root has a child OU that is named Department. The Department OU has a child OU that is named Engineering. The default FullAWSAccess policy is attached to the root, the Department OU, and the Engineering OU.\n\nThe company has many AWS accounts in the Engineering OU. Each account has an administrative IAM role with the AdministratorAccess IAM policy attached. The default FullAWSAccessPolicy is also attached to each account.\n\nA DevOps engineer plans to remove the FullAWSAccess policy from the Department OU. The DevOps engineer will replace the policy with a policy that contains an Allow statement for all Amazon EC2 API operations.\n\nWhat will happen to the permissions of the administrative 1AM roles as a result of this change?",
      "choices": {
        "C": "All API actions on all resources will be denied.",
        "B": "All API actions on EC2 resources will be allowed. All other API actions will be denied.",
        "A": "All API actions on all resources will be allowed.",
        "D": "All API actions on EC2 resources will be denied. All other API actions will be allowed."
      },
      "correct_answer": "B",
      "answer_ET": "B",
      "answers_community": [
        "B (77%)",
        "A (23%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/146260-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-08-21 16:03:00",
      "unix_timestamp": 1724248980,
      "discussion_count": 6,
      "discussion": [
        {
          "poster": "teo2157",
          "timestamp": "1734338400.0",
          "content": "Selected Answer: B\nIt's B based on this url\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps_evaluation.html",
          "comment_id": "1327225",
          "upvote_count": "2"
        },
        {
          "poster": "aws_god",
          "content": "Selected Answer: A\nThe default FullAWSAccess policy is attached to the root, the Department OU, and the Engineering OU. So even if it is removed from the Department OU, it is still attached on the Engineering OU.",
          "upvote_count": "2",
          "comment_id": "1283504",
          "timestamp": "1726294200.0"
        },
        {
          "poster": "ApacheKafkaAWS",
          "content": "Selected Answer: B\nI'ts B",
          "timestamp": "1724937780.0",
          "comment_id": "1274505",
          "upvote_count": "2"
        },
        {
          "upvote_count": "2",
          "timestamp": "1724646840.0",
          "poster": "siheom",
          "comment_id": "1272405",
          "content": "Selected Answer: B\nvote B.."
        },
        {
          "content": "Selected Answer: B\nWhen the FullAWSAccess policy is replaced with a policy that allows only EC2 actions, this new SCP will act as a boundary. Even if an IAM role or user within the account has a broader permission set (like AdministratorAccess), the SCP limits what can be done.",
          "upvote_count": "4",
          "timestamp": "1724631540.0",
          "comment_id": "1272339",
          "poster": "hzaki"
        },
        {
          "comment_id": "1270177",
          "content": "Selected Answer: A\nThe answer is A \nStill, the root has attached a full access policy.",
          "upvote_count": "1",
          "poster": "hzaki",
          "comments": [
            {
              "comment_id": "1272340",
              "upvote_count": "1",
              "timestamp": "1724631600.0",
              "content": "Sorry the Answer: B\nWhen the FullAWSAccess policy is replaced with a policy that allows only EC2 actions, this new SCP will act as a boundary. Even if an IAM role or user within the account has a broader permission set (like AdministratorAccess), the SCP limits what can be done.",
              "poster": "hzaki"
            }
          ],
          "timestamp": "1724248980.0"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:15:43.322Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "SH1A1U1X6hC3XdOso3cU",
      "question_number": 203,
      "page": 41,
      "question_text": "A company manages AWS accounts in AWS Organizations. The company needs a solution to send Amazon CloudWatch Logs data to an Amazon S3 bucket in a dedicated AWS account. The solution must support all existing and future CloudWatch Logs log groups.\n\nWhich solution will meet these requirements?",
      "choices": {
        "A": "Enable Organizations backup policies to back up all log groups to a dedicated S3 bucket. Add an S3 bucket policy that allows access from all accounts that belong to the company.",
        "B": "Create a backup plan in AWS Backup. Specify a dedicated S3 bucket as a backup vault. Assign all CloudWatch Logs log group resources to the backup plan. Create resource assignments in the backup plan for all accounts that belong to the company.",
        "D": "Create a CloudWatch Logs destination and an Amazon Kinesis Data Firehose delivery stream in the dedicated AWS account. Specify the S3 bucket as the destination of the delivery stream. Create subscription filters for all existing log groups in all accounts. Create an AWS Lambda function to call the CloudWatch Logs PutSubscriptionFilter API operation. Create an Amazon EventBridge rule to invoke the Lambda function when a CreateLogGroup event occurs.",
        "C": "Create a backup plan in AWS Backup. Specify a dedicated S3 bucket as a backup vault. Assign all existing log groups to the backup plan. Create resource assignments in the backup plan for all accounts that belong to the company. Create an AWS Systems Manager Automation runbook to assign log groups to a backup plan. Create an AWS Config rule that has an automatic remediation action for all noncompliant log groups. Specify the runbook as the rule's target."
      },
      "correct_answer": "D",
      "answer_ET": "D",
      "answers_community": [
        "D (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/146217-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-08-21 09:23:00",
      "unix_timestamp": 1724224980,
      "discussion_count": 3,
      "discussion": [
        {
          "poster": "aws_god",
          "comment_id": "1283509",
          "timestamp": "1726294860.0",
          "upvote_count": "2",
          "content": "Selected Answer: D\nC seems like the most elegant solution, but I was not able to find any documentation that supports it.\n\nFor option D I found this - https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/SubscriptionFilters.html#FirehoseExample"
        },
        {
          "poster": "hzaki",
          "upvote_count": "2",
          "content": "Selected Answer: D\nCorrect answer D",
          "timestamp": "1724245680.0",
          "comment_id": "1270135"
        },
        {
          "content": "Correct answer D",
          "upvote_count": "2",
          "timestamp": "1724224980.0",
          "comment_id": "1269923",
          "poster": "hzaki"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:15:43.322Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "xofSaTTyqjS0z6dLLrIE",
      "question_number": 204,
      "page": 41,
      "question_text": "A DevOps engineer manages a Java-based application that runs in an Amazon Elastic Container Service (Amazon ECS) cluster on AWS Fargate. Auto scaling has not been configured for the application.\n\nThe DevOps engineer has determined that the Java Virtual Machine (JVM) thread count is a good indicator of when to scale the application. The application serves customer traffic on port 8080 and makes JVM metrics available on port 9404.\n\nApplication use has recently increased. The DevOps engineer needs to configure auto scaling for the application.\n\nWhich solution will meet these requirements with the LEAST operational overhead? (Choose two.)",
      "choices": {
        "B": "Deploy the Amazon CloudWatch agent as a container sidecar. Configure a metric filter for the JVM thread count metric on the CloudWatch log group for the CloudWatch agent. Add a target tracking policy in Fargate. Select the metric from the metric filter as a scale target.",
        "C": "Create an Amazon Managed Service for Prometheus workspace. Deploy AWS Distro for OpenTelemetry as a container sidecar to publish the JVM metrics from port 9404 to the Prometheus workspace. Configure rules for the workspace to use the JVM thread count metric to scale the application. Add a step scaling policy in Fargate. Select the Prometheus rules to scale up and scaling down.",
        "A": "Deploy the Amazon CloudWatch agent as a container sidecar. Configure the CloudWatch agent to retrieve JVM metrics from port 9404. Create CloudWatch alarms on the JVM thread count metric to scale the application. Add a step scaling policy in Fargate to scale up and scale down based on the CloudWatch alarms.",
        "D": "Create an Amazon Managed Service for Prometheus workspace. Deploy AWS Distro for OpenTelemetry as a container sidecar to retrieve JVM metrics from port 9404 to publish the JVM metrics from port 9404 to the Prometheus workspace. Add a target tracking policy in Fargate. Select the Prometheus metric as a scale target."
      },
      "correct_answer": "AD",
      "answer_ET": "AD",
      "answers_community": [
        "AD (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/153026-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-12-16 09:59:00",
      "unix_timestamp": 1734339540,
      "discussion_count": 1,
      "discussion": [
        {
          "timestamp": "1734339540.0",
          "comment_id": "1327239",
          "poster": "teo2157",
          "content": "Selected Answer: AD\nBoth A and D are feasible based on this links:\nhttps://docs.aws.amazon.com/autoscaling/application/userguide/application-auto-scaling-step-scaling-policies.html\nhttps://aws.amazon.com/blogs/containers/autoscaling-amazon-ecs-services-based-on-custom-metrics-with-application-auto-scaling/",
          "upvote_count": "3"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:15:43.322Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "KDTpguTbjEfygO0TWafp",
      "question_number": 205,
      "page": 41,
      "question_text": "A company has an application that runs in a single AWS Region. The application runs on an Amazon Elastic Kubernetes Service (Amazon EKS) cluster and connects to an Amazon Aurora MySQL cluster. The application is built in an AWS CodeBuild project. The container images are published to Amazon Elastic Container Registry (Amazon ECR).\n\nThe company needs to replicate the state of the application for the container images and the database to a second Region.\n\nWhich solution will meet these requirements in the MOST operationally efficient way?",
      "choices": {
        "A": "Turn on Amazon S3 Cross-Region Replication (CRR) on the bucket that holds the ECR container images. Deploy the application to an EKS cluster in the second Region by referencing the new S3 bucket object URL for the container image in a Kubernetes deployment file. Configure a cross-Region Aurora Replica in the second Region. Configure the new application deployment to use the endpoints for the cross-Region Aurora Replica.",
        "B": "Create an Amazon EventBridge rule that reacts to image pushes to the ECR repository. Configure the EventBridge rule to invoke an AWS Lambda function to replicate the image to a new ECR repository in the second Region. Deploy the application to an EKS cluster in the second Region by referencing the new ECR repository in a Kubernetes deployment file. Configure a cross-Region Aurora Replica in the second Region. Configure the new application deployment to use the endpoints for the cross-Region Aurora Replica.",
        "C": "Turn on Cross-Region Replication to replicate the ECR repository to the second Region. Deploy the application to an EKS cluster in the second Region by referencing the new ECR repository in a Kubernetes deployment file. Configure an Aurora global database with clusters in the initial Region and the second Region. Configure the new application deployment to use the endpoints for the second Region's cluster in the Aurora global database.",
        "D": "Configure the CodeBuild project to also push the container image to an ECR repository in the second Region. Deploy the application to an EKS cluster in the second Region by referencing the new ECR repository in a Kubernetes deployment file. Configure an Aurora MySQL cluster in the second Region as the target for binary log replication from the Aurora MySQL cluster in the initial Region. Configure the new application deployment to use the endpoints for the second Region's cluster."
      },
      "correct_answer": "C",
      "answer_ET": "C",
      "answers_community": [
        "C (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/152993-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-12-15 12:02:00",
      "unix_timestamp": 1734260520,
      "discussion_count": 2,
      "discussion": [
        {
          "comment_id": "1327257",
          "content": "Selected Answer: C\nAgree with C based on this link:\nhttps://aws.amazon.com/blogs/containers/cross-region-replication-in-amazon-ecr-has-landed/",
          "poster": "teo2157",
          "upvote_count": "2",
          "timestamp": "1734343440.0"
        },
        {
          "timestamp": "1734260520.0",
          "comment_id": "1326794",
          "upvote_count": "2",
          "content": "Selected Answer: C\nWhy this is the most operationally efficient:\n\n • Minimal Management Overhead: AWS manages the replication for both the ECR images and the Aurora database. There is no need to configure or maintain additional Lambda functions or EventBridge rules.\n • Scalability: Aurora global databases and ECR replication are designed for production-grade systems and can scale to meet large workloads.\n • Reliability: The solution leverages AWS-native features, reducing the chance of errors or operational disruptions.",
          "poster": "Ky_24"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:15:43.322Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "1ThpcDql48ZuOAhlXhYF",
      "question_number": 206,
      "page": 42,
      "question_text": "A company is building a serverless application that uses AWS Lambda functions to process data.\n\nA BeginResponse Lambda function initializes data in response to specific application events. The company needs to ensure that a large number of Lambda functions are invoked after the BeginResponse Lambda function runs. Each Lambda function must be invoked in parallel and depends on only the outputs of the BeginResponse Lambda function. Each Lambda function has retry logic for invocation and must be able to fine-tune concurrency without losing data.\n\nWhich solution will meet these requirements with the MOST operational efficiency?",
      "choices": {
        "B": "Create an Amazon Simple Queue Service (Amazon SQS) queue for each Lambda function that needs to run after the BeginResponse Lambda function runs. Subscribe each Lambda function to its own SQS queue. Create an Amazon Simple Notification Service (Amazon SNS) topic. Subscribe each SQS queue to the SNS topic. Modify the BeginResponse function to publish to the SNS topic when it finishes running.",
        "D": "Create an AWS Step Functions Standard Workflow. Configure states in the workflow to invoke the Lambda functions sequentially. Create an Amazon Simple Notification Service (Amazon SNS) topic. Modify the BeginResponse Lambda function to publish to the SNS topic before the Lambda function finishes running. Create a new Lambda function that is subscribed to the SNS topic and that invokes the Step Functions workflow.",
        "C": "Create an Amazon Simple Queue Service (Amazon SQS) queue for each Lambda function that needs to run after the BeginResponse Lambda function runs. Subscribe the Lambda function to the SQS queue. Create an Amazon Simple Notification Service (Amazon SNS) topic for each SQS queue. Subscribe the SQS queues to the SNS topics. Modify the BeginResponse function to publish to the SNS topics when the function finishes running.",
        "A": "Create an Amazon Simple Notification Service (Amazon SNS) topic. Modify the BeginResponse Lambda function to publish to the SNS topic before the BeginResponse Lambda function finishes running. Subscribe all Lambda functions that need to invoke after the BeginResponse Lambda function runs to the SNS topic. Subscribe any new Lambda functions to the SNS topic."
      },
      "correct_answer": "B",
      "answer_ET": "B",
      "answers_community": [
        "B (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/152994-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-12-15 12:19:00",
      "unix_timestamp": 1734261540,
      "discussion_count": 2,
      "discussion": [
        {
          "content": "Selected Answer: B\nFan-out",
          "upvote_count": "2",
          "timestamp": "1735309620.0",
          "poster": "youonebe",
          "comment_id": "1332439"
        },
        {
          "comment_id": "1326804",
          "timestamp": "1734261540.0",
          "content": "Selected Answer: B\nHow B Works:\n\n 1. BeginResponse Function: Publishes to a single SNS topic when it completes its processing.\n 2. SNS Fan-Out: The SNS topic sends the message to multiple SQS queues, one for each dependent Lambda function.\n 3. SQS Queues and Lambda: Each Lambda function is subscribed to its respective SQS queue and processes messages independently.\n 4. Retry and Concurrency: SQS queues ensure messages are retried if processing fails, and Lambda concurrency can be tuned for each queue.",
          "upvote_count": "4",
          "poster": "Ky_24"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:15:53.737Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "EiVikDY3Bd7cBxk3Tjos",
      "question_number": 207,
      "page": 42,
      "question_text": "A company operates a globally deployed product out of multiple AWS Regions. The company's DevOps team needs to use Amazon API Gateway to deploy an API to support the product.\n\nThe API must be deployed redundantly. The deployment must provide independent availability from each company location. The deployment also must respond to a custom domain URL and must optimize performance for the API user requests.\n\nWhich solution will meet these requirements?",
      "choices": {
        "B": "Deploy an API Gateway regional API endpoint in the us-east-1 Region. Integrate the API Gateway API with a public Application Load Balancer (ALB). Create an AWS Global Accelerator standard accelerator. Associate the endpoint with the ALCreate an Amazon Route 53 alias record set that points the custom domain name to the DNS name that is assigned to the accelerator.",
        "D": "Deploy an API Gateway edge-optimized API endpoint in the us-east-1 Region. Create an Amazon CloudFront distribution. Configure the CloudFront distribution with an alternate domain name. Specify the API Gateway Invoke URL as the origin domain. Create an Amazon Route 53 alias record set with a simple routing policy. Point the routing policy to the CloudFront distribution domain name.",
        "C": "Deploy an API Gateway regional API endpoint in every AWS Region where the company's product is deployed. Create an API Gateway custom domain in each Region for the deployed API Gateway API. Create an Amazon Route 53 record set that has a latency routing policy for every deployed API Gateway custom domain.",
        "A": "Deploy an API Gateway edge-optimized API endpoint in the us-east-1 Region. Create an API Gateway custom domain for the API. Create an Amazon Route 53 record set with a geoproximity routing policy for the API's custom domain. Increase the geographic bias to the maximum allowed value."
      },
      "correct_answer": "C",
      "answer_ET": "C",
      "answers_community": [
        "C (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/153028-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-12-16 11:33:00",
      "unix_timestamp": 1734345180,
      "discussion_count": 1,
      "discussion": [
        {
          "timestamp": "1734345180.0",
          "upvote_count": "3",
          "content": "Selected Answer: C\n1. **Deploy Amazon API Gateway in Multiple Regions**:\n - Deploy your API Gateway in multiple AWS Regions to ensure redundancy and independent availability.\n2. **Set Up a Custom Domain Name for API Gateway**:\n - Create a custom domain name in API Gateway and map it to your APIs in each Region.\n3. **Use Amazon Route 53 for Global DNS and Latency-Based Routing**:\n - Use Amazon Route 53 to create a global DNS entry for your custom domain and configure latency-based routing to direct user requests to the nearest API Gateway deployment.",
          "comment_id": "1327275",
          "poster": "teo2157"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:15:53.737Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "gfx9AuYt8SExfHX7uOOn",
      "question_number": 208,
      "page": 42,
      "question_text": "A DevOps engineer uses AWS CodeBuild to frequently produce software packages. The CodeBuild project builds large Docker images that the DevOps engineer can use across multiple builds.\n\nThe DevOps engineer wants to improve build performance and minimize costs.\n\nWhich solution will meet these requirements?",
      "choices": {
        "C": "Store the Docker images in an Amazon Elastic Container Registry (Amazon ECR) repository. Modify the CodeBuild project runtime configuration to always use the most recent image version.",
        "D": "Create custom AMIs that contain the cached Docker images. In the CodeBuild build, launch Amazon EC2 instances from the custom AMIs.",
        "A": "Store the Docker images in an Amazon Elastic Container Registry (Amazon ECR) repository. Implement a local Docker layer cache for CodeBuild.",
        "B": "Cache the Docker images in an Amazon S3 bucket that is available across multiple build hosts. Expire the cache by using an S3 Lifecycle policy."
      },
      "correct_answer": "A",
      "answer_ET": "A",
      "answers_community": [
        "A (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/152474-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-12-02 17:48:00",
      "unix_timestamp": 1733158080,
      "discussion_count": 3,
      "discussion": [
        {
          "comment_id": "1400221",
          "poster": "DKM",
          "timestamp": "1742317740.0",
          "content": "Selected Answer: A\nThe LOCAL_DOCKER_LAYER_CACHE is a feature in Docker that allows you to store build cache layers locally on your filesystem. This can significantly speed up the build process by reusing layers that haven't changed, reducing the need to rebuild them from scratch",
          "upvote_count": "1"
        },
        {
          "content": "Selected Answer: A\n1. **Use Amazon ECR to Store Docker Images**:\n - Push the built Docker images to Amazon ECR after a successful build.\n - Pull the Docker images from Amazon ECR at the start of each build to use as a cache.\n\n2. **Configure CodeBuild to Use Docker Layer Caching**:\n - Enable Docker layer caching in your CodeBuild project to further improve build performance.",
          "comment_id": "1327284",
          "timestamp": "1734346020.0",
          "poster": "teo2157",
          "upvote_count": "3"
        },
        {
          "upvote_count": "2",
          "poster": "Ky_24",
          "comment_id": "1326827",
          "timestamp": "1734263640.0",
          "content": "Selected Answer: A\n1. Amazon ECR for Centralized Storage:\n • Storing Docker images in Amazon ECR allows you to maintain and reuse images across builds. This reduces the need to rebuild base images repeatedly, improving build performance.\n • ECR integrates seamlessly with AWS CodeBuild, making it a scalable and cost-effective solution for managing container images.\n 2. Local Docker Layer Caching:\n • Docker layer caching significantly reduces build time by avoiding the re-download or re-creation of unchanged layers.\n • CodeBuild supports enabling local caching with type: LOCAL_DOCKER_LAYER_CACHE. This caching stores intermediate image layers locally on the build host, which speeds up subsequent builds."
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:15:53.737Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "J9bZRCj1o2z6ldJ3oqzB",
      "question_number": 209,
      "page": 42,
      "question_text": "A large company recently acquired a small company. The large company invited the small company to join the large company's existing organization in AWS Organizations as a new OU.\n\nA DevOps engineer determines that the small company needs to launch t3.small Amazon EC2 instance types for the company's application workloads. The small company needs to deploy the instances only within US-based AWS Regions.\n\nThe DevOps engineer needs to use an SCP in the small company's new OU to ensure that the small company can launch only the required instance types.\n\nWhich solution will meet these requirements?",
      "choices": {
        "D": "Configure a statement to allow the ec2:RunInstances action for all EC2 instance resources when the ec2:InstanceType condition is equal to t3.small.\nConfigure another statement to allow the ec2:RunInstances action for all EC2 instance resources when the aws:RequestedRegion condition is equal to us-*.",
        "B": "Configure a statement to allow the ec2:RunInstances action for all EC2 instance resources when the ec2:InstanceType condition is not equal to t3.small.\nConfigure another statement to allow the ec2:RunInstances action for all EC2 instance resources when the aws:RequestedRegion condition is not equal to us-*.",
        "C": "Configure a statement to deny the ec2:RunInstances action for all EC2 instance resources when the ec2:InstanceType condition is equal to t3.small.\nConfigure another statement to deny the ec2:RunInstances action for all EC2 instance resources when the aws:RequestedRegion condition is equal to us-*.",
        "A": "Configure a statement to deny the ec2:RunInstances action for all EC2 instance resources when the ec2:InstanceType condition is not equal to t3.small.\nConfigure another statement to deny the ec2:RunInstances action for all EC2 instance resources when the aws:RequestedRegion condition is not equal to us-*."
      },
      "correct_answer": "A",
      "answer_ET": "A",
      "answers_community": [
        "A (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/153424-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-12-25 06:23:00",
      "unix_timestamp": 1735104180,
      "discussion_count": 2,
      "discussion": [
        {
          "content": "Selected Answer: A\nVery important: \n• An Allow statement in an SCP permits the Resource element to only have a \"*\" entry.\n• An Allow statement in an SCP can't have a Condition element at all.\nBecause of the second point, B and D are not right because these are adding conditions in the allow statement. C does not mean and deny the requirements so A is the right answer.",
          "poster": "c87b433",
          "timestamp": "1738682520.0",
          "upvote_count": "2",
          "comment_id": "1351461"
        },
        {
          "upvote_count": "3",
          "comment_id": "1331712",
          "poster": "CHRIS12722222",
          "timestamp": "1735162980.0",
          "content": "Selected Answer: A\nA\n\n deny the ec2:RunInstances action for all EC2 instance resources when the ec2:InstanceType condition is not equal to t3.small\n\ndeny the ec2:RunInstances action for all EC2 instance resources when the aws:RequestedRegion condition is not equal to us-*"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:15:53.737Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "swMCpQkrS7Zba668qfO7",
      "question_number": 210,
      "page": 42,
      "question_text": "A DevOps team manages infrastructure for an application. The application uses long-running processes to process items from an Amazon Simple Queue Service (Amazon SQS) queue. The application is deployed to an Auto Scaling group.\n\nThe application recently experienced an issue where items were taking significantly longer to process. The queue exceeded the expected size, which prevented various business processes from functioning properly. The application records all logs to a third-party tool.\n\nThe team is currently subscribed to an Amazon Simple Notification Service (Amazon SNS) topic that the team uses for alerts. The team needs to be alerted if the queue exceeds the expected size.\n\nWhich solution will meet these requirements with the MOST operational efficiency?",
      "choices": {
        "C": "Create an AWS Lambda function that retrieves the ApproximateNumberOfMessages SQS queue attribute value and publishes the value as a new CloudWatch custom metric. Create an Amazon EventBridge rule that is scheduled to run every 5 minutes and that invokes the Lambda function. Configure a CloudWatch metrics alarm with a period of 1 hour and a static threshold to alarm if the sum of the new custom metric is greater than the expected value.",
        "A": "Create an Amazon CloudWatch metric alarm with a period of 1 hour and a static threshold to alarm if the average of the ApproximateNumberOfMessagesDelayed metric is greater than the expected value. Configure the alarm to notify the SNS topic.",
        "B": "Create an Amazon CloudWatch metric alarm with a period of 1 hour and a static threshold to alarm if the sum of the ApproximateNumberOfMessagesVisible metric is greater than the expected value. Configure the alarm to notify the SNS topic.",
        "D": "Create an AWS Lambda function that checks the ApproximateNumberOfMessagesDelayed SQS queue attribute and compares the value to a defined expected size in the function. Create an Amazon EventBridge rule that is scheduled to run every 5 minutes and that invokes the Lambda function. When the ApproximateNumberOfMessagesDelayed SQS queue attribute exceeds the expected size, send a notification the SNS topic."
      },
      "correct_answer": "B",
      "answer_ET": "B",
      "answers_community": [
        "B (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/153029-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-12-16 12:02:00",
      "unix_timestamp": 1734346920,
      "discussion_count": 2,
      "discussion": [
        {
          "upvote_count": "4",
          "comment_id": "1331715",
          "content": "Selected Answer: B\nApproximateNumberOfMessagesVisible, which provides the number of messages available for retrieval from the queue. This is the primary metric that reflects how many messages are waiting to be processed.",
          "poster": "CHRIS12722222",
          "timestamp": "1735163580.0"
        },
        {
          "content": "Selected Answer: B\nBetween CloudWatch alarms and Lambda functios is more operational the CloudWatch alarms. The key point here is that ApproximateNumberOfMessagesVisible corresponds to \nThe number of messages to be processed while ApproximateNumberOfMessagesDelayed is the number of messages in the queue that are delayed and not available for reading immediately. Based on that it's B.",
          "upvote_count": "3",
          "comment_id": "1327291",
          "poster": "teo2157",
          "timestamp": "1734346920.0"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:15:53.737Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "tea2tY3KJiDeKD1MFXnz",
      "question_number": 211,
      "page": 43,
      "question_text": "A large company runs critical workloads in multiple AWS accounts. The AWS accounts are managed under AWS Organizations with all features enabled. The company stores confidential customer data in an Amazon S3 bucket. Access to the S3 bucket requires multiple levels of approval.\n\nThe company wants to monitor when the S3 bucket is accessed by using the AWS CLI. The company also wants insights into the various activities performed by other users on all other S3 buckets in the AWS accounts to detect any issues.\n\nWhich solution will meet these requirements?",
      "choices": {
        "B": "Create an AWS CloudTrail organization trail that is delivered to Amazon CloudWatch in the Organizations management account. Enable data events logs for all S3 buckets. Use Amazon CloudWatch anomaly detection in all the AWS accounts. Use Amazon Athena to perform SQL queries on the custom metrics created from the CloudTrail logs.",
        "C": "Create an AWS CloudTrail organization trail that is delivered to Amazon CloudWatch in the Organizations management account. Enable data events logs for all S3 buckets. Use Amazon CloudWatch anomaly detection in all the AWS accounts. Use Amazon CloudWatch Metrics Insights to perform SQL queries on the custom metrics created from the CloudTrail logs.",
        "A": "Create an AWS CloudTrail trail that is delivered to Amazon CloudWatch in each AWS account. Enable data events logs for all S3 buckets. Use Amazon GuardDuty for anomaly detection in all the AWS accounts. Use Amazon Athena to perform SQL queries on the custom metrics created from the CloudTrail logs.",
        "D": "Create an AWS CloudTrail trail that is delivered to Amazon CloudWatch in each AWS account. Enable data events logs for all S3 buckets. Use a custom solution for anomaly detection in all the AWS accounts. Use Amazon CloudWatch Metrics Insights to perform SQL queries on the custom metrics created from the CloudTrail logs."
      },
      "correct_answer": "C",
      "answer_ET": "C",
      "answers_community": [
        "C (75%)",
        "B (25%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/153031-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-12-16 12:18:00",
      "unix_timestamp": 1734347880,
      "discussion_count": 5,
      "discussion": [
        {
          "upvote_count": "1",
          "timestamp": "1738496700.0",
          "content": "Selected Answer: C\nAthena can only perform queries in S3 buckets, not in cloudwatch metrics.",
          "poster": "jojewi8143",
          "comment_id": "1350381"
        },
        {
          "comment_id": "1348928",
          "poster": "teo2157",
          "timestamp": "1738221300.0",
          "upvote_count": "2",
          "content": "Selected Answer: C\nAthena can only permorm queries in S3 buckets, not in cloudwatch metrics. Based on that, it's C."
        },
        {
          "upvote_count": "1",
          "timestamp": "1735896060.0",
          "comment_id": "1335961",
          "content": "Selected Answer: B\nAthena allows for ad-hoc analysis of log data, enabling you to investigate specific events or trends without the need to set up complex data processing pipelines.",
          "poster": "Slays"
        },
        {
          "content": "Selected Answer: C\nAmazon CloudWatch Metrics Insights can perform SQL queries",
          "poster": "matt200",
          "timestamp": "1735543320.0",
          "comment_id": "1333950",
          "upvote_count": "3"
        },
        {
          "timestamp": "1734465840.0",
          "content": "Selected Answer: B\nYou have now set up an AWS CloudTrail organization trail that sends logs to CloudWatch, enabled anomaly detection on the CloudTrail logs, and configured Amazon Athena to query those logs with SQL. You can further optimize this setup by incorporating Lambda functions, setting more complex anomaly detection configurations, or using AWS Security Hub for better monitoring and automation.",
          "upvote_count": "1",
          "poster": "spring21",
          "comment_id": "1328127"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:16:04.200Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "0yVcDaWkkzpOGmR1DIUN",
      "question_number": 212,
      "page": 43,
      "question_text": "A company has an organization in AWS Organizations. The organization includes workload accounts that contain enterprise applications. The company centrally manages users from an operations account. No users can be created in the workload accounts. The company recently added an operations team and must provide the operations team members with administrator access to each workload account.\nWhich combination of actions will provide this access? (Choose three.)",
      "choices": {
        "E": "In the operations account, create an IAM user group that is named SysAdmins. Add an IAM policy that allows the sts:AssumeRole action for the SysAdmin role in each workload account. Add all operations team members to the group.",
        "A": "Create a SysAdmin role in the operations account. Attach the AdministratorAccess policy to the role. Modify the trust relationship to allow the sts:AssumeRole action from the workload accounts.",
        "B": "Create a SysAdmin role in each workload account. Attach the AdministratorAccess policy to the role. Modify the trust relationship to allow the sts:AssumeRole action from the operations account.",
        "D": "In the operations account, create an IAM user for each operations team member.",
        "F": "Create an Amazon Cognito user pool in the operations account. Create an Amazon Cognito user for each operations team member.",
        "C": "Create an Amazon Cognito identity pool in the operations account. Attach the SysAdmin role as an authenticated role."
      },
      "correct_answer": "BDE",
      "answer_ET": "BDE",
      "answers_community": [
        "BDE (84%)",
        "Other"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/106179-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-04-14 14:41:00",
      "unix_timestamp": 1681476060,
      "discussion_count": 18,
      "discussion": [
        {
          "content": "Selected Answer: BDE\nAny thing Cognito, safe to remove (it is only used for application identity management)\nStep 1: Create each role in each workload account. Set trust relationship to only sts:AssumeRole via the operations user in operations account\nStep 2: Self explanatory: whatever permission you needs once the user assumed the role\nStep 3: Voila",
          "upvote_count": "7",
          "timestamp": "1688807040.0",
          "comment_id": "946339",
          "poster": "habros"
        },
        {
          "timestamp": "1743281520.0",
          "comment_id": "1411856",
          "upvote_count": "1",
          "poster": "Srikantha",
          "content": "Selected Answer: ABE\nChatGPT Explanation"
        },
        {
          "content": "Selected Answer: BDE\nBDE\nNot A - Create SysAdmin role for workload accounts. \nNot C F - No Cognito require.",
          "poster": "jamesf",
          "comment_id": "1254859",
          "timestamp": "1721896140.0",
          "upvote_count": "1"
        },
        {
          "upvote_count": "1",
          "comment_id": "1225945",
          "timestamp": "1717743660.0",
          "poster": "HarryLy",
          "content": "Selected Answer: BDE\nOperation account:\n - Need to create a role to assume role in workload account --> E\n- Create a group of users can perform assume role --> D\nworkload account\n- Need to create a role with have admin perssion for operation account assume -->B"
        },
        {
          "comment_id": "1194220",
          "poster": "c3518fc",
          "timestamp": "1712906400.0",
          "content": "Selected Answer: BEF\nNot sure why everyone is saying BDE. Why would you create an IAM user for each member and also create for the group? Make it make sense",
          "upvote_count": "2"
        },
        {
          "upvote_count": "1",
          "comment_id": "1168804",
          "poster": "4555894",
          "content": "Selected Answer: BDE\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-account-with-roles.html",
          "timestamp": "1709903340.0"
        },
        {
          "timestamp": "1709125140.0",
          "comment_id": "1161634",
          "upvote_count": "1",
          "content": "Selected Answer: BDE\nEBD looks like the best choice",
          "poster": "Vitalydt"
        },
        {
          "content": "Selected Answer: BDE\nsts:AssumeRole is one of the AWS Security Token Service (STS) actions used to obtain temporary security credentials and assume the role of another AWS account.",
          "comment_id": "1153765",
          "timestamp": "1708329360.0",
          "poster": "dzn",
          "upvote_count": "1"
        },
        {
          "upvote_count": "3",
          "poster": "thanhnv142",
          "content": "BDE: No cognito here. \n-step 1: create role in workload accounts\n-step 2: create IAM user for each member\n-step 3: move all member to the group that has permission to assume the role in step 1",
          "timestamp": "1706536680.0",
          "comment_id": "1135021"
        },
        {
          "timestamp": "1686384720.0",
          "poster": "madperro",
          "comment_id": "919904",
          "content": "Selected Answer: BDE\nBDE seems to be right.",
          "upvote_count": "2"
        },
        {
          "poster": "rdoty",
          "timestamp": "1685535240.0",
          "upvote_count": "1",
          "comment_id": "911223",
          "content": "Selected Answer: BDE\ndef BDE cause role must be created in workload accounts and assumed by the operations account"
        },
        {
          "comment_id": "910390",
          "content": "Selected Answer: BDE\nCorrect: BDE\nCognito has nothing to do with this, so C and F are wrong.\nThe roles must be created in the workload accounts and assumed from the operations account. So A is wrong.",
          "upvote_count": "1",
          "timestamp": "1685461200.0",
          "poster": "bcx"
        },
        {
          "content": "Selected Answer: BDE\nBDE seems the correct strategy",
          "timestamp": "1682749020.0",
          "comment_id": "884075",
          "upvote_count": "3",
          "poster": "ParagSanyashiv"
        },
        {
          "poster": "5aga",
          "upvote_count": "1",
          "timestamp": "1682482500.0",
          "comment_id": "881085",
          "content": "Why do we need option A when question is asking access to workload account?"
        },
        {
          "content": "A,B,E it is",
          "comment_id": "871163",
          "poster": "alce2020",
          "upvote_count": "1",
          "timestamp": "1681579860.0"
        },
        {
          "upvote_count": "2",
          "content": "Selected Answer: BDE\nBDE is right answer, nothing to do with cognito",
          "poster": "ele",
          "comment_id": "870809",
          "timestamp": "1681551120.0"
        },
        {
          "comment_id": "870376",
          "timestamp": "1681494360.0",
          "comments": [
            {
              "poster": "vvndx",
              "timestamp": "1683886080.0",
              "content": "Should be BDE, Why the need to create two roles?",
              "upvote_count": "1",
              "comment_id": "895816"
            }
          ],
          "upvote_count": "1",
          "poster": "jqso234",
          "content": "Selected Answer: ABE\nOptions C, D, and F are incorrect because they do not provide a way for the operations team members to assume a role in the workload accounts, which is necessary to access the resources in those accounts."
        },
        {
          "upvote_count": "1",
          "poster": "boledadian",
          "content": "BDE is correct",
          "comment_id": "870180",
          "timestamp": "1681476060.0"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:16:04.200Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "BkXf0Iwjzzq90nq0Oa7f",
      "question_number": 213,
      "page": 43,
      "question_text": "A DevOps team is deploying microservices for an application on an Amazon Elastic Kubernetes Service (Amazon EKS) cluster. The cluster uses managed node groups. The DevOps team wants to enable auto scaling for the microservice Pods based on a specific CPU utilization percentage. The DevOps team has already installed the Kubernetes Metrics Server on the cluster.\n\nWhich solution will meet these requirements in the MOST operationally efficient way?",
      "choices": {
        "C": "Run the AWS Systems Manager AWS-UpdateEKSManagedNodeGroup Automation document. Modify the values for NodeGroupDesiredSize, NodeGroupMaxSize, and NodeGroupMinSize to be based on an estimate for the required node size.",
        "A": "Edit the Auto Scaling group that is associated with the worker nodes of the EKS cluster. Configure the Auto Scaling group to use a target tracking scaling policy to scale when the average CPU utilization of the Auto Scaling group reaches a specific percentage.",
        "D": "Deploy the Kubernetes Horizontal Pod Autoscaler (HPA) and the Kubernetes Cluster Autoscaler in the cluster. Configure the HPA to scale based on the target CPU utilization percentage. Configure the Cluster Autoscaler to use the auto-discovery setting.",
        "B": "Deploy the Kubernetes Horizontal Pod Autoscaler (HPA) and the Kubernetes Vertical Pod Autoscaler (VPA) in the cluster. Configure the HPA to scale based on the target CPU utilization percentage. Configure the VPA to use the recommender mode setting."
      },
      "correct_answer": "D",
      "answer_ET": "D",
      "answers_community": [
        "D (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/152995-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-12-15 13:30:00",
      "unix_timestamp": 1734265800,
      "discussion_count": 1,
      "discussion": [
        {
          "timestamp": "1734265800.0",
          "upvote_count": "3",
          "content": "Selected Answer: D\nWhy Option D is Correct:\n\nHorizontal Pod Autoscaler (HPA):\nHPA automatically adjusts the number of Pods in a Kubernetes Deployment, ReplicaSet, or StatefulSet based on observed CPU or memory utilization or custom metrics.\nWith the Kubernetes Metrics Server already installed, HPA can monitor CPU utilization metrics and scale the Pods to maintain the target CPU usage percentage.\nCluster Autoscaler:\nWhen HPA increases the number of Pods and there aren’t enough resources in the cluster, Cluster Autoscaler adds nodes to the managed node group dynamically.\nThe auto-discovery feature enables Cluster Autoscaler to automatically detect the appropriate node group and scale it up or down as needed.\nOperational Efficiency:\nThis combination ensures Pods scale first at the workload level (HPA) and then at the infrastructure level (Cluster Autoscaler) only if required. This approach minimizes cost and ensures optimal resource utilization.",
          "poster": "Ky_24",
          "comment_id": "1326846"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:16:04.200Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "jGWLcgzdO0wJTNYg4gQG",
      "question_number": 214,
      "page": 43,
      "question_text": "A company has multiple AWS accounts. The company uses AWS IAM Identity Center that is integrated with a third-party SAML 2.0 identity provider (IdP).\n\nThe attributes for access control feature is enabled in IAM Identity Center. The attribute mapping list maps the department key from the IdP to the ${path:enterprise.department} attribute. All existing Amazon EC2 instances have a d1, d2, d3 department tag that corresponds to three company’s departments.\n\nA DevOps engineer must create policies based on the matching attributes. The policies must grant each user access to only the EC2 instances that are tagged with the user’s respective department name.\n\nWhich condition key should the DevOps engineer include in the custom permissions policies to meet these requirements?",
      "choices": {
        "C": "",
        "A": "",
        "D": "",
        "B": ""
      },
      "correct_answer": "C",
      "answer_ET": "C",
      "answers_community": [
        "C (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/151022-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-11-08 20:06:00",
      "unix_timestamp": 1731092760,
      "discussion_count": 4,
      "discussion": [
        {
          "timestamp": "1734466920.0",
          "comment_id": "1328137",
          "upvote_count": "2",
          "poster": "spring21",
          "content": "Selected Answer: C\nit must also be named exactly the same in your aws:PrincipalTag condition key (that is, \"ec2:ResourceTag/CostCenter\": \"${aws:PrincipalTag/CostCenter}\")."
        },
        {
          "poster": "Impromptu",
          "comment_id": "1316329",
          "content": "Selected Answer: C\nAnswer is indeed C\nhttps://docs.aws.amazon.com/singlesignon/latest/userguide/configure-abac-attributes.html",
          "upvote_count": "3",
          "timestamp": "1732283700.0"
        },
        {
          "poster": "koo_kai",
          "comment_id": "1309429",
          "content": "Selected Answer: C\nit's c",
          "timestamp": "1731241920.0",
          "upvote_count": "2"
        },
        {
          "poster": "Jefff9997",
          "content": "The answer is C. Not B.",
          "timestamp": "1731092760.0",
          "upvote_count": "3",
          "comment_id": "1308891"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:16:04.200Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "EsC8EugQsjSsvHTBe1Yl",
      "question_number": 215,
      "page": 43,
      "question_text": "A security team wants to use AWS CloudTrail to monitor all actions and API calls in multiple accounts that are in the same organization in AWS Organizations. The security team needs to ensure that account users cannot turn off CloudTrail in the accounts.\n\nWhich solution will meet this requirement?",
      "choices": {
        "A": "Apply an SCP to all OUs to deny the cloudtrail:StopLogging action and the cloudtrail:DeleteTrail action.",
        "D": "Use AWS Config to automatically re-enable CloudTrail if a user disables CloudTrail in an account.",
        "B": "Create IAM policies in each account to deny the cloudtrail:StopLogging action and the cloudtrail:DeleteTrail action.",
        "C": "Set up Amazon CloudWatch alarms to notify the security team when a user disables CloudTrail in an account."
      },
      "correct_answer": "A",
      "answer_ET": "A",
      "answers_community": [
        "A (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/153425-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-12-25 06:24:00",
      "unix_timestamp": 1735104240,
      "discussion_count": 1,
      "discussion": [
        {
          "poster": "matt200",
          "timestamp": "1735544340.0",
          "content": "Selected Answer: A\nshould be A",
          "comment_id": "1333959",
          "upvote_count": "2"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:16:04.200Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "bJMkysH24EtxARMKRDRf",
      "question_number": 216,
      "page": 44,
      "question_text": "A DevOps engineer needs to configure a blue/green deployment for an existing three-tier application. The application runs on Amazon EC2 instances and uses an Amazon RDS database. The EC2 instances run behind an Application Load Balancer (ALB) and are in an Auto Scaling group.\n\nThe DevOps engineer has created launch templates, Auto Scaling groups, and ALB target groups for the blue environment and the green environment. Each target group specifies which application version, blue or green, will be loaded on the EC2 instances. An Amazon Route 53 record for www.example.com points to the ALB.\n\nThe deployment must shift traffic all at once from the blue environment to the green environment.\n\nWhich solution will meet these requirements?",
      "choices": {
        "C": "Update the launch template to deploy the green environment's application version to the blue environment's EC2 instances. Do not change the target groups or the Auto Scaling groups in either environment. Perform a rolling restart of the blue environments EC2 instances.",
        "A": "Starta rolling restart of the Auto Scaling group for the green environment to deploy the new application version to the green environment's EC2 instances. When the rolling restart is complete, use an AWS CLI command to update the ALB to send traffic to the green environment's target group.",
        "D": "Starta rolling restart of the Auto Scaling group for the green environment to deploy the new application version to the green environment's EC2 instances. When the rolling restart is complete, update Route 53 to point to the green environment's endpoint on the ALB.",
        "B": "Use an AWS CLI command to update the ALB to send traffic to the green environments target group. Start a rolling restart of the Auto Scaling group for the green environment to deploy the new application version to the green environment's EC2 instances."
      },
      "correct_answer": "A",
      "answer_ET": "A",
      "answers_community": [
        "A (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/151064-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-11-10 13:41:00",
      "unix_timestamp": 1731242460,
      "discussion_count": 4,
      "discussion": [
        {
          "content": "it's A\nQ.120",
          "poster": "koo_kai",
          "upvote_count": "6",
          "timestamp": "1731242460.0",
          "comment_id": "1309430"
        },
        {
          "content": "Selected Answer: A\nShould be A",
          "upvote_count": "2",
          "timestamp": "1735313220.0",
          "comment_id": "1332470",
          "poster": "youonebe"
        },
        {
          "upvote_count": "2",
          "timestamp": "1733745900.0",
          "content": "Selected Answer: A\nFor sure, it's \"A\". First we update new (meaning \"green\") cluster with a new version. And after that we shift traffic of ALB from first target group to the second.",
          "comment_id": "1324023",
          "poster": "eugene2owl"
        },
        {
          "timestamp": "1732589340.0",
          "content": "Selected Answer: A\nA it's correct",
          "poster": "fnameni",
          "comment_id": "1317869",
          "upvote_count": "2"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:16:14.722Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "heYn45wVhVzzfyTKCQtN",
      "question_number": 217,
      "page": 44,
      "question_text": "A company has an application that runs on Amazon EC2 instances in an Auto Scaling group. The application processes a high volume of messages from an Amazon Simple Queue Service (Amazon SQS) queue.\n\nA DevOps engineer noticed that the application took several hours to process a group of messages from the SQS queue. The average CPU utilization of the Auto Scaling group did not cross the threshold of a target tracking scaling policy when processing the messages. The application that processes the SQS queue publishes logs to ‘Amazon CloudWatch Logs.\n\nThe DevOps engineer needs to ensure that the queue is processed quickly.\n\nWhich solution meets these requirements with the LEAST operational overhead?",
      "choices": {
        "A": "Create an AWS Lambda function. Configure the Lambda function to publish a custom metric by using the ApproximateNumberOfMessagesVisible SQS queue attribute and the GroupInServiceInstances Auto Scaling group attribute to publish the queue messages for each instance. Schedule an Amazon EventBridge rule to run the Lambda function every hour. Create a target tracking scaling policy for the Auto Scaling group that uses the custom metric to scale in and out.",
        "C": "Create a target tracking scaling policy for the Auto Scaling group. In the target tracking policy, use the ApproximateNumberOfMessagesVisible SQS queue attribute and the GroupInServiceInstances Auto Scaling group attribute to calculate how many messages are in the queue for each number of instances by using metric math. Use the calculated attribute to scale in and out.",
        "B": "Create an AWS Lambda function. Configure the Lambda function to publish a custom metric by using the ApproximateNumberOfMessagesVisible SQS queue attribute and the GroupInServiceInstances Auto Scaling group attribute to publish the queue messages for each instance. Create a CloudWatch subscription filter for the application logs with the Lambda function as the target. Create a target tracking scaling policy for the Auto Scaling group that uses the custom metric to scale in and out.",
        "D": "Create an AWS Lambda function that logs the ApproximateNumberOfMessagesVisible attribute of the SQS queue to a CloudWatch Logs log group. Schedule an Amazon EventBridge rule to run the Lambda function every 5 minutes. Create a metric filer to count the number of log events from a CloudWatch logs group. Create a target tracking scaling policy for the Auto Scaling group that uses the custom metric to scale in and out."
      },
      "correct_answer": "C",
      "answer_ET": "C",
      "answers_community": [
        "C (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/152997-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-12-15 14:34:00",
      "unix_timestamp": 1734269640,
      "discussion_count": 2,
      "discussion": [
        {
          "poster": "GripZA",
          "upvote_count": "1",
          "comment_id": "1562716",
          "timestamp": "1745319000.0",
          "content": "Selected Answer: C\nboth these metrics available in CloudWatch:\nGroupInServiceInstances: The number of instances that are running as part of the Auto Scaling group. This metric does not include instances that are pending or terminating.\nApproximateNumberOfMessagesVisible: The number of messages to be processed.\n\nno need for Lambda"
        },
        {
          "comment_id": "1326873",
          "poster": "Ky_24",
          "upvote_count": "4",
          "timestamp": "1734269640.0",
          "content": "Selected Answer: C\nWhy Option C is Correct:\n • Target Tracking Scaling Policy: The question specifies that the scaling policy should help process the queue quickly. Using metric math to combine the ApproximateNumberOfMessagesVisible SQS queue attribute with the GroupInServiceInstances Auto Scaling group attribute provides an efficient way to scale based on the actual number of messages and the available instances.\n • Direct Integration with Auto Scaling: This solution integrates directly with the Auto Scaling group’s target tracking policy, so the scaling of EC2 instances happens automatically and dynamically based on the metrics from SQS, ensuring faster processing of messages."
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:16:14.722Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "SMx1tBAc6A7lSIcyB81D",
      "question_number": 218,
      "page": 44,
      "question_text": "A company has a single AWS account that runs hundreds of Amazon EC2 instances in a single AWS Region. The company launches and terminates new EC2 instances every hour. The account includes existing EC2 instances that have been running for longer than a week.\n\nThe company's security policy requires all running EC2 instances to have an EC2 instance profile attached. The company has created a default EC2 instance profile. The default EC2 instance profile must be attached to any EC2 instances that do not have a profile attached.\n\nWhich solution will meet these requirements?",
      "choices": {
        "A": "Configure an Amazon EventBridge rule that matches the Amazon EC2 RunInstances API calls. Configure the rule to invoke an AWS Lambda function to attach the default instance profile to the EC2 instances.",
        "B": "Configure AWS Config. Deploy an AWS Config ec2-instance-profile-attached managed rule. Configure an automatic remediation action that invokes an AWS Systems Manager Automation runbook to attach the default instance profile to the EC2 instances.",
        "D": "Configure AWS Config. Deploy an AWS Config iam-role-managed-policy-check managed rule. Configure an automatic remediation action that invokes an AWS Lambda function to attach the default instance profile to the EC2 instances.",
        "C": "Configure an Amazon EventBridge rule that matches the Amazon EC2 StartInstances API calls. Configure the rule to invoke an AWS Systems Manager Automation runbook to attach the default instance profile to the EC2 instances."
      },
      "correct_answer": "B",
      "answer_ET": "B",
      "answers_community": [
        "B (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/153426-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-12-25 06:25:00",
      "unix_timestamp": 1735104300,
      "discussion_count": 2,
      "discussion": [
        {
          "upvote_count": "1",
          "poster": "Adzz",
          "timestamp": "1748153400.0",
          "comment_id": "1572077",
          "content": "Selected Answer: B\noption b"
        },
        {
          "poster": "CHRIS12722222",
          "timestamp": "1735166580.0",
          "upvote_count": "3",
          "comment_id": "1331722",
          "content": "Selected Answer: B\nhttps://docs.aws.amazon.com/config/latest/developerguide/ec2-instance-profile-attached.html"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:16:14.722Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "Hbn5T72WH0U3crUvf0st",
      "question_number": 219,
      "page": 44,
      "question_text": "A company uses AWS Organizations to manage hundreds of AWS accounts. The company has a team that is responsible for AWS Identity and Access Management (IAM).\n\nThe IAM team wants to implement AWS IAM Identity Center. The IAM team must have only the minimum required permissions to manage IAM Identity Center. The IAM team must not be able to gain unnecessary access to the Organizations management account. The IAM team must be able to provision new IAM Identity Center permission sets and assignments for new and existing member accounts.\n\nWhich combination of steps will meet these requirements? (Choose three.)",
      "choices": {
        "A": "Create a new AWS account for the IAM team. Enable IAM Identity Center in the new account. In the Organizations management account, register the new account as a delegated administrator for IAM Identity Center.",
        "C": "Create an SCP in Organizations. Create a new OU for the Organizations management account, and link the new SCP to the OU. Configure the SCP to deny all access to IAM Identity Center.",
        "E": "Assign the new permission set to the Organizations management account. Allow the IAM team's group to use the permission set.",
        "F": "Assign the new permission set to the new AWS account. Allow the IAM team's group to use the permission set.",
        "B": "Create a new AWS account for the IAM team. Enable IAM Identity Center in the Organizations management account. In the Organizations management account, register the new account as a delegated administrator for IAM Identity Center.",
        "D": "Create IAM users and an IAM group for the IAM team in IAM Identity Center. Add the users to the group. Create a new permission set. Attach the AWSSSOMemberAccountAdministrator managed IAM policy to the group."
      },
      "correct_answer": "BDF",
      "answer_ET": "BDF",
      "answers_community": [
        "BDF (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/151838-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-11-22 15:06:00",
      "unix_timestamp": 1732284360,
      "discussion_count": 2,
      "discussion": [
        {
          "poster": "tinyshare",
          "timestamp": "1733268240.0",
          "content": "Selected Answer: BDF\nA is wrong. you need to enable the Identity Center in the management account first, then create a delegated account.\nC is wrong SCP is different from IdP\nE is wrong. The permission set should be assigned to members, not the management account.",
          "comment_id": "1321579",
          "upvote_count": "5"
        },
        {
          "content": "Selected Answer: BDF\nShould be B instead of A: https://docs.aws.amazon.com/singlesignon/latest/userguide/get-set-up-for-idc.html\nAlthough the administration can be delegated to a member account, the enabling of Identity Center is still in the management account.",
          "poster": "Impromptu",
          "upvote_count": "5",
          "comment_id": "1316339",
          "timestamp": "1732284360.0"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:16:14.722Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "HTCojAKRKg3ITxpCgeJX",
      "question_number": 220,
      "page": 44,
      "question_text": "A company uses an Amazon Aurora PostgreSQL global database that has two secondary AWS Regions. A DevOps engineer has configured the database parameter group to guarantee an RPO of 60 seconds. Write operations on the primary cluster are occasionally blocked because of the RPO setting.\n\nThe DevOps engineer needs to reduce the frequency of blocked write operations.\n\nWhich solution will meet these requirements?",
      "choices": {
        "B": "Enable write forwarding for the global database.",
        "D": "Configure synchronous replication for the global database.",
        "C": "Remove one of the secondary clusters from the global database.",
        "A": "Add an additional secondary cluster to the global database."
      },
      "correct_answer": "C",
      "answer_ET": "C",
      "answers_community": [
        "C (67%)",
        "B (22%)",
        "11%"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/153142-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-12-18 01:15:00",
      "unix_timestamp": 1734480900,
      "discussion_count": 4,
      "discussion": [
        {
          "timestamp": "1734480900.0",
          "comment_id": "1328214",
          "poster": "spring21",
          "upvote_count": "5",
          "content": "Selected Answer: C\nTo reduce the frequency of blocked write operations in an Amazon Aurora PostgreSQL global database, particularly when you're using a 60-second Recovery Point Objective (RPO), the goal is to improve replication performance so that the lag between the primary cluster and its secondary regions does not exceed the 60-second threshold."
        },
        {
          "timestamp": "1742313960.0",
          "comment_id": "1400203",
          "poster": "DKM",
          "content": "Selected Answer: C\nRemoving a secondary cluster can help alleviate write contention on the primary cluster, improving overall performance.",
          "upvote_count": "1"
        },
        {
          "comment_id": "1341738",
          "upvote_count": "2",
          "timestamp": "1737040560.0",
          "content": "Selected Answer: B\nEnabling write forwarding for an Amazon Aurora global database can be a viable solution to improve write performance and reduce the frequency of blocked write operations. Write forwarding allows secondary clusters in a global database to forward write requests to the primary cluster. This can help distribute the write load and improve the overall performance of the global database.",
          "poster": "teo2157"
        },
        {
          "poster": "youonebe",
          "content": "Selected Answer: D\nExplanation: Aurora global databases use asynchronous replication by default to provide fast cross-region replication. However, asynchronous replication can cause delays in synchronizing data between regions, and if the RPO is set too aggressively, it can cause write operations to block when replication is behind.\n\nBy configuring synchronous replication, writes to the primary region will only be acknowledged once the changes are successfully replicated to the secondary regions, reducing the likelihood of replication lag. This means that write operations will be blocked if the replication is lagging and the RPO setting is not met, but the frequency of blocked writes may be reduced because synchronous replication guarantees data consistency across regions.",
          "upvote_count": "1",
          "comment_id": "1332497",
          "timestamp": "1735314660.0"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:16:14.722Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "ttS3JFkhp6VNRMf7tLGV",
      "question_number": 221,
      "page": 45,
      "question_text": "A company has a web application that is hosted on an Amazon Elastic Kubernetes Service (Amazon EKS) cluster. The EKS cluster runs on AWS Fargate that is available through an internet-facing Application Load Balancer.\n\nThe application is experiencing stability issues that lead to longer response times. A DevOps engineer needs to configure observability in Amazon CloudWatch to troubleshoot the issue. The solution must provide only the minimum necessary permissions.\n\nWhich combination of steps will meet these requirements? (Choose three.)",
      "choices": {
        "F": "Enable EKS control plane logging for the EKS cluster.",
        "E": "Configure an IAM OpenID Connect (OIDC) provider for the EKS cluster.",
        "B": "Deploy the AWS Distro for OpenTelemetry Collector as a Kubernetes DaemonSet to the EKS cluster.",
        "D": "Associate a Kubernetes service account with an IAM role by using IAM roles for service accounts in Amazon EKS. Use the CloudWatchAgentAdminPolicy AWS managed policy.",
        "C": "Associate a Kubernetes service account with an IAM role by using IAM roles for service accounts in Amazon EKS. Use the CloudWatchAgentServerPolicy AWS managed policy.",
        "A": "Deploy the CloudWatch agent as a Kubernetes StatefulSet to the EKS cluster."
      },
      "correct_answer": "ACF",
      "answer_ET": "ACF",
      "answers_community": [
        "ACF (50%)",
        "BCE (38%)",
        "6%"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/153427-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-12-25 06:27:00",
      "unix_timestamp": 1735104420,
      "discussion_count": 6,
      "discussion": [
        {
          "poster": "matt200",
          "upvote_count": "5",
          "comment_id": "1333983",
          "content": "Selected Answer: BCE\nB: AWS Distro for OpenTelemetry (ADOT) is the recommended solution for collecting metrics and traces from EKS clusters\nE: OIDC provider is required to use IRSA\n\nA: Incorrect because For EKS on Fargate, ADOT is the recommended solution\nF: Incorrect because this alone won't provide the application-level observability needed",
          "timestamp": "1735546200.0"
        },
        {
          "content": "Selected Answer: ACF\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/deploy-container-insights-EKS.html\n\n- No daemonsets for fargate\n- CloudWatchAgentServerPolicy is correct\n- controlplane logging",
          "comment_id": "1331726",
          "upvote_count": "5",
          "timestamp": "1735169460.0",
          "poster": "CHRIS12722222"
        },
        {
          "upvote_count": "1",
          "content": "Selected Answer: BCF\nB not A - it's best practice to use Container Insights to collect metrics on Amazon EKS and Kubernetes. To use Container Insights with Fargate, you must use AWS Distro for OpenTelemetry.\n\nC not D for minimum necessary permissions.\n\nF - EKS control plane logging provides audit and diagnostic logs directly from the Amazon EKS control plane to CloudWatch Logs in your account",
          "comment_id": "1562722",
          "poster": "GripZA",
          "timestamp": "1745321340.0"
        },
        {
          "comment_id": "1363257",
          "timestamp": "1740778680.0",
          "poster": "SysOps",
          "upvote_count": "1",
          "content": "Selected Answer: ACE\nA instead of B only because Daemonsets aren’t supported on Fargate"
        },
        {
          "upvote_count": "3",
          "timestamp": "1739728680.0",
          "poster": "jojewi8143",
          "comment_id": "1357323",
          "content": "Selected Answer: ACF\nChanged mind to ACF. Daemonsets arent supported on Fargate, B is not possible. https://docs.aws.amazon.com/eks/latest/userguide/fargate.html"
        },
        {
          "comment_id": "1350396",
          "content": "Selected Answer: BCE\nim for bce",
          "upvote_count": "1",
          "timestamp": "1738497780.0",
          "poster": "jojewi8143"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:16:25.301Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "u1DBkBEK226bJmWbgOSO",
      "question_number": 222,
      "page": 45,
      "question_text": "A company stores its Python-based application code in AWS CodeCommit. The company uses AWS CodePipeline to deploy the application. The CodeCommit repository and the CodePipeline pipeline are deployed to the same AWS account.\n\nThe company's security team requires all code to be scanned for vulnerabilities before the code is deployed to production. If any vulnerabilities are found, the deployment must stop.\n\nWhich solution will meet these requirements?",
      "choices": {
        "A": "Create a new CodeBuild project. Configure the project to run a security scan on the code by using Amazon CodeGuru Security. Configure the CodeBuild project to raise an error if CodeGuru Security finds vulnerabilities. Create a new IAM role that has sufficient permissions to run CodeGuru Security scans. Assign the role to the CodeBuild project. In the CodePipeline pipeline, add a new stage before the deployment stage. Select AWS CodeBuild as the action provider for the new stage. Use the source artifact from the CodeCommit repository. Configure the action to use the CodeBuild project.",
        "B": "Create a new CodeBuild project. Configure the project to run a security scan on the code by using Amazon Inspector. Configure the CodeBuild project to raise an error if Amazon Inspector finds vulnerabilities. Create a new IAM role that has sufficient permissions to run Amazon Inspector scans. Assign the role to the CodeBuild project. In the CodePipeline pipeline, add a new stage before the deployment stage. Select AWS CodeBuild as the action provider for the new stage. Use the source artifact from the CodeCommit repository. Configure the action to use the CodeBuild project.",
        "D": "Update the IAM role that is attached to CodePipeline to include sufficient permissions to invoke Amazon DevOps Guru. In the CodePipeline pipeline, add a new stage before the deployment stage. Select CodeGuru Security as the action provider for the new stage. Use the source artifact from the CodeCommit repository.",
        "C": "Update the IAM role that is attached to CodePipeline to include sufficient permissions to invoke Amazon DevOps Guru. In the CodePipeline pipeline, add a new stage before the deployment stage. Select DevOps Guru as the action provider for the new stage. Use the source artifact from the CodeCommit repository."
      },
      "correct_answer": "A",
      "answer_ET": "A",
      "answers_community": [
        "A (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/153428-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-12-25 06:28:00",
      "unix_timestamp": 1735104480,
      "discussion_count": 2,
      "discussion": [
        {
          "timestamp": "1735169820.0",
          "content": "Selected Answer: A\nit is codeguru for static code analysis\n\nNOT devops guru\nNo need for inspector",
          "upvote_count": "5",
          "poster": "CHRIS12722222",
          "comment_id": "1331728"
        },
        {
          "content": "Selected Answer: A\nAmazon CodeGuru Security detects, tracks, and fixes code security vulnerabilities anywhere in the development cycle using ML and automated reasoning",
          "upvote_count": "3",
          "comment_id": "1355502",
          "poster": "teo2157",
          "timestamp": "1739344200.0"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:16:25.301Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "SuyZTontsYZZkOZfT1pm",
      "question_number": 223,
      "page": 45,
      "question_text": "A company is adopting AWS CodeDeploy to automate its application deployments for a Java-Apache Tomcat application with an Apache Webserver. The development team started with a proof of concept, created a deployment group for a developer environment, and performed functional tests within the application. After completion, the team will create additional deployment groups for staging and production.\nThe current log level is configured within the Apache settings, but the team wants to change this configuration dynamically when the deployment occurs, so that they can set different log level configurations depending on the deployment group without having a different application revision for each group.\nHow can these requirements be met with the LEAST management overhead and without requiring different script versions for each deployment group?",
      "choices": {
        "B": "Create a script that uses the CodeDeploy environment variable DEPLOYMENT_GROUP_ NAME to identify which deployment group the instance is part of. Use this information to configure the log level settings. Reference this script as part of the BeforeInstall lifecycle hook in the appspec.yml file.",
        "A": "Tag the Amazon EC2 instances depending on the deployment group. Then place a script into the application revision that calls the metadata service and the EC2 API to identify which deployment group the instance is part of. Use this information to configure the log level settings. Reference the script as part of the AfterInstall lifecycle hook in the appspec.yml file.",
        "D": "Create a script that uses the CodeDeploy environment variable DEPLOYMENT_GROUP_ID to identify which deployment group the instance is part of to configure the log level settings. Reference this script as part of the Install lifecycle hook in the appspec.yml file.",
        "C": "Create a CodeDeploy custom environment variable for each environment. Then place a script into the application revision that checks this environment variable to identify which deployment group the instance is part of. Use this information to configure the log level settings. Reference this script as part of the ValidateService lifecycle hook in the appspec.yml file."
      },
      "correct_answer": "B",
      "answer_ET": "B",
      "answers_community": [
        "B (73%)",
        "D (27%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/105490-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-04-07 10:41:00",
      "unix_timestamp": 1680856860,
      "discussion_count": 18,
      "discussion": [
        {
          "content": "B. In the docs: https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html you'll find a Note: \"The Start, Install, TestTraffic, AllowTraffic, and End events in the deployment cannot be scripted, which is why they appear in gray in this diagram.\" Thats why its not D.",
          "poster": "Schubibubi",
          "timestamp": "1681798680.0",
          "comment_id": "873325",
          "upvote_count": "17"
        },
        {
          "comment_id": "1078707",
          "timestamp": "1727165340.0",
          "poster": "y0eri",
          "upvote_count": "12",
          "content": "Answer: B\nRead this blog. \nhttps://aws.amazon.com/blogs/devops/using-codedeploy-environment-variables/\n\nif [ \"$DEPLOYMENT_GROUP_NAME\" == \"Staging\" ]\nthen\n sed -i -e 's/LogLevel warn/LogLevel debug/g' /etc/httpd/conf/httpd.conf\nfi"
        },
        {
          "comment_id": "1316824",
          "content": "Selected Answer: B\nOn top of what others wrote the other difference between B and D is the BeforeInstall, since you need to configure log level before deploying the code/serivce.",
          "timestamp": "1732400280.0",
          "upvote_count": "1",
          "poster": "steli0"
        },
        {
          "content": "Selected Answer: B\nB is correct.\nDEPLOYMENT_ID : This variables contains the deployment ID of the current deployment.\n\nDEPLOYMENT_GROUP_NAME : This variable contains the name of the deployment group. A deployment group is a set of instances associated with an application that you target for a deployment.",
          "comment_id": "1064231",
          "poster": "Sazeka",
          "timestamp": "1727165340.0",
          "upvote_count": "2"
        },
        {
          "timestamp": "1727165340.0",
          "poster": "zijo",
          "content": "Answer B. You only need to consider options B and D which are the least complex ones. The option B gives the CodeDeploy environment variable DEPLOYMENT_GROUP_ NAME that points to different instances and gives the option to set different log-level configurations in the same script depending on the deployment group without having a different application revision for each group. Also, The BeforeInstall lifecycle hook in the appspec.yml file refers to a script that will run on the instance before the application revision files are installed.",
          "upvote_count": "3",
          "comment_id": "1082863"
        },
        {
          "poster": "c3518fc",
          "upvote_count": "1",
          "content": "Selected Answer: B\nversion: 0.0\nos: linux\nfiles:\n - source: /\n destination: /var/www/html/\nhooks:\n BeforeInstall:\n - location: scripts/update_log_level.sh\n timeout: 300\n runas: root",
          "comment_id": "1209009",
          "timestamp": "1727165340.0"
        },
        {
          "comment_id": "1214653",
          "content": "Selected Answer: B\nThis reference specifies the exact scenario described in \"B\", so I have to go with that\n\"Set the log level according to the deployment group.\"\nhttps://aws.amazon.com/blogs/devops/using-codedeploy-environment-variables/\n\ncat install_dependencies.sh\n [...]\n if [ \"$DEPLOYMENT_GROUP_NAME\" == \"Staging\" ]\n [...]\n\ncat appspec.yml\nhooks:\n BeforeInstall:\n - location: scripts/install_dependencies",
          "timestamp": "1727165340.0",
          "poster": "Gomer",
          "upvote_count": "1"
        },
        {
          "comments": [
            {
              "content": "B is correct: <without having a different application revision for each group> means A and C is incorrect. \nA and C: <place a script into the application revision> both mention this, indicating a revision of the app, which is contradicted to the question\nD: there is no Install lifecycle hook",
              "poster": "thanhnv142",
              "comment_id": "1145263",
              "timestamp": "1707458460.0",
              "upvote_count": "1"
            }
          ],
          "timestamp": "1706354100.0",
          "poster": "thanhnv142",
          "upvote_count": "2",
          "comment_id": "1133273",
          "content": "B is correct"
        },
        {
          "timestamp": "1704079680.0",
          "upvote_count": "3",
          "content": "Selected Answer: D\nAnswer D. \nYou only need to consider options B and D which are the least complex ones. The option B gives the CodeDeploy environment variable DEPLOYMENT_GROUP_ NAME, but DEPLOYMENT_GROUP_ID is recommended because it's more reliable and less prone to changes or inconsistencies. Answer D with settings DEPLOYMENT_GROUP_ID environment variable, which contains the unique identifier for the deployment group. This allows you to identify the deployment group without relying on custom scripts or metadata services.",
          "poster": "hoakhanh281",
          "comment_id": "1110952"
        },
        {
          "content": "Based on the analysis, Option B is the most efficient and straightforward approach. It uses the built-in DEPLOYMENT_GROUP_NAME environment variable provided by CodeDeploy and involves minimal management overhead. The script can easily read this variable to determine the deployment group and set the log level accordingly, eliminating the need for different script versions for each group. This method aligns well with the requirement of least management overhead and simplicity.",
          "upvote_count": "1",
          "timestamp": "1700329920.0",
          "comment_id": "1074196",
          "poster": "wem"
        },
        {
          "content": "B is the most straightforward and efficient solution to meet the requirements with the least management overhead and without requiring different script versions for each deployment group.",
          "timestamp": "1686529560.0",
          "poster": "SanChan",
          "upvote_count": "2",
          "comment_id": "921046",
          "comments": [
            {
              "poster": "Aja1",
              "timestamp": "1691304840.0",
              "upvote_count": "1",
              "content": "https://aws.amazon.com/blogs/devops/using-codedeploy-environment-variables/",
              "comment_id": "973594"
            }
          ]
        },
        {
          "content": "Answer is B practical use case",
          "poster": "mywogunleye",
          "timestamp": "1686378180.0",
          "comment_id": "919848",
          "upvote_count": "1"
        },
        {
          "upvote_count": "2",
          "content": "Selected Answer: B\nRunning the hook during the Install or AfterInstall would make more sense but hooks for Install are not available (like in answer D) and AfterInstall is not included in answers so the best answer is B.",
          "poster": "madperro",
          "comment_id": "918017",
          "timestamp": "1686213300.0"
        },
        {
          "content": "Selected Answer: B\nB is the only correct answer",
          "upvote_count": "4",
          "comment_id": "888276",
          "poster": "vherman",
          "timestamp": "1683100440.0"
        },
        {
          "upvote_count": "3",
          "comment_id": "870439",
          "poster": "alce2020",
          "timestamp": "1681501680.0",
          "content": "B. Create a script that uses the CodeDeploy environment variable DEPLOYMENT_GROUP_NAME to identify which deployment group the instance is part of, and use this information to configure the log level settings. Reference this script as part of the BeforeInstall lifecycle hook in the appspec.yml file, would be the option with the least management overhead and without requiring different script versions for each deployment group"
        },
        {
          "content": "Option B is the best solution for this use case. By using the CodeDeploy environment variable DEPLOYMENT_GROUP_NAME, the script can identify the deployment group that the instance is part of, without requiring any additional configuration or management overhead. The script can then dynamically configure the log level settings based on the identified deployment group.",
          "poster": "jqso234",
          "timestamp": "1681492200.0",
          "upvote_count": "1",
          "comment_id": "870351"
        },
        {
          "upvote_count": "1",
          "poster": "henryyvr",
          "timestamp": "1681340760.0",
          "content": "B See: https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html",
          "comment_id": "868884"
        },
        {
          "content": "Selected Answer: D\nD is valid and LEAST management overhead",
          "comments": [
            {
              "content": "typo, B is the right: BeforeInstall lifecycle hook",
              "comment_id": "877106",
              "poster": "ele",
              "upvote_count": "2",
              "timestamp": "1682147700.0"
            }
          ],
          "poster": "ele",
          "timestamp": "1680856860.0",
          "comment_id": "863659",
          "upvote_count": "1"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:16:25.301Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "TRsIVKH3EVfSls9oGwq5",
      "question_number": 224,
      "page": 45,
      "question_text": "A company has multiple accounts in an organization in AWS Organizations. The company's SecOps team needs to receive an Amazon Simple Notification Service (Amazon SNS) notification if any account in the organization turns off the Block Public Access feature on an Amazon S3 bucket. A DevOps engineer must implement this change without affecting the operation of any AWS accounts. The implementation must ensure that individual member accounts in the organization cannot turn off the notification.\nWhich solution will meet these requirements?",
      "choices": {
        "B": "Create an AWS CloudFormation template that creates an SNS topic and subscribes the SecOps team’s email address to the SNS topic. In the template, include an Amazon EventBridge rule that uses an event pattern of CloudTrail activity for s3:PutBucketPublicAccessBlock and a target of the SNS topic. Deploy the stack to every account in the organization by using CloudFormation StackSets.",
        "C": "Turn on AWS Config across the organization. In the delegated administrator account, create an SNS topic. Subscribe the SecOps team's email address to the SNS topic. Deploy a conformance pack that uses the s3-bucket-level-public-access-prohibited AWS Config managed rule in each account and uses an AWS Systems Manager document to publish an event to the SNS topic to notify the SecOps team.",
        "D": "Turn on Amazon Inspector across the organization. In the Amazon Inspector delegated administrator account, create an SNS topic. Subscribe the SecOps team’s email address to the SNS topic. In the same account, create an Amazon EventBridge rule that uses an event pattern for public network exposure of the S3 bucket and publishes an event to the SNS topic to notify the SecOps team.",
        "A": "Designate an account to be the delegated Amazon GuardDuty administrator account. Turn on GuardDuty for all accounts across the organization. In the GuardDuty administrator account, create an SNS topic. Subscribe the SecOps team's email address to the SNS topic. In the same account, create an Amazon EventBridge rule that uses an event pattern for GuardDuty findings and a target of the SNS topic."
      },
      "correct_answer": "C",
      "answer_ET": "C",
      "answers_community": [
        "C (69%)",
        "A (25%)",
        "6%"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/105252-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-04-05 06:30:00",
      "unix_timestamp": 1680669000,
      "discussion_count": 46,
      "discussion": [
        {
          "timestamp": "1698502020.0",
          "content": "Answer is C.\n* AWS AWS Systems Manager Automation provides predefined runbooks(ex. AWS-PublishSNSNotification ) for Amazon Simple Notification Service - https://docs.aws.amazon.com/systems-manager-automation-runbooks/latest/userguide/automation-aws-publishsnsnotification.html\n* Running automations in multiple AWS Regions and accounts (https://docs.aws.amazon.com/systems-manager/latest/userguide/running-automations-multiple-accounts-regions.html )\n\nB seems to be old approach. With cloudformation stackset, each account can still change resource config (ex. SNS) that causes drift.... so I choose C because it utilize AWS organization fully with aws systems manager automation in multiple regions and multiple accounts with delegated administrator account( or management account )",
          "comment_id": "1056245",
          "upvote_count": "12",
          "comments": [
            {
              "timestamp": "1723834440.0",
              "comment_id": "1267308",
              "content": "With option B, you will get notifications when user accounts turn off the block public access feature but it doesn't stop them from doing it. The question requires that the implementation stops users from being able to carry out that operation successfully altogether.",
              "poster": "flaacko",
              "upvote_count": "2",
              "comments": [
                {
                  "upvote_count": "1",
                  "poster": "Impromptu",
                  "timestamp": "1732108200.0",
                  "comments": [
                    {
                      "content": "To bad I can't edit, so to correct myself: PutBucketPublicAccessBlock is indeed the IAM permission and what you should filter on.\nAnd the cloudformation solution in option B also lacks the safeguard to prevent users from disabling the eventbridge rule (and therefore disabling the notification)",
                      "poster": "Impromptu",
                      "upvote_count": "1",
                      "timestamp": "1732108500.0",
                      "comment_id": "1315254"
                    }
                  ],
                  "comment_id": "1315247",
                  "content": "Just to go more into detail, as the answer C seems correct indeed. But I'd like to point out some extra details on why B is wrong.\nThe questions asks that a user cannot turn off the notification. They should be able to turn off the block public access feature. So B is not wrong because it doesn't implement the latter.\nB is wrong because it's PutPublicAccessBlock (does not contain \"Bucket\" in it). And additionally, you should add a condition to the eventbridge rule that checks the content of the action: that BlockPublicPolicy is set to False. Without the condition you will get notification on all PutPublicAccessBlock events, so also those that are considered to be valid."
                }
              ]
            }
          ],
          "poster": "rlf"
        },
        {
          "poster": "Srikantha",
          "upvote_count": "2",
          "timestamp": "1744506360.0",
          "content": "Selected Answer: C\nGoal: Detect and notify when Block Public Access is turned off on any S3 bucket across all AWS accounts in an organization.\nAWS Config tracks resource configuration changes, like changes to S3 bucket access settings.\nThe s3-bucket-level-public-access-prohibited managed rule evaluates whether S3 buckets have Block Public Access settings enabled.\nConformance packs allow deployment of these rules organization-wide in a managed way.\nA Systems Manager automation document can be triggered on noncompliance to publish to an SNS topic for notifications.\nCentral management via a delegated administrator ensures member accounts can't disable the rule or notification.",
          "comment_id": "1560207"
        },
        {
          "poster": "Gomer",
          "timestamp": "1725570780.0",
          "upvote_count": "1",
          "content": "Selected Answer: A\nGuardDuty Policy\nPolicy:S3/BucketBlockPublicAccessDisabled\n\"An IAM entity invoked an API used to disable S3 Block Public Access on a bucket.\"\n\"Data source: CloudTrail management events\"\n\"This finding informs you that Block Public Access was disabled for the listed S3 bucket. When enabled, S3 Block Public Access settings are used to filter the policies or access control lists (ACLs) applied to buckets as a security measure to prevent inadvertent public exposure of data.\"\nhttps://docs.aws.amazon.com/guardduty/latest/ug/guardduty_finding-types-s3.html#policy-s3-bucketblockpublicaccessdisabled",
          "comment_id": "1279181"
        },
        {
          "timestamp": "1721896740.0",
          "upvote_count": "1",
          "content": "Selected Answer: C\nC\n\n\"A conformance pack is a collection of AWS Config rules and remediation actions that can be easily deployed as a single entity in an account and a Region or across an organization in AWS Organizations.\"\nhttps://docs.aws.amazon.com/config/latest/developerguide/conformance-packs.html\nhttps://docs.aws.amazon.com/config/latest/developerguide/WhatIsConfig.html",
          "comment_id": "1254866",
          "poster": "jamesf"
        },
        {
          "comment_id": "1239956",
          "upvote_count": "1",
          "poster": "aefuen1",
          "timestamp": "1719806820.0",
          "content": "Selected Answer: A\nIt's A. GuardDuty echieves this with no effort."
        },
        {
          "upvote_count": "2",
          "comment_id": "1239045",
          "content": "Selected Answer: A\nA DevOps engineer must implement this change without affecting the operation of any AWS accounts.",
          "poster": "xdkonorek2",
          "timestamp": "1719638160.0"
        },
        {
          "poster": "Gomer",
          "upvote_count": "1",
          "timestamp": "1717093860.0",
          "comments": [
            {
              "comment_id": "1279180",
              "upvote_count": "1",
              "poster": "Gomer",
              "content": "GuardDuty Policy\nPolicy:S3/BucketBlockPublicAccessDisabled\n\"An IAM entity invoked an API used to disable S3 Block Public Access on a bucket.\"\n\"Data source: CloudTrail management events\"\n\"This finding informs you that Block Public Access was disabled for the listed S3 bucket. When enabled, S3 Block Public Access settings are used to filter the policies or access control lists (ACLs) applied to buckets as a security measure to prevent inadvertent public exposure of data.\" \nhttps://docs.aws.amazon.com/guardduty/latest/ug/guardduty_finding-types-s3.html#policy-s3-bucketblockpublicaccessdisabled",
              "timestamp": "1725570720.0"
            }
          ],
          "content": "I was sure the answer was \"C\" until I started reading through some of the requirements and comments. The words \"implementation must ensure that individual member accounts in the organization cannot turn off the notification\" incline me to lean towards \"A\", because with \"C\", someone with admin privileges on a single account could turn off the notification in that account. As pointed out by others, there are a number of GuardDuty findings associates with S3 public access. Having GuardDuty and EventBridge pattern trigger SNS for some key words such as \"s3\" and \"Public\" seems to make sense in enforcing this across an organization. I don't have enough experience with GuardDuty in an Organization to be 100% confident, but the emphasis on SNS requirement makes me think this could be a trick question.",
          "comment_id": "1221764"
        },
        {
          "content": "Selected Answer: C\nC is only correct option.",
          "comment_id": "1205163",
          "upvote_count": "1",
          "timestamp": "1714579140.0",
          "poster": "seetpt"
        },
        {
          "comment_id": "1199655",
          "content": "Technically A would be sufficient here.\n\nThe question is only asking to be NOTIFIED when block public access gets disabled.\n\nSee the following GuardDuty finding: https://docs.aws.amazon.com/guardduty/latest/ug/guardduty_finding-types-s3.html#policy-s3-bucketblockpublicaccessdisabled\n\nManaging multiple GuardDuty accounts is simplified using the AWS Organizations delegated administrator feature. With this feature, the AWS Organizations management account can designate a member account to be the GuardDuty administrator for the entire organization. The delegated GuardDuty administrator is then granted permission to enable and manage GuardDuty for all existing and future accounts in the organization.",
          "poster": "that1guy",
          "upvote_count": "3",
          "timestamp": "1713703980.0"
        },
        {
          "timestamp": "1710327060.0",
          "poster": "Cervus18",
          "content": "Selected Answer: A\nWe can leverage AWS Organizations to enable Guarduty in all accounts. \nThere is an S3 finding called Policy:S3/AccountBlockPublicAccessDisabled\nThen we setup a single EventBrdige rule in the delegated account that publish the event to the SNS topic in the same account.\n\nThis is the easisest solution to be implemented and monitoring the public access seamlessly across all Organization's accounts\n\nThis is a common multi-account strategy for GuardDuty with AWS organizations, to collect such finding from hundred of accounts",
          "upvote_count": "4",
          "comment_id": "1172455"
        },
        {
          "content": "Selected Answer: C\nAmazon GuardDuty is primarily on threat detection and response, not configuration monitoring. A conformance pack is a collection of AWS Config rules and remediation actions that can be easily deployed as a single entity in an account and a Region or across an organization in AWS Organizations. https://docs.aws.amazon.com/config/latest/developerguide/conformance-\npacks.htmlhttps://docs.aws.",
          "upvote_count": "4",
          "poster": "4555894",
          "timestamp": "1709903460.0",
          "comment_id": "1168808"
        },
        {
          "comment_id": "1166665",
          "poster": "zijo",
          "timestamp": "1709662620.0",
          "content": "Answer is C\nA conformance pack is a collection of AWS Config rules and remediation actions that can be easily deployed as a single entity in an account and a Region or across an organization in AWS Organizations. You can also use AWS Systems Manager documents (SSM documents) to store your conformance pack templates on AWS and directly deploy conformance packs using SSM document names.",
          "upvote_count": "3"
        },
        {
          "upvote_count": "1",
          "comment_id": "1155947",
          "timestamp": "1708559940.0",
          "poster": "Rocky007",
          "content": "Hi can somebody with contributors access, would please forward all the questions pdf to me on telegram @rater250 , I'm willing to pay"
        },
        {
          "comment_id": "1135030",
          "timestamp": "1706537400.0",
          "comments": [
            {
              "timestamp": "1706799180.0",
              "upvote_count": "2",
              "content": "Let me clarify: B cannot be correct because of this reason: \"Deploy the stack to every account in the organization by using CloudFormation StackSets\" means in every accounts of this AWS org (canbe up to hundreds of account), we will deploy a SNS topic and an EventBridge rule. This would be an extremely expensive deployment",
              "poster": "thanhnv142",
              "comment_id": "1137708"
            }
          ],
          "content": "C is correct: AWS config can only be modify by admin, not member accounts",
          "upvote_count": "1",
          "poster": "thanhnv142"
        },
        {
          "comment_id": "1133274",
          "content": "Option B is also not a valid case because we can direct use config with eventbrige why to go for clod trail we can use aws config rule s3-bucket-public-read-prohibited if rule changes eventbridge will trigger sns",
          "timestamp": "1706354160.0",
          "poster": "hotblooded",
          "upvote_count": "2"
        },
        {
          "upvote_count": "1",
          "content": "I got confused with option B and C , but Lets think in C option when I will use system manager to trigger SNS I can simply use eventbridge run that checks for config rule compliance change , IF compliance changes then as a target we will specify SNS.\nYes , We can also specify system manager automation document to trigger sns but why I will use it I will directly use SNS.\n\nSo from above I still by looking words B is correct option. Main reason is you do not need system manager here to trigger SNS.\n\nPlus there is no mention for eventbridge rule that will trigger system manager , from config we cannot directly trigger it.",
          "poster": "hotblooded",
          "comment_id": "1133267",
          "timestamp": "1706353920.0"
        },
        {
          "upvote_count": "1",
          "content": "I got confused with option B and C , but Lets think in C option when I will use system manager to trigger SNS I can simply use eventbridge run that checks for config rule compliance change , IF compliance changes then as a target we will specify SNS.\nYes , We can also specify system manager automation document to trigger sns but why I will use it I will directly use SNS.\n\nSo from above I still by looking words B is correct option. Main reason is you do not need system manager here to trigger SNS.",
          "poster": "hotblooded",
          "comment_id": "1133265",
          "timestamp": "1706353800.0"
        },
        {
          "content": "Selected Answer: C\nThis is the type of thing that AWS Config is used for.",
          "timestamp": "1704023340.0",
          "comment_id": "1110500",
          "poster": "Jay_2pt0_1",
          "upvote_count": "2"
        },
        {
          "upvote_count": "3",
          "poster": "csG13",
          "content": "Selected Answer: C\nIt’s not B because other users can turn it off. With AWS config in organisations only the admin in the root can do it.",
          "comment_id": "1106440",
          "timestamp": "1703632200.0"
        },
        {
          "content": "Selected Answer: C\nAnswer is C:\n\nWith B you can deploy fix the problem, but it is installed in every account, so a user with admin rights in that account can delete/modify the configuration",
          "timestamp": "1703165940.0",
          "poster": "zolthar_z",
          "comment_id": "1102563",
          "upvote_count": "2"
        },
        {
          "content": "Selected Answer: C\nC is right using Config Rules and Conformational Packs with SSM .",
          "poster": "z_inderjot",
          "timestamp": "1703134200.0",
          "upvote_count": "3",
          "comment_id": "1102187"
        },
        {
          "upvote_count": "1",
          "poster": "zolthar_z",
          "timestamp": "1700418000.0",
          "comment_id": "1074829",
          "content": "Selected Answer: B\nI don't have a technical reason but others dumps shows B as the Answer"
        },
        {
          "comment_id": "1059246",
          "upvote_count": "2",
          "poster": "DZ_Ben",
          "comments": [
            {
              "poster": "Jaguaroooo",
              "upvote_count": "1",
              "comment_id": "1112693",
              "timestamp": "1704279960.0",
              "content": "it cannot be A, this has nothing to do with remediating the issue. I think it is C, aws config can be used to monitor changes at the Org level and corrective actions can be taken using lambda. C is correct for sure"
            },
            {
              "timestamp": "1702198020.0",
              "content": "This solution does not address the requirement of notifying the SecOps team if a bucket's Block Public Access feature is turned off. GuardDuty focuses on detecting security threats and vulnerabilities, not specifically monitoring S3 bucket configurations.",
              "comment_id": "1092357",
              "upvote_count": "1",
              "poster": "harithzainudin"
            }
          ],
          "timestamp": "1698794460.0",
          "content": "Should be A. The finding type should be \"Policy:S3/AccountBlockPublicAccessDisabled\". C is incorrect because to my knowledge you will also need to at least create an EventBridge rule and Lambda to make this work."
        },
        {
          "upvote_count": "1",
          "content": "C SHOULD BE FINE",
          "poster": "examok",
          "comment_id": "1014191",
          "timestamp": "1695391860.0"
        },
        {
          "upvote_count": "1",
          "timestamp": "1695034380.0",
          "content": "Selected Answer: C\nA will send email to secops team for all Gaurdduty findigs. We want email for S3 Public access enabled event only",
          "comment_id": "1010488",
          "poster": "RVivek"
        },
        {
          "comments": [
            {
              "comment_id": "1009181",
              "poster": "daburahjail",
              "upvote_count": "1",
              "timestamp": "1694879880.0",
              "content": "And I say this because they are more focused on monitoring and alerting, and not PREVENTION"
            },
            {
              "upvote_count": "1",
              "comment_id": "1071395",
              "content": "Yup agree with you. SCP is missing from all answers.",
              "poster": "robertohyena",
              "timestamp": "1700049300.0"
            }
          ],
          "comment_id": "1009180",
          "upvote_count": "2",
          "timestamp": "1694879820.0",
          "content": "Selected Answer: C\nAll answers fail the requirement \"The implementation must ensure that individual member accounts in the organization cannot turn off the notification.\". The correct approach here would be to use SCP to prevent users from changing this option.\nSince we have to chose one, C is the best alternative.",
          "poster": "daburahjail"
        },
        {
          "timestamp": "1691914260.0",
          "content": "C\nAWS Config allows you to manage AWS Config rules across all AWS accounts within an organization. You can:\nCentrally create, update, and delete AWS Config rules across all accounts in your organization.\nDeploy a common set of AWS Config rules across all accounts and specify accounts where AWS Config rules should not be created.\nUse the APIs from the management account in AWS Organizations to enforce governance by ensuring that the underlying AWS Config rules are not modifiable by your organization’s member accounts.",
          "comment_id": "979857",
          "poster": "ixdb",
          "upvote_count": "3"
        },
        {
          "upvote_count": "2",
          "poster": "vherman",
          "timestamp": "1690879980.0",
          "content": "Selected Answer: A\nA - 100%\nC - you can deploy Config Rules centrally via Config Service. Aggregation only works that way.",
          "comment_id": "968898"
        },
        {
          "upvote_count": "3",
          "content": "Selected Answer: C\nPlease the questions.\nA. Nope I don't want all GD findings.\nB. No actual protection - nope.\nD. Nope.\nC. Only viable option.",
          "poster": "lunt",
          "comment_id": "965788",
          "timestamp": "1690574160.0"
        },
        {
          "timestamp": "1687973940.0",
          "content": "Selected Answer: C\ncompliance and governance",
          "comment_id": "936967",
          "poster": "pepecastr0",
          "upvote_count": "3"
        },
        {
          "upvote_count": "3",
          "content": "Selected Answer: C\nAnswer is C.",
          "timestamp": "1687686540.0",
          "poster": "FunkyFresco",
          "comment_id": "933435"
        },
        {
          "comments": [
            {
              "timestamp": "1692607620.0",
              "content": "Agree, B is correct.",
              "upvote_count": "1",
              "poster": "fanq10",
              "comment_id": "986333"
            }
          ],
          "timestamp": "1687207260.0",
          "poster": "tartarus23",
          "content": "Selected Answer: B\nAnswer is B.\nThis solution uses Amazon CloudTrail, a service that enables governance, compliance, operational auditing, and risk auditing of your AWS account. CloudTrail allows you to log, continuously monitor, and retain account activity related to actions across your AWS infrastructure, including actions taken in Amazon S3.\n\nAWS CloudFormation StackSets extends the functionality of stacks by enabling you to create, update, or delete stacks across multiple accounts and regions with a single operation. The CloudFormation template will create the necessary SNS topic and subscription, and the EventBridge rule to monitor for the s3:PutBucketPublicAccessBlock CloudTrail event. This event is generated when the Block Public Access feature is modified for an S3 bucket.",
          "upvote_count": "3",
          "comment_id": "927909"
        },
        {
          "comments": [
            {
              "content": "We can use aws systems manager automation in mutiple aws regions and accounts with a delegated administrator account ( or management account) \nhttps://docs.aws.amazon.com/systems-manager-automation-runbooks/latest/userguide/automation-aws-publishsnsnotification.html",
              "upvote_count": "1",
              "timestamp": "1698502560.0",
              "comment_id": "1056248",
              "poster": "rlf"
            }
          ],
          "poster": "allen_devops",
          "upvote_count": "1",
          "content": "Both A and C support monitoring the change of bucket block public access setting but I didn't see how to use system manager to push event to sns. There isn't event thing mentioned in the option c and how it integrates with system manager, sns and aws config. So I go for option A",
          "comment_id": "925878",
          "timestamp": "1686993660.0"
        },
        {
          "upvote_count": "3",
          "content": "Selected Answer: C\nABC may be technically feasible but Config is the AWS recommended solution for configuration changes tracking and alerting.",
          "poster": "madperro",
          "comment_id": "924155",
          "timestamp": "1686830700.0"
        },
        {
          "upvote_count": "2",
          "timestamp": "1685535360.0",
          "poster": "rdoty",
          "content": "Selected Answer: C\nconfig always for these, guard duty is not suitable for tracking configuration changes over time. it is meant for threat detection",
          "comment_id": "911226"
        },
        {
          "upvote_count": "1",
          "timestamp": "1685461560.0",
          "content": "One problem with B is that the SNS topic must be in the same region as the EventBridge rule. So you need as many SNS topics as regions. It does not seems feasible.\n\nOn the other hand centrailzed AWS Config is a much better solution IMHO",
          "comment_id": "910393",
          "poster": "bcx"
        },
        {
          "poster": "qan1257",
          "content": "A and D are incorrect.\nAWS GuardDuty focuses on threat detection and monitoring for malicious activity within your AWS environment. It analyzes various data sources such as VPC flow logs, CloudTrail logs, and DNS logs, but it does not specifically examine S3 bucket policies.\n\nAWS Inspector is a security assessment service that helps in assessing the security and compliance of your EC2 instances. It performs security assessments against common vulnerabilities and provides recommendations for remediation. \n\nC is incorrect, because the aggregator should be setup, but not to create rule in each account. https://docs.aws.amazon.com/organizations/latest/userguide/services-that-can-integrate-config.html\n\nB is correct. The unmanaged changes can be detected by dirft detection if the rule was deleted by individual account.\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/what-is-cfnstacksets.html",
          "comment_id": "907211",
          "timestamp": "1685090040.0",
          "upvote_count": "1"
        },
        {
          "comment_id": "905486",
          "content": "Selected Answer: C\nIt's C. Follow the link of Zoe",
          "poster": "Akaza",
          "upvote_count": "2",
          "timestamp": "1684904640.0"
        },
        {
          "content": "Selected Answer: A\nA for sure. To be honest there are many wrong answers in this dump, previous was better. Requirement was disable turnoff in the accounts, using config you could delete rules, but with guarduty you couldn’t do this",
          "poster": "Dimidrol",
          "comment_id": "896120",
          "timestamp": "1683914400.0",
          "upvote_count": "2",
          "comments": [
            {
              "poster": "ipsingh",
              "timestamp": "1683995280.0",
              "comments": [
                {
                  "timestamp": "1690879860.0",
                  "upvote_count": "1",
                  "comment_id": "968896",
                  "content": "You can't deploy a config rule in multiple accounts via Config Service. Config aggregator does not work that way. A is correct",
                  "poster": "vherman"
                }
              ],
              "content": "When Config is centrally managed/delegated then rules can't be deleted by individual accounts.",
              "comment_id": "896881",
              "upvote_count": "1"
            }
          ]
        },
        {
          "upvote_count": "1",
          "comment_id": "888328",
          "content": "C \nhttps://docs.aws.amazon.com/config/latest/developerguide/s3-account-level-public-access-blocks.html",
          "poster": "Zoe_zoe",
          "timestamp": "1683104880.0"
        },
        {
          "timestamp": "1682749260.0",
          "poster": "ParagSanyashiv",
          "comment_id": "884079",
          "upvote_count": "4",
          "content": "Selected Answer: C\nC is most nearest answer to the question"
        },
        {
          "upvote_count": "3",
          "timestamp": "1682483820.0",
          "poster": "5aga",
          "comment_id": "881103",
          "content": "Selected Answer: C\nAmazon GuardDuty is primarily on threat detection and response, not configuration monitoring\n\nA conformance pack is a collection of AWS Config rules and remediation actions that can be easily deployed as a single entity in an account and a Region or across an organization in AWS Organizations.\n\nhttps://docs.aws.amazon.com/config/latest/developerguide/conformance-packs.html"
        },
        {
          "poster": "henryyvr",
          "upvote_count": "2",
          "timestamp": "1682296980.0",
          "comment_id": "878895",
          "content": "Selected Answer: C\nC ... aws config"
        },
        {
          "poster": "jqso234",
          "content": "Selected Answer: A\nOption A allows for a centralized management approach unlike B. A leverages Amazon GuardDuty, which is already integrated with Amazon SNS and provides the ability to detect threats in the AWS environment. On the other hand, option B requires using CloudFormation to deploy a custom rule for CloudTrail, which can be more complex to manage and monitor.",
          "timestamp": "1681494600.0",
          "upvote_count": "1",
          "comment_id": "870379"
        },
        {
          "poster": "Dimidrol",
          "upvote_count": "4",
          "timestamp": "1680726240.0",
          "comment_id": "862453",
          "content": "Selected Answer: A\nA for me. We did such solution"
        },
        {
          "content": "Why not C?",
          "comment_id": "861754",
          "timestamp": "1680669000.0",
          "upvote_count": "1",
          "poster": "lqpO_Oqpl"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:16:25.301Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "EStmMoAWVLjsvORtDxnF",
      "question_number": 225,
      "page": 45,
      "question_text": "A DevOps engineer deploys an application to a fleet of Amazon Linux EC2 instances. The DevOps engineer needs to monitor system metrics across the fleet. The DevOps engineer wants to monitor the relationship between network traffic and memory utilization for the application code. The DevOps engineer wants to track the data on a 60 second interval.\n\nWhich solution will meet these requirements?",
      "choices": {
        "A": "Use Amazon CloudWatch basic monitoring to collect the NetworkIn metric and the MemoryBytesUsed metric. Graph the metrics in CloudWatch.",
        "B": "Use Amazon CloudWatch detailed monitoring to collect the NetworkIn metric and the MemoryBytesUsed metric. Graph the metrics in CloudWatch.",
        "D": "Use Amazon CloudWatch basic monitoring to collect the built-in NetworkIn metric. Install the CloudWatch agent on the EC2 instances to collect the mem_used metric. Graph the metrics in CloudWatch.",
        "C": "Use Amazon CloudWatch detailed monitoring to collect the NetworkIn metric. Install the CloudWatch agent on the EC2 instances to collect the mem_used metric. Graph the metrics in CloudWatch."
      },
      "correct_answer": "C",
      "answer_ET": "C",
      "answers_community": [
        "C (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/151517-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-11-17 22:15:00",
      "unix_timestamp": 1731878100,
      "discussion_count": 2,
      "discussion": [
        {
          "content": "Answer: C\nDetailed Monitoring for High Resolution: Detailed monitoring provides metrics at a 1-minute frequency, which meets the requirement of tracking data on a 60-second interval. Basic monitoring only provides 5-minute data.\nNetworkIn Metric: The NetworkIn metric is a standard metric available through detailed monitoring, providing insights into incoming network traffic.\nCloudWatch Agent for Memory Utilization: While MemoryBytesUsed is available with basic monitoring, the CloudWatch agent allows you to collect more specific memory metrics like mem_used, which might be more relevant for analyzing application code memory usage. This provides greater flexibility and granularity in monitoring memory.\nCorrelating Metrics: By collecting both NetworkIn and mem_used in CloudWatch, you can graph and analyze them together to understand the relationship between network traffic and memory utilization for your application.",
          "timestamp": "1731878100.0",
          "comment_id": "1313740",
          "poster": "f4b18ba",
          "upvote_count": "7"
        },
        {
          "content": "Selected Answer: C\nUse Amazon CloudWatch detailed monitoring to collect the NetworkIn metric. Install the CloudWatch agent on the EC2 instances to collect the mem_used metric. Graph the metrics in CloudWatch.",
          "comment_id": "1331729",
          "poster": "CHRIS12722222",
          "upvote_count": "2",
          "timestamp": "1735169940.0"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:16:25.301Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "cY8kd2Ew1zpHskSaWzxQ",
      "question_number": 226,
      "page": 46,
      "question_text": "A company uses AWS Systems Manager to manage a fleet of Amazon Linux EC2 instances that have SSM Agent installed. All EC2 instances are configured to use Instance Metadata Service Version 2 (IMDSv2) and are running in the same AWS account and AWS Region. Company policy requires developers to use only Amazon Linux.\n\nThe company wants to ensure that all new EC2 instances are automatically managed by Systems Manager after creation.\n\nWhich solution will meet these requirements with the MOST operational efficiency?",
      "choices": {
        "A": "Create an IAM role that has a trust policy that allows Systems Manager to assume the role. Attach the AmazonSSMManagedEC2InstanceDefaultPolicy policy to the role. Configure the default-ec2-instance-management-role SSM service setting to use the role.",
        "C": "Configure Systems Manager Patch Manager. Create a patch baseline that automatically installs SSM Agent on all new EC2 instances. Create a patch group for all EC2 instances. Attach the patch baseline to the patch group. Create a maintenance window and maintenance window task to start installing SSM Agent daily.",
        "D": "Create an EC2 instance role that has a trust policy that allows Amazon EC2 to assume the role. Attach the AmazonSSMManagedInstanceCore policy to the role. Ensure that AWS Config is set up. Use the ec2-instance-profile-attached managed AWS Config rule to validate if an EC2 instance has the role attached. Configure the rule to run on EC2 configuration changes. Configure automatic remediation for the rule to run the AWS-SetupManagedRoleOnEc2Instance SSM document to attach the role to the EC2 instance.",
        "B": "Ensure that AWS Config is set up. Create an AWS Config rule that validates if an EC2 instance has SSM Agent installed. Configure the rule to run on EC2 configuration changes. Configure automatic remediation for the rule to run the AWS-InstallSSMAgent SSM document to install SSM Agent."
      },
      "correct_answer": "A",
      "answer_ET": "A",
      "answers_community": [
        "A (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/152237-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-11-28 09:31:00",
      "unix_timestamp": 1732782660,
      "discussion_count": 3,
      "discussion": [
        {
          "timestamp": "1743381180.0",
          "upvote_count": "1",
          "content": "Selected Answer: A\nBy setting the default-ec2-instance-management-role service setting, new EC2 instances will automatically assume the correct role, allowing seamless management by AWS Systems Manager.\n\nThus, Option A is the best choice.",
          "comment_id": "1413859",
          "poster": "Srikantha"
        },
        {
          "comment_id": "1327035",
          "upvote_count": "3",
          "poster": "Ky_24",
          "timestamp": "1734292740.0",
          "content": "Selected Answer: A\n1. Automatic Role Association:\n • AWS Systems Manager supports a default instance management role that is automatically attached to new EC2 instances upon creation.\n • By configuring the default-ec2-instance-management-role SSM service setting, any new EC2 instance will automatically be associated with the specified IAM role.\n 2. IAM Role and Policy:\n • The AmazonSSMManagedEC2InstanceDefaultPolicy provides the necessary permissions for SSM Agent to manage instances, including access to Systems Manager services, Amazon S3, and AWS Config logs.\n 3. Operational Efficiency:\n • This solution ensures new EC2 instances are automatically registered with Systems Manager without requiring additional manual steps or configuration changes.\n • It eliminates the need for AWS Config rules, patch baselines, or remediation documents, simplifying the management process."
        },
        {
          "timestamp": "1732782660.0",
          "upvote_count": "3",
          "poster": "ArunRav",
          "content": "Selected Answer: A\nAmazon Linux has the agent already installed. So A perform the rest of the steps to manage the instances using SSM",
          "comment_id": "1319097"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:16:35.727Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "AqGN9OUJGu6wF9O34VkU",
      "question_number": 227,
      "page": 46,
      "question_text": "A company configured an Amazon S3 event source for an AWS Lambda function. The company needs the Lambda function to run when a new object is created or an existing object is modified in a specific S3 bucket. The Lambda function will use the S3 bucket name and the S3 object key of the incoming event to read the contents of the new or modified S3 object. The Lambda function will parse the contents and save the parsed contents to an Amazon DynamoDB table.\n\nThe Lambda function's execution role has permissions to read from the S3 bucket and to write to the DynamoDB table. During testing, a DevOps engineer discovers that the Lambda function does not run when objects are added to the S3 bucket or when existing objects are modified.\n\nWhich solution will resolve these problems?",
      "choices": {
        "B": "Create a resource policy for the Lambda function to grant Amazon S3 permission to invoke the Lambda function on the S3 bucket.",
        "D": "Configure an Amazon Simple Queue Service (Amazon SQS) queue as the destination for the S3 bucket event notifications. Update the Lambda function's execution role to have permission to read from the SQS queue. Update the Lambda function to consume messages from the SQS queue.",
        "A": "Create an S3 bucket policy for the S3 bucket that grants the S3 bucket permission to invoke the Lambda function.",
        "C": "Configure an Amazon Simple Queue Service (Amazon SQS) queue as an OnFailure destination for the Lambda function. Update the Lambda function to process messages from the SQS queue and the S3 event notifications."
      },
      "correct_answer": "B",
      "answer_ET": "B",
      "answers_community": [
        "B (90%)",
        "10%"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/151700-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-11-20 17:10:00",
      "unix_timestamp": 1732119000,
      "discussion_count": 5,
      "discussion": [
        {
          "content": "Selected Answer: B\nEvent Trigger Issue:\nThe Lambda function is not being triggered when objects are created or modified in the S3 bucket.\nA common reason for this issue is that Amazon S3 does not have the necessary permissions to invoke the Lambda function.\nLambda Function Resource Policy:\nUnlike some AWS services (e.g., SNS, SQS), which can automatically invoke Lambda if they have permissions in their service role, S3 requires an explicit resource-based policy on the Lambda function to grant it invocation permissions.\nThis means S3 needs explicit permission to trigger the Lambda function when events occur.\nWhy Option B Works:\nAdding a resource-based policy to the Lambda function allows S3 to invoke the function when an event (PUT/POST operation) occurs.\nThe policy should include an \"s3.amazonaws.com\" principal and an action of \"lambda:InvokeFunction\", granting S3 the ability to trigger Lambda.",
          "poster": "Srikantha",
          "timestamp": "1743466920.0",
          "comment_id": "1415313",
          "upvote_count": "1"
        },
        {
          "upvote_count": "2",
          "comment_id": "1327043",
          "poster": "Ky_24",
          "content": "Selected Answer: B\n1. S3-to-Lambda Invocation:\n • When you configure Amazon S3 to trigger an AWS Lambda function, S3 must have explicit permission to invoke the function. This is done by attaching a resource-based policy to the Lambda function.\n • Without this policy, even if S3 event notifications are configured, the Lambda function will not be triggered because S3 does not have the necessary permissions to invoke the function.\n 2. Solution Details:\n • A resource policy for the Lambda function specifies that the S3 bucket is allowed to invoke the Lambda function. This is configured by using the AWS CLI, AWS SDKs, or directly in the AWS Management Console.",
          "timestamp": "1734293940.0"
        },
        {
          "upvote_count": "3",
          "poster": "ArunRav",
          "comment_id": "1319104",
          "timestamp": "1732783260.0",
          "content": "Selected Answer: B\nLambda should allow to be invoked by S3 bucket Hence B"
        },
        {
          "upvote_count": "3",
          "comment_id": "1316344",
          "timestamp": "1732285320.0",
          "poster": "Impromptu",
          "content": "Selected Answer: B\nhttps://docs.aws.amazon.com/lambda/latest/dg/access-control-resource-based.html\n\nThe lambda resource policy should allow S3. Or in cloudformation terms: adding the AWS::Lambda::Permission"
        },
        {
          "comment_id": "1315366",
          "upvote_count": "1",
          "comments": [
            {
              "content": "In your link it states \"To invoke your function, Amazon S3 needs permission from the function's resource-based policy.\"\n\nNote \"FUNCTION's resource-based policy\"\n\nSo therefore it is B. We do not have to edit the bucket policy for this.",
              "comment_id": "1316342",
              "upvote_count": "1",
              "timestamp": "1732285140.0",
              "poster": "Impromptu"
            }
          ],
          "content": "Selected Answer: A\nEvent Source Mapping and Permission Requirements:\n\nFor Amazon S3 to invoke a Lambda function, the Lambda function must have a resource-based policy that grants Amazon S3 permission to invoke it.\nWithout this resource-based policy, even if the event source is correctly configured, the Lambda function will not be triggered because S3 is not authorized to invoke it.\nResource Policy for Lambda Function:\n\nA resource policy on the Lambda function explicitly allows Amazon S3 to invoke the function for the specified bucket events (e.g., object creation or modification).\nThis resolves the issue by enabling S3 to trigger the Lambda function\n\nAction - lambda:InvokeFunction allows S3 to invoke the Lambda function.\n\nhttps://docs.aws.amazon.com/lambda/latest/dg/with-s3.html",
          "timestamp": "1732119000.0",
          "poster": "f4b18ba"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:16:35.727Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "eEqURI3IFwLKam4WihJG",
      "question_number": 228,
      "page": 46,
      "question_text": "A company recently configured AWS Control Tower in its organization in AWS Organizations. The company enrolled all existing AWS accounts in AWS Control Tower. The company wants to ensure that all new AWS accounts are automatically enrolled in AWS Control Tower.\n\nThe company has an existing AWS Step Functions workflow that creates new AWS accounts and performs any actions required as part of account creation. The Step Functions workflow is defined in the same AWS account as AWS Control Tower.\n\nWhich combination of steps should the company add to the Step Functions workflow to meet these requirements? (Choose two.)",
      "choices": {
        "D": "Call the AWS Service Catalog ProvisionProduct API operation with the details of the new AWS account.",
        "E": "Call the Organizations EnableAWSServiceAccess API operation with the controltower.amazonaws.com service name and the details of the new AWS account.",
        "A": "Create an Amazon EventBridge event that has an aws.controltower source and a CreateManagedAccount detail-type. Add the details of the new AWS account to the detail field of the event.",
        "B": "Create an Amazon EventBridge event that has an aws.controltower source and a SetupLandingZone detail-type. Add the details of the new AWS account to the detail field of the event.",
        "C": "Create an AWSControlTowerExecution role in the new AWS account. Configure the role to allow the AWS Control Tower administrator account to assume the role."
      },
      "correct_answer": "CD",
      "answer_ET": "CD",
      "answers_community": [
        "CD (78%)",
        "CE (22%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/151518-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-11-17 22:32:00",
      "unix_timestamp": 1731879120,
      "discussion_count": 6,
      "discussion": [
        {
          "comment_id": "1342060",
          "content": "Selected Answer: CE\nAgrees with CE based on the blog that CHRIS1272222 provided",
          "timestamp": "1737104340.0",
          "upvote_count": "1",
          "poster": "teo2157"
        },
        {
          "timestamp": "1735319820.0",
          "content": "Selected Answer: CE\nC - AWS Control Tower requires the AWSControlTowerExecution role to be created in each managed account. This role allows AWS Control Tower to manage the account and enforce governance and compliance rules. When a new account is created, AWS Control Tower will need this role to carry out management tasks.\n\nE - The EnableAWSServiceAccess API operation is used to enable AWS Control Tower service access in AWS Organizations. This action ensures that AWS Control Tower can operate across the organization and manage new accounts that are created within the organization. By enabling service access for Control Tower, new accounts can be automatically enrolled in the governance and management processes of Control Tower.",
          "poster": "youonebe",
          "comment_id": "1332550",
          "upvote_count": "1"
        },
        {
          "upvote_count": "3",
          "poster": "CHRIS12722222",
          "timestamp": "1735312740.0",
          "comment_id": "1332462",
          "content": "Selected Answer: CD\nRead blog\nhttps://aws.amazon.com/blogs/architecture/field-notes-enroll-existing-aws-accounts-into-aws-control-tower/\n\nDownload the python code and you will see it calls the ProvisionProduct API in method provision_sc_product"
        },
        {
          "comment_id": "1327046",
          "content": "Selected Answer: CD\nOption Details:\n\n 1. C. Create an AWSControlTowerExecution role:\n • AWS Control Tower requires an AWSControlTowerExecution role in new accounts.\n • This role allows AWS Control Tower to assume control of the account and apply the necessary guardrails, policies, and configurations.\n • Without this role, AWS Control Tower cannot manage the account.\n 2. D. Call the AWS Service Catalog ProvisionProduct API operation:\n • Account Factory uses AWS Service Catalog to create and enroll new accounts into AWS Control Tower.\n • The ProvisionProduct API operation allows programmatic provisioning of new accounts through Account Factory, ensuring enrollment into Control Tower governance.",
          "upvote_count": "4",
          "timestamp": "1734294240.0",
          "poster": "Ky_24"
        },
        {
          "poster": "f4b18ba",
          "comment_id": "1313744",
          "timestamp": "1731879180.0",
          "upvote_count": "2",
          "content": "Answer: CD (had a typo)"
        },
        {
          "upvote_count": "3",
          "content": "Answer: CE\nWSControlTowerExecution Role (Option C): For AWS Control Tower to manage accounts, each account must have the AWSControlTowerExecution role, which allows the AWS Control Tower administrator account to assume the role and apply required policies and controls. Creating this role in the new account enables Control Tower to perform management operations as needed.\n\nService Catalog ProvisionProduct API (Option D): AWS Control Tower uses AWS Service Catalog products to provision and manage accounts. Calling the ProvisionProduct API operation as part of the Step Functions workflow allows the new account to be enrolled in Control Tower by provisioning it through the appropriate Service Catalog product. This step ensures that the new account is enrolled in the AWS Control Tower landing zone.",
          "comment_id": "1313743",
          "poster": "f4b18ba",
          "timestamp": "1731879120.0"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:16:35.727Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "zdoESwgaKSX9YZyEYfFX",
      "question_number": 229,
      "page": 46,
      "question_text": "A company's web application uses an Application Load Balancer (ALB) to direct traffic to Amazon EC2 instances across three Availability Zones.\n\nThe company has deployed a newer version of the application to one Availability Zone for testing. If a problem is detected with the application, the company wants to direct traffic away from the affected Availability Zone until the deployment has been rolled back. The application must remain available and maintain static stability during the rollback.\n\nWhich solution will meet these requirements with the MOST operational efficiency?",
      "choices": {
        "B": "Disable cross-zone load balancing on the ALB's target group. Manually remove instances in the target group that belong to the affected Availability Zone.",
        "A": "Disable cross-zone load balancing on the ALB's target group. Initiate a zonal shift on the ALB to direct traffic away from the affected Availability Zone.",
        "C": "Configure cross-zone load balancing on the ALB's target group to inherit settings from the ALB. Initiate a zonal shift on the ALB to direct traffic away from the affected Availability Zone.",
        "D": "Configure cross-zone load balancing on the ALB's target group to inherit settings from the ALB. Remove the subnet that is associated with the affected Availability Zone."
      },
      "correct_answer": "A",
      "answer_ET": "A",
      "answers_community": [
        "A (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/151702-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-11-20 17:14:00",
      "unix_timestamp": 1732119240,
      "discussion_count": 4,
      "discussion": [
        {
          "content": "Selected Answer: A\nARC + Route53",
          "timestamp": "1748075940.0",
          "poster": "awspro_winjin",
          "upvote_count": "1",
          "comment_id": "1571838"
        },
        {
          "upvote_count": "1",
          "poster": "Srikantha",
          "timestamp": "1743467820.0",
          "comment_id": "1415318",
          "content": "Selected Answer: A\nWhy Option A?\nZonal Shifts Provide Quick and Controlled Traffic Redirection\nAWS Route 53 Application Recovery Controller allows you to initiate a zonal shift to temporarily remove an entire Availability Zone (AZ) from service.\nThis is operationally efficient as it does not require manually removing instances or modifying target groups.\nDisabling Cross-Zone Load Balancing Ensures Proper Traffic Steering\nIf cross-zone load balancing is enabled, the ALB will continue distributing traffic across all AZs even if one AZ is experiencing issues.\nDisabling it ensures that a zonal shift properly removes traffic from the affected AZ.\nMaintains Availability and Stability\nOther AZs will continue handling traffic without disruption.\nThe rollback process can proceed without affecting users."
        },
        {
          "timestamp": "1735104660.0",
          "upvote_count": "2",
          "poster": "matt200",
          "comment_id": "1331367",
          "content": "Selected Answer: A\nshould be A"
        },
        {
          "timestamp": "1732119240.0",
          "comment_id": "1315369",
          "poster": "f4b18ba",
          "upvote_count": "3",
          "content": "Disable Cross-Zone Load Balancing:\n\nDisabling cross-zone load balancing ensures that each Availability Zone handles traffic only for its own targets. This enables the ability to isolate traffic away from a specific zone when needed.\nInitiate a Zonal Shift:\n\nAWS offers Zonal Shift (through AWS Elastic Disaster Recovery or AWS Global Accelerator) to temporarily shift traffic away from a specific Availability Zone. This is highly operationally efficient because it allows automated rerouting without manual changes to target groups or instance configurations.\nA zonal shift can quickly redirect traffic while maintaining high availability and stability for the application during rollback or testing.\nRef -https://docs.aws.amazon.com/r53recovery/latest/dg/arc-zonal-shift.html"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:16:35.727Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "ntBtd9BrbvmusVVv0iI3",
      "question_number": 230,
      "page": 46,
      "question_text": "A company has several AWS accounts. An Amazon Connect instance runs in each account. The company uses an Amazon EventBridge default event bus in each account for event handling.\n\nA DevOps team needs to receive all the Amazon Connect events in a single DevOps account.\n\nWhich solution meets these requirements?",
      "choices": {
        "B": "Update the resource-based policy of the default event bus in each account to allow the DevOps account to receive events. Configure an EventBridge rule in the DevOps account that matches Amazon Connect events and has a target of the default event bus in the other accounts.",
        "C": "Update the resource-based policy of the default event bus in the DevOps account. Update the policy to allow events to be received from the accounts. Configure an EventBridge rule in each account that matches Amazon Connect events and has a target of the DevOps account's default event bus.",
        "A": "Update the resource-based policy of the default event bus in each account to allow the DevOps account to replay events. Configure an EventBridge rule in the DevOps account that matches Amazon Connect events and has a target of the default event bus in the other accounts.",
        "D": "Update the resource-based policy of the default event bus in the DevOps account. Update the policy to allow events to be replayed by the accounts. Configure an EventBridge rule in each account that matches Amazon Connect events and has a target of the DevOps account's default event bus."
      },
      "correct_answer": "C",
      "answer_ET": "C",
      "answers_community": [
        "C (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/151704-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-11-20 17:21:00",
      "unix_timestamp": 1732119660,
      "discussion_count": 4,
      "discussion": [
        {
          "upvote_count": "1",
          "timestamp": "1743468360.0",
          "content": "Selected Answer: C\nExplanation:\n1. Event Flow Across AWS Accounts\n\nThe goal is to consolidate Amazon Connect events from multiple AWS accounts into a single DevOps account.\nAmazon EventBridge allows cross-account event ingestion using a resource-based policy on the receiving event bus (DevOps account).\n2. Correct Configuration Steps\n\n✅ Step 1: Update the resource-based policy of the default event bus in the DevOps account\n\nThis allows it to receive events from other accounts.\nThe policy must specify the source accounts and the events.amazonaws.com principal.\n✅ Step 2: Create EventBridge rules in each Amazon Connect account\n\nThese rules match Amazon Connect events and forward them to the DevOps account's default event bus.",
          "comment_id": "1415320",
          "poster": "Srikantha"
        },
        {
          "timestamp": "1737109680.0",
          "comment_id": "1342095",
          "poster": "teo2157",
          "upvote_count": "2",
          "content": "Selected Answer: C\nAgree with f4b18ba comments"
        },
        {
          "timestamp": "1734295800.0",
          "upvote_count": "3",
          "poster": "Ky_24",
          "comment_id": "1327053",
          "content": "Selected Answer: C\nExplanation:\n\nTo centralize Amazon Connect events from multiple AWS accounts into a single account’s EventBridge event bus, the following steps are required:\n 1. Update the resource-based policy of the EventBridge event bus in the DevOps account:\n • This policy allows the DevOps account’s event bus to accept events from the other accounts.\n • The policy must specify the sending account IDs in the Principal field and grant permissions for actions like events:PutEvents.\n 2. Create EventBridge rules in each Amazon Connect account:\n • These rules match the specific Amazon Connect events (e.g., contact events, agent status updates) and forward them to the default event bus in the DevOps account."
        },
        {
          "poster": "f4b18ba",
          "comment_id": "1315376",
          "upvote_count": "3",
          "content": "Selected Answer: C\nResource-Based Policy on the DevOps Account's Event Bus:\n\nTo allow cross-account event routing, the DevOps account's EventBridge event bus must have a resource-based policy that grants permissions to other accounts to send events to it.\n\nEventBridge Rule in Each Account:\n\nEach account needs an EventBridge rule that matches the desired Amazon Connect events and sends them to the DevOps account's event bus as the target. This ensures all relevant events are aggregated in the DevOps account.\n\nCross-Account Event Routing:\nEventBridge supports cross-account event routing with a combination of resource-based policies and properly configured rules in the source accounts.\n\nhttps://docs.aws.amazon.com/eventbridge/latest/userguide/eb-cross-account.html\nhttps://docs.aws.amazon.com/eventbridge/latest/userguide/eb-event-bus-permissions.html\nhttps://docs.aws.amazon.com/connect/latest/adminguide/eventbridge.html",
          "timestamp": "1732119660.0"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:16:35.727Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "oPjFthQkSmoDkvBiFn8m",
      "question_number": 231,
      "page": 47,
      "question_text": "A company has deployed an Amazon Elastic Kubernetes Service (Amazon EKS) cluster with Amazon EC2 node groups. The company's DevOps team uses the Kubernetes Horizontal Pod Autoscaler and recently installed a supported EKS cluster Autoscaler.\n\nThe DevOps team needs to implement a solution to collect metrics and logs of the EKS cluster to establish a baseline for performance. The DevOps team will create an initial set of thresholds for specific metrics and will update the thresholds over time as the cluster is used. The DevOps team must receive an Amazon Simple Notification Service (Amazon SNS) email notification if the initial set of thresholds is exceeded or if the EKS cluster Autoscaler is not functioning properly.\n\nThe solution must collect cluster, node, and pod metrics. The solution also must capture logs in Amazon CloudWatch.\n\nWhich combination of steps should the DevOps team take to meet these requirements? (Choose three.)",
      "choices": {
        "C": "Create CloudWatch alarms to monitor the CPU, memory, and node failure metrics of the cluster. Configure the alarms to send an SNS email notification to the DevOps team if thresholds are exceeded.",
        "E": "Create a CloudWatch alarm to monitor the logs of the Autoscaler deployments for errors. Configure the alarm to send an SNS email notification to the DevOps team if thresholds are exceeded.",
        "F": "Create a CloudWatch alarm to monitor a metric log filter of the Autoscaler deployments for errors. Configure the alarm to send an SNS email notification to the DevOps team if thresholds are exceeded.",
        "A": "Deploy the CloudWatch agent and Fluent Bit to the cluster. Ensure that the EKS cluster has appropriate permissions to send metrics and logs to CloudWatch.",
        "D": "Create a CloudWatch composite alarm to monitor a metric log filter of the CPU, memory, and node metrics of the cluster. Configure the alarm to send an SNS email notification to the DevOps team when anomalies are detected.",
        "B": "Deploy AWS Distro for OpenTelemetry to the cluster. Ensure that the EKS cluster has appropriate permissions to send metrics and logs to CloudWatch."
      },
      "correct_answer": "ACF",
      "answer_ET": "ACF",
      "answers_community": [
        "ACF (73%)",
        "9%",
        "Other"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/152245-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-11-28 10:01:00",
      "unix_timestamp": 1732784460,
      "discussion_count": 6,
      "discussion": [
        {
          "content": "Selected Answer: ADF\nYou need the logs, then you go A instead of B. \nAnd if you are going A, you cannot go C, you have to go D, then of course F",
          "timestamp": "1747818060.0",
          "poster": "robotgeek",
          "upvote_count": "1",
          "comment_id": "1570912"
        },
        {
          "upvote_count": "1",
          "content": "Selected Answer: ACF\nExplanation\nTo collect logs and metrics from the EKS cluster, nodes, and pods, and to ensure notifications are sent when thresholds are exceeded, we need:\n\nA mechanism to collect logs and metrics (Option A).\nAlarms for key cluster performance metrics (Option C).\nAlarms to detect Autoscaler failures (Option F).",
          "timestamp": "1743547740.0",
          "poster": "Srikantha",
          "comment_id": "1418775"
        },
        {
          "upvote_count": "1",
          "content": "Selected Answer: BCE\nCorrect:\n\n B. Deploy AWS Distro for OpenTelemetry (ADOT):\nADOT is the recommended solution for collecting metrics and logs from EKS clusters\n\n\n E. Create CloudWatch alarm for Autoscaler logs:\n Monitors Autoscaler functionality through log analysis\n\nWrong:\nA. Deploy CloudWatch agent and Fluent Bit:\n While this would work, it's not the recommended approach for EKS\nF. Create CloudWatch alarm with metric log filter for Autoscaler:\n Direct log monitoring (Option E) is more appropriate",
          "comment_id": "1333991",
          "timestamp": "1735548000.0",
          "comments": [
            {
              "comment_id": "1333999",
              "content": "change my mind to ACF",
              "upvote_count": "2",
              "poster": "matt200",
              "timestamp": "1735548600.0"
            }
          ],
          "poster": "matt200"
        },
        {
          "timestamp": "1733837760.0",
          "poster": "luisfsm_111",
          "content": "Selected Answer: ACF\nA collects metrics and logs, C create the alarms and F monitors the auto scaler",
          "upvote_count": "3",
          "comment_id": "1324597"
        },
        {
          "comment_id": "1321788",
          "upvote_count": "1",
          "content": "Selected Answer: ACE\nThe question asks to \"collect metrics and logs\".\nYou need to install the CloudWatch Agent which is A.\nYou need to collect metrics which is C.\nYou need to collect logs which is E.",
          "poster": "tinyshare",
          "timestamp": "1733303640.0"
        },
        {
          "comment_id": "1319126",
          "timestamp": "1732784460.0",
          "content": "Selected Answer: ACF\nA- To collect the cloudwatch logs and send to cloudwatch service.\nC - To setup Alarms\nF - To monitor and alert",
          "poster": "ArunRav",
          "upvote_count": "4"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:16:46.185Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "LpbQFAnP9a0BqTkJHeTH",
      "question_number": 232,
      "page": 47,
      "question_text": "A company discovers that its production environment and disaster recovery (DR) environment are deployed to the same AWS Region. All the production applications run on Amazon EC2 instances and are deployed by AWS CloudFormation. The applications use an Amazon FSx for NetApp ONTAP volume for application storage. No application data resides on the EC2 instances.\n\nA DevOps engineer copies the required AMIs to a new DR Region. The DevOps engineer also updates the CloudFormation code to accept a Region as a parameter. The storage needs to have an RPO of 10 minutes in the DR Region.\n\nWhich solution will meet these requirements?",
      "choices": {
        "B": "Use AWS Backup to create a backup vault and a custom backup plan that has a 10-minute frequency. Specify the DR Region as the target Region. Assign the EC2 instances in the production Region to the backup plan.",
        "A": "Create an Amazon S3 bucket in both Regions. Configure S3 Cross-Region Replication (CRR) for the S3 buckets. Create a scheduled AWS Lambda function to copy any new content from the FSx for ONTAP volume to the S3 bucket in the production Region.",
        "C": "Create an AWS Lambda function to create snapshots of the instance store volumes that are attached to the EC2 instances. Configure the Lambda function to copy the snapshots to the DR Region and to remove the previous copies. Create an Amazon EventBridge scheduled rule that invokes the Lambda function every 10 minutes.",
        "D": "Create an FSx for ONTAP instance in the DR Region. Configure a 5-minute schedule for a volume-level NetApp SnapMirror to replicate the volume from the production Region to the DR Region."
      },
      "correct_answer": "D",
      "answer_ET": "D",
      "answers_community": [
        "D (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/151709-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-11-20 17:54:00",
      "unix_timestamp": 1732121640,
      "discussion_count": 3,
      "discussion": [
        {
          "poster": "Srikantha",
          "upvote_count": "1",
          "timestamp": "1743548640.0",
          "comment_id": "1418788",
          "content": "Selected Answer: D\nOption D: Use FSx for ONTAP SnapMirror with a 5-minute replication schedule.\nThis ensures continuous, low-latency replication and meets the 10-minute RPO requirement efficiently."
        },
        {
          "upvote_count": "2",
          "poster": "matt200",
          "timestamp": "1735104720.0",
          "comment_id": "1331368",
          "content": "Selected Answer: D\nshould be D"
        },
        {
          "poster": "f4b18ba",
          "timestamp": "1732121640.0",
          "comment_id": "1315406",
          "upvote_count": "2",
          "content": "Implementation Steps :\nCreate an FSx for ONTAP Instance in the DR Region:\n\nDeploy an FSx for NetApp ONTAP instance in the new DR Region.\nSet Up SnapMirror Replication:\n\nConfigure SnapMirror from the FSx volume in the production Region to the FSx instance in the DR Region.\nSet the replication schedule to 5 minutes to meet the 10-minute RPO requirement.\nTest the DR Setup:\n\nVerify that the replicated volume in the DR Region is consistent and accessible.\nEnsure that the DR environment can failover to the replicated FSx volume.\nUpdate CloudFormation:\n\nEnsure the updated CloudFormation templates can deploy EC2 instances in the DR Region and mount the replicated FSx volume."
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:16:46.185Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "VJrf73jxurpEwoRtUFSN",
      "question_number": 233,
      "page": 47,
      "question_text": "During a security audit, a company discovered that some security groups allow SSH traffic from 0.0.0.0/0. A security team must implement a solution to detect and remediate this issue as soon as possible. The company uses one organization in AWS Organizations to manage all the company's AWS accounts.\n\nWhich solution will meet these requirements?",
      "choices": {
        "A": "Enable AWS Config for all AWS accounts. Use a periodic trigger to activate the vpe-sg-port-restriction-check AWS Config rule. Create an AWS Lambda function to remediate any noncompliant rules.",
        "D": "Create an AWS Systems Manager Automation document in each account to inspect all security groups and to delete noncompliant rules. Use an Amazon EventBridge rule to run the Automation document every hour.",
        "B": "Create an AWS Lambda function in each AWS account to delete all the security group rules. Create an Amazon EventBridge rule to match security group update events or creation events. Set the Lambda function in each account as a target for the rule.",
        "C": "Enable AWS Config for all AWS accounts. Create a custom AWS Config rule to run on the restricted-ssh configuration change trigger. Configure the rule to invoke an AWS Lambda function to remediate any noncompliant resources."
      },
      "correct_answer": "C",
      "answer_ET": "C",
      "answers_community": [
        "C (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/151655-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-11-20 02:47:00",
      "unix_timestamp": 1732067220,
      "discussion_count": 3,
      "discussion": [
        {
          "poster": "phu0298",
          "upvote_count": "5",
          "timestamp": "1732067220.0",
          "comment_id": "1315004",
          "content": "C\nwhy not A:\n\nThe vpe-sg-port-restriction-check AWS Config rule is not specific to this use case.\nThe periodic trigger does not provide real-time detection, potentially delaying remediation."
        },
        {
          "poster": "Srikantha",
          "comment_id": "1418792",
          "content": "Selected Answer: C\nOption C: AWS Config + real-time rule evaluation + Lambda remediation.\nThis provides real-time security enforcement across all accounts.",
          "upvote_count": "1",
          "timestamp": "1743549240.0"
        },
        {
          "upvote_count": "4",
          "comment_id": "1326843",
          "timestamp": "1734264960.0",
          "content": "Selected Answer: C\nThis option involves enabling AWS Config across all accounts, deploying the restricted-ssh rule, and setting up automatic remediation to address non-compliant security groups, thereby meeting the requirements efficiently.",
          "poster": "Slays"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:16:46.185Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "T2h8PSZ8yFwVMBohUhRe",
      "question_number": 234,
      "page": 47,
      "question_text": "A company's DevOps engineer must install a software package on 30 on-premises VMs and 15 Amazon EC2 instances.\n\nThe DevOps engineer needs to ensure that all VMs receive the package in a process that is auditable and that any configuration drift on the VMs is automatically identified and alerted on. The company uses AWS Direct Connect to connect its on-premises data center to AWS.\n\nWhich solution will meet these requirements with the MOST operational efficiency?",
      "choices": {
        "D": "Log in to each VM. Use a local package manager to install the package. Use AWS Config to monitor the AWS resources for configuration changes. Write a script to monitor the on-premises resources.",
        "C": "Write a script that checks if the package is installed across the environment. Configure the script to create a list of all VMs that are noncompliant. Configure the script to send the list to the system administrator, who will install the package on the noncompliant VMs.",
        "A": "Write a script that iterates through the list of VMs once a week. Configure the script to check for the package and install the package if the package is not found. Configure the script to send an email message notification to the system administrator if the package is not found.",
        "B": "Install the AWS Systems Manager Agent (SSM Agent) on all VMs. Use the SSM Agent to install the package. Use AWS Config to monitor for configuration drift. Use Amazon Simple Notification Service (Amazon SNS) to notify the system administrator if any drift is found."
      },
      "correct_answer": "B",
      "answer_ET": "B",
      "answers_community": [
        "B (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/152430-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-12-01 15:05:00",
      "unix_timestamp": 1733061900,
      "discussion_count": 2,
      "discussion": [
        {
          "poster": "Srikantha",
          "content": "Selected Answer: B\nption B: Install the AWS Systems Manager Agent (SSM Agent) on all VMs. Use the SSM Agent to install the package. Use AWS Config to monitor for configuration drift. Use Amazon SNS to notify the system administrator if any drift is found.",
          "comment_id": "1418795",
          "upvote_count": "1",
          "timestamp": "1743549900.0"
        },
        {
          "comment_id": "1320583",
          "content": "Selected Answer: B\nCoorect Ans B",
          "upvote_count": "3",
          "poster": "gunjan229",
          "timestamp": "1733061900.0"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:16:46.185Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "UNFkRuClqm6S8wu5Qn3z",
      "question_number": 235,
      "page": 47,
      "question_text": "A company has migrated its container-based applications to Amazon EKS and want to establish automated email notifications. The notifications sent to each email address are for specific activities related to EKS components. The solution will include Amazon SNS topics and an AWS Lambda function to evaluate incoming log events and publish messages to the correct SNS topic.\nWhich logging solution will support these requirements?",
      "choices": {
        "C": "Enable Amazon S3 logging for the EKS components. Configure an Amazon CloudWatch subscription filter for each component with Lambda as the subscription feed destination.",
        "D": "Enable Amazon S3 logging for the EKS components. Configure S3 PUT Object event notifications with AWS Lambda as the destination.",
        "B": "Enable Amazon CloudWatch Logs to log the EKS components. Create CloudWatch Logs Insights queries linked to Amazon EventBridge events that invoke Lambda.",
        "A": "Enable Amazon CloudWatch Logs to log the EKS components. Create a CloudWatch subscription filter for each component with Lambda as the subscription feed destination."
      },
      "correct_answer": "A",
      "answer_ET": "A",
      "answers_community": [
        "A (97%)",
        "3%"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/105336-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-04-05 22:25:00",
      "unix_timestamp": 1680726300,
      "discussion_count": 14,
      "discussion": [
        {
          "upvote_count": "16",
          "timestamp": "1703025780.0",
          "comment_id": "927913",
          "poster": "tartarus23",
          "content": "Selected Answer: A\nCorrect Answer is A. \nExplanation:\nAmazon EKS integrates with CloudWatch Logs to provide detailed logs of the state and execution of the services in the cluster. CloudWatch subscription filters can be used to route specific log events from a CloudWatch Logs group to a Lambda function. The Lambda function can then process the events and publish notifications to the appropriate Amazon SNS topic."
        },
        {
          "upvote_count": "3",
          "poster": "4555894",
          "comment_id": "1168810",
          "timestamp": "1725793980.0",
          "content": "Selected Answer: A\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/SubscriptionFilters.html#LambdaFunctionExample\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/SubscriptionFilters.html"
        },
        {
          "poster": "zijo",
          "comment_id": "1166684",
          "upvote_count": "1",
          "content": "AWS EKS itself does not offer native S3 logging for container logs. CloudWatch Logs Insights queries cannot directly link to Amazon EventBridge events. So the answer here is A.",
          "timestamp": "1725554940.0"
        },
        {
          "poster": "dzn",
          "upvote_count": "1",
          "content": "Selected Answer: A\nCloudWatch Logs subscription filtering is a feature that allows capture log data in real time and forward it to other AWS services such as Kinesis Data Firehose, Kinesis Streams, and Lambda.",
          "timestamp": "1724109180.0",
          "comment_id": "1154396"
        },
        {
          "comment_id": "1135118",
          "upvote_count": "3",
          "content": "A is correct: Use cloudwatch logs to collect logs from EKS. Use subcription filter to filter out logs and only send relevant logs to lambda to trigger it. \nB: CloudWatch Logs Insights is for data analysis. Additionally, using EventBridge events to trigger lambda incur costs\nC and D: Amazon S3 logging is used for monitoring actions on S3 itself, not EKS",
          "poster": "thanhnv142",
          "timestamp": "1722264300.0"
        },
        {
          "comment_id": "1102190",
          "content": "Selected Answer: A\nA is right \nC, D are wrong , because there is not integration in EKS to send logs to s3. \nB is for log analysis , and aggreation",
          "upvote_count": "4",
          "poster": "z_inderjot",
          "timestamp": "1718938740.0"
        },
        {
          "upvote_count": "1",
          "comments": [
            {
              "timestamp": "1716135600.0",
              "content": "No, sorry, this was for the previous questions",
              "upvote_count": "2",
              "comment_id": "1074827",
              "poster": "zolthar_z"
            }
          ],
          "content": "Selected Answer: B\nI don't have a technical reason but others dumps shows B as the Answer",
          "comment_id": "1074810",
          "poster": "zolthar_z",
          "timestamp": "1716134340.0"
        },
        {
          "comment_id": "924159",
          "poster": "madperro",
          "upvote_count": "1",
          "timestamp": "1702649220.0",
          "content": "Selected Answer: A\nA, metric filter can call Lambda."
        },
        {
          "content": "Selected Answer: A\ncertainly cloudwatch logs metric filter A",
          "upvote_count": "1",
          "poster": "rdoty",
          "comment_id": "911231",
          "timestamp": "1701353880.0"
        },
        {
          "upvote_count": "1",
          "comment_id": "886945",
          "content": "Selected Answer: A\nA. Enable Amazon CloudWatch Logs to log the EKS components. Create a CloudWatch subscription filter for each component with Lambda as the subscription feed destination.\n\nThis solution involves enabling Amazon CloudWatch Logs to log the EKS components and creating a CloudWatch subscription filter for each component with AWS Lambda as the subscription feed destination. This approach will allow the Lambda function to evaluate incoming log events and publish messages to the correct Amazon SNS topic. Amazon SNS can then send email notifications to each email address based on the messages it receives from the corresponding SNS topic.",
          "timestamp": "1698895440.0",
          "poster": "haazybanj"
        },
        {
          "timestamp": "1697379300.0",
          "content": "Selected Answer: A\nA, clear",
          "comment_id": "870978",
          "poster": "ele",
          "upvote_count": "1"
        },
        {
          "poster": "alce2020",
          "upvote_count": "1",
          "content": "A is the correct answer",
          "timestamp": "1697319600.0",
          "comment_id": "870485"
        },
        {
          "timestamp": "1697305860.0",
          "content": "Selected Answer: A\nAmazon CloudWatch Logs can log the EKS components, and subscription filters can be created for each component with AWS Lambda as the subscription feed destination. The Lambda function can evaluate incoming log events and publish messages to the appropriate Amazon SNS topic, enabling automated email notifications to be sent. Therefore, option A is the correct solution. Option C is incorrect because Amazon S3 logging is not designed for logging EKS components.",
          "poster": "jqso234",
          "upvote_count": "1",
          "comment_id": "870380"
        },
        {
          "comment_id": "862454",
          "poster": "Dimidrol",
          "timestamp": "1696537500.0",
          "upvote_count": "3",
          "content": "Selected Answer: A\nA for sure"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:16:46.185Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "GEwTRAT7vi6emEKoQwtZ",
      "question_number": 236,
      "page": 48,
      "question_text": "A company has an AWS CodePipeline pipeline in the eu-west-1 Region. The pipeline stores the build artifacts in an Amazon S3 bucket. The pipeline builds and deploys an AWS Lambda function by using an AWS CloudFormation deploy action.\n\nA DevOps engineer needs to update the existing pipeline to also deploy the Lambda function to the us-east-1 Region. The pipeline has already been updated to create an additional artifact to deploy to us-east-1.\n\nWhich combination of steps should the DevOps engineer take to meet these requirements? (Choose two.)",
      "choices": {
        "E": "Modify the pipeline to include the S3 bucket for us-east-1 as an artifact store. Create a new CloudFormation deploy action for us-east-1 in the pipeline. Configure the new deploy action to use the CloudFormation template from the us-east-1 artifact.",
        "D": "Create an S3 bucket in us-east-1. Configure S3 Cross-Region Replication (CRR) from the S3 bucket in eu-west-1 to the S3 bucket in us-east-1.",
        "B": "Create a new CloudFormation deploy action for us-east-1 in the pipeline. Configure the new deploy action to use the CloudFormation template from the additional artifact that was created for us-east-1.",
        "C": "Create an S3 bucket in us-east-1. Configure the S3 bucket policy to allow CodePipeline to have read and write access.",
        "A": "Modify the CloudFormation template to include a parameter for the Lambda function code's .zip file location. Create a new CloudFormation deploy action for us-east-1 in the pipeline. Configure the new deploy action to pass in the us-east-1 artifact location as a parameter override."
      },
      "correct_answer": "CE",
      "answer_ET": "CE",
      "answers_community": [
        "CE (92%)",
        "8%"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/152429-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-12-01 15:05:00",
      "unix_timestamp": 1733061900,
      "discussion_count": 5,
      "discussion": [
        {
          "timestamp": "1743944880.0",
          "content": "Selected Answer: CE\nQuestion says pipeline already modified to create the artifact in us-east-1",
          "poster": "Srikantha",
          "upvote_count": "2",
          "comment_id": "1558247"
        },
        {
          "timestamp": "1741876920.0",
          "poster": "asimohat",
          "comment_id": "1389278",
          "upvote_count": "1",
          "content": "Selected Answer: BC\nThe question states “The pipeline has already been updated to create additional deliverables to deploy to us-east-1.\nOption B simply adds a deploy action in a way that leverages this existing update."
        },
        {
          "content": "Selected Answer: CE\nCorrect",
          "comment_id": "1331595",
          "upvote_count": "3",
          "timestamp": "1735137600.0",
          "poster": "tubtab"
        },
        {
          "timestamp": "1733753220.0",
          "poster": "eugene2owl",
          "content": "Selected Answer: CE\nCorrect ansers \"C\" and \"E\".\n\"A\" is wrong for sure, because it's not enough to simply bypass a new Lambda code location as a parameter of existing CloudFormation Template: you need NOT only another Lambda code location, but also another region, another VPC id, etc.\nSo you need to have a second dedicated separate template.",
          "upvote_count": "4",
          "comment_id": "1324082"
        },
        {
          "comment_id": "1320582",
          "poster": "gunjan229",
          "timestamp": "1733061900.0",
          "upvote_count": "3",
          "content": "Selected Answer: CE\nCorrect Answer C, E"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:16:56.630Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "7VXbNHmDEthGnQ43Wl2Q",
      "question_number": 237,
      "page": 48,
      "question_text": "A company uses an AWS Cloud Development Kit (AWS CDK) application for its infrastructure. The AWS CDK application creates AWS Lambda functions and the IAM roles that are attached to the functions. The company also uses AWS Organizations. The company's developers can assume the AWS CDK application deployment role.\n\nThe company's security team discovered that the developers and the role used to deploy the AWS CDK application have more permissions than necessary. The security team also discovered that the roles attached to the Lambda functions that the CDK application creates have more permissions than necessary. The developers must not have the ability to grant additional permissions.\n\nWhich solution will meet these requirements with the LEAST operational overhead?",
      "choices": {
        "D": "Create an SCP that denies the iam:CreateRole action and the iam:UpdateRole action for the developer role. Give the AWS CDK deployment role access to create roles associated with Lambda functions. Run AWS Identity and Access Management Access Analyzer to verify that the Lambda functions role does not have permissions.",
        "B": "Create an IAM permission boundary policy. Define the maximum actions that the AWS CDK application requires in the policy. Update the account's AWS CDK bootstrapping to use the permission boundary. Update the configuration in the AWS CDK application for the default permissions boundary to use the policy.",
        "C": "Create an IAM permission boundary policy. Define the maximum actions that the AWS CDK application requires in the policy. Instruct the developers to use the permission boundary policy name when they create a role in the AWS CDK application code.",
        "A": "Create an SCP that denies the iam:CreateRole action and the iam:UpdateRole action for the developer role and the AWS CDK application deployment role. Centrally create new IAM roles to attach to the Lambda functions for the developers to use to provision Lambda functions."
      },
      "correct_answer": "B",
      "answer_ET": "B",
      "answers_community": [
        "B (63%)",
        "A (38%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/152094-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-11-27 04:05:00",
      "unix_timestamp": 1732676700,
      "discussion_count": 3,
      "discussion": [
        {
          "comment_id": "1419028",
          "timestamp": "1743552600.0",
          "poster": "Srikantha",
          "upvote_count": "1",
          "content": "Selected Answer: B\nOption B → Use a permission boundary at the AWS CDK bootstrap level to restrict both developer permissions and Lambda function roles.\n\nThis solution enforces least privilege, requires no manual IAM role creation, and keeps the AWS CDK workflow seamless."
        },
        {
          "poster": "teo2157",
          "content": "Selected Answer: A\nGoing for A as B doesn´t restrict any permission to the deverlopers, just to the CDK role...",
          "comment_id": "1342125",
          "upvote_count": "3",
          "timestamp": "1737116520.0"
        },
        {
          "comment_id": "1318399",
          "upvote_count": "4",
          "poster": "phu0298",
          "content": "Selected Answer: B\nIn Option B, the use of a permission boundary directly in the CDK workflow ensures that the created roles are inherently compliant, removing the need for IAM Access Analyzer’s reactive validation step. This approach reduces operational complexity and aligns with best practices for proactive security.",
          "timestamp": "1732676700.0"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:16:56.630Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "v3r9NcnlVsaDaIuQwOS3",
      "question_number": 238,
      "page": 48,
      "question_text": "A company uses Amazon Elastic Container Registry (Amazon ECR) private registries to store container images.\n\nA DevOps team needs to ensure that the container images are regularly scanned for software package vulnerabilities.\n\nWhich solution will meet this requirement?",
      "choices": {
        "B": "Enable basic continuous scanning for private registries in Amazon ECR.",
        "C": "Create an AWS System Manager Automation document to scan images by using the AWS SDK. Configure the Automation document to run when a new image is pushed to an ECR registry.",
        "D": "Create an AWS Lambda function that scans all images in Amazon ECR by using the AWS SDK. Create an Amazon EventBridge rule to invoke the Lambda function each day.",
        "A": "Enable enhanced scanning for private registries in Amazon ECR."
      },
      "correct_answer": "A",
      "answer_ET": "A",
      "answers_community": [
        "A (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/152096-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-11-27 04:10:00",
      "unix_timestamp": 1732677000,
      "discussion_count": 2,
      "discussion": [
        {
          "upvote_count": "1",
          "poster": "Srikantha",
          "content": "Selected Answer: A\nWhy is Option A the Best Choice?\nEnhanced scanning provides the most comprehensive security coverage.\nIt uses AWS-provided and third-party scanning tools (e.g., Amazon Inspector).\nIt scans continuously and provides detailed vulnerability reports.\nSupports automatic scanning on image push and periodic rescanning.\nThis ensures new images and existing images are continuously monitored.\nNo need for custom scripts or Lambda functions.\nFully managed by AWS → low operational overhead.",
          "comment_id": "1419029",
          "timestamp": "1743552660.0"
        },
        {
          "content": "Selected Answer: A\nAmazon ECR offers two levels of image scanning:\n 1. Basic Scanning: Detects vulnerabilities when images are pushed to the repository.\n 2. Enhanced Scanning: Provides continuous scanning of images, including detailed reports and automatic rescans when vulnerabilities are published.",
          "comment_id": "1318402",
          "poster": "phu0298",
          "timestamp": "1732677000.0",
          "upvote_count": "4"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:16:56.630Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "aLh4JXg1NPI71uFvuB8L",
      "question_number": 239,
      "page": 48,
      "question_text": "A security team sets up a workflow that invokes an AWS Step Functions workflow when Amazon EventBridge matches specific events. The events can be generated by several AWS services. AWS CloudTrail records user activities.\n\nThe security team notices that some important events do not invoke the workflow as expected. The CloudTrail logs do not indicate any direct errors related to the missing events.\n\nWhich combination of steps will identify the root cause of the missing event invocations? (Choose three.)",
      "choices": {
        "A": "Enable EventBridge schema discovery on the event bus to determine whether the event patterns match the expected schema.",
        "B": "Configure Amazon CloudWatch to monitor EventBridge metrics and Step Functions metrics. Set up alerts for anomalies in event patterns and workflow invocations.",
        "C": "Configure an AWS Lambda logging function to monitor and log events from EventBridge to provide more details about the processed events.",
        "E": "Review metrics for the EventBridge failed invocations to ensure that the IAM execution role that is attached to the rule has sufficient permissions.",
        "D": "Review the Step Functions execution history for patterns of failures or timeouts that could correlate to the missing event invocations.",
        "F": "Verify that the Step Functions workflow has the correct permissions to be invoked by EventBridge."
      },
      "correct_answer": "ABE",
      "answer_ET": "ABE",
      "answers_community": [
        "ABE (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/151539-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-11-18 10:05:00",
      "unix_timestamp": 1731920700,
      "discussion_count": 4,
      "discussion": [
        {
          "comment_id": "1419083",
          "poster": "Srikantha",
          "timestamp": "1743553200.0",
          "content": "Selected Answer: ABE\n✔ A. Enable EventBridge schema discovery to validate event structure.\n✔ B. Monitor CloudWatch metrics for EventBridge and Step Functions.\n✔ E. Check EventBridge failed invocations metrics for IAM permission issues.\n\nThis ensures you're covering schema validation, event monitoring, and IAM permission checks—all critical for debugging missing event invocations.",
          "upvote_count": "1"
        },
        {
          "comment_id": "1324670",
          "poster": "luisfsm_111",
          "content": "Selected Answer: ABE\nAgree with the execution role approach",
          "upvote_count": "2",
          "timestamp": "1733849580.0"
        },
        {
          "timestamp": "1732287540.0",
          "content": "Selected Answer: ABE\nI'd say E instead of F. The EventBridge rule contains the IAM execution role, that needs the permissions to invoke the step function. The permissions is not given at the step function side (no resource-based policy, see https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_aws-services-that-work-with-iam.html)",
          "upvote_count": "3",
          "poster": "Impromptu",
          "comment_id": "1316367"
        },
        {
          "content": "A,B,F\nThe best steps to identify the root cause of the missing event invocations are enabling schema discovery to ensure the event structure is correct, monitoring EventBridge and Step Functions metrics to detect anomalies, and verifying that the Step Functions workflow has the correct permissions to be invoked by EventBridge. These actions will help to narrow down the issue effectively and efficiently.",
          "timestamp": "1731920700.0",
          "comment_id": "1313909",
          "upvote_count": "2",
          "poster": "uncledana"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:16:56.630Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "GBVStzFUAJ7mLgk3EmgH",
      "question_number": 240,
      "page": 48,
      "question_text": "A company's DevOps engineer uses AWS Systems Manager to perform maintenance tasks. The company has a few Amazon EC2 instances that require a restart after notifications from AWS Health.\n\nThe DevOps engineer must implement an automated solution that uses Amazon EventBridge to remediate the notifications during the company's scheduled maintenance windows.\n\nHow should the DevOps engineer configure an EventBridge rule to meet these requirements?",
      "choices": {
        "B": "Configure an event source of Systems Manager. Configure an event type that indicates a maintenance window. Target the AWS-RestartEC2Instance Systems Manager Automation runbook to restart the EC2 instances.",
        "D": "Configure an event source of EC2. Configure an event type that indicates instance state notification. Target a newly created AWS Lambda function that registers a Systems Manager maintenance window task to restart the EC2 instances.",
        "A": "Configure an event source of AWS Health. Configure event types that indicate scheduled instance termination and retirement. Target the AWS-RestartEC2Instance Systems Manager Automation runbook to restart the EC2 instances.",
        "C": "Configure an event source of AWS Health. Configure event types that indicate scheduled instance termination and retirement. Target a newly created AWS Lambda function that registers a Systems Manager maintenance window task to restart the EC2 instances."
      },
      "correct_answer": "A",
      "answer_ET": "A",
      "answers_community": [
        "A (68%)",
        "C (32%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/151840-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-11-22 16:04:00",
      "unix_timestamp": 1732287840,
      "discussion_count": 8,
      "discussion": [
        {
          "poster": "tinyshare",
          "content": "Selected Answer: A\nSame as the question 20, A is correct.",
          "upvote_count": "6",
          "timestamp": "1733453520.0",
          "comment_id": "1322601"
        },
        {
          "upvote_count": "1",
          "poster": "Srikantha",
          "comment_id": "1419136",
          "timestamp": "1743553680.0",
          "content": "Selected Answer: A\nEvent Source of AWS Health:\nThe DevOps engineer wants to act on AWS Health notifications. AWS Health provides event types for scheduled instance terminations and instance retirements, which are common reasons for needing to restart EC2 instances during a maintenance window.\nEvent Types:\nEventBridge can be configured to listen for event types like EC2 Instance Termination or EC2 Instance Retirement to automate instance restarts when these events occur.\nTarget Systems Manager Automation Runbook:\nAWS Systems Manager Automation runbooks such as AWS-RestartEC2Instance are pre-defined automation workflows that can restart EC2 instances. When triggered by EventBridge, this runbook can automatically restart the EC2 instances that need remediation."
        },
        {
          "poster": "devops_1",
          "timestamp": "1739928480.0",
          "content": "Selected Answer: C\nOption A does not satisfy the maintenance window requirement",
          "upvote_count": "2",
          "comment_id": "1358540"
        },
        {
          "timestamp": "1738859580.0",
          "comment_id": "1352518",
          "poster": "c87b433",
          "content": "Selected Answer: C\nC is the right answer. A will do action immediately but using lambda we can create a maintenance window according to organization downtime activity. A could have been the right answer, but it takes immediate action on an event that is not required. We have to do this in a particular time.",
          "upvote_count": "1"
        },
        {
          "content": "Selected Answer: C\nquestion states during the maintenance window not immediately",
          "poster": "gildzeee",
          "comment_id": "1330942",
          "timestamp": "1734987000.0",
          "upvote_count": "1"
        },
        {
          "upvote_count": "3",
          "timestamp": "1732678560.0",
          "poster": "phu0298",
          "content": "Selected Answer: A\nC. Lambda Function to Register a Maintenance Window Task\n • While this approach could work, it unnecessarily complicates the solution. AWS Systems Manager and EventBridge provide native integrations that eliminate the need for a custom Lambda function.\nTherefore the answer should be A",
          "comment_id": "1318409"
        },
        {
          "content": "Selected Answer: A\nAWS Health provides notifications for events such as scheduled instance termination and decommissioning.",
          "poster": "Changwha",
          "comment_id": "1316641",
          "upvote_count": "3",
          "timestamp": "1732359420.0"
        },
        {
          "comment_id": "1316369",
          "timestamp": "1732287840.0",
          "poster": "Impromptu",
          "upvote_count": "2",
          "content": "Selected Answer: C\nAlthough a bit more complex than A, I think C is best. Option A would just restart the instance when the event was received, and not withing the company's maintenance windows."
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:16:56.630Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "2uI6VnEGMmFQ5t9FYV2F",
      "question_number": 241,
      "page": 49,
      "question_text": "A DevOps engineer manages an AWS CodePipeline pipeline that builds and deploys a web application on AWS. The pipeline has a source stage, a build stage, and a deploy stage. When deployed properly, the web application responds with a 200 OK HTTP response code when the URL of the home page is requested.\n\nThe home page recently returned a 503 HTTP response code after CodePipeline deployed the application. The DevOps engineer needs to add an automated test into the pipeline. The automated test must ensure that the application returns a 200 OK HTTP response code after the application is deployed. The pipeline must fail if the response code is not present during the test. The DevOps engineer has added a CheckURL stage after the deploy stage in the pipeline.\n\nWhat should the DevOps engineer do next to implement the automated test?",
      "choices": {
        "A": "Configure the CheckURL stage to use an Amazon CloudWatch action. Configure the action to use a canary synthetic monitoring check on the application URL and to report a success or failure to CodePipeline.",
        "D": "Deploy an Amazon API Gateway HTTP API that checks the response code status of the URL and that reports success or failure to CodePipeline. Configure the CheckURL stage to use the AWS Device Farm test action and to provide the API Gateway HTTP API as an input artifact.",
        "B": "Create an AWS Lambda function to check the response code status of the URL and to report a success or failure to CodePipeline. Configure an action in the CheckURL stage to invoke the Lambda function.",
        "C": "Configure the CheckURL stage to use an AWS CodeDeploy action. Configure the action with an input artifact that is the URL of the application and to report a success or failure to CodePipeline."
      },
      "correct_answer": "B",
      "answer_ET": "B",
      "answers_community": [
        "B (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/153429-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-12-25 06:33:00",
      "unix_timestamp": 1735104780,
      "discussion_count": 2,
      "discussion": [
        {
          "comment_id": "1419214",
          "poster": "Srikantha",
          "upvote_count": "1",
          "timestamp": "1743554220.0",
          "content": "Selected Answer: B\nThis is the correct approach because:\n\nAWS Lambda functions are ideal for running custom code that can check the HTTP response status of the application.\nThe Lambda function can make an HTTP request to the application’s home page URL, check the response code, and then return a success or failure depending on whether the code is 200 OK.\nCodePipeline can then be configured to run this Lambda function as part of the CheckURL stage, and the pipeline can fail if the response code is not 200 OK."
        },
        {
          "poster": "matt200",
          "timestamp": "1735104780.0",
          "comment_id": "1331369",
          "upvote_count": "2",
          "content": "Selected Answer: B\nshould be B"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:17:07.084Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "K5haXTqZZz8rDO7kZNoI",
      "question_number": 242,
      "page": 49,
      "question_text": "A company has an application that uploads access logs to an Amazon CloudWatch Logs log group. The fields in the log lines include the response code and the application name.\n\nThe company wants to create a CloudWatch metric to track the number of requests by response code in a specific range and with a specific application name.\n\nWhich solution will meet these requirements?",
      "choices": {
        "D": "Create a CloudWatch Logs Insights query on the CloudWatch Logs log group to match the response code range. Configure the Logs Insights query to increment a CloudWatch metric with the response code and application name as dimensions.",
        "B": "Create a CloudWatch Logs metric filter on the CloudWatch Logs log group to match the response code range. Configure the metric filter to increment a metric. Set the response code and application name as dimensions.",
        "A": "Create a CloudWatch Logs log event filter on the CloudWatch Logs log stream to match the response code range. Configure the log event filter to increment a metric. Set the response code and application name as dimensions.",
        "C": "Create a CloudWatch Contributor Insights rule on the CloudWatch Logs log stream with a filter to match the response code range. Configure the Contributor Insights rule to increment a CloudWatch metric with the response code and application name as dimensions."
      },
      "correct_answer": "B",
      "answer_ET": "B",
      "answers_community": [
        "B (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/153430-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-12-25 06:34:00",
      "unix_timestamp": 1735104840,
      "discussion_count": 2,
      "discussion": [
        {
          "comment_id": "1419293",
          "poster": "Srikantha",
          "content": "Selected Answer: B\nWhy this is the best choice:\n\nCloudWatch Logs Metric Filters allow you to extract specific patterns from log data, such as response codes and application names, and create metrics based on those patterns.\nYou can set conditions to match the desired range of response codes in the logs.\nYou can also specify dimensions (e.g., response code and application name) for the metric, which will allow you to track the counts of requests with specific response codes and application names.\nOnce created, this metric filter will automatically create CloudWatch metrics for the logs that match the criteria.",
          "timestamp": "1743555000.0",
          "upvote_count": "2"
        },
        {
          "poster": "matt200",
          "timestamp": "1735104840.0",
          "comment_id": "1331371",
          "upvote_count": "3",
          "content": "Selected Answer: B\nshould be B"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:17:07.084Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "uepg2K5ShyhN1SLa13Qm",
      "question_number": 243,
      "page": 49,
      "question_text": "A DevOps engineer provisioned an Amazon Elastic Kubernetes Service (Amazon EKS) cluster with managed node groups. The DevOps engineer associated an OpenID Connect (OIDC) issuer with the cluster.\n\nThe DevOps engineer is configuring Amazon Elastic Block Store (Amazon EBS) General Purpose SSD (gp3) volumes for the cluster. The DevOps engineer attempts to initiate a PersistentVolumeClaim (PVC) request but is unable to provision a volume. To troubleshoot the issue, the DevOps engineer runs the kubectl describe pyc command. The DevOps engineer receives a failed to provision volume with StorageClass error and a could not create volume in EC2:UnauthorizedOperation error.\n\nWhich solution will resolve these errors?",
      "choices": {
        "C": "Add the ebs.csi.aws.com/volumeType:gp3 annotation to the PersistentVolumeClaim object in the cluster.",
        "A": "Create a Kubernetes cluster role that allows the persistent volumes to perform get, list, watch, create, and delete operations. Configure the cluster role to allow get, list, and watch operations for storage in the cluster.",
        "B": "Create an Amazon EBS Container Storage Interface (CSI) driver IAM role that has the required permissions and trust relationships. Attach the IAM role to the Amazon EBS CSI driver add-on in the cluster.",
        "D": "Create a Kubernetes storage class object. Set the provisioner value to ebs.csi.aws.com. Set the volumeBindingMode value to WaitForFirstConsumer in the luster."
      },
      "correct_answer": "B",
      "answer_ET": "B",
      "answers_community": [
        "B (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/151540-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-11-18 10:46:00",
      "unix_timestamp": 1731923160,
      "discussion_count": 4,
      "discussion": [
        {
          "content": "Selected Answer: B\nThe key error is:\n\n\"could not create volume in EC2: UnauthorizedOperation\"\nThis indicates that the EBS CSI driver does not have the required IAM permissions to provision EBS volumes via the EC2 API. Since the cluster is using IAM roles for service accounts (IRSA) with OIDC, the EBS CSI driver must assume an IAM role with the right permissions.\n\nHere's what's likely missing:\nThe IAM role for the EBS CSI driver.\nProper trust relationship with the OIDC provider.\nNecessary permissions like ec2:CreateVolume, ec2:AttachVolume, etc.\n🛠 What to do:\nCreate an IAM policy with the required permissions.\nCreate an IAM role for the EBS CSI driver.\nUpdate the trust relationship to allow assumption via the cluster’s OIDC provider.\nPatch the EBS CSI driver deployment to use this IAM role (via Kubernetes service account).",
          "comment_id": "1537217",
          "poster": "Srikantha",
          "timestamp": "1743859560.0",
          "upvote_count": "2"
        },
        {
          "content": "Selected Answer: B\nB seems correct.",
          "comment_id": "1350544",
          "upvote_count": "1",
          "poster": "jojewi8143",
          "timestamp": "1738513020.0"
        },
        {
          "upvote_count": "3",
          "comment_id": "1327912",
          "timestamp": "1734442440.0",
          "content": "Selected Answer: B\nIt's B based on this:\nhttps://docs.aws.amazon.com/eks/latest/userguide/ebs-csi.html",
          "poster": "teo2157"
        },
        {
          "content": "The root cause of the error is that the EBS CSI driver does not have the necessary IAM permissions to create EBS volumes in EC2. Solution B resolves the issue by creating an appropriate IAM role and attaching it to the EBS CSI driver, giving it the required permissions.",
          "upvote_count": "3",
          "poster": "uncledana",
          "comment_id": "1313924",
          "timestamp": "1731923160.0"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:17:07.084Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "OqYDpy7QH2HKmAY3QJ0i",
      "question_number": 244,
      "page": 49,
      "question_text": "A company runs a fleet of Amazon EC2 instances in a VPC. The company's employees remotely access the EC2 instances by using the Remote Desktop Protocol (RDP).\n\nThe company wants to collect metrics about how many RDP sessions the employees initiate every day.\n\nWhich combination of steps will meet this requirement? (Choose three.)",
      "choices": {
        "E": "Create a log group metric filter.",
        "A": "Create an Amazon EventBridge rule that reacts to EC2 Instance State-change Notification events.",
        "B": "Create an Amazon CloudWatch Logs log group. Specify the log group as a target for the EventBridge rule.",
        "D": "Create an Amazon CloudWatch Logs log group. Specify the log group as a destination for the flow log.",
        "F": "Create a log group subscription filter. Use EventBridge as the destination.",
        "C": "Create a flow log in VPC Flow Logs."
      },
      "correct_answer": "CDE",
      "answer_ET": "CDE",
      "answers_community": [
        "CDE (90%)",
        "10%"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/151541-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-11-18 10:48:00",
      "unix_timestamp": 1731923280,
      "discussion_count": 6,
      "discussion": [
        {
          "timestamp": "1743859800.0",
          "comment_id": "1537391",
          "upvote_count": "1",
          "poster": "Srikantha",
          "content": "Selected Answer: CDE\nC. VPC Flow Logs\n\nFlow logs capture network traffic in your VPC, including RDP traffic (TCP port 3389).\nThis is the only way to detect RDP session attempts from a network perspective without needing to install agents on the instances.\nD. CloudWatch Logs destination for flow logs\n\nTo analyze flow logs, you must send them somewhere — CloudWatch Logs is a common destination.\nOnce in CloudWatch Logs, you can search and filter for RDP traffic patterns.\nE. Metric filter on the log group\n\nYou can create a CloudWatch metric filter to count log events that match RDP connections.\nFilter pattern would look for destination port 3389 and action \"ACCEPT\"."
        },
        {
          "poster": "Ky_24",
          "timestamp": "1734299820.0",
          "comment_id": "1327075",
          "upvote_count": "3",
          "content": "Selected Answer: CDE\nC. Create a flow log in VPC Flow Logs.\n • Why? VPC Flow Logs capture information about the traffic to and from network interfaces in your VPC. This is crucial for identifying and analyzing RDP sessions, which use TCP port 3389 by default.\n\nD. Create an Amazon CloudWatch Logs log group. Specify the log group as a destination for the flow log.\n • Why? The captured VPC Flow Logs must be stored in a destination to enable analysis. Specifying a CloudWatch Logs log group allows for centralized storage and querying of logs.\n\nE. Create a log group metric filter.\n • Why? A metric filter enables you to extract specific metrics from the flow logs. You can filter for traffic using port 3389 (RDP) and create a metric to count the sessions."
        },
        {
          "comment_id": "1325049",
          "timestamp": "1733925900.0",
          "poster": "luisfsm_111",
          "content": "Selected Answer: CDE\nI see CDE, no need for EventBridge",
          "upvote_count": "3"
        },
        {
          "comment_id": "1319571",
          "timestamp": "1732861800.0",
          "upvote_count": "2",
          "poster": "nqg54118",
          "content": "Selected Answer: CDE\nYou can use a subscription filter with Amazon Kinesis Data Streams, AWS Lambda, or Amazon Data Firehos\nhttps://docs.aws.amazon.com/ja_jp/AmazonCloudWatch/latest/logs/SubscriptionFilters.html"
        },
        {
          "upvote_count": "1",
          "poster": "f4b18ba",
          "content": "Selected Answer: CEF\nBy using an Amazon ECR pull through cache rule (Option C) and setting up the necessary VPC endpoints for private ECR (Option E) and S3 (Option F), the company can:\n\nEliminate Internet Access:\nRemove NAT gateways and internet gateways from the VPC.\nMaintain Image Access:\nAllow ECS tasks to pull images from both private and public ECR repositories without internet access.\nEnsure Image Updates:\nAutomatically receive updates to public images within 24 hours via the pull through cache.\nMinimize Operational Overhead:\nAvoid complex setups with additional services like CodeBuild, Lambda, or custom scripts.",
          "comment_id": "1316478",
          "timestamp": "1732305480.0"
        },
        {
          "poster": "uncledana",
          "timestamp": "1731923280.0",
          "comment_id": "1313927",
          "upvote_count": "1",
          "content": "The best approach for collecting metrics about RDP sessions is to use VPC Flow Logs, send them to CloudWatch Logs, and then create a metric filter to extract the relevant information (such as RDP traffic on port 3389). Option B, D, and E cover the necessary steps for implementing this solution."
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:17:07.084Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "PVYeA1hMCYhXNukZV1hd",
      "question_number": 245,
      "page": 49,
      "question_text": "A company is using Amazon Elastic Kubernetes Service (Amazon EKS) to run its applications. The EKS cluster is successfully running multiple pods. The company stores the pod images in Amazon Elastic Container Registry (Amazon ECR).\n\nThe company needs to configure Pod Identity access for the EKS cluster. The company has already updated the node IAM role by using the permissions for Pod Identity access.\n\nWhich solution will meet these requirements?",
      "choices": {
        "B": "Ensure that the nodes can reach the EKS Auth API. Add and configure the EKS Pod Identity Agent add-on for the EKS cluster.",
        "A": "Create an IAM OpenID Connect (OIDC) provider for the EKS cluster.",
        "C": "Create an EKS access entry that uses the API_AND-CONFIG_MAP cluster authentication mode.",
        "D": "Configure the AWS Security Token Service (AWS STS) endpoint for the Kubernetes service account that the pods in the EKS cluster use."
      },
      "correct_answer": "B",
      "answer_ET": "B",
      "answers_community": [
        "B (85%)",
        "A (15%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/151542-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-11-18 10:51:00",
      "unix_timestamp": 1731923460,
      "discussion_count": 8,
      "discussion": [
        {
          "timestamp": "1743860160.0",
          "comment_id": "1537643",
          "content": "Selected Answer: B\nThe company wants to configure EKS Pod Identity (a newer and simpler alternative to IAM Roles for Service Accounts / IRSA).\n\nSince they already updated the node IAM role to allow Pod Identity access, the next essential step is to:\n\nInstall the EKS Pod Identity Agent add-on to the cluster.\nEnsure that the nodes can reach the EKS Auth API, which the agent uses to request temporary credentials.\nThis is exactly what Option B outlines.",
          "poster": "Srikantha",
          "upvote_count": "1"
        },
        {
          "poster": "CHRIS12722222",
          "comment_id": "1332988",
          "content": "Selected Answer: B\nQuestion is not talking about IRSA\nPod identities do not need OIDC",
          "timestamp": "1735394460.0",
          "upvote_count": "3"
        },
        {
          "poster": "tubtab",
          "upvote_count": "2",
          "comment_id": "1331601",
          "content": "Selected Answer: B\nIT BBBB",
          "timestamp": "1735138320.0"
        },
        {
          "upvote_count": "2",
          "comment_id": "1330944",
          "content": "Selected Answer: B\nquestion doesnt state the pods are using irsa so the eks addon should work just fine with pod identity",
          "poster": "gildzeee",
          "timestamp": "1734987480.0"
        },
        {
          "upvote_count": "1",
          "timestamp": "1734446160.0",
          "comment_id": "1327956",
          "poster": "teo2157",
          "content": "Selected Answer: A\nIt's A based on this:\nhttps://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts.html"
        },
        {
          "timestamp": "1733307900.0",
          "upvote_count": "3",
          "comment_id": "1321808",
          "content": "Selected Answer: B\nPod Identity is a \"new\" way to provide Pod access to AWS services and does not rely on OIDC. Instead you have to setup the EKS Pod Identity Agent and must ensure kubernetes nodes can reach the EKS Auth API endpoint.\nhttps://docs.aws.amazon.com/eks/latest/userguide/pod-identities.html",
          "poster": "pma17"
        },
        {
          "comment_id": "1316457",
          "poster": "f4b18ba",
          "timestamp": "1732302780.0",
          "upvote_count": "1",
          "comments": [
            {
              "upvote_count": "1",
              "comment_id": "1332983",
              "content": "Question is not talking about IRSA\nPod identities do not need OIDC",
              "poster": "CHRIS12722222",
              "timestamp": "1735394220.0"
            }
          ],
          "content": "Selected Answer: A\nThis is the necessary first step to set up IRSA. Without the IAM OIDC provider, IAM cannot trust tokens from the EKS cluster, and service accounts cannot assume IAM roles.\nEnables the establishment of trust between Kubernetes service accounts and IAM roles, allowing pods to securely access AWS resources."
        },
        {
          "comment_id": "1313928",
          "upvote_count": "1",
          "poster": "uncledana",
          "timestamp": "1731923460.0",
          "content": "The best and most accurate solution is A. Create an IAM OpenID Connect (OIDC) provider for the EKS cluster."
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:17:07.084Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "jOIl5zjkBlkQawsSWCvz",
      "question_number": 246,
      "page": 50,
      "question_text": "A company is implementing an Amazon Elastic Container Service (Amazon ECS) cluster to run its workload. The company architecture will run multiple ECS services on the cluster. The architecture includes an Application Load Balancer on the front end and uses multiple target groups to route traffic.\nA DevOps engineer must collect application and access logs. The DevOps engineer then needs to send the logs to an Amazon S3 bucket for near-real-time analysis.\nWhich combination of steps must the DevOps engineer take to meet these requirements? (Choose three.)",
      "choices": {
        "A": "Download the Amazon CloudWatch Logs container instance from AWS. Configure this instance as a task. Update the application service definitions to include the logging task.",
        "E": "Activate access logging on the target groups that the ECS services use. Then send the logs directly to the logging S3 bucket.",
        "C": "Use Amazon EventBridge to schedule an AWS Lambda function that will run every 60 seconds and will run the Amazon CloudWatch Logs create-export-task command. Then point the output to the logging S3 bucket.",
        "D": "Activate access logging on the ALB. Then point the ALB directly to the logging S3 bucket.",
        "F": "Create an Amazon Kinesis Data Firehose delivery stream that has a destination of the logging S3 bucket. Then create an Amazon CloudWatch Logs subscription filter for Kinesis Data Firehose.",
        "B": "Install the Amazon CloudWatch Logs agent on the ECS instances. Change the logging driver in the ECS task definition to awslogs."
      },
      "correct_answer": "BDF",
      "answer_ET": "BDF",
      "answers_community": [
        "BDF (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/105337-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-04-05 22:32:00",
      "unix_timestamp": 1680726720,
      "discussion_count": 14,
      "discussion": [
        {
          "comment_id": "927918",
          "poster": "tartarus23",
          "upvote_count": "14",
          "content": "Selected Answer: BDF\nExplanation:\n\nOption B is correct because you can change the logging driver in the ECS task definition to awslogs, which will direct the logs to Amazon CloudWatch Logs. Then, the logs can be forwarded to the Amazon S3 bucket.\n\nOption D is correct because enabling access logging on the Application Load Balancer (ALB) allows the collection of access logs that can be sent directly to an S3 bucket.\n\nOption F is correct because you can create an Amazon Kinesis Data Firehose delivery stream that can deliver logs from CloudWatch Logs directly to an Amazon S3 bucket in near-real-time.",
          "timestamp": "1687207620.0"
        },
        {
          "content": "Selected Answer: BDF\nhttps://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-logging-monitoring.html",
          "timestamp": "1732482660.0",
          "poster": "steli0",
          "upvote_count": "2",
          "comment_id": "1317223"
        },
        {
          "timestamp": "1709903640.0",
          "content": "Selected Answer: BDF\nhttps://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-logging-monitoring.html",
          "comment_id": "1168812",
          "upvote_count": "2",
          "poster": "4555894"
        },
        {
          "poster": "dzn",
          "content": "Selected Answer: BDF\nEnable access logging using the ALB management console, CLI, or API. Specify the S3 bucket where the logs will be stored and, if necessary, set the log file prefix (e.g., production, staging.) to store the logs in different paths within the bucket.",
          "comment_id": "1154410",
          "upvote_count": "1",
          "timestamp": "1708393440.0"
        },
        {
          "comment_id": "1135125",
          "poster": "thanhnv142",
          "content": "BDF: There are two types of log that needs to be collected\nB: push app log to Cloudwatch log\nD: push access log to S3\nF: using Kinesis to push app log from cloudwatch log to S3 in near real-time\n\nA: wrong - we need cloudwatch agent, not container instance\nC: No need to use event bridge and lambda to trigger cloudwatch log to push log to s3. \nE: access logs lie in ALB, not ECS services.",
          "upvote_count": "4",
          "timestamp": "1706547600.0"
        },
        {
          "content": "Selected Answer: BDF\nBDF is the answer . \nbtw, can't we use cloudwatch ingists to collet the logs from containers in ecs there days , and then usign the subscription filter we can sends those logs to s3. \nwithout having to install cloud watch agent.",
          "poster": "z_inderjot",
          "comment_id": "1102195",
          "upvote_count": "2",
          "timestamp": "1703135580.0"
        },
        {
          "timestamp": "1702264740.0",
          "content": "Real time. so not E",
          "upvote_count": "1",
          "comment_id": "1093047",
          "poster": "imymoco"
        },
        {
          "content": "Selected Answer: BDF\nBDF makes sense. E is certainly wrong.",
          "timestamp": "1686833040.0",
          "upvote_count": "1",
          "poster": "madperro",
          "comment_id": "924187"
        },
        {
          "upvote_count": "2",
          "timestamp": "1685462040.0",
          "comment_id": "910400",
          "poster": "bcx",
          "content": "Selected Answer: BDF\nBDF\n\nAccess logs cannot be configured by ALB target group\n\nhttps://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-access-logs.html"
        },
        {
          "timestamp": "1685167380.0",
          "content": "Option B sends data to the Cloudwatch Log. This issue requires that logs be collected in S3.",
          "comment_id": "907796",
          "upvote_count": "1",
          "poster": "hanbj"
        },
        {
          "timestamp": "1682992620.0",
          "comment_id": "886955",
          "poster": "haazybanj",
          "upvote_count": "2",
          "content": "Selected Answer: BDF\nAnswer is bdf"
        },
        {
          "timestamp": "1681568400.0",
          "content": "Selected Answer: BDF\nB - get application logs to CW\nD - get access logs to S3\nF - get application logs from CW to S3 in near-real time",
          "comment_id": "870982",
          "poster": "ele",
          "upvote_count": "4"
        },
        {
          "poster": "jqso234",
          "content": "Selected Answer: BDF\nOption BDE can be cumbersome to manage in a large environment and may not be ideal for applications that generate large amounts of logs. Option BDF, on the other hand, captures both application and access logs, and uses the CloudWatch Logs driver to stream logs directly to CloudWatch Logs. This solution is more scalable as it does not require the CloudWatch Logs agent to be installed on each instance, and it can capture logs from multiple ECS tasks running on the same instance. In addition, the logs can be sent to an S3 bucket using a Kinesis Data Firehose delivery stream, which provides near-real-time analysis capabilities.",
          "timestamp": "1681495080.0",
          "comment_id": "870385",
          "upvote_count": "1"
        },
        {
          "comment_id": "862459",
          "content": "Selected Answer: BDF\nB D F for me",
          "poster": "Dimidrol",
          "upvote_count": "2",
          "comments": [
            {
              "content": "https://docs.aws.amazon.com/AmazonECS/latest/developerguide/using_cloudwatch_logs.html",
              "timestamp": "1680729120.0",
              "comment_id": "862474",
              "poster": "Dimidrol",
              "upvote_count": "1"
            }
          ],
          "timestamp": "1680726720.0"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:17:17.722Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "kB9x028JHitV7TRUXyUd",
      "question_number": 247,
      "page": 50,
      "question_text": "A company has multiple AWS accounts in an organization in AWS Organizations that has all features enabled. The company’s DevOps administrator needs to improve security across all the company's AWS accounts. The administrator needs to identify the top users and roles in use across all accounts.\n\nWhich solution will meet these requirements with the MOST operational efficiency?",
      "choices": {
        "B": "Create an unused access analysis for the organization by using AWS Identity and Access Management Access Analyzer. Review the analyzer results and determine if each finding has the intended level of permissions required for the workload.",
        "D": "Generate a Service access report for each account by using Organizations. From the results, pull the last accessed date and last accessed by account fields to find the top users and roles.",
        "C": "Create a new organization trail in AWS CloudTrail. Create a table in Amazon Athena that uses partition projection. Load the Athena table with CloudTrail data. Query the Athena table to find the top users and roles.",
        "A": "Create a new organization trail in AWS CloudTrail. Configure the trail to send log events to Amazon CloudWatch Logs. Create a CloudWatch Contributor Insights rule for the userIdentity.arn log field. View the results in CloudWatch Contributor Insights."
      },
      "correct_answer": "C",
      "answer_ET": "C",
      "answers_community": [
        "C (64%)",
        "A (36%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/151748-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-11-21 08:36:00",
      "unix_timestamp": 1732174560,
      "discussion_count": 9,
      "discussion": [
        {
          "timestamp": "1734300240.0",
          "upvote_count": "5",
          "poster": "Ky_24",
          "content": "Selected Answer: C\nYou can use partition projection in Athena to optimize your queries by specifying how the logs are structured in S3. This makes the process of querying CloudTrail logs across multiple AWS accounts much more efficient.",
          "comment_id": "1327079"
        },
        {
          "content": "Selected Answer: A\nWhich solution will meet these requirements with the MOST operational efficiency?\n\nAnswer is A",
          "upvote_count": "2",
          "poster": "AWSLoverLoverLoverLoverLover",
          "comment_id": "1571365",
          "timestamp": "1747934460.0"
        },
        {
          "timestamp": "1747389060.0",
          "comment_id": "1569324",
          "upvote_count": "1",
          "poster": "robotgeek",
          "content": "Selected Answer: A\nWhat do you think \"partition projection\" has to do with the required scenario guys?"
        },
        {
          "upvote_count": "1",
          "comment_id": "1537943",
          "poster": "Srikantha",
          "content": "Selected Answer: C\nThis option provides the MOST operational efficiency because it:\n\nAggregates CloudTrail logs from all AWS accounts using a single organization trail.\nLeverages Amazon Athena to analyze logs at scale with SQL-like queries.\nAllows for automated and repeatable querying to identify top users and roles across the entire organization.\nPartition projection reduces the need for manual partition management, improving performance and automation.",
          "timestamp": "1743860760.0"
        },
        {
          "comment_id": "1349436",
          "upvote_count": "4",
          "content": "Selected Answer: C\nVoting for C, agree with Ky_24",
          "timestamp": "1738314000.0",
          "poster": "teo2157"
        },
        {
          "comment_id": "1344080",
          "timestamp": "1737451380.0",
          "poster": "Erso",
          "content": "Selected Answer: A\nMOST operation efficency is the key point here",
          "upvote_count": "1"
        },
        {
          "poster": "teo2157",
          "comment_id": "1327961",
          "timestamp": "1734446520.0",
          "content": "Selected Answer: C\nAthena is much more efficient that CloudWatch Contributor Insights in this case",
          "upvote_count": "4"
        },
        {
          "upvote_count": "4",
          "comment_id": "1316460",
          "poster": "f4b18ba",
          "timestamp": "1732303020.0",
          "content": "Selected Answer: A\nOption A provides a solution that is operationally efficient, scalable, and directly addresses the requirement to identify the top users and roles in use across all AWS accounts. By leveraging AWS services like CloudTrail and CloudWatch Contributor Insights, the DevOps administrator can gain real-time insights with minimal setup and maintenance effort."
        },
        {
          "comment_id": "1315698",
          "content": "C\nA: While Contributor Insights can identify the top contributors (e.g., users and roles), it is limited to specific log patterns and is more suited for real-time analysis.\nThis option is not as operationally efficient for long-term, detailed analysis across all accounts.",
          "upvote_count": "4",
          "poster": "phu0298",
          "timestamp": "1732174560.0"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:17:17.722Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "bMVXS2mbIJIxIOdygGg8",
      "question_number": 248,
      "page": 50,
      "question_text": "A company has an organization in AWS Organizations with many Oils that contain many AWS accounts. The organization has a dedicated delegated administrator AWS account.\n\nThe company needs the accounts in one OU to have server-side encryption enforced for all Amazon Elastic Block Store (Amazon EBS) volumes and Amazon Simple Queue Service (Amazon SQS) queues that are created or updated on an AWS CloudFormation stack.\n\nWhich solution will enforce this policy before a CloudFormation stack operation in the accounts of this OU?",
      "choices": {
        "C": "Write an SCP to deny the creation of EBS volumes and SQS queues unless the EBS volumes and SQS queues have server-side encryption. Attach the SCP to the OU.",
        "A": "Activate trusted access to CloudFormation StackSets. Create a CloudFormation Hook that enforces server-side encryption on EBS volumes and SQS queues. Deploy the Hook across the accounts in the OU by using StackSets.",
        "B": "Set up AWS Config in all the accounts in the OU. Use AWS Systems Manager to deploy AWS Config rules that enforce server-side encryption for EBS volumes and SQS queues across the accounts in the OU.",
        "D": "Create an AWS Lambda function in the delegated administrator account that checks whether server-side encryption is enforced for EBS volumes and SQS queues. Create an IAM role to provide the Lambda function access to the accounts in the OU."
      },
      "correct_answer": "A",
      "answer_ET": "A",
      "answers_community": [
        "A (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/151849-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-11-22 20:48:00",
      "unix_timestamp": 1732304880,
      "discussion_count": 5,
      "discussion": [
        {
          "upvote_count": "1",
          "content": "Selected Answer: A\nThe company wants to enforce encryption at resource creation time before a CloudFormation stack operation is allowed. The best way to do that is by using CloudFormation Hooks, which can validate or block stack operations pre-deployment based on custom logic.\n\nCloudFormation Hooks allow you to enforce pre-provisioning checks.\nHooks can block stack creations or updates that don’t meet compliance (e.g., unencrypted EBS volumes or SQS queues).\nYou can deploy the Hook organization-wide using CloudFormation StackSets with trusted access enabled.\nThis provides automated, consistent enforcement across accounts in an OU.",
          "timestamp": "1743861180.0",
          "poster": "Srikantha",
          "comment_id": "1538121"
        },
        {
          "poster": "tubtab",
          "timestamp": "1735138740.0",
          "content": "Selected Answer: A\nKEYWORD enforce this policy before a CloudFormation stack operation",
          "upvote_count": "4",
          "comment_id": "1331605"
        },
        {
          "timestamp": "1734300420.0",
          "upvote_count": "4",
          "poster": "Ky_24",
          "comment_id": "1327080",
          "content": "Selected Answer: A\n• CloudFormation StackSets allows you to deploy a CloudFormation template across multiple AWS accounts and regions in your organization. By enabling trusted access to CloudFormation StackSets, you can manage resources and apply policies uniformly across multiple accounts within the OU.\n • A CloudFormation Hook is a way to enforce specific policies or checks during stack operations. In this case, you can create a Hook to ensure that all EBS volumes and SQS queues created or updated in the CloudFormation stack have server-side encryption enabled.\n • The StackSet and Hook can be deployed across all accounts in the specified OU, ensuring that server-side encryption is automatically enforced before any stack operation proceeds, thus satisfying the company’s policy."
        },
        {
          "comment_id": "1316634",
          "poster": "Changwha",
          "timestamp": "1732358520.0",
          "content": "Selected Answer: A\nThe answer is A",
          "upvote_count": "4"
        },
        {
          "timestamp": "1732304880.0",
          "poster": "f4b18ba",
          "comment_id": "1316475",
          "upvote_count": "4",
          "content": "Selected Answer: A\nCloudFormation Hooks allow you to intercept stack operations and perform validations or enforce policies before resources are created or updated.\nDevelop a CloudFormation Hook that checks whether EBS volumes and SQS queues in the CloudFormation templates have SSE enabled.\nUse CloudFormation StackSets with trusted access to deploy the Hook across all accounts in the OU.\nThe Hook will validate templates and prevent non-compliant resources from being created or updated during stack operations.\nApplies only to resources managed via CloudFormation, aligning with the company's requirement.\nCentralized Deployment: StackSets allow you to deploy the Hook across multiple accounts and regions efficiently.\nHooks do not interfere with non-CloudFormation operations, limiting the scope to what's required."
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:17:17.722Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "bOBx5npaWaJIPGWSmPQR",
      "question_number": 249,
      "page": 50,
      "question_text": "A company is running an internal application in an Amazon Elastic Container Service (Amazon ECS) cluster on Amazon EC2. The ECS cluster instances can connect to the public internet. The ECS tasks that run on the cluster instances are configured to use images from both private Amazon Elastic Container Registry (Amazon ECR) repositories and a public ECR registry repository.\n\nA new security policy requires the company to remove the ECS cluster's direct access to the internet. The company must remove any NAT gateways and internet gateways from the VPC that hosts the cluster. A DevOps engineer needs to ensure the ECS cluster can still download images from both the public ECR registry and the private ECR repositories. Images from the public ECR registry must remain up-to-date. New versions of the images must be available to the ECS cluster within 24 hours of publication.\n\nWhich combination of steps will meet these requirements with the LEAST operational overhead? (Choose three.)",
      "choices": {
        "C": "Create a new Amazon ECR pull through cache rule for the public ECR registry. Update each task definition in the ECS cluster to refer to the image from the pull through cache. Ensure each public image has been downloaded through the pull through cache at least once before removing internet access from the VPC.",
        "F": "Create an Amazon S3 gateway endpoint in the VPC.",
        "D": "Create an Amazon ECR interface VPC endpoint for the public ECR repositories that are in the VPC.",
        "E": "Create an Amazon ECR interface VPC endpoint for the private ECR repositories that are in the VPC.",
        "B": "Create a new Amazon ECR pull through cache rule for each image that is downloaded from the public ECR registry. Create an AWS Lambda function that invokes each pull through cache rule. Create an Amazon EventBridge rule that invokes the Lambda function once every 24 hours. Update each task definition in the ECS cluster to refer to the image from the pull through cache.",
        "A": "Create an AWS CodeBuild project and a new private ECR repository for each image that is downloaded from the public ECR registry. Configure each project to pull the image from the public ECR repository and push the image to the new private ECR repository. Create an Amazon EventBridge rule that invokes the CodeBuild project once every 24 hours. Update each task definition in the ECS cluster to refer to the new private ECR repository."
      },
      "correct_answer": "CEF",
      "answer_ET": "CEF",
      "answers_community": [
        "CEF (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/151565-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-11-18 23:40:00",
      "unix_timestamp": 1731969600,
      "discussion_count": 5,
      "discussion": [
        {
          "content": "Selected Answer: CEF\nC. ECR Pull Through Cache for Public Images\n\nThis allows public ECR images to be cached in your private ECR.\nYou avoid internet access by referencing the cached copy.\nOnce an image is pulled through the cache, it's stored in private ECR and available for future use even without internet access.\nEnsures that new versions can still be fetched (within 24h of publication), assuming it's referenced and updated in the cache.\nLeast operational overhead: No need for custom pipelines or daily sync jobs.\nE. Interface VPC Endpoint for Private ECR\n\nNeeded to let ECS pull private images without internet or NAT gateway.\nInterface VPC endpoints connect your VPC to ECR APIs via AWS PrivateLink.\nF. S3 Gateway Endpoint\n\nECR stores image layers in Amazon S3.\nWhen ECS pulls an image, it downloads layers from S3 — so the cluster needs S3 access even if ECR is private.\nA gateway endpoint provides private S3 access from the VPC.",
          "upvote_count": "2",
          "comment_id": "1538172",
          "timestamp": "1743861300.0",
          "poster": "Srikantha"
        },
        {
          "upvote_count": "3",
          "comment_id": "1327991",
          "poster": "teo2157",
          "timestamp": "1734449220.0",
          "content": "Selected Answer: CEF\nGoing with CEF as well"
        },
        {
          "upvote_count": "4",
          "content": "Selected Answer: CEF\nBy using an Amazon ECR pull through cache rule (Option C) and setting up the necessary VPC endpoints for private ECR (Option E) and S3 (Option F), the company can:\n\nEliminate Internet Access:\nRemove NAT gateways and internet gateways from the VPC.\nMaintain Image Access:\nAllow ECS tasks to pull images from both private and public ECR repositories without internet access.\nEnsure Image Updates:\nAutomatically receive updates to public images within 24 hours via the pull through cache.\nMinimize Operational Overhead:\nAvoid complex setups with additional services like CodeBuild, Lambda, or custom scripts.",
          "comment_id": "1316479",
          "poster": "f4b18ba",
          "timestamp": "1732305540.0"
        },
        {
          "content": "C, E, F\nhttps://docs.aws.amazon.com/AmazonECR/latest/userguide/vpc-endpoints.html\n\nVPC endpoints currently don't support Amazon ECR Public repositories. Consider using a pull through cache rule to host the public image in a private repository in the same Region as the VPC endpoint. For more information\n\nThe image metadata and layers in the ECR are stored in Amazon S3.\nCreating an S3 Gateway endpoint enables the ECS cluster to exchange data between ECR and S3 without the internet.",
          "timestamp": "1732087680.0",
          "comment_id": "1315101",
          "poster": "rainwalker",
          "upvote_count": "3"
        },
        {
          "poster": "uncledana",
          "content": "By implementing the pull through cache rule and setting up VPC endpoints for both public and private ECR repositories, the ECS cluster can securely access required container images without direct internet access. This approach ensures compliance with the security policy while maintaining operational efficiency and timely updates to images.",
          "upvote_count": "2",
          "comment_id": "1314267",
          "timestamp": "1731969600.0"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:17:17.722Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "8FjLYy9YELEJFX0athgj",
      "question_number": 250,
      "page": 50,
      "question_text": "A company has a continuous integration pipeline where the company creates container images by using AWS CodeBuild. The created images are stored in Amazon Elastic Container Registry (Amazon ECR).\n\nChecking for and fixing the vulnerabilities in the images takes the company too much time. The company wants to identify the image vulnerabilities quickly and notify the security team of the vulnerabilities.\n\nWhich combination of steps will meet these requirements with the LEAST operational overhead? (Choose two.)",
      "choices": {
        "B": "Create an Amazon EventBridge rule for Amazon Inspector findings. Set an Amazon Simple Notification Service (Amazon SNS) topic as the rule target.",
        "C": "Activate AWS Lambda enhanced scanning for Amazon ECR. Configure the enhanced scanning to use continuous scanning. Set up a topic in Amazon Simple Email Service (Amazon SES).",
        "E": "Activate default basic scanning for Amazon ECR for all container images. Configure the default basic scanning to use continuous scanning. Set up a topic in Amazon Simple Notification Service (Amazon SNS).",
        "A": "Activate Amazon Inspector enhanced scanning for Amazon ECR. Configure the enhanced scanning to use continuous scanning. Set up a topic in Amazon Simple Notification Service (Amazon SNS).",
        "D": "Create a new AWS Lambda function. Invoke the new Lambda function when scan findings are detected."
      },
      "correct_answer": "AB",
      "answer_ET": "AB",
      "answers_community": [
        "AB (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/153128-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-12-17 16:32:00",
      "unix_timestamp": 1734449520,
      "discussion_count": 2,
      "discussion": [
        {
          "comment_id": "1538259",
          "timestamp": "1743861480.0",
          "content": "Selected Answer: AB\n✅ A. Amazon Inspector Enhanced Scanning + SNS\nAmazon Inspector now supports enhanced container image scanning.\nIt can be configured to use continuous scanning, which means images are scanned as soon as they are pushed or updated in ECR.\nSNS is used to send notifications (email, SMS, or to other systems).\nThis setup requires minimal configuration and provides automated security insights.\n✅ B. EventBridge rule for Inspector findings\nInspector findings are emitted as EventBridge events.\nCreating an EventBridge rule to trigger SNS when findings occur allows you to immediately alert the security team.\nNo need for custom logic — EventBridge to SNS is low-maintenance and scalable.",
          "poster": "Srikantha",
          "upvote_count": "1"
        },
        {
          "content": "Selected Answer: AB\nIt's AB based on this https://docs.aws.amazon.com/AmazonECR/latest/userguide/image-scanning-enhanced.html",
          "timestamp": "1734449520.0",
          "poster": "teo2157",
          "comment_id": "1327997",
          "upvote_count": "4"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:17:17.722Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "5hcCoTHTIJ6XUabxhn4s",
      "question_number": 251,
      "page": 51,
      "question_text": "A DevOps administrator is configuring a repository to store a company's container images. The administrator needs to configure a lifecycle rule that automatically deletes container images that have a specific tag and that are older than 15 days.\n\nWhich solution will meet these requirements with the MOST operational efficiency?",
      "choices": {
        "B": "Create a repository in AWS CodeArtifact. Add a repository policy to the CodeArtifact repository to expire old assets that have the matching tag after 15 days.",
        "D": "Create an EC2 Image Builder container recipe. Add a build component to expire the container that has the matching tag after 15 days.",
        "C": "Create a bucket in Amazon S3. Add a bucket lifecycle policy to expire old objects that have the matching tag after 15 days",
        "A": "Create a repository in Amazon Elastic Container Registry (Amazon ECR). Add a lifecycle policy to the repository to expire images that have the matching tag after 15 days."
      },
      "correct_answer": "A",
      "answer_ET": "A",
      "answers_community": [
        "A (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/151567-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-11-18 23:51:00",
      "unix_timestamp": 1731970260,
      "discussion_count": 3,
      "discussion": [
        {
          "timestamp": "1743861540.0",
          "comment_id": "1538283",
          "poster": "Srikantha",
          "upvote_count": "2",
          "content": "Selected Answer: A\nAmazon ECR supports lifecycle policies that automatically manage the retention of container images. You can:\n\nMatch images by tags, including specific values or untagged.\nSpecify age conditions like deleting images older than 15 days.\nSet up the policy once for automatic cleanup, requiring minimal ongoing maintenance.\nThis is the most operationally efficient solution, purpose-built for container image lifecycle management."
        },
        {
          "timestamp": "1735104960.0",
          "content": "Selected Answer: A\nA. Create a repository in Amazon Elastic Container Registry (Amazon ECR).",
          "upvote_count": "4",
          "comment_id": "1331374",
          "poster": "matt200"
        },
        {
          "timestamp": "1731970260.0",
          "poster": "uncledana",
          "content": "The requirement is to automatically manage container images based on a specific tag and age. Amazon Elastic Container Registry (Amazon ECR) supports lifecycle policies, which are specifically designed to automate the management of image lifecycle events like expiration. Lifecycle policies in ECR allow you to define rules for expiring or retaining images based on criteria like image tags and age, providing an efficient solution with minimal operational overhead.",
          "comment_id": "1314269",
          "upvote_count": "3"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:17:28.292Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "LlL9haLAxrNgqL9pvcQg",
      "question_number": 252,
      "page": 51,
      "question_text": "A company uses Amazon Redshift as its data warehouse solution. The company wants to create a dashboard to view changes to the Redshift users and the queries the users perform.\n\nWhich combination of steps will meet this requirement? (Choose two.)",
      "choices": {
        "E": "Create an AWS Lambda function that uses Amazon Athena to query the Redshift logs. Create an Amazon CloudWatch dashboard that has a custom widget type that uses the Lambda function.",
        "C": "Configure the Redshift cluster database audit logging to include user activity logs. Configure Amazon CloudWatch as the target.",
        "A": "Create an Amazon CloudWatch log group. Create an AWS CloudTrail trail that writes to the CloudWatch log group.",
        "B": "Create a new Amazon S3 bucket. Configure default audit logging on the Redshift cluster. Configure the S3 bucket as the target.",
        "D": "Create an Amazon CloudWatch dashboard that has a log widget. Configure the widget to display user details from the Redshift logs."
      },
      "correct_answer": "CD",
      "answer_ET": "CD",
      "answers_community": [
        "CD (75%)",
        "BE (19%)",
        "6%"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/150837-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-11-06 06:10:00",
      "unix_timestamp": 1730869800,
      "discussion_count": 7,
      "discussion": [
        {
          "poster": "pma17",
          "comment_id": "1321814",
          "timestamp": "1733308860.0",
          "content": "Selected Answer: CD\nTo log users queries you need to include user activity logs which require additional setup on the Redshift cluster. So C, because it is not mentioned on B.\nThen to display this logs the only option is D, because E requires the logs to be on S3.",
          "upvote_count": "5"
        },
        {
          "content": "Selected Answer: BE\nTo track Redshift user activity and query execution, you need to enable Redshift audit logging. This allows you to analyze:\n\nUser connections and disconnections\nSQL queries run\nChanges to database users and privileges\nB. Enable Redshift Audit Logging to S3\n\nRedshift supports audit logging which includes user activity, connection logs, and user changes.\nLogs are delivered to an Amazon S3 bucket, which you can then query using tools like Athena.\nThis is the most efficient and supported way to persist and analyze historical Redshift activity.\nE. Query logs via Athena + Display on CloudWatch Dashboard\n\nOnce Redshift logs are in S3, you can set up an Athena table to query them.\nYou can use an AWS Lambda function to run these queries.\nA custom widget in an Amazon CloudWatch Dashboard can call the Lambda and display the results, such as recent user actions or frequent queries.\nThis enables a near-real-time, visual dashboard with low operational overhead.",
          "poster": "Srikantha",
          "timestamp": "1743861840.0",
          "comment_id": "1538399",
          "upvote_count": "2"
        },
        {
          "poster": "tdlAws",
          "timestamp": "1737562260.0",
          "content": "Selected Answer: BE\nSetting up the default audit log on your Redshift cluster will allow you to record database activity, including user changes and queries performed.\nSpecifying an Amazon S3 bucket as the destination will store the logs for later analysis.\nUsing Amazon Athena to query the logs stored in S3 allows you to create custom queries and extract the relevant data.\nThe Amazon CloudWatch dashboard with a custom AWS Lambda-based widget can display the information directly on the dashboard.",
          "comment_id": "1344858",
          "upvote_count": "1"
        },
        {
          "timestamp": "1732448280.0",
          "poster": "Impromptu",
          "upvote_count": "4",
          "content": "Selected Answer: CD\naudit logging can be sent to cloudwatch logs. So easier to just have them in cloudwatch and make a dashboard",
          "comment_id": "1317015"
        },
        {
          "poster": "siheom",
          "timestamp": "1732080060.0",
          "upvote_count": "1",
          "comment_id": "1315054",
          "content": "Selected Answer: BC\nvote BC"
        },
        {
          "poster": "uncledana",
          "upvote_count": "1",
          "content": "B. Create a new Amazon S3 bucket. Configure default audit logging on the Redshift cluster. Configure the S3 bucket as the target.\nE. Create an AWS Lambda function that uses Amazon Athena to query the Redshift logs. Create an Amazon CloudWatch dashboard that has a custom widget type that uses the Lambda function.\n\nExplanation:\n\nTo create a dashboard for viewing changes to Amazon Redshift users and the queries they perform, you need to capture the necessary audit logs and process them into a dashboard-friendly format.",
          "timestamp": "1731970500.0",
          "comment_id": "1314272"
        },
        {
          "poster": "tinyshare",
          "timestamp": "1731654120.0",
          "comment_id": "1312474",
          "upvote_count": "3",
          "content": "Selected Answer: CD\nYou need to use CloudWatch for dashboard, so the target must be CloudWatch. B is wrong.\nCloudTrail does not record user queries. A is wrong.\nUse Lambda to create your own solution is not recommended, E is wrong."
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:17:28.292Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "IpG3J2OvYqjFAMWevqRE",
      "question_number": 253,
      "page": 51,
      "question_text": "A company uses an organization in AWS Organizations to manage its 500 AWS accounts. The organization has all features enabled. The AWS accounts are in a single OU. The developers need to use the CostCenter tag key for all resources in the organization's member accounts. Some teams do not use the CostCenter tag key to tag their Amazon EC2 instances.\n\nThe cloud team wrote a script that scans all EC2 instances in the organization's member accounts. If the EC2 instances do not have a CostCenter tag key, the script will notify AWS account administrators. To avoid this notification, some developers use the CostCenter tag key with an arbitrary string in the tag value.\n\nThe cloud team needs to ensure that all EC2 instances in the organization use a CostCenter tag key with the appropriate cost center value.\n\nWhich solution will meet these requirements?",
      "choices": {
        "A": "Create an SCP that prevents the creation of EC2 instances without the CostCenter tag key. Create a tag policy that requires the CostCenter tag to be values from a known list of cost centers for all EC2 instances. Attach the policy to the OU. Update the script to scan the tag keys and tag values. Modify the script to update noncompliant resources with a default approved tag value for the CostCenter tag key.",
        "D": "Create a tag policy that requires the CostCenter tag to be values from a known list of cost centers for all EC2 instances. Attach the policy to the OU. Configure an AWS Lambda function that adds an empty CostCenter tag key to an EC2 instance. Create an Amazon EventBridge rule that matches events to the RunInstances API action with the Lambda function as the target.",
        "B": "Create an SCP that prevents the creation of EC2 instances without the CostCenter tag key. Attach the policy to the OU. Update the script to scan the tag keys and tag values and notify the administrators when the tag values are not valid.",
        "C": "Create an SCP that prevents the creation of EC2 instances without the CostCenter tag key. Attach the policy to the OU. Create an IAM permission boundary in the organization's member accounts that restricts the CostCenter tag values to a list of valid cost centers."
      },
      "correct_answer": "A",
      "answer_ET": "A",
      "answers_community": [
        "A (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/151850-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-11-22 21:18:00",
      "unix_timestamp": 1732306680,
      "discussion_count": 3,
      "discussion": [
        {
          "timestamp": "1732306680.0",
          "content": "Selected Answer: A\nService Control Policy (SCP): Creating an SCP ensures that any attempt to create EC2 instances without the CostCenter tag key is denied right from the start. This enforces the requirement at the organizational level.\n\nTag Policy: By creating a tag policy that enforces the CostCenter tag values to be from a known list, you can ensure that only valid cost center values are used across all EC2 instances.\n\nScript Update: Updating the script to not only scan for tag keys and values but also to update noncompliant resources with a default approved tag value ensures compliance and mitigates the issue of arbitrary string values.\n\nComprehensive Solution: This approach addresses both the presence of the CostCenter tag and the correctness of its value, providing a comprehensive solution to the problem.",
          "comment_id": "1316482",
          "upvote_count": "6",
          "poster": "f4b18ba"
        },
        {
          "content": "Selected Answer: A\nSCP to require the CostCenter tag key:\nPrevents users from launching EC2 instances without the required tag.\nTag policy to validate values:\nAWS tag policies can enforce allowed values for tag keys like CostCenter, across all accounts in an OU.\nScript enhancement for compliance:\nScript detects noncompliant resources and applies a default value, maintaining tag integrity and reducing manual intervention.",
          "poster": "Srikantha",
          "upvote_count": "2",
          "comment_id": "1538542",
          "timestamp": "1743862200.0"
        },
        {
          "content": "Selected Answer: A\nThe answer is A",
          "poster": "Changwha",
          "comment_id": "1316637",
          "timestamp": "1732358880.0",
          "upvote_count": "4"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:17:28.292Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "5h6rRjgxrbDy30W2z5x3",
      "question_number": 254,
      "page": 51,
      "question_text": "A DevOps engineer uses a pipeline in AWS CodePipeline. The pipeline has a build action and a deploy action for a single-page web application that is delivered to an Amazon S3 bucket. Amazon CloudFront serves the web application. The build action creates an artifact for the web application.\n\nThe DevOps engineer has created an AWS CloudFormation template that defines the S3 bucket and configures the S3 bucket to host the application. The DevOps engineer has configured a CloudFormation deploy action before the S3 action. The CloudFormation deploy action creates the S3 bucket. The DevOps engineer needs to configure the S3 deploy action to use the S3 bucket from the CloudFormation template.\n\nWhich combination of steps will meet these requirements? (Choose two.)",
      "choices": {
        "E": "Configure the build artifact from the build action and the AWS Systems Manager parameter as the inputs to the deploy action. Configure the deploy action to deploy to the S3 bucket by using the StackVariables.BucketName variable.",
        "D": "Configure the build artifact from the build action as the input to the CodePipeline S3 deploy action. Configure the deploy action to deploy to the S3 bucket by using the StackVariables.BucketName variable.",
        "A": "Add an output named BucketName to the CloudFormation template. Set the output's value to refer to the S3 bucket from the CloudFormation template. Configure the output value to export to an AWS::SSM::Parameter resource named Stackvariables.",
        "C": "Configure the output artifacts of the CloudFormation action in the pipeline to be an AWS Systems Manager Parameter Store parameter named StackVariables. Name the artifact BucketName.",
        "B": "Add an output named BucketName to the CloudFormation template. Set the output's value to refer to the S3 bucket from the CloudFormation template. Set the CloudFormation action's namespace to StackVariables in the pipeline."
      },
      "correct_answer": "BD",
      "answer_ET": "BD",
      "answers_community": [
        "BD (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/151568-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-11-19 00:15:00",
      "unix_timestamp": 1731971700,
      "discussion_count": 3,
      "discussion": [
        {
          "timestamp": "1732359240.0",
          "poster": "Changwha",
          "content": "Selected Answer: BD\n1. Define a BucketName output in the CloudFormation template and set the CloudFormation action’s namespace in the pipeline to StackVariables.\n2. Use the StackVariables.BucketName variable in the S3 deploy action to specify the S3 bucket dynamically, while using the build artifact as input.",
          "comment_id": "1316640",
          "upvote_count": "3"
        },
        {
          "poster": "f4b18ba",
          "upvote_count": "3",
          "comment_id": "1316486",
          "timestamp": "1732307520.0",
          "content": "Selected Answer: BD\nOption B:\nAdding an output to the CloudFormation template allows the pipeline to extract the S3 bucket name created during the stack deployment.\nSetting the namespace (StackVariables) in the CloudFormation action enables other actions in the pipeline to reference the output values using that namespace.\nOption D:\n\nThe build artifact needs to be input to the S3 deploy action to provide the files for deployment.\nBy using StackVariables.BucketName, the deploy action dynamically references the bucket created by the CloudFormation stack, ensuring that the files are deployed to the correct location."
        },
        {
          "content": "Selected Answer: BD\nTo dynamically deploy a single-page web application to an S3 bucket created by a CloudFormation deploy action in AWS CodePipeline, you should:\n\n 1. Define a BucketName output in the CloudFormation template and set the CloudFormation action’s namespace in the pipeline to StackVariables.\n 2. Use the StackVariables.BucketName variable in the S3 deploy action to specify the S3 bucket dynamically, while using the build artifact as input.\n\nThis approach ensures the pipeline is correctly configured to handle dynamic resource creation with minimal complexity.",
          "timestamp": "1731971700.0",
          "poster": "uncledana",
          "comment_id": "1314280",
          "upvote_count": "3"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:17:28.292Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "ujrrhOzwLSDMTW2jMJun",
      "question_number": 255,
      "page": 51,
      "question_text": "A company used a lift and shift strategy to migrate a workload to AWS. The company has an Auto Scaling group of Amazon EC2 instances. Each EC2 instance runs a web application, a database, and a Redis cache.\n\nUsers are experiencing large variations in the web application's response times. Requests to the web application go to a single EC2 instance that is under significant load. The company wants to separate the application components to improve availability and performance.\n\nWhich solution will meet these requirements?",
      "choices": {
        "D": "Create an Application Load Balancer and an Auto Scaling group for the web application. Migrate the database to an Amazon Aurora database that has a Multi-AZ deployment. Create an Amazon ElastiCache (Redis OSS) cluster for the cache.",
        "A": "Create a Network Load Balancer and an Auto Scaling group for the web application. Migrate the database to an Amazon Aurora Serverless database. Create an Application Load Balancer and an Auto Scaling group for the Redis cache.",
        "B": "Create an Application Load Balancer and an Auto Scaling group for the web application. Migrate the database to an Amazon Aurora database that has a Multi-AZ deployment. Create a Network Load Balancer and an Auto Scaling group in a single Availability Zone for the Redis cache.",
        "C": "Create a Network Load Balancer and an Auto Scaling group for the web application. Migrate the database to an Amazon Aurora Serverless database. Create an Amazon ElastiCache (Redis OSS) cluster for the cache. Create a target group that has a DNS target type that contains the ElastiCache (Redis OSS) cluster hostname."
      },
      "correct_answer": "D",
      "answer_ET": "D",
      "answers_community": [
        "D (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/153431-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-12-25 06:37:00",
      "unix_timestamp": 1735105020,
      "discussion_count": 2,
      "discussion": [
        {
          "poster": "matt200",
          "content": "Selected Answer: D\nshould be D",
          "timestamp": "1735105020.0",
          "comment_id": "1331375",
          "upvote_count": "5"
        },
        {
          "poster": "Srikantha",
          "content": "Selected Answer: D\nThis solution separates the components of the application to improve availability and performance while addressing the specific issues the company is facing, such as variations in response times and heavy load on a single EC2 instance.\n\nKey steps in this solution:\n\nApplication Load Balancer (ALB):\nDistributes incoming web traffic across multiple EC2 instances in an Auto Scaling group.\nEnsures high availability and horizontal scaling of the web application.\nAmazon Aurora (Multi-AZ):\nAurora is a fully managed relational database service, and using Multi-AZ deployment ensures high availability and automatic failover.\nHelps offload the database work from the EC2 instances.\nAmazon ElastiCache (Redis):\nOffloads caching from the EC2 instances to ElastiCache (Redis OSS), which is purpose-built for high-performance caching and can handle load more effectively than having Redis run on each EC2 instance.\nImproves response times by reducing the database load.",
          "timestamp": "1743864300.0",
          "comment_id": "1539970",
          "upvote_count": "2"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:17:28.292Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "mp7nCwUll94QvqPTBash",
      "question_number": 256,
      "page": 52,
      "question_text": "A company is using AWS Organizations and wants to implement a governance strategy with the following requirements:\n\n• AWS resource access is restricted to the same two Regions for all accounts.\n• AWS services are limited to a specific group of authorized services for all accounts.\n• Authentication is provided by Active Directory.\n• Access permissions are organized by job function and are identical in each account.\n\nWhich solution will meet these requirements?",
      "choices": {
        "B": "Establish a permission boundary in the management account to restrict Regions and authorized services. Use AWS CloudFormation StackSets to provision roles with permissions for each job function, including an IAM trust policy for IAM identity provider authentication in each account.",
        "D": "Establish a service control policy in the management account to restrict Regions and authorized services. Use AWS CloudFormation StackSets to provision roles with permissions for each job function, including an IAM trust policy for IAM identity provider authentication in each account.",
        "C": "Establish a service control policy in the management account to restrict Regions and authorized services. Use AWS Resource Access Manager (AWS RAM) to share management account roles with permissions for each job function, including AWS IAM Identity Center for authentication in each account.",
        "A": "Establish an organizational unit (OU) with group policies in the management account to restrict Regions and authorized services. Use AWS CloudFormation StackSets to provision roles with permissions for each job function, including an IAM trust policy for IAM identity provider authentication in each account."
      },
      "correct_answer": "D",
      "answer_ET": "D",
      "answers_community": [
        "D (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/153432-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-12-25 06:37:00",
      "unix_timestamp": 1735105020,
      "discussion_count": 2,
      "discussion": [
        {
          "content": "Selected Answer: D\nService Control Policies (SCPs):\nSCPs are used in AWS Organizations to apply restrictions at the organization level to control which AWS Regions and services can be used.\nSCPs will enforce the policy across all accounts in the organization, ensuring that resource access is restricted to only the allowed Regions and services.\nCloudFormation StackSets:\nAWS CloudFormation StackSets are used to automatically create and maintain roles and permissions across all accounts.\nThis allows for standardized job-function-based roles to be created consistently in each account, with the exact same structure, regardless of account.\nIAM Trust Policy with Active Directory:\nThe solution includes an IAM trust policy that allows Active Directory to authenticate users. This ensures that access is controlled by user identity and role, according to the organizational job functions.",
          "poster": "Srikantha",
          "comment_id": "1540247",
          "timestamp": "1743864540.0",
          "upvote_count": "2"
        },
        {
          "poster": "matt200",
          "comment_id": "1331376",
          "timestamp": "1735105020.0",
          "upvote_count": "4",
          "content": "Selected Answer: D\nshould be D"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:17:38.726Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "tvsG6f9KFKSw3VhrEZUF",
      "question_number": 257,
      "page": 52,
      "question_text": "A company that uses electronic health records is running a fleet of Amazon EC2 instances with an Amazon Linux operating system. As part of patient privacy requirements, the company must ensure continuous compliance for patches for operating system and applications running on the EC2 instances.\nHow can the deployments of the operating system and application patches be automated using a default and custom repository?",
      "choices": {
        "B": "Use AWS Direct Connect to integrate the corporate repository and deploy the patches using Amazon CloudWatch scheduled events, then use the CloudWatch dashboard to create reports.",
        "A": "Use AWS Systems Manager to create a new patch baseline including the custom repository. Run the AWS-RunPatchBaseline document using the run command to verify and install patches.",
        "C": "Use yum-config-manager to add the custom repository under /etc/yum.repos.d and run yum-config-manager-enable to activate the repository.",
        "D": "Use AWS Systems Manager to create a new patch baseline including the corporate repository. Run the AWS-AmazonLinuxDefaultPatchBaseline document using the run command to verify and install patches."
      },
      "correct_answer": "A",
      "answer_ET": "A",
      "answers_community": [
        "A (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/106198-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-04-14 20:01:00",
      "unix_timestamp": 1681495260,
      "discussion_count": 8,
      "discussion": [
        {
          "timestamp": "1724125500.0",
          "poster": "dzn",
          "comment_id": "1154505",
          "content": "Selected Answer: A\nAWS-AmazonLinuxDefaultPatchBaseline: defines which patches should be applied and which should be avoided.\nAWS-RunPatchBaseline: provides commands to actually run the patching process on the instance.",
          "upvote_count": "6"
        },
        {
          "content": "A is correct: AWS system manager and AWS-RunPatchBaseline to utilize a default and custom repo\nB and C are irrelevant\nD: AWS-AmazonLinuxDefaultPatchBaseline: this baseline has \"default\" in its name, it is a predefined baseline and cannot work with a custom repo",
          "upvote_count": "4",
          "poster": "thanhnv142",
          "comment_id": "1135140",
          "timestamp": "1722266040.0"
        },
        {
          "timestamp": "1720258920.0",
          "content": "Here are predefined documents that can not be modified (includes AWS-AmazonLinuxDefaultPatchBaseline)\nhttps://docs.aws.amazon.com/systems-manager/latest/userguide/patch-manager-predefined-and-custom-patch-baselines.html#patch-manager-baselines-custom\nAnd here is about the AWS-RunPatchBaseline\nhttps://docs.aws.amazon.com/systems-manager/latest/userguide/patch-manager-aws-runpatchbaseline.html",
          "comments": [
            {
              "content": "the Answer is A",
              "comment_id": "1115145",
              "poster": "davdan99",
              "upvote_count": "1",
              "timestamp": "1720258920.0"
            }
          ],
          "comment_id": "1115144",
          "poster": "davdan99",
          "upvote_count": "2"
        },
        {
          "poster": "z_inderjot",
          "content": "Selected Answer: A\nI was confused between A and D , i choose A instictinvily, D statement sounds like it only going to install default package for linux not from custom repo we add . But not sure any one can clarify",
          "upvote_count": "3",
          "timestamp": "1718940120.0",
          "comment_id": "1102197"
        },
        {
          "content": "Selected Answer: A\nA, SSM allows inclusion of custom repositories.",
          "timestamp": "1702651620.0",
          "upvote_count": "4",
          "poster": "madperro",
          "comment_id": "924191"
        },
        {
          "upvote_count": "1",
          "content": "Selected Answer: A\nA is it",
          "timestamp": "1698897600.0",
          "poster": "haazybanj",
          "comment_id": "886957"
        },
        {
          "upvote_count": "2",
          "comment_id": "870493",
          "content": "Selected Answer: A\nA is correct",
          "timestamp": "1697320080.0",
          "poster": "alce2020"
        },
        {
          "poster": "jqso234",
          "comment_id": "870386",
          "content": "Selected Answer: A\nTo automate the deployment of operating system and application patches using a default and custom repository in Amazon EC2 instances with Amazon Linux operating systems, you can use AWS Systems Manager. You can create a new patch baseline in Systems Manager that includes the custom repository, then run the AWS-RunPatchBaseline document using the run command to verify and install patches. This allows you to ensure continuous compliance for patches while also automating the patch deployment process.",
          "upvote_count": "1",
          "timestamp": "1697306460.0"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:17:38.726Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "qmmvWbbfe63hw5IIOIdt",
      "question_number": 258,
      "page": 52,
      "question_text": "A company detects unusual login attempts in many of its AWS accounts. A DevOps engineer must implement a solution that sends a notification to the company's security team when multiple failed login attempts occur. The DevOps engineer has already created an Amazon Simple Notification Service (Amazon SNS) topic and has subscribed the security team to the SNS topic.\n\nWhich solution will provide the notification with the LEAST operational effort?",
      "choices": {
        "A": "Configure AWS CloudTrail to send management events to an Amazon CloudWatch Logs log group. Create a CloudWatch Logs metric filter to match failed ConsoleLogin events. Create a CloudWatch alarm that is based on the metric filter. Configure an alarm action to send messages to the SNS topic.",
        "C": "Configure AWS CloudTrail to send data events to an Amazon CloudWatch Logs log group. Create a CloudWatch logs metric filter to match failed ConsoleLogin events. Create a CloudWatch alarm that is based on the metric filter. Configure an alarm action to send messages to the SNS topic.",
        "B": "Configure AWS CloudTrail to send management events to an Amazon S3 bucket. Create an Amazon Athena query that returns a failure if the query finds failed logins in the logs in the S3 bucket. Create an Amazon EventBridge rule to periodically run the query. Create a second EventBridge rule to detect when the query fails and to send a message to the SNS topic.",
        "D": "Configure AWS CloudTrail to send data events to an Amazon S3 bucket. Configure an Amazon S3 event notification for the s3:ObjectCreated event type. Filter the event type by ConsoleLogin failed events. Configure the event notification to forward to the SNS topic."
      },
      "correct_answer": "A",
      "answer_ET": "A",
      "answers_community": [
        "A (93%)",
        "7%"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/152488-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-12-03 08:52:00",
      "unix_timestamp": 1733212320,
      "discussion_count": 4,
      "discussion": [
        {
          "content": "Selected Answer: A\n\"A\" is indeed the most elegant and obvious solution.\n\"B\" might work but seems way more overcomplicated",
          "upvote_count": "6",
          "comment_id": "1324647",
          "poster": "eugene2owl",
          "timestamp": "1733847180.0"
        },
        {
          "content": "Selected Answer: A\nA as you can choose to send cloudtrail events to CloudWatch log groups.",
          "upvote_count": "5",
          "comment_id": "1328382",
          "timestamp": "1734516480.0",
          "poster": "teo2157"
        },
        {
          "upvote_count": "2",
          "poster": "Srikantha",
          "timestamp": "1743864660.0",
          "content": "Selected Answer: A\nThis solution leverages AWS CloudTrail for logging, CloudWatch Logs for capturing the log data, and CloudWatch Alarms for monitoring the failed login attempts, with SNS used for notifications. It provides the least operational effort for the following reasons:\n\nAWS CloudTrail captures management events, including failed login attempts (ConsoleLogin failures).\nThese events are sent to Amazon CloudWatch Logs, which is a straightforward way to centralize the log data for analysis.\nA CloudWatch Logs metric filter is created to match the ConsoleLogin failure events. This metric filter scans the CloudWatch logs for specific failed login attempts.\nCloudWatch Alarm is created based on the metric filter to trigger when there are multiple failed login attempts.\nThe alarm is configured to send a message to the SNS topic, notifying the security team.\nThis solution automates the detection of failed login attempts and provides a simple, efficient way to send notifications with minimal ongoing management.",
          "comment_id": "1540367"
        },
        {
          "poster": "On9son",
          "comment_id": "1321282",
          "content": "Selected Answer: B\nCloudTrail publishes log to S3. \nAnd management event contains login information\nhttps://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-events.html#cloudtrail-management-events",
          "upvote_count": "1",
          "timestamp": "1733212320.0"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:17:38.726Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "rWyCyWfEBxKMjrIz7NPy",
      "question_number": 259,
      "page": 52,
      "question_text": "A company has deployed a new REST API by using Amazon API Gateway. The company uses the API to access confidential data. The API must be accessed from only specific VPCs in the company.\n\nWhich solution will meet these requirements?",
      "choices": {
        "B": "Add a security group to the API Gateway API. Configure the inbound rules to allow only the specific VPC IP address ranges.",
        "C": "Create and attach an IAM role to the API Gateway API. Configure the IAM role to allow only the specific VPC IDs.",
        "A": "Create and attach a resource policy to the API Gateway API. Configure the resource policy to allow only the specific VPC IDs.",
        "D": "Add an ACL to the API Gateway API. Configure the outbound rules to allow only the specific VPC IP address ranges."
      },
      "correct_answer": "A",
      "answer_ET": "A",
      "answers_community": [
        "A (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/153039-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-12-16 14:06:00",
      "unix_timestamp": 1734354360,
      "discussion_count": 3,
      "discussion": [
        {
          "timestamp": "1743864780.0",
          "upvote_count": "1",
          "content": "Selected Answer: A\nAmazon API Gateway supports resource policies, which allow you to control who can access your API based on the source IP address, VPC ID, or even specific IP address ranges.\n\nIn this case, to restrict access to the API from only specific VPCs, you would create and attach a resource policy to the API Gateway. The resource policy allows you to specify which VPCs (via their VPC IDs) can access the API, ensuring that the API can only be accessed from the designated VPCs.\n\nThe resource policy is the most efficient and appropriate method for achieving this in API Gateway.",
          "poster": "Srikantha",
          "comment_id": "1540423"
        },
        {
          "poster": "CHRIS12722222",
          "upvote_count": "3",
          "timestamp": "1735405440.0",
          "content": "Selected Answer: A\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-private-api-create.html",
          "comment_id": "1333054"
        },
        {
          "content": "Selected Answer: A\nExplanation:\nAPI Gateway supports resource policies, which can restrict access based on specific conditions, such as VPC IDs or IP ranges. You can attach a resource policy to the API Gateway that allows access only from specific VPCs. This is the most direct and secure way to meet the requirement of allowing access only from specific VPCs.",
          "poster": "Ky_24",
          "timestamp": "1734354360.0",
          "upvote_count": "4",
          "comment_id": "1327372"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:17:38.726Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "E0G1N8bLCggGthh7GGIT",
      "question_number": 260,
      "page": 52,
      "question_text": "A company runs a website by using an Amazon Elastic Container Service (Amazon ECS) service that is connected to an Application Load Balancer (ALB). The service was in a steady state with tasks responding to requests successfully.\n\nA DevOps engineer updated the task definition with a new container image and deployed the new task definition to the service. The DevOps engineer noticed that the service is frequently stopping and starting new tasks because the ALB healtth checks are failing.\n\nWhat should the DevOps engineer do to troubleshoot the failed deployment?",
      "choices": {
        "B": "Increase the ALB health check grace period for the service.",
        "D": "Decrease the ALB health check interval.",
        "C": "Increase the service minimum healthy percent setting.",
        "A": "Ensure that a security group associated with the service allows traffic from the ALB."
      },
      "correct_answer": "B",
      "answer_ET": "B",
      "answers_community": [
        "B (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/150569-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-10-31 12:01:00",
      "unix_timestamp": 1730372460,
      "discussion_count": 5,
      "discussion": [
        {
          "poster": "Srikantha",
          "comment_id": "1540732",
          "content": "Selected Answer: B\nWhen the DevOps engineer deploys a new version of the ECS service (e.g., with a new container image), the ECS tasks might take some time to become healthy and respond to ALB health checks successfully. During this period, the ALB health checks could fail if they are not given enough time to detect the new tasks' readiness.\n\nBy increasing the ALB health check grace period, the engineer ensures that the new tasks have enough time to start up and become healthy before the ALB marks them as unhealthy. This can prevent the tasks from frequently stopping and starting due to health check failures.",
          "upvote_count": "2",
          "timestamp": "1743865200.0"
        },
        {
          "timestamp": "1742326560.0",
          "upvote_count": "1",
          "content": "Selected Answer: B\naws elbv2 modify-target-group-attributes \\\n --target-group-arn <target-group-arn> \\\n --attributes Key=health_check.grace_period,Value=<desired-value>",
          "comment_id": "1400261",
          "poster": "DKM"
        },
        {
          "timestamp": "1734354480.0",
          "content": "Selected Answer: B\nB - ALB health check is failing the new task",
          "comment_id": "1327377",
          "poster": "Ky_24",
          "upvote_count": "4"
        },
        {
          "upvote_count": "4",
          "timestamp": "1733848200.0",
          "poster": "eugene2owl",
          "content": "Selected Answer: B\n\"A\" is wrong because in the question it's said that Engineer has only updated image in the task definition. So he could not affect security group.\n\"B\" might be correct explanation, because new Docker container (based on a new image) takes longer time to start. So increasing a grace period might help to eventually get a successful health-check result and quit restarting the container.",
          "comment_id": "1324658"
        },
        {
          "content": "Selected Answer: B\nB",
          "timestamp": "1730372460.0",
          "upvote_count": "4",
          "poster": "awsarchitect5",
          "comment_id": "1305377"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:17:38.726Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "9Q2nE0nd5Nvd1wW5ZoCt",
      "question_number": 261,
      "page": 53,
      "question_text": "A company that uses electronic patient health records runs a fleet of Amazon EC2 instances with an Amazon Linux operating system. The company must continuously ensure that the EC2 instances are running operating system patches and application patches that are in compliance with current privacy regulations. The company uses a custom repository to store application patches.\n\nA DevOps engineer needs to automate the deployment of operating system patches and application patches. The DevOps engineer wants to use both the default operating system patch repository and the custom patch repository.\n\nWhich solution will meet these requirements with the LEAST effort?",
      "choices": {
        "D": "Use AWS Systems Manager to create a patch baseline for the default operating system repository and a second patch baseline for the custom repository. Run the AWS-RunPatchBaseline document by using the Run command to verify and install patches. Use the BaselineOverride API to configure the default patch baseline and the custom patch baseline.",
        "B": "Use AWS Direct Connect to integrate the custom repository with the EC2 instances. Use Amazon EventBridge events to deploy the patches.",
        "C": "Use the yum-config-manager command to add the custom repository to the /etc/yum.repos.d configuration. Run the yum-config-manager-enable command to activate the new repository.",
        "A": "Use AWS Systems Manager to create a new custom patch baseline that includes the default operating system repository and the custom repository. Run the AWS-RunPatchBaseline document by using the Run command to verify and install patches. Use the BaselineOverride API to configure the new custom patch baseline."
      },
      "correct_answer": "A",
      "answer_ET": "A",
      "answers_community": [
        "A (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/151818-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-11-22 09:50:00",
      "unix_timestamp": 1732265400,
      "discussion_count": 4,
      "discussion": [
        {
          "comment_id": "1316215",
          "poster": "phu0298",
          "content": "A\nWhy Not Option D?\nTwo Separate Patch Baselines:\nCreating and managing two separate patch baselines (one for OS patches and one for the custom repository) increases complexity.\nRunning AWS-RunPatchBaseline twice (once for each baseline) adds unnecessary operational overhead.",
          "upvote_count": "6",
          "timestamp": "1732265400.0"
        },
        {
          "poster": "Srikantha",
          "upvote_count": "2",
          "timestamp": "1743949140.0",
          "content": "Selected Answer: A\nD. Systems Manager does not support using two separate patch baselines at once. You must define a single patch baseline that includes all patch sources.",
          "comment_id": "1558269"
        },
        {
          "content": "Selected Answer: A\nTo automate patching while using both the default OS patch repo and a custom app patch repo, the most efficient and scalable solution is to:\n\nUse AWS Systems Manager Patch Manager, which supports:\nCustom patch baselines\nCombining default repositories with custom ones\nCentralized, automated patch management\nCreate a custom patch baseline that:\nIncludes the Amazon Linux OS patching rules\nDefines custom sources (your custom app patch repository)\nUse AWS-RunPatchBaseline SSM document to apply patches.\nOptionally, use the BaselineOverride parameter if you want to temporarily apply a different baseline (e.g., for testing).",
          "upvote_count": "2",
          "poster": "Srikantha",
          "comment_id": "1558268",
          "timestamp": "1743949080.0"
        },
        {
          "content": "Selected Answer: A\nAWS Systems Manager allows you to create a custom patch baseline that includes both the default operating system repository and the custom repository. This centralizes the management of patch baselines.\nThe AWS-RunPatchBaseline document can be run using the Systems Manager Run Command to automate the verification and installation of patches, ensuring compliance with current privacy regulations.\nUsing the BaselineOverride API provides flexibility to override the default settings with a custom patch baseline, streamlining the patching process across all EC2 instances.",
          "upvote_count": "4",
          "timestamp": "1732313940.0",
          "poster": "f4b18ba",
          "comment_id": "1316511"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:17:49.194Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "MmZu6QzFNZTl8EhSQJu5",
      "question_number": 262,
      "page": 53,
      "question_text": "A company use an organization in AWS Organizations to manage multiple AWS accounts. The company has enabled all features enabled for the organization. The company configured the organization as a hierarchy of OUs under the root OU. The company recently registered all its OUs and enrolled all its AWS accounts in AWS Control Tower.\n\nThe company needs to customize the AWS Control Tower managed AWS Config configuration recorder in each of the company's AWS accounts. The company needs to apply the customizations to both the existing AWS accounts and to any new AWS accounts that the company enrolls in AWS Control Tower in the future.\n\nWhich combination of steps will meet these requirements? (Choose three.)",
      "choices": {
        "B": "Create a new AWS account as an AWS Config delegated administrator. Create an AWS Lambda function in the delegated administrator account to apply the customizations to the AWS Config configuration recorder in the delegated administrator account.",
        "C": "Configure an Amazon EventBridge rule in the AWS Control Tower management account to invoke an AWS Lambda function when the Organizations OU is registered or reregistered. Re-register the root Organizations OU.",
        "F": "Configure an Amazon EventBridge rule in the AWS Control Tower management account to invoke an AWS Lambda function when an AWS account is updated or enrolled in AWS Control Tower or when the landing zone is updated. Re-register each Organizations OU in the organization.",
        "A": "Create a new AWS account. Create an AWS Lambda function in the new account to apply the customizations to the AWS Config configuration recorder in each AWS account in the organization.",
        "D": "Configure the AWSControlTowerExecution IAM role in each AWS account in the organization to be assumable by an AWS Lambda function. Configure the Lambda function to assume the AWSControlTowerExecution IAM role.",
        "E": "Create an IAM role in the AWS Control Tower management account that an AWS Lambda function can assume. Grant the IAM role permission to assume the AWSControlTowerExecution IAM role in any account in the organization. Configure the Lambda function to use the new IAM role."
      },
      "correct_answer": "AEF",
      "answer_ET": "AEF",
      "answers_community": [
        "AEF (63%)",
        "13%",
        "13%",
        "13%"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/151820-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-11-22 10:23:00",
      "unix_timestamp": 1732267380,
      "discussion_count": 6,
      "discussion": [
        {
          "timestamp": "1743949860.0",
          "poster": "Srikantha",
          "content": "Selected Answer: BEF\n❌ Why not the others?\nA. While a Lambda in a new account can work, it’s better to use a delegated admin for AWS Config management.\nC. EventBridge doesn’t emit events when an OU is registered or re-registered. This is not a valid trigger.\nD. Modifying the AWSControlTowerExecution role in each account breaks the Control Tower managed roles and is not recommended.",
          "upvote_count": "1",
          "comment_id": "1558271"
        },
        {
          "timestamp": "1742996400.0",
          "content": "Selected Answer: BDF\nThese steps ensure that the customizations are applied consistently across all existing and new AWS accounts, leveraging the delegated administrator account for centralized management and automation123.",
          "poster": "DKM",
          "upvote_count": "1",
          "comment_id": "1410378"
        },
        {
          "comment_id": "1337014",
          "content": "Selected Answer: CEF\nC covers OU registration/re-registration events⁠\nF handles account enrollment, updates, and landing zone changes⁠\n⁠E's role in providing the necessary IAM permissions structure:\nCreates proper IAM role in Control Tower management account\nEnables assumption of AWSControlTowerExecution IAM role across accounts",
          "timestamp": "1736150160.0",
          "poster": "fcbflo",
          "upvote_count": "1"
        },
        {
          "poster": "CHRIS12722222",
          "timestamp": "1735466760.0",
          "comment_id": "1333444",
          "upvote_count": "3",
          "content": "Selected Answer: AEF\nhttps://aws.amazon.com/solutions/guidance/customizing-aws-config-resources-in-aws-control-tower/\n\n- Need eventbridge in CT management acct to react to CT lifecycle events\n- need CT management acct lambda function to assume AWSControlTowerExecution role and customise config. \n- If lambda is not in CT management acct then it will need to assume a role in CT management acct which has trust with AWSControlTowerExecution role in member accts"
        },
        {
          "content": "Selected Answer: AEF\nI think there is a misspelling in the A option as it's said just \"Create a new AWS account\" when it should said \" Create a new AWS account as an AWS Config delegated administrator.\", said that, I go for AEF.",
          "poster": "teo2157",
          "timestamp": "1734519540.0",
          "upvote_count": "2",
          "comment_id": "1328413"
        },
        {
          "upvote_count": "3",
          "poster": "phu0298",
          "comment_id": "1316222",
          "timestamp": "1732267380.0",
          "content": "B, E, and F.\nB: AWS Config supports delegated administrators, allowing a central account to manage configurations across the organization.\nBy creating a Lambda function in the delegated administrator account, you can apply the customizations to the AWS Config configuration recorder in all member accounts centrally.\n\nE: The AWSControlTowerExecution IAM role exists in each enrolled account and allows centralized operations.\nThe IAM role in the management account needs permissions to assume the AWSControlTowerExecution role in member accounts.\n\nF: AWS Control Tower emits events when an account is enrolled or updated, or when the landing zone is updated.\nAn EventBridge rule can trigger the Lambda function to ensure that any new or updated accounts automatically receive the customizations.\nRe-registering each OU ensures that Control Tower applies its governance to all accounts and OUs consistently."
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:17:49.195Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "bt05AyVF94XL0z97Bl9U",
      "question_number": 263,
      "page": 53,
      "question_text": "A company runs an application in an Auto Scaling group of Amazon EC2 instances behind an Application Load Balancer (ALB). The EC2 instances run Docker containers that make requests to a MySQL database that runs on separate EC2 instances.\n\nA DevOps engineer needs to update the application to use a serverless architecture.\n\nWhich solution will meet this requirement with the FEWEST changes?",
      "choices": {
        "B": "Replace the containers that run on EC2 instances with AWS Fargate. Replace the MySQL database with an Amazon Aurora Serverless v2 database that is compatible with MySQL.",
        "C": "Replace the containers that run on EC2 instances and the ALB with AWS Lambda functions. Replace the MySQL database with Amazon DynamoDB tables.",
        "D": "Replace the containers that run on EC2 instances with AWS Fargate. Replace the MySQL database with Amazon DynamoDB tables.",
        "A": "Replace the containers that run on EC2 instances and the ALB with AWS Lambda functions. Replace the MySQL database with an Amazon Aurora Serverless v2 database that is compatible with MySQL."
      },
      "correct_answer": "B",
      "answer_ET": "B",
      "answers_community": [
        "B (86%)",
        "14%"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/152811-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-12-10 17:42:00",
      "unix_timestamp": 1733848920,
      "discussion_count": 3,
      "discussion": [
        {
          "poster": "Srikantha",
          "comment_id": "1550484",
          "timestamp": "1743883860.0",
          "upvote_count": "1",
          "content": "Selected Answer: A\nReplace EC2 + ALB with AWS Lambda → Moves compute to serverless.\nReplace MySQL with Aurora Serverless v2 (MySQL-compatible) → Keeps compatibility with MySQL, meaning minimal code/database query changes.\n✅ Best choice because it fully transitions to serverless (compute + database) with the least disruption to existing code.\n\n\nIt offers a fully serverless solution with minimum code changes, since:\n\nLambda can take over the logic in containers.\nAurora Serverless v2 provides MySQL compatibility, so the data layer is minimally impacted."
        },
        {
          "poster": "CHRIS12722222",
          "comment_id": "1333446",
          "timestamp": "1735466880.0",
          "upvote_count": "3",
          "content": "Selected Answer: B\nfargate and aurora serverless v2"
        },
        {
          "comment_id": "1324666",
          "upvote_count": "3",
          "poster": "eugene2owl",
          "timestamp": "1733848920.0",
          "content": "Selected Answer: B\n\"B\" is the most easy-to-implement option - as the question asks.\nThe rest options require moving from SQL to NoSQL (which requires multiple code changes) and/or moving from Docker containers to Lambda (which requires multiple code changes)."
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:17:49.195Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "UmVGr1suO9MPyO2E7IoE",
      "question_number": 264,
      "page": 53,
      "question_text": "A company uses an organization in AWS Organizations to manage 10 AWS accounts. All features are enabled, and trusted access for AWS CloudFormation is enabled.\n\nA DevOps engineer needs to use CloudFormation to deploy an IAM role to the Organizations management account and all member accounts in the organization.\n\nWhich solution will meet these requirements with the LEAST operational overhead?",
      "choices": {
        "A": "Create a CloudFormation StackSet that has service-managed permissions. Set the root OU as a deployment target.",
        "D": "Create a CloudFormation StackSet that has self-managed permissions. Set the root OU as a deployment target. Deploy a separate CloudFormation stack in the Organizations management account.",
        "C": "Create a CloudFormation StackSet that has self-managed permissions. Set the root OU as a deployment target.",
        "B": "Create a CloudFormation StackSet that has service-managed permissions. Set the root OU as a deployment target. Deploy a separate CloudFormation stack in the Organizations management account."
      },
      "correct_answer": "B",
      "answer_ET": "B",
      "answers_community": [
        "B (61%)",
        "A (39%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/151855-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-11-22 23:31:00",
      "unix_timestamp": 1732314660,
      "discussion_count": 7,
      "discussion": [
        {
          "timestamp": "1735473600.0",
          "comment_id": "1333484",
          "content": "Selected Answer: B\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/stacksets-orgs-associate-stackset-with-org.html#:~:text=StackSets%20doesn%27t%20deploy%20stack%20instances%20to%20the%20organization%27s%20management%20account%2C%20even%20if%20the%20management%20account%20is%20in%20your%20organization%20or%20in%20an%20OU%20in%20your%20organization\n\nStackset cant deploy to management acct",
          "upvote_count": "6",
          "poster": "CHRIS12722222"
        },
        {
          "poster": "Srikantha",
          "timestamp": "1743884820.0",
          "comment_id": "1550489",
          "content": "Selected Answer: A\nCloudFormation StackSets with service-managed permissions use AWS Organizations integration to automatically assume roles in target accounts — no manual role setup required.\nSetting the root OU as the deployment target means that the stack set will deploy to all existing and future accounts in the organization.\nYou don’t need to separately deploy anything to the management account unless it must be treated differently (which is not mentioned here).\nThis is the most hands-off, scalable approach — ideal for centrally managing IAM roles across all accounts in an organization.",
          "upvote_count": "1"
        },
        {
          "comment_id": "1350607",
          "poster": "jojewi8143",
          "timestamp": "1738521480.0",
          "upvote_count": "2",
          "content": "Selected Answer: B\nStackset cant deploy to management account."
        },
        {
          "upvote_count": "2",
          "comment_id": "1329753",
          "timestamp": "1734739680.0",
          "comments": [
            {
              "comment_id": "1329754",
              "timestamp": "1734739680.0",
              "content": "Service-managed permissions\nStackSets automatically generate the IAM roles required to deploy stack instances. You can create the IAM roles required by the AWS CloudFormation StackSets feature in the management account of AWS Organizations. You can also delegate the management of StackSets to member accounts.",
              "poster": "spring21",
              "upvote_count": "1"
            }
          ],
          "poster": "spring21",
          "content": "Selected Answer: A\nStackSets automatically generates the IAM roles required to deploy stack instances. You can create the IAM roles required by the AWS CloudFormation StackSets feature in the management account of AWS Organizations."
        },
        {
          "timestamp": "1732452060.0",
          "poster": "Impromptu",
          "upvote_count": "3",
          "content": "Selected Answer: B\nShould be B I think. A stackset with service-managed permissions does not deploy to the management account.\n\n\"StackSets doesn't deploy stack instances to the organization's management account, even if the management account is in your organization or in an OU in your organization.\"\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/stacksets-getting-started-create.html#stacksets-orgs-associate-stackset-with-org",
          "comment_id": "1317029"
        },
        {
          "upvote_count": "1",
          "comment_id": "1316636",
          "content": "Selected Answer: A\nA. Create a CloudFormation StackSet that has service-managed permissions. Set the root OU as a deployment target.",
          "poster": "Changwha",
          "timestamp": "1732358760.0"
        },
        {
          "comment_id": "1316518",
          "timestamp": "1732314660.0",
          "content": "Selected Answer: A\nUsing service-managed permissions simplifies the deployment process because AWS manages the permissions required for deploying the StackSet. This reduces the complexity and effort involved in setting up and managing permissions manually.\nBy setting the root Organizational Unit (OU) as the deployment target, the StackSet will automatically deploy the IAM role to all AWS accounts under the root OU, including both existing and future accounts. This ensures comprehensive and automatic coverage.\nService-managed StackSets provide a streamlined and scalable solution, requiring minimal manual intervention and oversight, thus reducing operational overhead.",
          "upvote_count": "3",
          "poster": "f4b18ba"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:17:49.195Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "DPWyFfs35lpsC0BnOw3P",
      "question_number": 265,
      "page": 53,
      "question_text": "A company runs an application that stores artifacts in an Amazon S3 bucket. The application has a large user base. The application writes a high volume of objects to the S3 bucket. The company has enabled event notifications for the S3 bucket.\n\nWhen the application writes an object to the S3 bucket, several processing tasks need to be performed simultaneously. The company's DevOps team needs to create an AWS Step Functions workflow to orchestrate the processing tasks.\n\nWhich combination of steps should the DevOps team take to meet these requirements with the LEAST operational overhead? (Choose two.)",
      "choices": {
        "A": "Create a Standard workflow that contains a parallel state that defines the processing tasks. Create an Asynchronous Express workflow that contains a parallel state that defines the processing tasks.",
        "D": "Create an Amazon EventBridge rule to match when a new S3 object is created. Configure the EventBridge rule to start the processing workflow.",
        "C": "Create an Amazon EventBridge rule to match when a new S3 object is created. Configure the EventBridge rule to invoke an AWS Lambda function. Configure the Lambda function to start the processing workflow.",
        "B": "Create a Synchronous Express workflow that contains a map state that defines the processing tasks."
      },
      "correct_answer": "AD",
      "answer_ET": "AD",
      "answers_community": [
        "AD (83%)",
        "Other"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/151613-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-11-19 09:56:00",
      "unix_timestamp": 1732006560,
      "discussion_count": 6,
      "discussion": [
        {
          "timestamp": "1743950520.0",
          "comment_id": "1558275",
          "upvote_count": "2",
          "poster": "Srikantha",
          "content": "Selected Answer: AD\n✅ A. Create a Standard workflow that contains a parallel state that defines the processing tasks.\nStandard workflows in AWS Step Functions are best suited for long-running, durable processes and support parallel states, which are perfect for triggering multiple tasks simultaneously.\nSince the processing involves several tasks, this structure provides reliability, retries, and observability out of the box.\n✅ D. Create an Amazon EventBridge rule to match when a new S3 object is created. Configure the EventBridge rule to start the processing workflow.\nEventBridge can directly invoke a Step Functions workflow (without needing an intermediate Lambda function), which reduces operational overhead.\nIt simplifies the event-driven architecture by removing custom code and improves maintainability."
        },
        {
          "comment_id": "1343652",
          "content": "Selected Answer: D\nChanging my mind, as I said previously, there's a misspelling but the problem is that \" Create an Asynchronous Express workflow that contains a parallel state that defines the processing tasks. \" should be a separate option as it's the correct first part. D is pretty evident.",
          "upvote_count": "1",
          "timestamp": "1737384000.0",
          "poster": "teo2157"
        },
        {
          "upvote_count": "3",
          "timestamp": "1734525120.0",
          "poster": "teo2157",
          "content": "Selected Answer: AD\nThere's a misspelling and the option B is included in the option A, said that, going for standard workflow as it's recommended for long-running (up to one year), durable, and auditable workflows instead of Express Workflows that are ideal for high-volume, event-processing workloads such as IoT data ingestion, streaming data processing and transformation, and mobile application backends and can run for up to five minutes. .",
          "comment_id": "1328506"
        },
        {
          "timestamp": "1733918460.0",
          "upvote_count": "3",
          "content": "Selected Answer: AD\nBecause of the requirement for parallel task processing, I vote for A and D.",
          "poster": "luisfsm_111",
          "comment_id": "1325008"
        },
        {
          "upvote_count": "1",
          "comment_id": "1323412",
          "content": "Selected Answer: BD\nStandard workflow is for long-running. Express workflow is for high-volume.\nSo B is better than A.\nhttps://docs.aws.amazon.com/step-functions/latest/dg/choosing-workflow-type.html",
          "timestamp": "1733639640.0",
          "poster": "tinyshare"
        },
        {
          "upvote_count": "2",
          "timestamp": "1732006560.0",
          "comment_id": "1314521",
          "content": "Selected Answer: AD\nUse a Standard Step Functions workflow with parallel states to handle the processing tasks (Option A).\nUse an EventBridge rule to directly trigger the Step Functions workflow (Option D).",
          "poster": "uncledana"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:17:49.195Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "8irdSQ6jCxUwQeH4Y0TL",
      "question_number": 266,
      "page": 54,
      "question_text": "A DevOps team supports an application that runs in an Amazon Elastic Container Service (Amazon ECS) cluster behind an Application Load Balancer (ALB). Currently, the DevOps team uses AWS CodeDeploy to deploy the application by using a blue/green all-at-once strategy. Recently, the DevOps team had to roll back a deployment when a new version of the application dramatically increased response times for requests.\n\nThe DevOps team needs use to a deployment strategy that will allow the team to monitor a new version of the application before the team shifts all traffic to the new version. If a new version of the application increases response times, the deployment should be rolled back as quickly as possible.\n\nWhich combination of steps will meet these requirements? (Choose two.)",
      "choices": {
        "C": "Create an Amazon CloudWatch alarm to monitor the UnHealthyHostCount metric for the ALB. Set the alarm to activate if the metric is higher than the desired value. Associate the alarm with the CodeDeploy deployment group. Modify the deployment group to roll back when a deployment fails.",
        "B": "Modify the CodeDeploy deployment to use the CodeDeployDefault.ECSLinear10PercentEvery3Minutes configuration.",
        "A": "Modify the CodeDeploy deployment to use the CodeDeployDefault.ECSCanary10Percent5Minutes configuration.",
        "E": "Create an Amazon CloudWatch alarm to monitor the TargetConnectionErrorCount metric for the ALB. Set the alarm to activate if the metric is higher than the desired value. Associate the alarm with the CodeDeploy deployment group. Modify the deployment group to roll back when alarm thresholds are met.",
        "D": "Create an Amazon CloudWatch alarm to monitor the TargetResponseTime metric for the ALB. Set the alarm to activate if the metric is higher than the desired value. Associate the alarm with the CodeDeploy deployment group. Modify the deployment group to roll back when alarm thresholds are met."
      },
      "correct_answer": "AD",
      "answer_ET": "AD",
      "answers_community": [
        "AD (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/151614-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-11-19 10:04:00",
      "unix_timestamp": 1732007040,
      "discussion_count": 2,
      "discussion": [
        {
          "timestamp": "1732315380.0",
          "upvote_count": "4",
          "content": "Selected Answer: AD\nUsing a canary deployment strategy with CodeDeployDefault.ECSCanary10Percent5Minutes allows for controlled, gradual deployment, providing the ability to monitor the new version's performance. Simultaneously, monitoring the TargetResponseTime with CloudWatch alarms ensures that any issues with response times are detected early, allowing for quick rollbacks to the previous stable version.",
          "poster": "f4b18ba",
          "comment_id": "1316519"
        },
        {
          "poster": "uncledana",
          "content": "Selected Answer: AD\n• Use the CodeDeployDefault.ECSCanary10Percent5Minutes configuration (Option A) for canary deployment to test with minimal traffic initially.\n • Monitor the ALB’s TargetResponseTime metric (Option D) with an alarm to detect performance issues and trigger a rollback if needed.",
          "timestamp": "1732007040.0",
          "comment_id": "1314525",
          "upvote_count": "3"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:17:59.605Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "pg20Kac86FqjScNkNBYB",
      "question_number": 267,
      "page": 54,
      "question_text": "A security team must record the configuration of AWS resources, detect issues, and send notifications for findings. The main workload in the AWS account consists of an Amazon EC2 Auto Scaling group that scales in and out several times during the day.\n\nThe team wants to be notified within 2 days if any Amazon EC2 security group allows traffic on port 22 for 0.0.0.0/0. The team also needs a snapshot of the configuration of the AWS resources to be taken routinely.\n\nThe security team has already created and subscribed to an Amazon Simple Notification Service (Amazon SNS) topic.\n\nWhich solution meets these requirements?",
      "choices": {
        "A": "Configure AWS Config to use periodic recording for the AWS account. Deploy the vpc-sg-port-restriction-check AWS Config managed rule. Configure AWS Config to use the SNS topic as the target for notifications.",
        "D": "Create an AWS Lambda function to evaluate security groups and publish a message to the SNS topic. Use an Amazon EventBridge rule to schedule the Lambda function to run once a day.",
        "C": "Configure AWS Config to use configuration change recording for the AWS account. Deploy the ssh-restricted AWS Config managed rule. Configure AWS Config to use the SNS topic as the target for notifications.",
        "B": "Configure AWS Config to use configuration change recording for the AWS account. Deploy the vpc-sg-open-only-to-authorized-ports AWS Config managed rule. Configure AWS Config to use the SNS topic as the target for notifications."
      },
      "correct_answer": "C",
      "answer_ET": "C",
      "answers_community": [
        "C (79%)",
        "A (16%)",
        "5%"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/151615-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-11-19 10:11:00",
      "unix_timestamp": 1732007460,
      "discussion_count": 8,
      "discussion": [
        {
          "content": "Selected Answer: C\nConfiguration Change Recording: By configuring AWS Config to use configuration change recording, the system will continuously monitor and record configurations of your AWS resources whenever there are changes. This ensures real-time compliance monitoring and reduces the delay in detection.\n\nAppropriate Managed Rule: The ssh-restricted AWS Config managed rule specifically checks for security groups that allow unrestricted SSH (port 22) access. This rule directly addresses the requirement to be notified if any EC2 security group allows traffic on port 22 for 0.0.0.0/0.\n\nNotification Setup: Configuring AWS Config to use the SNS topic ensures that the security team will be notified within the specified time frame if the rule is violated. AWS Config can send notifications to the SNS topic as soon as a non-compliant resource is detected.",
          "poster": "f4b18ba",
          "comment_id": "1316520",
          "upvote_count": "5",
          "timestamp": "1732315620.0"
        },
        {
          "upvote_count": "1",
          "timestamp": "1743951000.0",
          "comment_id": "1558277",
          "content": "Selected Answer: C\nAWS Config with configuration change recording ensures that every time a resource configuration changes, it's recorded—ideal for dynamic environments like Auto Scaling groups.\nThe ssh-restricted managed rule specifically checks whether port 22 (SSH) is open to 0.0.0.0/0, which directly satisfies the requirement.\nConfiguring AWS Config to publish to an existing SNS topic ensures the security team is notified of any findings within 2 days or sooner, depending on when the change occurs.",
          "poster": "Srikantha"
        },
        {
          "poster": "jojewi8143",
          "upvote_count": "2",
          "content": "Selected Answer: C\nssh-restricted",
          "timestamp": "1738521900.0",
          "comment_id": "1350611"
        },
        {
          "content": "Selected Answer: B\nWhy not B?\n\nhttps://docs.aws.amazon.com/config/latest/developerguide/vpc-sg-open-only-to-authorized-ports.html",
          "poster": "matt200",
          "upvote_count": "1",
          "comment_id": "1334169",
          "timestamp": "1735566180.0",
          "comments": [
            {
              "poster": "matt200",
              "upvote_count": "2",
              "timestamp": "1736146800.0",
              "content": "change my mind to C",
              "comment_id": "1336996"
            }
          ]
        },
        {
          "upvote_count": "1",
          "content": "Selected Answer: A\ni think we need to use periodic or daily recording instead of continuous one",
          "timestamp": "1735485900.0",
          "poster": "CHRIS12722222",
          "comment_id": "1333566"
        },
        {
          "timestamp": "1734810480.0",
          "comment_id": "1330179",
          "upvote_count": "4",
          "poster": "spring21",
          "content": "Selected Answer: C\nFor monitoring if any EC2 security group allows traffic on port 22 (SSH) from 0.0.0.0/0:\nUse the managed AWS Config rule:\n\nrestricted-ssh\nThis rule checks that security groups do not allow unrestricted incoming SSH traffic (port 22) from 0.0.0.0/0."
        },
        {
          "timestamp": "1733914560.0",
          "upvote_count": "2",
          "comment_id": "1324989",
          "content": "Selected Answer: A\n\"A\" because \"vpc-sg-port-restriction-check\" fits requested check well, and the condition says \"... to be taken routinely\", which means \"periodically\", \"regularly\".\n\"B\" and \"C\" propose running \"on-change\" instead, which does not fit condition \"routinely\"",
          "poster": "eugene2owl"
        },
        {
          "poster": "uncledana",
          "upvote_count": "3",
          "comment_id": "1314527",
          "content": "Selected Answer: C\nThe correct answer is C",
          "timestamp": "1732007460.0"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:17:59.605Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "17AbhC7njplKCOxqlVaM",
      "question_number": 268,
      "page": 54,
      "question_text": "A company is using AWS CodePipeline to automate its release pipeline. AWS CodeDeploy is being used in the pipeline to deploy an application to Amazon Elastic Container Service (Amazon ECS) using the blue/green deployment model. The company wants to implement scripts to test the green version of the application before shifting traffic. These scripts will complete in 5 minutes or less. If errors are discovered during these tests, the application must be rolled back.\nWhich strategy will meet these requirements?",
      "choices": {
        "A": "Add a stage to the CodePipeline pipeline between the source and deploy stages. Use AWS CodeBuild to create a runtime environment and build commands in the buildspec file to invoke test scripts. If errors are found, use the aws deploy stop-deployment command to stop the deployment.",
        "C": "Add a hooks section to the CodeDeploy AppSpec file. Use the AfterAllowTestTraffic lifecycle event to invoke an AWS Lambda function to run the test scripts. If errors are found, exit the Lambda function with an error to initiate rollback.",
        "D": "Add a hooks section to the CodeDeploy AppSpec file. Use the AfterAllowTraffic lifecycle event to invoke the test scripts. If errors are found, use the aws deploy stop-deployment CLI command to stop the deployment.",
        "B": "Add a stage to the CodePipeline pipeline between the source and deploy stages. Use this stage to invoke an AWS Lambda function that will run the test scripts. If errors are found, use the aws deploy stop-deployment command to stop the deployment."
      },
      "correct_answer": "C",
      "answer_ET": "C",
      "answers_community": [
        "C (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/106213-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-04-14 23:53:00",
      "unix_timestamp": 1681509180,
      "discussion_count": 8,
      "discussion": [
        {
          "timestamp": "1698897780.0",
          "content": "Add a hooks section to the CodeDeploy AppSpec file. The AppSpec file is a YAML file that describes how to deploy an application to Amazon ECS using CodeDeploy. We can use the AfterAllowTestTraffic lifecycle event to run the test scripts. This event is triggered after the new version of the application is deployed, and before traffic is shifted to the new version.\n\nIn the AfterAllowTestTraffic lifecycle event, invoke an AWS Lambda function to run the test scripts. The Lambda function can be written in any programming language supported by Lambda, such as Python, Node.js, or Java.\n\nIf the test scripts detect any errors, exit the Lambda function with an error code. This will cause the deployment to fail, and CodeDeploy will initiate a rollback.",
          "comment_id": "886961",
          "poster": "haazybanj",
          "upvote_count": "10"
        },
        {
          "poster": "zijo",
          "content": "AfterAllowTestTraffic lifecycle event in the hooks section will not shift the whole traffic to the green application but only a small percentage of traffic to the newly deployed version. C is the answer",
          "upvote_count": "1",
          "comment_id": "1167190",
          "timestamp": "1725623220.0"
        },
        {
          "poster": "dzn",
          "timestamp": "1724132640.0",
          "upvote_count": "2",
          "content": "Selected Answer: C\nCodeDeploy Blue/Green deployments, the AfterAllowTestTraffic hook is triggered after the test traffic redirection to the new version (Green) is set. Additional verification, testing, or other custom actions can be automated by executing Lambda functions at this time.",
          "comment_id": "1154547"
        },
        {
          "upvote_count": "4",
          "timestamp": "1722266700.0",
          "poster": "thanhnv142",
          "content": "C is correct: we can initiate the script using lambda for advanced features\nA and B are wrong: Both trigger the test script befor deploy stages\nD is wrong: It only stops the deployment, not rollback it",
          "comment_id": "1135144"
        },
        {
          "poster": "ixdb",
          "upvote_count": "1",
          "comment_id": "979860",
          "timestamp": "1707820080.0",
          "content": "C is right."
        },
        {
          "content": "Selected Answer: C\nC is the right answer.\nhttps://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html#appspec-hooks-ecs",
          "timestamp": "1702652580.0",
          "comment_id": "924205",
          "upvote_count": "4",
          "poster": "madperro"
        },
        {
          "timestamp": "1697380080.0",
          "content": "Selected Answer: C\nLifecycle event hooks for an Amazon ECS deployment:\n\nAfterAllowTraffic – Use to run tasks after the second target group serves traffic to the replacement task set. The results of a hook function at this lifecycle event can trigger a rollback.",
          "comment_id": "870989",
          "comments": [
            {
              "poster": "ele",
              "comment_id": "870990",
              "content": "Correction: \nAfterAllowTestTraffic – Use to run tasks after the test listener serves traffic to the replacement task set. The results of a hook function at this point can trigger a rollback.",
              "upvote_count": "1",
              "timestamp": "1697380140.0"
            }
          ],
          "upvote_count": "1",
          "poster": "ele"
        },
        {
          "content": "Selected Answer: C\nC is the correct answer",
          "timestamp": "1697320380.0",
          "comment_id": "870498",
          "upvote_count": "1",
          "poster": "alce2020"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:17:59.605Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "xMSmWueJ6bkH2kULgxVs",
      "question_number": 269,
      "page": 54,
      "question_text": "A company has proprietary data available by using an Amazon CloudFront distribution. The company needs to ensure that the distribution is accessible by only users from the corporate office that have a known set of IP address ranges. An AWS WAF web ACL is associated with the distribution and has a default action set to Count.\n\nWhich solution will meet these requirements with the LEAST operational overhead?",
      "choices": {
        "D": "Create a WAF IP address set that matches the corporate office IP address range. Set the default action on the existing web ACL to Block. Add a rule that has priority 0 that allows traffic from the IP address set.",
        "C": "Create a new regex pattern set. Add the regex pattern set to a new rule group. Set the default action on the existing web ACL to Allow. Add a rule that has priority 0 that allows traffic based on the regex pattern set.",
        "B": "Create an AWS WAF IP address set that matches the corporate office IP address range. Create a new web ACL that has a default action set to Allow. Associate the web ACL with the CloudFront distribution. Add a rule that allows traffic from the IP address set.",
        "A": "Create a new regex pattern set. Add the regex pattern set to a new rule group. Create a new web ACL that has a default action set to Block. Associate the web ACL with the CloudFront distribution. Add a rule that allows traffic based on the new rule group."
      },
      "correct_answer": "D",
      "answer_ET": "D",
      "answers_community": [
        "D (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/151616-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-11-19 10:20:00",
      "unix_timestamp": 1732008000,
      "discussion_count": 4,
      "discussion": [
        {
          "poster": "Srikantha",
          "upvote_count": "2",
          "content": "Selected Answer: D\nGoal: Allow access only from specific IP ranges (corporate office), and block everything else.\nWAF IP set: This is the right tool for matching specific source IP addresses.\nDefault action = Block: Ensures all traffic is blocked unless explicitly allowed.\nRule priority 0 (highest priority): Ensures that corporate IPs are evaluated first and allowed.\nUses existing web ACL: Minimizes overhead by not needing to create a new ACL.",
          "comment_id": "1558281",
          "timestamp": "1743951060.0"
        },
        {
          "poster": "teo2157",
          "upvote_count": "4",
          "timestamp": "1734534240.0",
          "content": "Selected Answer: D\nAgreee with D as prioty 0 is the highest priority rule",
          "comment_id": "1328576"
        },
        {
          "comment_id": "1316522",
          "poster": "f4b18ba",
          "timestamp": "1732315920.0",
          "content": "Selected Answer: D\nUsing Existing Web ACL: This approach leverages the existing web ACL, minimizing the need to create a new one, which reduces operational overhead.\n\nIP Address Set: By creating a WAF IP address set that matches the corporate office IP address range, you precisely define which IP addresses are allowed access.\n\nBlocking by Default: Setting the default action to Block ensures that only traffic from the defined IP addresses is allowed, meeting the security requirement.\n\nHigh Priority Rule: Adding a high-priority rule (priority 0) to allow traffic from the IP address set ensures that legitimate traffic from the corporate office is not blocked.",
          "upvote_count": "4"
        },
        {
          "comment_id": "1314530",
          "poster": "uncledana",
          "timestamp": "1732008000.0",
          "content": "Selected Answer: D\nThe requirements are:\n\n 1. Restrict access to the CloudFront distribution to users from a known set of IP address ranges (the corporate office).\n 2. Minimize operational overhead.\n 3. Use the existing AWS WAF web ACL, which has the default action set to Count.\n\nOption D: Create a WAF IP address set that matches the corporate office IP address range. Set the default action on the existing web ACL to Block. Add a rule that has priority 0 that allows traffic from the IP address set.",
          "upvote_count": "4"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:17:59.605Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "n2n4lz8OlzVM2OumokEK",
      "question_number": 270,
      "page": 54,
      "question_text": "A company runs several applications in the same AWS account. The applications send logs to Amazon CloudWatch.\n\nA data analytics team needs to collect performance metrics and custom metrics from the applications. The analytics team needs to transform the metrics data before storing the data in an Amazon S3 bucket. The analytics team must automatically collect any new metrics that are added to the CloudWatch namespace.\n\nWhich solution will meet these requirements with the LEAST operational overhead?",
      "choices": {
        "D": "Configure subscription filters on the application log groups to target an Amazon Data Firehose delivery stream. Configure the Firehose delivery stream to invoke an AWS Lambda function to transform the data. Configure the delivery stream to send the transformed data to the S3 bucket.",
        "A": "Configure a CloudWatch metric stream to include metrics from the application and the CloudWatch namespace. Configure the metric stream to deliver the metrics to an Amazon Data Firehose delivery stream. Configure the Firehose delivery stream to invoke an AWS Lambda function to transform the data. Configure the delivery stream to send the transformed data to the S3 bucket.",
        "B": "Configure a CloudWatch metrics stream to include all the metrics and to deliver the metrics to an Amazon Data Firehose delivery stream. Configure the Firehose delivery stream to invoke an AWS Lambda function to transform the data. Configure the delivery stream to send the transformed data to the S3 bucket.",
        "C": "Configure metric filters for the CloudWatch logs to create custom metrics. Configure a CloudWatch metric stream to deliver the application metrics to the S3 bucket."
      },
      "correct_answer": "A",
      "answer_ET": "A",
      "answers_community": [
        "A (54%)",
        "B (46%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/151617-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-11-19 10:45:00",
      "unix_timestamp": 1732009500,
      "discussion_count": 5,
      "discussion": [
        {
          "upvote_count": "1",
          "timestamp": "1743951180.0",
          "comment_id": "1558282",
          "poster": "Srikantha",
          "content": "Selected Answer: B\nCloudWatch metric streams are designed specifically to stream metric data in near real-time to destinations like Firehose.\nSelecting \"all metrics\" ensures that any new metrics added in the future are automatically included, satisfying the requirement to \"automatically collect any new metrics.\"\nAmazon Kinesis Data Firehose supports Lambda transforms, so the analytics team can modify data before it's delivered.\nOutput to S3 is natively supported by Firehose, which satisfies the storage requirement."
        },
        {
          "upvote_count": "1",
          "content": "Selected Answer: B\nB is better than A because new metrics must be automatically collected. With A they have to be manually selected. With B they are automatically collected.",
          "poster": "rinip86277",
          "timestamp": "1739773260.0",
          "comment_id": "1357619"
        },
        {
          "comments": [
            {
              "timestamp": "1745340600.0",
              "upvote_count": "1",
              "comment_id": "1562783",
              "content": "I think his Selected Answer supposed to be A.",
              "poster": "GripZA"
            }
          ],
          "timestamp": "1735487160.0",
          "comment_id": "1333573",
          "content": "Selected Answer: B\nWhy not option B?\n\nJust collect all the metrics in the namespace, if new ones are added later, it will also be collected",
          "upvote_count": "4",
          "poster": "CHRIS12722222"
        },
        {
          "timestamp": "1734380700.0",
          "poster": "Ky_24",
          "content": "Selected Answer: A\nA is correct",
          "upvote_count": "3",
          "comment_id": "1327620"
        },
        {
          "poster": "uncledana",
          "comment_id": "1314539",
          "upvote_count": "4",
          "content": "Selected Answer: A\nExplanation:\n\nThe requirements are:\n\n 1. Collect performance metrics and custom metrics from CloudWatch.\n 2. Automatically include new metrics added to the namespace.\n 3. Transform the metrics data.\n 4. Store the transformed data in an S3 bucket.\n 5. Minimize operational overhead.\n\nOption A: Configure a CloudWatch metric stream to include metrics from the application and the CloudWatch namespace. Configure the metric stream to deliver the metrics to an Amazon Data Firehose delivery stream. Configure the Firehose delivery stream to invoke an AWS Lambda function to transform the data. Configure the delivery stream to send the transformed data to the S3 bucket.",
          "timestamp": "1732009500.0"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:17:59.605Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "dDy0obWmCz6jITd8oYK3",
      "question_number": 271,
      "page": 55,
      "question_text": "A company uses an HPC platform to run analysis jobs for data. The company uses AWS CodeBuild to create container images and store the images on Amazon Elastic Container Registry (Amazon ECR). The images are then deployed on Amazon Elastic Kubernetes Service (Amazon EKS).\n\nTo maintain compliance, the company needs to ensure that the images are signed before the images are deployed on Amazon EKS. The signing keys must be rotated periodically and must be managed automatically. The company needs to track who generates the signatures.\n\nWhich solution will meet these requirements with the LEAST operational effort?",
      "choices": {
        "D": "Use CodeBuild to build the image. Sign the image by using AWS Signer before pushing the image to Amazon ECR. Use AWS CloudTrail to track who generates the signatures.",
        "C": "Use AWS Lambda to retrieve the image that was previously pushed to Amazon ECR. Use AWS Signer to sign the image. Use Amazon CloudWatch to track who generates the signatures.",
        "B": "Use AWS Lambda to retrieve the image that was previously pushed to Amazon ECR. Use a Lambda function to sign the image. Use Amazon CloudWatch to track who generates the signatures.",
        "A": "Use CodeBuild to retrieve the image that was previously pushed to Amazon ECR. Use AWS Signer to sign the image. Use AWS CloudTrail to track who generates the signatures."
      },
      "correct_answer": "D",
      "answer_ET": "D",
      "answers_community": [
        "D (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/151618-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-11-19 10:58:00",
      "unix_timestamp": 1732010280,
      "discussion_count": 2,
      "discussion": [
        {
          "content": "Selected Answer: D\nCodeBuild is already in use and integrates well with image build workflows.\nAWS Signer supports container image signing, especially with Amazon ECR.\nSigning before pushing ensures that only signed images are stored and used — a best practice for compliance.\nKey rotation is managed automatically by AWS Key Management Service (KMS) when used with AWS Signer.\nAWS CloudTrail tracks who signs the image (i.e., tracks the AWS identity that invokes Signer).\nThis setup has the least operational overhead because it stays within managed AWS services and integrates smoothly with the existing build pipeline.",
          "upvote_count": "1",
          "timestamp": "1743951300.0",
          "comment_id": "1558283",
          "poster": "Srikantha"
        },
        {
          "upvote_count": "3",
          "poster": "uncledana",
          "comment_id": "1314542",
          "timestamp": "1732010280.0",
          "content": "Selected Answer: D\nExplanation:\n\nThis solution meets all of the requirements with the least operational effort because:\n\n 1. Image Signing with AWS Signer:\n 2. Automated Key Rotation:\n 3. Tracking Who Signs the Images:\n 4. Using CodeBuild for Image Creation:\n 5. Least Operational Effort:"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:18:10.053Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "oUTSYMRlfFaezAA7b0wf",
      "question_number": 272,
      "page": 55,
      "question_text": "A company uses an AWS CodeArtifact repository to store Python packages that the company developed internally. A DevOps engineer needs to use AWS CodeDeploy to deploy an application to an Amazon EC2 instance. The application uses a Python package that is stored in the CodeArtifact repository. A BeforeInstall lifecycle event hook will install the package.\n\nThe DevOps engineer needs to grant the EC2 instance access to the CodeArtifact repository.\n\nWhich solution will meet this requirement?",
      "choices": {
        "A": "Create a service-linked role for CodeArtifact. Associate the role with the EC2 instance. Use the aws codeartifact get-authorization-token CLI command on the instance.",
        "D": "Create an instance profile that contains an IAM role that has access to CodeArtifact. Associate the instance profile with the EC2 instance. Use the aws codeartifact login CLI command on the instance.",
        "B": "Configure a resource-based policy for the CodeArtifact repository that allows the ReadFromRepository action for the EC2 instance principal.",
        "C": "Configure ACLs on the CodeArtifact repository to allow the EC2 instance to access the Python package."
      },
      "correct_answer": "D",
      "answer_ET": "D",
      "answers_community": [
        "D (71%)",
        "B (29%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/151619-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-11-19 11:00:00",
      "unix_timestamp": 1732010400,
      "discussion_count": 6,
      "discussion": [
        {
          "timestamp": "1748106000.0",
          "comment_id": "1571966",
          "content": "Selected Answer: D\nB. Configure a resource-based policy for the CodeArtifact repository:\nIssue: CodeArtifact repositories do not support resource-based policies (unlike S3 buckets or SNS topics). Access to CodeArtifact is controlled entirely through IAM policies attached to roles or users, making this option invalid.",
          "poster": "nickp84",
          "upvote_count": "1"
        },
        {
          "timestamp": "1745341140.0",
          "comments": [
            {
              "comment_id": "1568992",
              "content": "That is not saying that \"specifically\" you can not use IBPs, IBPs are prefered when you want to give individual access",
              "poster": "robotgeek",
              "timestamp": "1747293120.0",
              "upvote_count": "1"
            }
          ],
          "upvote_count": "1",
          "comment_id": "1562786",
          "poster": "GripZA",
          "content": "Selected Answer: B\njust like other resource policies, CodeArtifact uses resource-based permissions to control access. Resource-based permissions let you specify who has access to a repository and what actions they can perform on it. By default, only the repository owner has access to a repository so implicit deny unless you apply a policy document that allows other IAM principals to access your repository.\n\nhttps://docs.aws.amazon.com/codeartifact/latest/ug/repo-policies.html"
        },
        {
          "content": "Selected Answer: D\nIAM role with CodeArtifact permissions: You need an IAM role attached to the EC2 instance (via instance profile) that grants permission to read from CodeArtifact.\naws codeartifact login sets up your Python environment (e.g., pip) to authenticate to the CodeArtifact repository using temporary credentials tied to the instance’s IAM role.\nThis is the recommended approach to grant secure and scalable access to CodeArtifact from EC2 instances.",
          "upvote_count": "1",
          "comment_id": "1558284",
          "timestamp": "1743951360.0",
          "poster": "Srikantha"
        },
        {
          "poster": "teo2157",
          "timestamp": "1734536100.0",
          "content": "Selected Answer: D\nVote for D based on https://docs.aws.amazon.com/codeartifact/latest/ug/security-iam.html",
          "upvote_count": "4",
          "comment_id": "1328596"
        },
        {
          "timestamp": "1733648880.0",
          "upvote_count": "3",
          "comment_id": "1323457",
          "content": "Selected Answer: B\nIt is the resource allows who can use it, in this case, CodeArtifact.\nhttps://docs.aws.amazon.com/codeartifact/latest/ug/repo-policies.html",
          "poster": "tinyshare"
        },
        {
          "poster": "uncledana",
          "upvote_count": "4",
          "content": "Selected Answer: D\nD. Create an instance profile that contains an IAM role that has access to CodeArtifact. Associate the instance profile with the EC2 instance. Use the aws codeartifact login CLI command on the instance.\n\nExplanation:\n\nTo allow the EC2 instance to access the CodeArtifact repository, the EC2 instance must have the necessary IAM permissions to interact with AWS CodeArtifact. Here’s why option D is the best solution:",
          "comment_id": "1314545",
          "timestamp": "1732010400.0"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:18:10.053Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "Ovojz4j0GQQ3Tu9CODaD",
      "question_number": 273,
      "page": 55,
      "question_text": "A company has a file-reading application that saves files to a database that runs on Amazon EC2 instances. Regulations require the company to delete files from EC2 instances every day at a specific time. The company must delete database records that are older than 60 days.\n\nThe database record deletion must occur after the file deletions. The company has created scripts to delete files and database records. The company needs to receive an email notification for any failure of the deletion scripts.\n\nWhich solution will meet these requirements with the LEAST development effort?",
      "choices": {
        "A": "Use AWS Systems Manager State Manager to automatically invoke a Systems Manager Automation document at the specified time each day. Configure the Automation document to use a run command to run the deletion scripts in sequential order. Create an Amazon EventBridge rule to use Amazon Simple Notification Service (Amazon SNS) to send failure notifications to the company.",
        "B": "Use AWS Systems Manager State Manager to automatically invoke a Systems Manager Automation document at the specified time each day. Configure the Automation document to use a run command to run the deletion scripts in sequential order. Create a conditional statement inside the Automation document as the last step to check for errors. Use Amazon Simple Email Service (Amazon SES) to send failure notifications as email messages to the company.",
        "D": "Create an Amazon EventBridge rule that invokes an AWS Lambda function at the specified time. Add the necessary permissions for the invocation to the Lambda function's resource-based policy. Configure the Lambda function to run the deletion scripts in sequential order. Configure the Lambda function to use Amazon Simple Email Service (Amazon SES) to send failure notifications as email messages to the company.",
        "C": "Create an Amazon EventBridge rule that invokes an AWS Lambda function at the specified time. Add the necessary permissions for the invocation to the Lambda function's resource-based policy. Configure the Lambda function to run the deletion scripts in sequential order. Configure the Lambda function to use Amazon Simple Notification Service (Amazon SNS) to send failure notifications to the company."
      },
      "correct_answer": "A",
      "answer_ET": "A",
      "answers_community": [
        "A (83%)",
        "B (17%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/151620-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-11-19 11:04:00",
      "unix_timestamp": 1732010640,
      "discussion_count": 4,
      "discussion": [
        {
          "upvote_count": "7",
          "comment_id": "1314547",
          "timestamp": "1732010640.0",
          "poster": "uncledana",
          "content": "Selected Answer: A\nOption A provides the most efficient solution with the least development effort. It uses AWS Systems Manager Automation for orchestrating the execution of deletion scripts and uses Amazon SNS to handle failure notifications, which is simpler and more streamlined than using SES."
        },
        {
          "upvote_count": "1",
          "content": "Selected Answer: A\nLess operational efficiency\nState Manager and Automation Documents already manage scheduling and execution without the need for custom code.\n\nRunCommand executes commands directly on EC2 instances without an external agent.\n\nEventBridge and SNS are already natively integrated with Automation Documents execution failures.\n\nReduced development effort: Uses managed and integrated services without writing code (unlike Lambda).",
          "comment_id": "1589949",
          "poster": "Jonalb",
          "timestamp": "1753346280.0"
        },
        {
          "timestamp": "1743951480.0",
          "poster": "Srikantha",
          "upvote_count": "2",
          "comment_id": "1558286",
          "content": "Selected Answer: A\nSystems Manager State Manager: Can schedule tasks like running scripts on EC2 without having to manage cron jobs or Lambda triggers.\nAutomation Document with Run Command: Easily runs existing scripts on EC2 instances in sequence, fulfilling the requirement to delete files before database records.\nAmazon SNS + EventBridge: Out-of-the-box integration to notify on Automation execution failures — no need to build error-handling logic yourself."
        },
        {
          "comment_id": "1317022",
          "timestamp": "1732450380.0",
          "poster": "tinyshare",
          "comments": [
            {
              "timestamp": "1745341320.0",
              "content": "No, it uses SES not SNS.",
              "poster": "GripZA",
              "comment_id": "1562787",
              "upvote_count": "1"
            }
          ],
          "content": "Selected Answer: B\nA and B are about the same, but B has specifics to check errors by using the return values.",
          "upvote_count": "2"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:18:10.053Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "J23gSbvRAtWpbkfi2Ptj",
      "question_number": 274,
      "page": 55,
      "question_text": "A company uses an organization in AWS Organizations that has all features enabled to manage its AWS accounts. Amazon EQ instances run in the AWS accounts.\n\nThe company requires that all current EC2 instances must use Instance Metadata Service Version 2 (IMDSv2). The company needs to block AWS API calls that originate from EC2 instances that do not use IMDSv2.\n\nWhich solution will meet these requirements?",
      "choices": {
        "D": "Create a new SCP statement that denies when the ec2:MetadataHttpTokens condition key value is not equal to required. Attach the SCP to the root of the organization.",
        "C": "Create a new SCP statement that denies \"*\" when the ec2:RoleDelivery condition key value is less than two. Attach the SCP to the root of the organization.",
        "A": "Create a new SCP statement that denies the ec2:RunInstances action when the ec2:MetadataHttpTokens condition key is not equal to the value of required. Attach the SCP to the root of the organization.",
        "B": "Create a new SCP statement that denies the ec2:RunInstances action when the ec2:MetadataHttpPutResponseHopLimit condition key value is greater than two. Attach the SCP to the root of the organization."
      },
      "correct_answer": "D",
      "answer_ET": "D",
      "answers_community": [
        "D (68%)",
        "A (24%)",
        "8%"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/151621-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-11-19 11:07:00",
      "unix_timestamp": 1732010820,
      "discussion_count": 8,
      "discussion": [
        {
          "poster": "Impromptu",
          "timestamp": "1732453680.0",
          "upvote_count": "7",
          "content": "Selected Answer: D\nI think it's D.\nIt must indeed use the ec2:MetadataHttpTokens condition key, but if we only deny the ec2:RunInstances, then the already running EC2 instances can still do AWS API calls. Even if they are not using IMDSv2.",
          "comment_id": "1317040"
        },
        {
          "poster": "teo2157",
          "content": "Selected Answer: D\nGoing for D, as A just enforce that the new EC2 instances to use IMDSv2 but there can be old instances not running IDMSv2 that can still do API calls...",
          "timestamp": "1734599280.0",
          "upvote_count": "6",
          "comment_id": "1328917"
        },
        {
          "upvote_count": "2",
          "content": "Selected Answer: C\nIt is C. From the official docs, not from ChatGPT: \n\"Furthermore, you can choose an additional layer of protection to enforce the change from IMDSv1 to IMDSv2. At the access management layer with respect to the APIs called via EC2 Role credentials, you can use a new condition key in either IAM policies or AWS Organizations service control policies (SCPs). Specifically, by using the condition key ec2:RoleDelivery with a value of 2.0 in your IAM policies, API calls made with EC2 Role credentials obtained from IMDSv1 will receive an UnauthorizedOperation response.\"\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-metadata-transition-to-version-2.html",
          "poster": "robotgeek",
          "comment_id": "1568991",
          "timestamp": "1747292760.0"
        },
        {
          "content": "Selected Answer: A\nService Control Policies (SCPs) allow you to control which actions are allowed or denied across an entire organization or specific organizational units (OUs) in AWS Organizations.\nThe ec2:MetadataHttpTokens condition key is used to enforce IMDSv2. Setting the value of required ensures that the EC2 instances launched must use IMDSv2, as IMDSv1 would be denied.\nBy denying ec2:RunInstances when the IMDSv2 condition is not met, you are enforcing the policy for all EC2 instances launched, preventing the creation of instances without IMDSv2.",
          "poster": "Srikantha",
          "upvote_count": "1",
          "timestamp": "1743951660.0",
          "comment_id": "1558288"
        },
        {
          "upvote_count": "1",
          "timestamp": "1742997240.0",
          "poster": "DKM",
          "comment_id": "1410391",
          "content": "Selected Answer: A\nThis Service Control Policy (SCP) ensures that any attempt to launch EC2 instances without using IMDSv2 will be denied. By attaching this SCP to the root of the organization, it will apply to all accounts within the organization, ensuring compliance across the board.\n\nHere is an example of the SCP statement:\n\n{\n \"Version\": \"2012-10-17\",\n \"Statement\": [\n {\n \"Effect\": \"Deny\",\n \"Action\": \"ec2:RunInstances\",\n \"Resource\": \"*\",\n \"Condition\": {\n \"StringNotEquals\": {\n \"ec2:MetadataHttpTokens\": \"required\"\n }\n }\n }\n ]\n}"
        },
        {
          "content": "Selected Answer: A\nHere's why:\n\nService Control Policies (SCPs): SCPs allow you to set permission guardrails for all accounts in your organization. By creating an SCP that denies the ec2:RunInstances action when the ec2:MetadataHttpTokens condition key is not set to required, you ensure that only instances configured to use IMDSv2 can be launched1.\nCondition Key: The ec2:MetadataHttpTokens condition key ensures that the instance metadata service requires the use of IMDSv21.\nThis approach enforces the use of IMDSv2 across all EC2 instances in the organization, enhancing security by preventing the use of the less secure IMDSv1.",
          "upvote_count": "1",
          "comment_id": "1410171",
          "poster": "DKM",
          "timestamp": "1742938380.0"
        },
        {
          "upvote_count": "4",
          "timestamp": "1735489860.0",
          "poster": "CHRIS12722222",
          "content": "Selected Answer: D\nOption A will prevent creating ec2 instances, allowing existing ones to violate policy",
          "comment_id": "1333598"
        },
        {
          "timestamp": "1732010820.0",
          "comment_id": "1314548",
          "upvote_count": "3",
          "poster": "uncledana",
          "content": "Selected Answer: A\nOption A provides the correct solution by using the ec2:MetadataHttpTokens condition key in an SCP to deny the ec2:RunInstances action for instances that do not have IMDSv2 enabled. This is the most effective way to ensure compliance with the company’s requirement."
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:18:10.053Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "Fp4Y4FRNTe2JEj2vdRe1",
      "question_number": 275,
      "page": 55,
      "question_text": "A DevOps team supports an application that runs on a large number of Amazon EC2 instances in an Auto Scaling group. The DevOps team uses AWS CloudFormation to deploy the EC2 instances. The application recently experienced an issue. A single instance returned errors to a large percentage of requests. The EC2 instance responded as healthy to both Amazon EC2 and Elastic Load Balancing health checks.\n\nThe DevOps team collects application logs in Amazon CloudWatch by using the embedded metric format. The DevOps team needs to receive an alert if any EC2 instance is responsible for more than half of all errors.\n\nWhich combination of steps will meet these requirements with the LEAST operational overhead? (Choose two.)",
      "choices": {
        "B": "Create a resource group in AWS Resource Groups. Use the CloudFormation stack to group the resources for the application. Add the application to CloudWatch Application Insights. Use the resource group to identify the application.",
        "E": "Create a CloudWatch subscription filter for the application logs that filters for errors and invokes an AWS Lambda function. Configure the Lambda function to send the instance ID and error and in a notification to an Amazon Simple Notification Service (Amazon SNS) topic to notify the DevOps team.",
        "A": "Create a CloudWatch Contributor Insights rule that groups logs from the CloudWatch application logs based on instance ID and errors.",
        "C": "Create a metric filter for the application logs to count the occurrence of the term \"Error.'' Create a CloudWatch alarm that uses the METRIC_COUNT function to determine whether errors have occurred. Configure the CloudWatch alarm to send a notification to an Amazon Simple Notification Service (Amazon SNS) topic to notify the DevOps team.",
        "D": "Create a CloudWatch alarm that uses the INSIGHT_RULE_METRIC function to determine whether a specific instance is responsible for more than half of all errors reported by EC2 instances. Configure the CloudWatch alarm to send a notification to an Amazon Simple Notification Service (Amazon SNS) topic to notify the DevOps team."
      },
      "correct_answer": "AD",
      "answer_ET": "AD",
      "answers_community": [
        "AD (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/151622-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-11-19 11:13:00",
      "unix_timestamp": 1732011180,
      "discussion_count": 3,
      "discussion": [
        {
          "upvote_count": "5",
          "content": "Selected Answer: AD\ncontributor insight for top N",
          "poster": "CHRIS12722222",
          "comment_id": "1333609",
          "timestamp": "1735490820.0"
        },
        {
          "upvote_count": "2",
          "comment_id": "1558291",
          "content": "Selected Answer: AD\nCloudWatch Contributor Insights (A):\nCloudWatch Contributor Insights can be used to analyze CloudWatch logs and aggregate them by custom dimensions, such as the instance ID in this case. This will allow you to identify which EC2 instance is contributing the most to errors in the application logs, which directly addresses the requirement of identifying if any EC2 instance is responsible for more than half of the errors.\nCloudWatch Alarm with INSIGHT_RULE_METRIC (D):\nAfter creating the Contributor Insights rule to track error patterns and group logs by instance, you can use the INSIGHT_RULE_METRIC function to create a CloudWatch alarm. This metric can monitor the Contributor Insights rule for instances that contribute disproportionately to errors (e.g., more than 50% of all errors).\nYou can configure the CloudWatch alarm to trigger notifications via Amazon SNS, alerting the DevOps team when the threshold is breached.",
          "poster": "Srikantha",
          "timestamp": "1743951780.0"
        },
        {
          "upvote_count": "4",
          "timestamp": "1732011180.0",
          "comment_id": "1314551",
          "poster": "uncledana",
          "content": "Selected Answer: AD\nA and D provide a streamlined and automated way to track which EC2 instance is contributing to a high percentage of errors and send an alert when this threshold is crossed, with minimal manual intervention and operational overhead."
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:18:10.053Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "cz2eh0s0gxZOA4IOGhvK",
      "question_number": 276,
      "page": 56,
      "question_text": "A company is using AWS CloudFormation to perform deployments of its application environment. A deployment failed during a recent update to the existing CloudFormation stack. A DevOps engineer discovered that some resources in the stack were manually modified.\n\nThe DevOps engineer needs a solution that detects manual modification of resources and sends an alert to the DevOps lead.\n\nWhich solution will meet these requirements with the LEAST operational effort?",
      "choices": {
        "D": "Create an AWS Config managed rule that has the CLOUDFORMATION_STACK_DRIFT_DETECTION_CHECK identifier. Create an Amazon EventBridge rule that is invoked on the NON_COMPLIANT resources status. Create an AWS Lambda function that sends an email message to the DevOps lead. Set the Lambda function as the rule target.",
        "B": "Tag all CloudFormation resources with a specific tag. Create an AWS Config custom rule by using the AWS Config Rules Development Kit Library (RDKlib) that checks all resource changes that have the specific tag. Configure the custom rule to mark all the tagged resource changes as NON_COMPLIANT when the change is not performed by CloudFormation. Create an Amazon EventBridge rule that is invoked on the NON_COMPUANT resources status. Create an AWS Lambda function that sends an email message to the DevOps lead. Set the Lambda function as the rule target.",
        "A": "Create an Amazon Simple Notification Service (Amazon SNS) topic. Subscribe the DevOps lead to the topic by using an email address. Create an AWS Config managed rule that has the CLOUDFORMATION_STACK_DRIFT_DETECTION_CHECK identifier. Create an Amazon EventBridge rule that is invoked on the NON_COMPLIANT resources status. Set the SNS topic as the rule target.",
        "C": "Create an Amazon Simple Notification Service (Amazon SNS) topic. Subscribe the DevOps lead to the topic by using an email address. Create an AWS Config managed rule that has the CLOUDFORMATION_STACK_DRIFT_DETECTION_CHECK identifier. Create an Amazon EventBridge rule that is invoked on the COMPLIANT resources status. Set the SNS topic as the rule target."
      },
      "correct_answer": "A",
      "answer_ET": "A",
      "answers_community": [
        "A (93%)",
        "7%"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/151623-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-11-19 11:17:00",
      "unix_timestamp": 1732011420,
      "discussion_count": 5,
      "discussion": [
        {
          "poster": "Srikantha",
          "content": "Selected Answer: A\nAWS Config Managed Rule (CLOUDFORMATION_STACK_DRIFT_DETECTION_CHECK):\nThe CLOUDFORMATION_STACK_DRIFT_DETECTION_CHECK is a built-in AWS Config rule that automatically detects drift on resources managed by CloudFormation. Drift refers to manual changes made to CloudFormation-managed resources, and this rule identifies such changes.\nEventBridge Rule:\nYou can create an EventBridge rule that listens for NON_COMPLIANT events triggered by the AWS Config rule when drift is detected. This will ensure that whenever there are manual modifications on CloudFormation-managed resources, the event will be captured.\nSNS Notification:\nUsing Amazon SNS, you can set up an email notification for the DevOps lead whenever the event is triggered. Subscribing the DevOps lead to the SNS topic ensures that they are immediately notified without requiring manual intervention.",
          "upvote_count": "2",
          "comment_id": "1558293",
          "timestamp": "1743952020.0"
        },
        {
          "content": "Selected Answer: A\nKey Requirements:\n\n 1. Detect manual modification of CloudFormation-managed resources.\n 2. Send an alert to the DevOps lead when such changes are detected.\n 3. Achieve this with minimal operational effort.",
          "poster": "Ky_24",
          "comment_id": "1327499",
          "timestamp": "1734366240.0",
          "upvote_count": "4"
        },
        {
          "upvote_count": "4",
          "timestamp": "1733920380.0",
          "content": "Selected Answer: A\nLeast operational overhead always will involve using AWS-Managed services instead of developing code, for example. So, A in my opinion.",
          "comment_id": "1325019",
          "poster": "luisfsm_111"
        },
        {
          "upvote_count": "4",
          "poster": "Impromptu",
          "comment_id": "1317050",
          "timestamp": "1732454340.0",
          "content": "Selected Answer: A\nA is less complex by just using SNS for notifying, instead of creating a lambda function just to do that."
        },
        {
          "poster": "uncledana",
          "timestamp": "1732011420.0",
          "upvote_count": "1",
          "content": "Selected Answer: D\nOption D is the most efficient and least operationally complex solution because it uses AWS Config’s drift detection rule, integrates with EventBridge for event handling, and leverages a Lambda function to send notifications. This approach directly addresses the need to detect manual changes in CloudFormation-managed resources and alert the DevOps lead.",
          "comment_id": "1314553"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:18:20.495Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "ETrs6VG5jlZ8pNxO3OdO",
      "question_number": 277,
      "page": 56,
      "question_text": "A DevOps engineer deployed multiple AWS accounts by using AWS Control Tower to support different business, technical, and administrative units in a company. A security team needs the DevOps engineer to automate AWS Control Tower guardrails for the company. The guardrails must be applied to all accounts in an OU of the company's organization in AWS Organizations.\n\nThe security team needs a solution that has version control and can be reviewed and rolled back if necessary. The security team will maintain the management of the solution in its OU. The security team wants to limit the type of guardrails that are allowed and allow only new guardrails that are approved by the security team.\n\nWhich solution will meet these requirements with the MOST operational efficiency?",
      "choices": {
        "C": "Create individual AWS CloudFormation templates that align to a guardrail. Store the templates in an AWS CodeCommit repository. Create an AWS::ControlTower::EnableControl logical resource in the template for each OU in the organization. Configure an AWS CodePipeline pipeline in the security team's account that an Amazon EventBridge rule will invoke for the security team's CodeCommit changes.",
        "D": "Configure an AWS CodePipeline pipeline in the security team's account that an Amazon EventBridge rule will invoke for PutObject events to an Amazon S3 bucket. Create individual AWS CloudFormation templates that align to a guardrail. Store the templates in the S3 bucket. Create an AWS::ControlTower::EnableControl logical resource in the template for each OU in the organization.",
        "A": "Create individual AWS CloudFormation templates that align to a guardrail. Store the templates in an AWS CodeCommit repository. Create an AWS::ControlTower::EnableControl logical resource in the template for each OU in the organization. Configure an AWS Code Build project that an Amazon EventBridge rule will invoke for the security team's AWS CodeCommit changes.",
        "B": "Create individual AWS CloudFormation templates that align to a guardrail. Store the templates in an AWS CodeCommit repository. Create an AWS::ControlTower::EnableControl logical resource in the template for each account in the organization. Configure an AWS CodePipeline pipeline in the security team's account. Advise the security team to invoke the pipeline and provide these parameters when starting the pipeline."
      },
      "correct_answer": "C",
      "answer_ET": "C",
      "answers_community": [
        "C (78%)",
        "D (22%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/151624-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-11-19 11:22:00",
      "unix_timestamp": 1732011720,
      "discussion_count": 3,
      "discussion": [
        {
          "content": "Selected Answer: C\nOption C is the most efficient and scalable solution for automating AWS Control Tower guardrails while meeting the security team’s requirements for version control, approval, and rollback, with minimal operational overhead. It uses CodeCommit, CodePipeline, and EventBridge, leveraging the best AWS services for this purpose.",
          "comment_id": "1314556",
          "upvote_count": "5",
          "poster": "uncledana",
          "timestamp": "1732011720.0"
        },
        {
          "upvote_count": "2",
          "timestamp": "1743952080.0",
          "poster": "Srikantha",
          "content": "Selected Answer: C\nVersion control is managed easily with CodeCommit, and the changes to the guardrails can be reviewed and rolled back if necessary.\nApproval and governance are built into the process, with the security team controlling the changes and ensuring that only approved guardrails are applied.\nAutomation through CodePipeline and EventBridge ensures that the guardrails are applied to the correct OUs automatically, without the need for manual processes or additional operational overhead.\nThe solution is scalable as it can be applied to multiple OUs and accounts.",
          "comment_id": "1558294"
        },
        {
          "upvote_count": "2",
          "content": "Selected Answer: D\nD is not right because solution should be like AWS CodePipeline pipeline must be invooked by security team commits. But in D, PutObject events to an Amazon S3 bucket is used to invoke CodePipeline.\nA is using AWS Code Build unnecesaarily on Amazon EventBridge rule. It does not say anything automated and involve manual efforts.\nB is completely manual steps mentioned in the line so can't be efficient.\n\nC is completely automated so its a right answer.",
          "comment_id": "1352956",
          "timestamp": "1738927020.0",
          "poster": "c87b433"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:18:20.495Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "m9gt2VinrrUOVWUqVeuE",
      "question_number": 278,
      "page": 56,
      "question_text": "A company runs a web application on Amazon Elastic Kubernetes Service (Amazon EKS). The company uses Amazon CloudFront to distribute the application. The company recently enabled AWS WAF. The company set up Amazon CloudWatch Logs to send logs to an aws-waf-logs log group.\n\nThe company wants a DevOps engineer to receive alerts if there are sudden changes in blocked traffic. The company does not want to receive alerts for other changes in AWS WAF log behavior. The company will tune AWS WAF rules over time.\n\nThe DevOps engineer is currently subscribed to an Amazon Simple Notification Service (Amazon SNS) topic in the environment.\n\nWhich solution will meet these requirements?",
      "choices": {
        "D": "Create a CloudWatch anomaly detector for the log group. Create a CloudWatch alarm by using metrics that the CloudWatch anomaly detector publishes. Use the medium setting for the LogAnomalyPriority metric. Configure the alarm to go into alarm state if a sum of anomalies over 1 hour is greater than an expected value. Configure the alarm to notify the SNS topic to alert the DevOps engineer.",
        "C": "Create a CloudWatch metrics filter for counted requests on the AWS WAF log group to create a custom metric. Create a CloudWatch alarm that activates when the sum of blocked requests in the custom metric during a period of 1 hour is greater than a static estimate for the acceptable number of blocked requests in 1 hour. Configure the alarm to notify the SNS topic to alert the DevOps engineer.",
        "A": "Create a CloudWatch Logs metrics filter for blocked requests on the AWS WAF log group to create a custom metric. Create a CloudWatch alarm by using CloudWatch anomaly detection and the published custom metric. Configure the alarm to notify the SNS topic to alert the DevOps engineer.",
        "B": "Create a CloudWatch anomaly detector for the log group. Create a CloudWatch alarm by using metrics that the CloudWatch anomaly detector publishes. Use the high setting for the LogAnomalyPriority metric. Configure the alarm to go into alarm state if a static threshold of one anomaly is detected. Configure the alarm to notify the SNS topic to alert the DevOps engineer."
      },
      "correct_answer": "A",
      "answer_ET": "A",
      "answers_community": [
        "A (91%)",
        "9%"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/151626-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-11-19 11:24:00",
      "unix_timestamp": 1732011840,
      "discussion_count": 4,
      "discussion": [
        {
          "timestamp": "1743952200.0",
          "poster": "Srikantha",
          "comment_id": "1558295",
          "upvote_count": "2",
          "content": "Selected Answer: A\nOption A is the most efficient solution because it directly monitors blocked requests through a custom CloudWatch metric and uses CloudWatch anomaly detection to identify significant deviations, which is precisely what the company needs to monitor."
        },
        {
          "poster": "CHRIS12722222",
          "comment_id": "1333643",
          "content": "Selected Answer: A\nI go with option A\nif we want to detect SUDDEN change in blocked request, we cant do so in 1hr period as that would be too long a time and what if the blocked request normalised quickly within that 1hr. I think using anomaly detection will provide some upper and lower limit and free us from defining and tuning a static threshold",
          "timestamp": "1735493400.0",
          "upvote_count": "4"
        },
        {
          "poster": "Slays",
          "comment_id": "1333114",
          "content": "Selected Answer: C\nUnc the question said: \"The company does not want to receive alerts for other changes in AWS WAF log behavior\"\n\nThey only want notifications when blocked traffic increase, so anomaly detection doesn't fit the requirements.\n\nGotta be C",
          "timestamp": "1735411980.0",
          "upvote_count": "1"
        },
        {
          "timestamp": "1732011840.0",
          "comments": [],
          "upvote_count": "4",
          "comment_id": "1314558",
          "poster": "uncledana",
          "content": "Selected Answer: A\nOption A provides the most precise and scalable solution to meet the company’s requirements. It focuses on blocked requests, uses anomaly detection for adaptive monitoring, and provides alerting through SNS when a sudden change in blocked traffic occurs."
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:18:20.495Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "q6GVN6202nrQjG3Tiyhm",
      "question_number": 279,
      "page": 56,
      "question_text": "A company uses AWS Storage Gateway in file gateway mode in front of an Amazon S3 bucket that is used by multiple resources. In the morning when business begins, users do not see the objects processed by a third party the previous evening. When a DevOps engineer looks directly at the S3 bucket, the data is there, but it is missing in Storage Gateway.\nWhich solution ensures that all the updated third-party files are available in the morning?",
      "choices": {
        "C": "Modify Storage Gateway to run in volume gateway mode.",
        "A": "Configure a nightly Amazon EventBridge event to invoke an AWS Lambda function to run the RefreshCache command for Storage Gateway.",
        "D": "Use S3 Same-Region Replication to replicate any changes made directly in the S3 bucket to Storage Gateway.",
        "B": "Instruct the third party to put data into the S3 bucket using AWS Transfer for SFTP."
      },
      "correct_answer": "A",
      "answer_ET": "A",
      "answers_community": [
        "A (97%)",
        "3%"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/106199-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-04-14 20:03:00",
      "unix_timestamp": 1681495380,
      "discussion_count": 9,
      "discussion": [
        {
          "upvote_count": "16",
          "poster": "tartarus23",
          "timestamp": "1703026140.0",
          "content": "Selected Answer: A\nExplanation:\n\nAWS Storage Gateway's file gateway mode provides a bridge between your on-premises servers and Amazon S3. File gateway caches frequently accessed files in your on-premises environment to provide low-latency access. However, if the S3 bucket's data is modified by another service, the cache does not automatically refresh. Thus, to ensure all the updated third-party files are available in the morning, you can use an AWS Lambda function triggered by Amazon EventBridge to run the RefreshCache command for Storage Gateway. This will ensure the cache is updated with the latest changes.",
          "comment_id": "927922"
        },
        {
          "comment_id": "870992",
          "poster": "ele",
          "comments": [
            {
              "timestamp": "1715771460.0",
              "content": "Thanks for this.\nAlso found https://repost.aws/knowledge-center/storage-gateway-automate-refreshcache\n\nStorage Gateway allows you to automate the RefreshCache operation based on a Time To Live (TTL) value. TTL is the length of time since the last refresh. When a user accesses the file directory after the TTL value, the file gateway refreshes the directory's contents from the S3 bucket. Valid TTL values for automating the RefreshCache operation range from 300 seconds to 2,592,000 seconds (5 minutes to 30 days).",
              "upvote_count": "3",
              "comment_id": "1071461",
              "poster": "robertohyena"
            }
          ],
          "upvote_count": "12",
          "timestamp": "1697380440.0",
          "content": "Selected Answer: A\nA: refresh cache: https://repost.aws/knowledge-center/storage-gateway-s3-changes-not-showing"
        },
        {
          "poster": "Gomer",
          "content": "Selected Answer: A\nRead and concede:\n\"Configure an automated cache refresh schedule using AWS Lambda with an Amazon CloudWatch rule\"\nhttps://docs.aws.amazon.com/filegateway/latest/files3/refresh-cache.html#auto-refresh-lambda-procedure",
          "comment_id": "1221911",
          "timestamp": "1732943520.0",
          "upvote_count": "1"
        },
        {
          "poster": "bhond",
          "content": "where is it saying files are written directly to s3 ?",
          "comment_id": "981736",
          "timestamp": "1708012740.0",
          "comments": [
            {
              "upvote_count": "3",
              "content": "You do make a point but if you read the phrase \" When a DevOps engineer looks directly at the S3 bucket \" it kinda implies besides you dont have any other better choice anyway. if you look at user \"ele\" comments and follow the link below it will get clear[hope that helps];\nhttps://repost.aws/knowledge-center/storage-gateway-s3-changes-not-showing",
              "poster": "yorkicurke",
              "timestamp": "1716484920.0",
              "comment_id": "1078695"
            }
          ],
          "upvote_count": "1"
        },
        {
          "comment_id": "979872",
          "poster": "ixdb",
          "content": "A is right.\nStorage Gateway updates the file share cache automatically when you write files to the cache\nlocally using the file share. However, Storage Gateway doesn't automatically update the cache\nwhen you upload a file directly to Amazon S3. When you do this, you must perform a\nRefreshCache operation to see the changes on the file share.",
          "timestamp": "1707822120.0",
          "upvote_count": "1"
        },
        {
          "timestamp": "1702653000.0",
          "comment_id": "924214",
          "poster": "madperro",
          "upvote_count": "1",
          "content": "Selected Answer: A\nA is the answer."
        },
        {
          "comment_id": "886962",
          "upvote_count": "2",
          "poster": "haazybanj",
          "timestamp": "1698897840.0",
          "content": "Selected Answer: A\nThe issue appears to be related to the Storage Gateway cache not being updated. To ensure that all the updated third-party files are available in the morning, you can use the RefreshCache API to manually refresh the cache or configure automatic cache refresh.\n\nOption A is a possible solution to configure automatic cache refresh, but it is not necessary to run the RefreshCache command every night if you can ensure that cache refresh occurs frequently enough to meet your requirements."
        },
        {
          "timestamp": "1697320560.0",
          "content": "Selected Answer: A\nA is correct",
          "poster": "alce2020",
          "upvote_count": "1",
          "comment_id": "870500"
        },
        {
          "comments": [
            {
              "comment_id": "910449",
              "content": "Transfer SFTP has the same effect in this case as adding files to S3 with PutObject. The cache in the storage gateway would not be updated, requiring the same refresh as in option A.",
              "upvote_count": "1",
              "timestamp": "1701370560.0",
              "poster": "bcx"
            }
          ],
          "comment_id": "870387",
          "upvote_count": "1",
          "timestamp": "1697306580.0",
          "poster": "jqso234",
          "content": "Selected Answer: B\nOption B appears to be the correct choice. Configuring the third party to put data into the S3 bucket using AWS Transfer for SFTP would ensure that the data is immediately available in both the S3 bucket and Storage Gateway, avoiding any potential caching issues. Option A of configuring a nightly event to refresh the cache may not be an optimal solution as it could result in stale data being served during the day."
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:18:20.495Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "5UZhIVGOWC4VqD7AbmiS",
      "question_number": 280,
      "page": 56,
      "question_text": "A video platform company is migrating its video catalog to AWS. The company will host MP4 videos files in an Amazon S3 bucket. The company will use Amazon CloudFront and Amazon EC2 instances to serve the video files.\n\nUsers first connect to a frontend application that redirects to a video URL. The video URL contains an authorization token in CloudFront. The cache is activated on the CloudFront distribution. Authorization token check activity needs to be logged in Amazon CloudWatch.\n\nThe company wants to prevent direct access to video files on CloudFront and Amazon S3 and wants to implement checks of the authorization token that the frontend application provides. The company also wants to perform regular rolling updates of the code that checks the authorization token signature.\n\nWhich solution will meet these requirements with the LEAST operational effort?",
      "choices": {
        "D": "Implement an authorization token check in CloudFront Functions. Enable CloudWatch logging for the CloudFront function. Attach the CloudFront function to the CloudFront distribution. Implement a second CloudFront distribution. Migrate the traffic from the first CloudFront distribution by using Amazon Route 53 weighted routing.",
        "B": "Implement an authorization token check in CloudFront Functions. Enable CloudWatch logging for the CloudFront function. Attach the CloudFront function to the CloudFront distribution. Implement CloudFront continuous deployment to perform updates.",
        "A": "Implement an authorization token check in Lambda@Edge as a trigger on the CloudFront distribution. Enable CloudWatch logging for the Lambda@Edge function. Attach the Lambda@Edge function to the CloudFront distribution. Implement CloudFront continuous deployment to perform updates.",
        "C": "Implement an authorization token check in the application code that is installed on the EC2 instances. Install the CloudWatch agent on the EC2 instances. Configure the application to log to the CloudWatch agent. Implement a second CloudFront distribution. Migrate the traffic from the first CloudFront distribution by using Amazon Route 53 weighted routing."
      },
      "correct_answer": "B",
      "answer_ET": "B",
      "answers_community": [
        "B (80%)",
        "A (20%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/151628-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-11-19 11:35:00",
      "unix_timestamp": 1732012500,
      "discussion_count": 6,
      "discussion": [
        {
          "upvote_count": "1",
          "poster": "nickp84",
          "comment_id": "1571977",
          "content": "Selected Answer: A\nB. Use CloudFront Functions:\nCloudFront Functions are lightweight and designed for simple request/response modifications (e.g., URL rewriting or header manipulation) but have limitations in runtime (Node.js or Python), memory, and execution time (less than 1 second). They are not well-suited for complex token signature validation (e.g., cryptographic operations for JWTs). Additionally, CloudFront Functions do not support direct CloudWatch logging; logging requires additional setup (e.g., via CloudFront access logs), increasing complexity. While continuous deployment is supported, the functional limitations make this less suitable.",
          "timestamp": "1748107320.0"
        },
        {
          "upvote_count": "1",
          "poster": "GripZA",
          "comment_id": "1562796",
          "timestamp": "1745343600.0",
          "content": "Selected Answer: B\nCloudFront Functions over Lambda@Edge here since its integrated directly with CloudFront and CloudFront continuous deployment."
        },
        {
          "comment_id": "1558296",
          "poster": "Srikantha",
          "upvote_count": "1",
          "content": "Selected Answer: B\nOption B is the most efficient and operationally effective solution. It uses CloudFront Functions to check the authorization token directly at the edge, minimizing latency, and integrates seamlessly with CloudWatch for logging. Additionally, CloudFront continuous deployment simplifies updates, making it the optimal solution for the company’s requirements.",
          "timestamp": "1743952380.0"
        },
        {
          "timestamp": "1735568820.0",
          "content": "Selected Answer: A\nOption A is correct. Here's why:\nLambda@Edge:\n\nPerfect for token authorization checks\nSupports CloudWatch logging\nCan handle complex validation logic\nBuilt for CloudFront integration\nRolling updates via continuous deployment",
          "poster": "matt200",
          "upvote_count": "1",
          "comment_id": "1334187"
        },
        {
          "timestamp": "1732335000.0",
          "comment_id": "1316557",
          "poster": "f4b18ba",
          "content": "Selected Answer: B\nCloudFront Functions is a lightweight JavaScript-based environment that runs at the edge and is designed for high performance with low latency. It's ideal for simple tasks like authorization checks.\nEnabling CloudWatch logging for CloudFront Functions ensures that the authorization token check activities are logged, providing visibility into the process.\n Implementing CloudFront continuous deployment simplifies the process of rolling updates for the function, ensuring that new code can be deployed quickly and seamlessly.",
          "upvote_count": "3"
        },
        {
          "poster": "uncledana",
          "comment_id": "1314567",
          "timestamp": "1732012500.0",
          "content": "Selected Answer: B\nOption B provides the most efficient solution with the least operational overhead. It uses CloudFront Functions for token validation, enables CloudWatch logging, and supports continuous deployment for easy updates, meeting the company’s requirements in a scalable and cost-effective manner.",
          "upvote_count": "3"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:18:20.495Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "pueU1T1E6bFH5TkKDXQd",
      "question_number": 281,
      "page": 57,
      "question_text": "A company uses an organization in AWS Organizations to manage multiple AWS accounts in a hierarchical structure. An SCP that is associated with the organization root allows IAM users to be created.\n\nA DevOps team must be able to create IAM users with any level of permissions. Developers must also be able to create IAM users. However, developers must not be able to grant new IAM users excessive permissions. The developers have the CreateAndManageUsers role in each account. The DevOps team must be able to prevent other users from creating IAM users.\n\nWhich combination of steps will meet these requirements? (Choose two.)",
      "choices": {
        "A": "Create an SCP in the organization to deny users the ability to create and modify IAM users. Attach the SCP to the root of the organization. Attach the CreateAndManageUsers role to developers.",
        "D": "Create an IAM permissions policy named PermissionBoundaries within each account. Configure PermissionsBoundaries to allow users who have the PermissionBoundaries policy to create new IAM users.",
        "C": "Create an IAM permissions policy named PermissionBoundaries within each account. Configure the PermissionBoundaries policy to specify the maximum permissions that a developer can grant to a new IAM user.",
        "E": "Create an IAM permissions policy named DeveloperBoundary within each account. Configure the DeveloperBoundary policy to allow developers to create IAM users and to assign policies to IAM users of only if the developer includes the PermissionBoundaries policy as the permissions boundary. Attach the DeveloperBoundary policy to the CreateAndManageUsers role within each account.",
        "B": "Create an SCP in the organization to grant users that have the DeveloperBoundary policy attached the ability to create new IAM users and to modify IAM users. Configure the SCP to require users to attach the PermissionBoundaries policy to any new IAM user. Attach the SCP to the root of the organization."
      },
      "correct_answer": "CE",
      "answer_ET": "CE",
      "answers_community": [
        "CE (93%)",
        "7%"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/151629-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-11-19 11:39:00",
      "unix_timestamp": 1732012740,
      "discussion_count": 5,
      "discussion": [
        {
          "upvote_count": "5",
          "timestamp": "1734366840.0",
          "comment_id": "1327507",
          "poster": "Ky_24",
          "content": "Selected Answer: CE\n1. IAM user creation:\n • Both the DevOps team and developers should be able to create IAM users.\n 2. Permissions control:\n • Developers should be restricted from granting excessive permissions to the IAM users they create.\n 3. Prevention of unauthorized IAM user creation:\n • Only the designated roles (DevOps and developers) should create IAM users.\n\nTo achieve this, AWS Permissions Boundaries provide an effective way to enforce limits on the permissions that developers can assign"
        },
        {
          "content": "Selected Answer: CE\nA would prevent anyone to create IAM users, so both DevOps teams and Developers cannot create IAM users.\nB would prevent DevOps team to create IAM users \"with any level of permissions\".\nC would create the permission boundary that defines the maximum permissions of a user created by the Developers.\nD does not work like that. The permission boundary would be used for preventing too many permissions on a user created by the Developers, and not for giving them user creation rights as well.\nE would give the Developers the permissions to create users, but would force them to also attach the permission boundary (created in C) to the new user, limiting their permissions correctly (even if the Developer would give that user too many permissions)",
          "poster": "Impromptu",
          "comment_id": "1317064",
          "upvote_count": "5",
          "timestamp": "1732456200.0"
        },
        {
          "timestamp": "1753344900.0",
          "upvote_count": "1",
          "content": "Selected Answer: CE\n✅ C.\n\nCreate an IAM permissions policy named PermissionBoundaries within each account. Configure the PermissionBoundaries policy to specify the maximum permissions a developer can grant to a new IAM user.\n\n✅ Reason: This clearly defines the permissions limits that developers can grant to users they create—exactly the expected behavior when using permissions boundaries.\n\n✅ E.\n\nCreate an IAM permissions policy named DeveloperBoundary within each account. Configure the DeveloperBoundary policy to allow developers to create IAM users and assign policies to IAM users only if the developer includes the PermissionBoundaries policy as the permissions boundary. Attach the DeveloperBoundary policy to the CreateAndManageUsers role within each account.\n\n✅ Reason: This step ensures that developers can only create users if they are using the boundary (PermissionBoundaries). This enforces the golden rule: you can only create users if you use the boundary.",
          "poster": "Jonalb",
          "comment_id": "1589944"
        },
        {
          "comment_id": "1319089",
          "poster": "ArunRav",
          "timestamp": "1732781160.0",
          "content": "Selected Answer: CE\nSCP in A denies the access to everyone but it doesnt explain the details about PermissionBoundaries policy used in C option. When you combine the C option with E option ie Creation of PermissionBoundaries policy to create the boundary and Creation of Developer boundary policy which allow developers to have access with boundaries mentioned in PermissionBoundaries make sense.\nHence CE",
          "upvote_count": "3"
        },
        {
          "comment_id": "1314571",
          "content": "Selected Answer: AE\nOption A provides the control at the organizational level to deny IAM user creation by non-DevOps users.\n • Option E ensures that developers can create users with limited permissions by enforcing Permission Boundaries, ensuring they cannot assign excessive permissions.\n\nThis combination effectively meets the requirements with the least operational overhead.",
          "timestamp": "1732012740.0",
          "upvote_count": "1",
          "poster": "uncledana"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:18:31.046Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "pRDl6BNhXLb2Vgc5fXWD",
      "question_number": 282,
      "page": 57,
      "question_text": "A company has deployed a landing zone that has a well-defined AWS Organizations structure and an SCP. The company's development team can create their AWS resources only by using AWS CloudFormation and the AWS Cloud Development Kit (AWS CDK).\n\nA DevOps engineer notices that Amazon Simple Queue Service (Amazon SQS) queues that are deployed in different CloudFormation stacks have different configurations. The DevOps engineer also notices that the application cost allocation tag is not always set.\n\nThe DevOps engineer needs a solution that will enforce tagging and promote the reuse of code. The DevOps engineer needs to avoid different configurations for the deployed SQS queues.\n\nWhat should the DevOps engineer do to meet these requirements?",
      "choices": {
        "B": "Update the SCP to enforce the cost allocation tag in CloudFormation stacks. Instruct the development team to use CloudFormation modules to define SQS queues. Instruct the development team to deploy the SQS queues by using CloudFormation stacks.",
        "A": "Create an Organizations tag policy to enforce the cost allocation tag in CloudFormation stacks. Instruct the development team to use CloudFormation to define SQS queues. Instruct the development team to deploy the SQS queues by using CloudFormation StackSets.",
        "C": "Use AWS CDK tagging to enforce the cost allocation tag in CloudFormation StackSets. Instruct the development team to use the AWS CDK to define SQS queues. Instruct the development team to deploy the SQS queues by using CDK stacks.",
        "D": "Use AWS CDK tagging to enforce the cost allocation tag in CloudFormation stacks. Instruct the development team to use the AWS CDK to define SQS queues. Instruct the development team to deploy the SQS queues by using CDK feature flags."
      },
      "correct_answer": "C",
      "answer_ET": "C",
      "answers_community": [
        "C (58%)",
        "B (25%)",
        "D (17%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/151630-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-11-19 11:40:00",
      "unix_timestamp": 1732012800,
      "discussion_count": 7,
      "discussion": [
        {
          "comment_id": "1323485",
          "content": "Selected Answer: C\nC: \nEnforce tagging across all accounts via StackSets. \nUse CDK Stacks to deploy same configuration SQS.",
          "poster": "sn61613",
          "upvote_count": "6",
          "timestamp": "1733653620.0"
        },
        {
          "timestamp": "1747288860.0",
          "poster": "robotgeek",
          "content": "Selected Answer: B\nGuys, the question is poorly redacted but clearly it is not C \n- Option A: Within the Tag Policy itself, you can enable enforcement for specific resource types (so it would be the typical correct response if it was not for the \"reuse\" thing)\n\nSource: https://docs.aws.amazon.com/whitepapers/latest/tagging-best-practices/implementing-and-enforcing-tagging.html\n\n- Option B: \"Instruct the development team to use CloudFormation modules\", that improves \"promote the reuse of code\" while assuring enforcing\n\n- Option C and D: BOTH options rely on programmers not forgetting to implement lines of codes to configure the tags, that is not how Devops and enforcement works guys.",
          "comment_id": "1568984",
          "upvote_count": "1"
        },
        {
          "timestamp": "1744246740.0",
          "comment_id": "1559436",
          "upvote_count": "1",
          "content": "Selected Answer: D\nTagging enforcement via CDK stack-level tagging.\nReusable constructs for consistent SQS configuration.\nCDK feature flags to help with best practices and configuration enforcement.\nAll with the least operational overhead and maximum developer productivity.",
          "poster": "Srikantha"
        },
        {
          "timestamp": "1738523340.0",
          "poster": "jojewi8143",
          "comment_id": "1350623",
          "upvote_count": "1",
          "content": "Selected Answer: C\nGoing with C"
        },
        {
          "poster": "teo2157",
          "comment_id": "1343743",
          "upvote_count": "1",
          "timestamp": "1737391140.0",
          "content": "Selected Answer: B\nGoing with B which enforce tagging using SCPs and promote the reuse of code using CF modules for the SQS."
        },
        {
          "poster": "ArunRav",
          "timestamp": "1732782300.0",
          "comment_id": "1319095",
          "upvote_count": "1",
          "content": "Selected Answer: B\nThough it is a straightforward solution...It doesn't give the level of enforcing with SCP has. Hence for restricting SCP and CFT to deploy.\nHence B"
        },
        {
          "timestamp": "1732012800.0",
          "comment_id": "1314573",
          "content": "Selected Answer: D\nOption D provides a straightforward, flexible, and scalable solution by using AWS CDK for both tagging enforcement and reusing code, while also ensuring that developers can maintain standardization in their deployments with feature flags. This solution promotes best practices in terms of both infrastructure consistency and operational efficiency.",
          "poster": "uncledana",
          "upvote_count": "1"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:18:31.046Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "dDQBqIfzI2XcSXwYHXED",
      "question_number": 283,
      "page": 57,
      "question_text": "A DevOps team manages a company's AWS account. The company wants to ensure that specific AWS resource configuration changes are automatically reverted.\n\nWhich solution will meet this requirement?",
      "choices": {
        "A": "Use AWS Config rules to detect changes in resource configurations. Configure remediation action that uses AWS Systems Manager Automation documents to revert the configuration changes.",
        "C": "Use AWS CloudFormation to create a stack that deploys the necessary configuration changes. Update the stack when configuration changes need to be reverted.",
        "B": "Use Amazon CloudWatch alarms to monitor resource metrics. When an alarm is activated, use an Amazon Simple Notification Service (Amazon SNS) topic to notify an administrator to manually reverts the configuration changes.",
        "D": "Use AWS Trusted Advisor to check for noncompliant configurations. Manually apply necessary changes based on Trusted Advisor recommendations."
      },
      "correct_answer": "A",
      "answer_ET": "A",
      "answers_community": [
        "A (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/151631-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2024-11-19 11:42:00",
      "unix_timestamp": 1732012920,
      "discussion_count": 2,
      "discussion": [
        {
          "poster": "uncledana",
          "comment_id": "1314575",
          "content": "Selected Answer: A\nThe best solution is A, where AWS Config rules are used to monitor resource configuration changes, and remediation actions using AWS Systems Manager Automation documents are configured to automatically revert any non-compliant configuration changes. This provides an automated, scalable solution that meets the requirement to ensure that specific AWS resource configuration changes are automatically reverted.",
          "timestamp": "1732012920.0",
          "upvote_count": "6"
        },
        {
          "content": "Selected Answer: A\nAWS Systems Manager Automation & AWS Config",
          "comment_id": "1579261",
          "poster": "gbemimatti",
          "timestamp": "1750459920.0",
          "upvote_count": "1"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:18:31.046Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "uJmpY5wwVxrrF1GOHlfe",
      "question_number": 284,
      "page": 57,
      "question_text": "A company hosts an application in its AWS account. The application uses an Amazon S3 bucket to store objects that contain sensitive information.\n\nThe company needs to capture object-level S3 API calls, including calls that are rejected because the calls were made by using credentials that are not valid.\n\nWhich solution will meet these requirements?",
      "choices": {
        "A": "Create an AWS CloudTrail trail in the account. Enable S3 data events logging. Configure the trail to log to Amazon CloudWatch.",
        "C": "Configure Amazon GuardDuty with S3 protection enabled for the account. Create an Amazon EventBridge rule that matches findings that are associated with the S3 bucket. Configure the rule to use an Amazon Simple Queue Service (Amazon SQS) queue as the target.",
        "B": "Create a new S3 bucket. Configure access logging on the application's S3 bucket. Deliver the access logs to the new S3 bucket.",
        "D": "Create an AWS CloudTrail trail and a new S3 bucket in the account. Configure the trail to log to the S3 new bucket."
      },
      "correct_answer": "A",
      "answer_ET": "A",
      "answers_community": [
        "A (67%)",
        "B (33%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/302826-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2025-04-22 20:14:00",
      "unix_timestamp": 1745345640,
      "discussion_count": 3,
      "discussion": [
        {
          "content": "Selected Answer: A\nTo capture API calls at object level in the Amazon S3, including calls rejected by invalid credentials, you need to enable the \"data events\" in AWS CloudTrail.\n\nCloudtrail is the only service that records all API calls, including:\n\nPutobject, GetObject, Deleteobject, etc.\n\nAuthorized and unauthorized calls (eg, AccessDenied error or invalidationSKeyid).\n\nDate events are required to register object level operations in S3.\n\nSending logs to Cloudwatch allows for real -time alerts and queries (optional, but helps visibility).",
          "upvote_count": "1",
          "comment_id": "1591073",
          "timestamp": "1753720260.0",
          "poster": "Jonalb"
        },
        {
          "poster": "gbemimatti",
          "upvote_count": "1",
          "timestamp": "1750460280.0",
          "comment_id": "1579262",
          "content": "Selected Answer: A\nOption A\nS3 Access Logs are great for basic access auditing and billing analysis, but they do not capture denied requests or IAM-level details.\n\nCloudTrail data events are the only service that logs both accepted and rejected S3 object-level API calls, fulfilling the security requirement."
        },
        {
          "content": "Selected Answer: B\nServer access logging provides detailed records for the requests that are made to a bucket. Server access logs are useful for many applications. For example, access log information can be useful in security and access audits. This information can also help you learn about your customer base and understand your Amazon S3 bill.\n\nbest practice to use a new S3 bucket: When your source bucket and destination bucket are the same bucket, additional logs are created for the logs that are written to the bucket, which creates an infinite loop of logs. We do not recommend doing this because it could result in a small increase in your storage billing. \n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/ServerLogs.html",
          "upvote_count": "1",
          "poster": "GripZA",
          "timestamp": "1745345640.0",
          "comment_id": "1562799"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:18:31.046Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "ngjVJfayon8qUHJkKQ68",
      "question_number": 285,
      "page": 57,
      "question_text": "A DevOps administrator is responsible for managing the security of a company's Amazon CloudWatch Logs log groups. The company's security policy states that employee IDs must not be visible in logs except by authorized personnel. Employee IDs follow the pattern of Emp-XXXXXX, where each X is a digit.\n\nAn audit discovered that employee IDs are found in a single log file. The log file is available to engineers, but the engineers are not authorized to view employee IDs. Engineers currently have an AWS IAM Identity Center permission that allows logs:* on all resources in the account.\n\nThe administrator must mask the employee ID so that new log entries that contain the employee ID are not visible to unauthorized personnel.\n\nWhich solution will meet these requirements with the MOST operational efficiency?",
      "choices": {
        "D": "Create an Amazon Data Firehose delivery stream that has an Amazon S3 bucket as the destination. Create a Firehose subscription filter on the log group that uses the Firehose delivery stream. Remove the \"logs:*\" permission on the engineering accounts. Create an Amazon Macie job on the S3 bucket that has an Emp-\\d{6} custom identifier.",
        "C": "Create an AWS Lambda function to parse a log file entry, remove the employee ID, and write the results to a new log file. Create a Lambda subscription filter on the log group and select the Lambda function. Grant the lambda:InvokeFunction permission to the log group.",
        "B": "Create a new data protection policy on the log group. Add managed data identifiers for the personal data category. Create an IAM policy that has a Deny action for the \"NotAction\":\"logs:Unmask\" permission on the resource. Attach the policy to the engineering accounts.",
        "A": "Create a new data protection policy on the log group. Add an Emp-\\d{6} custom data identifier configuration. Create an IAM policy that has a Deny action for the Action\":\"logs:Unmask\" permission on the resource. Attach the policy to the engineering accounts."
      },
      "correct_answer": "A",
      "answer_ET": "A",
      "answers_community": [
        "A (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/302827-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2025-04-22 20:23:00",
      "unix_timestamp": 1745346180,
      "discussion_count": 1,
      "discussion": [
        {
          "upvote_count": "1",
          "content": "Selected Answer: A\nCloudWatch Logs uses data protection policies to select the sensitive data for which you want to scan, and the actions that you want to take to protect that data. To select the sensitive data of interest, you use data identifiers. To act upon data identifiers that are found, you can define audit and de-identify operations.\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/cloudwatch-logs-data-protection-policies.html\n\nCustom data identifiers (CDIs) let you define your own custom regular expressions that can be used in your data protection policy. Using custom data identifiers, you can target business-specific personally identifiable information (PII) use cases that managed data identifiers can't provide. \nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/CWL-custom-data-identifiers.html",
          "comment_id": "1562800",
          "timestamp": "1745346180.0",
          "poster": "GripZA"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:18:31.046Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "X2XbDYxqq1UOpiOoW0nX",
      "question_number": 286,
      "page": 58,
      "question_text": "A company uses an organization in AWS Organizations to manage many AWS accounts. The company has enabled all features for the organization. The company uses AWS CloudFormation StackSets to deploy configurations to the accounts. The company uses AWS Config to monitor an Amazon S3 bucket.\n\nThe company needs to ensure that all object uploads to the S3 bucket use AWS Key Management Service (AWS KMS) encryption.\n\nWhich solution will meet these requirements?",
      "choices": {
        "B": "Create an SCP that includes a deny statement for the s3:createBucket action and a condition statement where s3:x-amz-server-side-encryption is not aws:kms. Attach the SCP to the root of the organization.",
        "C": "Create an AWS CloudFormation stack set to enable an AWS CloudTrail trail to capture S3 data events for the organization. In the stack set, create an Amazon EventBridge rule to match S3 PutObject events that do not use AWS KMS encryption. Configure the rule to target an Amazon Simple Notification Service (Amazon SNS) topic.",
        "D": "Create an SCP that includes a deny statement for the s3:putObject action and a condition where s3:x-amz-server-side-encryption is not aws:kms. Attach the SCP to the root of the organization.",
        "A": "Create an AWS Config conformance pack that includes the s3-bucket-server-side-encryption-enabled rule. Deploy the conformance pack to the accounts. Configure the rule to target an Amazon Simple Notification Service (Amazon SNS) topic."
      },
      "correct_answer": "D",
      "answer_ET": "D",
      "answers_community": [
        "D (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/302828-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2025-04-22 20:28:00",
      "unix_timestamp": 1745346480,
      "discussion_count": 1,
      "discussion": [
        {
          "timestamp": "1745346480.0",
          "upvote_count": "1",
          "comment_id": "1562801",
          "content": "Selected Answer: D\nAn SCP allows you to enforce guardrails across all accounts in an AWS Organization.\nThis policy:\nDenies the action s3:PutObject\nWhen the request does not specify aws:kms in s3:x-amz-server-side-encryption\n\nThis enforces encryption at upload time, and it blocks any upload attempts without KMS encryption.",
          "poster": "GripZA"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:18:41.592Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "WjiaWGMGfIiA1eaNa2z7",
      "question_number": 287,
      "page": 58,
      "question_text": "A company uses an Amazon Aurora PostgreSQL DB cluster and loads transactional data into the database every 5 hours. Data analysts use the Aurora PostgreSQL database to run short-running queries, create complex aggregated queries, and create simple reports that use the data. The data analysts also manually update the data, including deleting and inserting data.\n\nThe data analysts have reported performance issues. The database team recently identified a long-running idle transaction connection that affected performance by blocking other queries and preventing VACUUM operations. The team wants to be proactively notified about these potential operational issues and about the recommended actions to fix the issues.\n\nThe company's AWS account uses Amazon DevOps Guru to monitor all the applications in the account.\n\nWhich solution will meet these requirements?",
      "choices": {
        "D": "Turn on Performance Insights in the existing Aurora PostrgreSQL DB cluster. Configure Amazon EventBridge to receive events from the existing Aurora PostgreSQL DB cluster. Configure DevOps Guru to send notifications to the database team by using Amazon Simple Notification Service (Amazon SNS).",
        "A": "Turn on Performance Insights and DevOps Guru in the existing Aurora PostgreSQL DB cluster. Configure DevOps Guru to send notifications to the database team by using Amazon Simple Notification Service (Amazon SNS).",
        "C": "Turn on Performance Insights and DevOps Guru in the existing Aurora PostgreSQL DB cluster. Configure the Aurora PostgreSQL DB cluster to send notifications to the database team by using Amazon Simple Email Service (Amazon SES).",
        "B": "Turn on Performance Insights in the existing Aurora PostrgreSQL DB cluster. Configure Amazon EventBridge to receive events from the existing Aurora PostgreSQL DB cluster. Configure the Aurora PostgreSQL DB cluster to send notifications to the database team by using Amazon Simple Notification Service (Amazon SNS)."
      },
      "correct_answer": "A",
      "answer_ET": "A",
      "answers_community": [
        "A (67%)",
        "D (33%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/302829-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2025-04-22 20:34:00",
      "unix_timestamp": 1745346840,
      "discussion_count": 3,
      "discussion": [
        {
          "timestamp": "1750461420.0",
          "comment_id": "1579263",
          "content": "Selected Answer: A\nEventBridge not needed here",
          "poster": "gbemimatti",
          "upvote_count": "1"
        },
        {
          "poster": "AWSLoverLoverLoverLoverLover",
          "timestamp": "1748241660.0",
          "comment_id": "1572381",
          "content": "Selected Answer: A\nDevOps Guru can detect anomalous behavior and operational issues and provide recommendations to fix them.\n\nSince the company's AWS account already uses DevOps Guru, it’s best to enable it for the Aurora PostgreSQL DB cluster, so it can start monitoring it specifically.\n\nPerformance Insights provides additional visibility into database performance\n\nAmazon SNS is a standard method for alerting or notifying teams when an operational issue is detected.\n\nNot D, assumes DevOps Guru is already monitoring the Aurora cluster, but DevOps Guru must be explicitly enabled per resource, and D omits that step.",
          "upvote_count": "1"
        },
        {
          "comment_id": "1562803",
          "timestamp": "1745346840.0",
          "upvote_count": "1",
          "poster": "GripZA",
          "content": "Selected Answer: D\nD over A, since the company's AWS account already uses Amazon DevOps Guru to monitor all the applications in the account."
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:18:41.592Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "jK772kBD2uxvye7CfLLZ",
      "question_number": 288,
      "page": 58,
      "question_text": "A DevOps engineer is implementing governance controls for a company that requires its infrastructure to be housed within the United States. The company has many AWS accounts in an organization in AWS Organizations that has all features enabled.\n\nThe engineer must restrict which AWS Regions the company can use. The engineer must also ensure that an alert is sent as soon as possible if any activity outside the governance policy occurs. The controls must be automatically enabled on any new Region outside the United States.\n\nWhich combination of steps will meet these requirements? (Choose two.)",
      "choices": {
        "B": "Configure AWS CloudTrail to send logs to Amazon CloudWatch Logs. Enable CloudTrail for all Regions. Use a CloudWatch Logs metric filter to create a metric in non-US Regions. Configure a CloudWatch alarm to send an alert if the metric is greater than 0.",
        "C": "Use an AWS Lambda function that checks for AWS service activity. Deploy the Lambda function to all Regions. Write an Amazon EventBridge rule that runs the Lambda function every hour. Configure the rule to send an alert if the Lambda function finds any activity in a non-US Region.",
        "E": "Create an Organizations SCP allow policy that has a condition that the aws:RequestedRegion property matches a list of all US Regions. Include an exception in the policy for global services. Attach the policy to the root of the organization.",
        "A": "Create an Organizations SCP deny policy that has a condition that the aws:RequestedRegion property does not match a list of all US Regions. Include an exception in the policy for global services. Attach the policy to the root of the organization.",
        "D": "Use an AWS Lambda function to query Amazon Inspector to look for service activity in non-US Regions. Configure the Lambda function to send alerts if Amazon Inspector finds any activity."
      },
      "correct_answer": "AB",
      "answer_ET": "AB",
      "answers_community": [
        "AB (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/302864-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2025-04-23 04:15:00",
      "unix_timestamp": 1745374500,
      "discussion_count": 1,
      "discussion": [
        {
          "comment_id": "1562907",
          "upvote_count": "1",
          "timestamp": "1745374500.0",
          "poster": "GripZA",
          "content": "Selected Answer: AB\nSimilar to Question #184"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:18:41.592Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "4zKgaqnx46HQwrrOuYhI",
      "question_number": 289,
      "page": 58,
      "question_text": "A company runs applications on Amazon EC2 instances that are in an Amazon EC2 Auto Scaling group. The EC2 instances are behind an Application Load Balancer (ALB). Users recently began to experience errors when traffic was directed to some of the EC2 instances.\n\nA DevOps engineer discovers that the Auto Scaling group reports the problematic instances are healthy despite the application errors. User experience returns to normal after the DevOps engineer resolves the application errors on the problematic instances.\n\nThe company wants to ensure that traffic is routed only to healthy instances that are not experiencing application errors. The company also wants a support team to receive a notification if the traffic routing configuration changes.\n\nWhich solution will meet these requirements?",
      "choices": {
        "A": "Configure the Auto Scaling group to use ELB health checks. Enable AWS Config. Create an AWS Config rule to ensure that any new Auto Scaling group will use ELB health checks. Create an Amazon Simple Notification Service (Amazon SNS) topic to notify the support team if the traffic routing configuration changes. Configure the AWS Config rule to send a notification to the topic.",
        "C": "Configure the Auto Scaling group to use EC2 health checks. Create an Amazon CloudWatch synthetic canary to monitor the application. Create a CloudWatch alarm that is triggered when the CloudWatch canary fails. Configure the alarm to notify the support team when the alarm state is in alarm.",
        "B": "Configure the Auto Scaling group to use EC2 health checks. Enable AWS Config. Create an AWS Config rule to ensure that any new Auto Scaling group will use EC2 health checks. Create an Amazon Simple Notification Service (Amazon SNS) topic to notify the support team if the traffic routing configuration changes. Configure the AWS Config rule to send a notification to the topic.",
        "D": "Configure the Auto Scaling group to use ELB health checks. Create an Amazon CloudWatch synthetic canary to monitor the application. Create a CloudWatch alarm that is triggered when the CloudWatch canary fails. Configure the alarm to notify the support team when the alarm state is in alarm."
      },
      "correct_answer": "D",
      "answer_ET": "D",
      "answers_community": [
        "D (60%)",
        "A (40%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/302871-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2025-04-23 06:43:00",
      "unix_timestamp": 1745383380,
      "discussion_count": 4,
      "discussion": [
        {
          "poster": "Adzz",
          "comment_id": "1572167",
          "timestamp": "1748176320.0",
          "upvote_count": "1",
          "content": "Selected Answer: D\noption D"
        },
        {
          "content": "Selected Answer: D\nUse CloudWatch Synthetics (canary) to simulate real user behavior (e.g., making HTTP requests to the app).\n\nIf the canary fails (e.g., due to application errors or routing problems), a CloudWatch alarm is triggered.\n\nThis alarm can send a notification via Amazon SNS to the support team.\n\nD includes a canary and CloudWatch alarm with SNS notification — this provides deep visibility and alerts the support team.",
          "timestamp": "1747924020.0",
          "poster": "AWSLoverLoverLoverLoverLover",
          "upvote_count": "1",
          "comment_id": "1571343"
        },
        {
          "upvote_count": "1",
          "timestamp": "1746716640.0",
          "content": "Selected Answer: D\nThis solution uses AWS Config to enforce ELB health checks in Auto Scaling groups, but this is not necessary.\n\nA CloudWatch synthetic canary simulates user interactions with the application",
          "comment_id": "1567438",
          "poster": "connorhoehn"
        },
        {
          "upvote_count": "2",
          "content": "Selected Answer: A\nuse ALB as health check type in ASG settings, not EC2, for application errors.\n\nWhile CloudWatch synthetic canaries can monitor application endpoints, they do not prevent traffic from being routed to unhealthy instances. So config over synthetic canary here.",
          "poster": "GripZA",
          "comment_id": "1562947",
          "timestamp": "1745383380.0"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:18:41.592Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "XMlRAAqE2nyUrUqiRM4y",
      "question_number": 290,
      "page": 58,
      "question_text": "A DevOps engineer needs to back up sensitive Amazon S3 objects that are stored within an S3 bucket with a private bucket policy using S3 cross-Region replication functionality. The objects need to be copied to a target bucket in a different AWS Region and account.\nWhich combination of actions should be performed to enable this replication? (Choose three.)",
      "choices": {
        "E": "Create a replication rule in the source bucket to enable the replication.",
        "F": "Create a replication rule in the target bucket to enable the replication.",
        "C": "Add statements to the source bucket policy allowing the replication IAM role to replicate objects.",
        "B": "Create a replication I AM role in the target account.",
        "D": "Add statements to the target bucket policy allowing the replication IAM role to replicate objects.",
        "A": "Create a replication IAM role in the source account"
      },
      "correct_answer": "ADE",
      "answer_ET": "ADE",
      "answers_community": [
        "ADE (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/106261-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-04-15 16:39:00",
      "unix_timestamp": 1681569540,
      "discussion_count": 8,
      "discussion": [
        {
          "upvote_count": "7",
          "comment_id": "894541",
          "content": "Selected Answer: ADE\nS3 cross-Region replication (CRR) automatically replicates data between buckets across different AWS Regions. To enable CRR, you need to add a replication configuration to your source bucket that specifies the destination bucket, the IAM role, and the encryption type (optional). You also need to grant permissions to the IAM role to perform replication actions on both the source and destination buckets. Additionally, you can choose the destination storage class and enable additional replication options such as S3 Replication Time Control (S3 RTC) or S3 Batch Replication.",
          "timestamp": "1699680600.0",
          "poster": "tschenhau"
        },
        {
          "comments": [
            {
              "poster": "Gomer",
              "timestamp": "1733011740.0",
              "comment_id": "1222359",
              "content": "Source Account: (Source Bucket)(Versioning)(Role/Policy to \"Enable\" Replicaton)\nTarget Account: (Target Bucket)(Versioning)(Role/Policy to \"Allow\" Replicaton)",
              "upvote_count": "2"
            }
          ],
          "upvote_count": "4",
          "timestamp": "1733010900.0",
          "comment_id": "1222353",
          "content": "Selected Answer: ADE\nTricky question because they are trying to get one to confuse the \"enable\" replicaton \"role\"/policy (\"rule\") in source account with the \"allow\" replicaton role/\"policy\" in target account. These references helped me work up some summary steps:\nSteps to configure S3 replication between different accounts\n1. Create source and destination buckets in different accounts and regions (acctA, acctB)\n2. Enable versioning on the buckets (acctA, acctB)\n3. Create IAM role and attach a policy granting S3 permission to replicate objects (acctA)\n4. Add the replication configuration to source bucket (acctA)\n5. Add bucket \"policy on the destination bucket to allow\" objects replication (acctB)(req. 2nd role)\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/replication-walkthrough1.html#enable-replication\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/replication-walkthrough-2.html",
          "poster": "Gomer"
        },
        {
          "poster": "thanhnv142",
          "content": "ADF is correct: this task is done by S3 itself\nA: Create role in the source to allow S3 access permission\nD: add policy to allow repication in the target\nE: enable replication in the source",
          "timestamp": "1722301140.0",
          "comment_id": "1135457",
          "upvote_count": "2"
        },
        {
          "upvote_count": "2",
          "poster": "bugincloud",
          "content": "Selected Answer: ADE\nADE make sense.",
          "timestamp": "1710952020.0",
          "comment_id": "1012408"
        },
        {
          "poster": "madperro",
          "timestamp": "1702653360.0",
          "upvote_count": "2",
          "comment_id": "924220",
          "content": "Selected Answer: ADE\nADE make sense."
        },
        {
          "timestamp": "1698898200.0",
          "content": "Selected Answer: ADE\nConfirmed",
          "poster": "haazybanj",
          "upvote_count": "2",
          "comment_id": "886964"
        },
        {
          "content": "ADE confirmed!",
          "upvote_count": "1",
          "timestamp": "1697421000.0",
          "comment_id": "871403",
          "poster": "alce2020"
        },
        {
          "content": "Selected Answer: ADE\nADE\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/replication-walkthrough-2.html",
          "timestamp": "1697380740.0",
          "comment_id": "870996",
          "poster": "ele",
          "upvote_count": "3"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:18:41.592Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "85Gw1KoxOZjNduKAn2G6",
      "question_number": 291,
      "page": 59,
      "question_text": "A DevOps engineer needs to troubleshoot a pipeline that uses a GitHub code repository. The pipeline contains a source stage, a build stage, and a deploy stage. The pipeline also has an AWS CodeStar connection to the GitHub code repository.\n\nThe build stage uses an AWS CodeBuild build project. The build project needs to perform a git clone of the repository as part of the build process.\n\nThe DevOps engineer validates that the source stage is working properly. However, the build stage fails each time the pipeline runs.\n\nWhat is the reason that the build stage fails in the pipeline?",
      "choices": {
        "B": "The AWS CodeStar connection to GitHub contains incorrect credentials.",
        "D": "The AWS CodeBuild service role does not have permission to use the AWS CodeStar connection.",
        "C": "The AWS CodePipeline service role does not have permission to use the AWS CodeStar connection.",
        "A": "The build stage within the pipeline needs to use the AWS CodeStar connection action."
      },
      "correct_answer": "D",
      "answer_ET": "D",
      "answers_community": [
        "D (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/302872-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2025-04-23 06:59:00",
      "unix_timestamp": 1745384340,
      "discussion_count": 2,
      "discussion": [
        {
          "comment_id": "1579265",
          "poster": "gbemimatti",
          "content": "Selected Answer: D\nA. The build stage doesn't support a separate CodeStar action—it relies on CodeBuild having the correct permissions.\n\nB. The source stage is working, so CodeStar credentials are correct.\n\nC. The CodePipeline service role already can access CodeStar (since the Source stage works); the issue is within the CodeBuild role, not the pipeline role.",
          "timestamp": "1750462380.0",
          "upvote_count": "1"
        },
        {
          "comment_id": "1562950",
          "content": "Selected Answer: D\nThe initial pipeline run will fail because the CodeBuild service role must be updated with permissions to use connections. Add the codestar-connections:UseConnection IAM permission to your service role policy.\n\nhttps://docs.aws.amazon.com/codepipeline/latest/userguide/tutorials-github-gitclone.html",
          "poster": "GripZA",
          "upvote_count": "1",
          "timestamp": "1745384340.0"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:18:52.444Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "9Wk65BiaWF5Vhy2K8fcV",
      "question_number": 292,
      "page": 59,
      "question_text": "A company's DevOps team uses Node Package Manager (NPM) open source libraries to build applications. The DevOps team runs its application build process in an AWS CodeBuild project that downloads the NPM libraries from public NPM repositories.\n\nThe company wants to host the NPM libraries in private NPM repositories. The company also needs to be able to run checks on new versions of the libraries before the DevOps team uses the libraries.\n\nWhich solution will meet these requirements with the LEAST operational effort?",
      "choices": {
        "B": "Enable Amazon S3 caching in the CodeBuild project configuration. Add a step in the buildspec.yaml config file to perform the required checks on the package versions in the cache.",
        "D": "Create an AWS CodeCommit repository for each library. Clone the required NPM libraries to the appropriate CodeCommit repository. Modify the CodeBuild buildspec.yaml config file so that NPM uses the private CodeCommit repositories. Add an AWS CodePipeline pipeline that performs the required checks on the package versions for each new commit to the repositories. Configure the pipeline to revert to the most recent commit in the event of a failure.",
        "C": "Create an AWS CodeCommit repository for each library. Clone the required NPM libraries to the appropriate CodeCommit repository. Modify the CodeBuild appspec.yaml config file to use the private CodeCommit repositories. Add a step to perform the required checks on the package versions.",
        "A": "Create an AWS CodeArtifact repository with an upstream repository named npm-store. Configure the application build process to use the CodeArtifact repository as the default source for NPM. Create an AWS CodePipeline pipeline to perform the required checks on package versions in the CodeArtifact repository. Set the package status to unlisted if a failure occurs."
      },
      "correct_answer": "A",
      "answer_ET": "A",
      "answers_community": [
        "A (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/157101-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2025-02-25 18:30:00",
      "unix_timestamp": 1740504600,
      "discussion_count": 2,
      "discussion": [
        {
          "timestamp": "1745384820.0",
          "poster": "GripZA",
          "upvote_count": "1",
          "comment_id": "1562953",
          "content": "Selected Answer: A\nCodeArtifact stores software packages in repositories. \nYou can create a connection between your private CodeArtifact repository and an external, public repository, such as npmjs.com. CodeArtifact will then fetch and store packages on demand from the public repository when they're requested by a package manager. \n\nhttps://docs.aws.amazon.com/codeartifact/latest/ug/welcome.html"
        },
        {
          "timestamp": "1740504600.0",
          "comment_id": "1361515",
          "poster": "Bwhizzy",
          "upvote_count": "1",
          "content": "Selected Answer: A\nCodeArtifact with Upstream Repository\n\nProxy Public Packages: The npm-store upstream repository acts as a proxy for npm.org, automatically caching public packages in CodeArtifact13.\n\nCentralized Control: The build process uses CodeArtifact as the default NPM source, ensuring all dependencies are managed privately.\n\nCodePipeline for Version Checks\n\nAutomated Validation: The pipeline runs tests or checks on new package versions before they’re approved for use.\n\nUnlisted Status: Failed packages are marked as unlisted, preventing their inclusion in builds until resolved16.\n\nWhy Other Options Fail:\n\nB/C/D: Cloning libraries into CodeCommit (C/D) or using S3 caching (B) adds manual effort and complexity compared to CodeArtifact’s native proxying.\n\nNo Need for Manual Cloning: CodeArtifact’s upstream eliminates the need to clone public packages into private repositories"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:18:52.444Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "zsTRFaIjEE9RmNtdgq7V",
      "question_number": 293,
      "page": 59,
      "question_text": "A company has a search application that has a web interface. The company uses Amazon CloudFront, Application Load Balancers (ALBs), and Amazon EC2 instances in an Auto Scaling group with a desired capacity of 3. The company uses prebaked AMIs. The application starts in 1 minute. The application queries an Amazon OpenSearch Service cluster.\n\nThe application is deployed to multiple Availability Zones. Because of compliance requirements, the application needs to have a disaster recovery (DR) environment in a separate AWS Region. The company wants to minimize the ongoing cost of the DR environment and requires an RTO and an RPO of under 30 minutes. The company has created an ALB in the DR Region.\n\nWhich solution will meet these requirements?",
      "choices": {
        "B": "Create a new CloudFront distribution in the DR Region and add the new ALB as an origin. Use Amazon Route 53 DNS for Regional failover. Copy the AMI to the DR Region. Create a launch template and an Auto Scaling group with a desired capacity of 0 in the DR Region. Reconfigure the OpenSearch Service cluster as a Multi-AZ with Standby deployment. Ensure that the standby nodes are in the DR Region.",
        "D": "Add the new ALB as an origin in the CloudFront distribution. Configure origin failover functionality. Copy the AMI to the DR Region. Create a launch template and an Auto Scaling group with a desired capacity of 3 in the DR Region. Create a new OpenSearch Service cluster in the DR Region. Set up cross-cluster replication for the cluster.",
        "A": "Add the new ALB as an origin in the CloudFront distribution. Configure origin failover functionality. Copy the AMI to the DR Region. Create a launch template and an Auto Scaling group with a desired capacity of 0 in the DR Region. Create a new OpenSearch Service cluster in the DR Region. Set up cross-cluster replication for the cluster.",
        "C": "Create a new CloudFront distribution in the DR Region and add the new ALB as an origin. Use Amazon Route 53 DNS for Regional failover. Copy the AMI to the DR Region. Create a launch template and an Auto Scaling group with a desired capacity of 3 in the DR Region. Reconfigure the OpenSearch Service cluster as a Multi-AZ with Standby deployment. Ensure that the standby nodes are in the DR Region."
      },
      "correct_answer": "A",
      "answer_ET": "A",
      "answers_community": [
        "A (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/302873-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2025-04-23 07:18:00",
      "unix_timestamp": 1745385480,
      "discussion_count": 1,
      "discussion": [
        {
          "comment_id": "1562956",
          "upvote_count": "2",
          "timestamp": "1745385480.0",
          "poster": "GripZA",
          "content": "Selected Answer: A\nactive-passive strategy with desired capacity of 0. 1min application start time gives enough time to increase desired count when you failover to DR region. \n\nOpenSearch supports cross-cluster replication for DR scenarios. \"With cross-cluster replication in Amazon OpenSearch Service, you can replicate user indexes, mappings, and metadata from one OpenSearch Service domain to another. Using cross-cluster replication helps to ensure disaster recovery if there is an outage, and allows you to replicate data across geographically distant data centers to reduce latency.\"\n\nhttps://docs.aws.amazon.com/opensearch-service/latest/developerguide/replication.html\n\nnot B or C since DR env is in a separate region. OpenSearch multi-AZ won't help.\n\nnot D since it will have running 3 instances with associated runtime costs."
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:18:52.444Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "nUrIMKsFr7jjZsEZ6rWj",
      "question_number": 294,
      "page": 59,
      "question_text": "A DevOps engineer uses AWS WAF to manage web ACLs across an AWS account. The DevOps engineer must ensure that AWS WAF is enabled for all Application Load Balancers (ALBs) in the account. The DevOps engineer uses an AWS CloudFormation template to deploy an individual ALB and AWS WAF as part of each application stack's deployment process. If AWS WAF is removed from the ALB after the ALB is deployed, AWS WAF must be added to the ALB automatically.\n\nWhich solution will meet these requirements with the MOST operational efficiency?",
      "choices": {
        "A": "Enable AWS Config. Add the alb-waf-enabled managed rule. Create an AWS Systems Manager Automation document to add AWS WAF to an ALB. Edit the rule to automatically remediate. Select the Systems Manager Automation document as the remediation action.",
        "B": "Enable AWS Config. Add the alb-waf-enabled managed rule. Create an Amazon EventBridge rule to send all AWS Config ConfigurationItemChangeNotification notification types to an AWS Lambda function. Configure the Lambda function to call the AWS Config start-resource-evaluation API in detective mode.",
        "C": "Configure an Amazon EventBridge rule to periodically call an AWS Lambda function that calls the detect-stack-drift API on the CloudFormation template. Configure the Lambda function to modify the ALB attributes with waf.fail_open.enabled set to true if the AWS::WAFv2::WebACLAssociation resource shows a status of drifted.",
        "D": "Configure an Amazon EventBridge rule to periodically call an AWS Lambda function that calls the detect-stack-drift API on the CloudFormation template. Configure the Lambda function to delete and redeploy the CloudFormation stack if the AWS::WAFv2::WebACLAssociation resource shows a status of drifted."
      },
      "correct_answer": "A",
      "answer_ET": "A",
      "answers_community": [
        "A (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/169071-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2025-03-14 21:20:00",
      "unix_timestamp": 1741983600,
      "discussion_count": 3,
      "discussion": [
        {
          "comment_id": "1589946",
          "upvote_count": "1",
          "poster": "Jonalb",
          "content": "Selected Answer: A\nAWS Config can detect non-compliance, such as an ALB without a WAF.\n\nThe alb-waf-enabled managed rule checks for exactly that: whether an AWS WAF ACL is associated with the ALB.\n\nBy configuring an automatic remediation action with Systems Manager Automation, you can automatically reattach the Web ACL to the ALB as soon as non-compliance is detected.\n\nThis approach is 100% automated, based on managed services, with no scripting or manual detection required.",
          "timestamp": "1753345320.0"
        },
        {
          "comment_id": "1562958",
          "poster": "GripZA",
          "upvote_count": "1",
          "timestamp": "1745386020.0",
          "content": "Selected Answer: A\nelb-waf-enabled managed rule checks if AWS WAF is enabled on ALBs\nSystems Manager Automation provides predefined runbooks for Elastic Load Balancing."
        },
        {
          "timestamp": "1741983600.0",
          "comment_id": "1395712",
          "poster": "2b80c69",
          "upvote_count": "1",
          "content": "Selected Answer: A\nAWS Config has elb-waf-enabled managed rule which solves the problem in a most operationally efficient manner.\nNot B coz, it focus more on sending notifications which is not the purpose of the question\nNot C & D coz, it has drift detection, which is again not the purpose of the question"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:18:52.444Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "5KsRakjH3WKuoPNBJByA",
      "question_number": 295,
      "page": 59,
      "question_text": "A company has multiple member accounts that are part of an organization in AWS Organizations. The security team needs to review every Amazon EC2 security group and their inbound and outbound rules. The security team wants to programmatically retrieve this information from the member accounts using an AWS Lambda function in the management account of the organization.\nWhich combination of access changes will meet these requirements? (Choose three.)",
      "choices": {
        "F": "Create an IAM role in the management account that has access to the AmazonEC2ReadOnlyAccess managed policy.",
        "C": "Create an IAM role in each member account that has access to the AmazonEC2ReadOnlyAccess managed policy.",
        "A": "Create a trust relationship that allows users in the member accounts to assume the management account IAM role.",
        "D": "Create an I AM role in each member account to allow the sts:AssumeRole action against the management account IAM role's ARN.",
        "B": "Create a trust relationship that allows users in the management account to assume the IAM roles of the member accounts.",
        "E": "Create an I AM role in the management account that allows the sts:AssumeRole action against the member account IAM role's ARN."
      },
      "correct_answer": "BCE",
      "answer_ET": "BCE",
      "answers_community": [
        "BCE (81%)",
        "Other"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/106307-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-04-16 03:54:00",
      "unix_timestamp": 1681610040,
      "discussion_count": 15,
      "discussion": [
        {
          "comment_id": "927930",
          "content": "Selected Answer: BCE\nExplanation:\n\n(B) The trust relationship enables an IAM entity (user, group, or role) to assume a role. In this case, the entities in the management account need to assume roles in the member accounts.\n\n(C) The IAM role in each member account should have a policy attached that grants read-only access to EC2 instances. The AmazonEC2ReadOnlyAccess managed policy provides this access.\n\n(E) An IAM role in the management account should be created that has the permission to perform the sts:AssumeRole action against the member account IAM role's ARN. This allows entities assuming this role to switch to the roles in the member accounts and perform actions according to the permissions of those roles.",
          "poster": "tartarus23",
          "timestamp": "1703026380.0",
          "upvote_count": "9"
        },
        {
          "content": "BCE are correct: \nB: create trust relationship for management to assume role in member accounts\nC: create role in member account that has access to AmazoneEC2\nE: Create IAM role in management account that allow access to member account IAM role",
          "upvote_count": "4",
          "comment_id": "1135472",
          "poster": "thanhnv142",
          "timestamp": "1722303240.0"
        },
        {
          "poster": "svjl",
          "content": "The security team wants to programmatically retrieve this information from the member accounts using an AWS Lambda function in the management account of the organization.\n\nReadOnlyAccess and option B grant the assumeRole\nBesides that the correct resource is \"IAM\" not \"I AM\" So BCF is correct",
          "timestamp": "1716733260.0",
          "comment_id": "1080792",
          "upvote_count": "1"
        },
        {
          "upvote_count": "1",
          "poster": "RVivek",
          "comment_id": "991865",
          "timestamp": "1709110080.0",
          "content": "Selected Answer: BCE\nB- Member accounts should trust Management account\nC- Memeber accounts should have a Role athat has the necessary permission\nE- Managment account should have a IAM user account that has stsAssume role permission for the roles created in member accounts"
        },
        {
          "comment_id": "985439",
          "upvote_count": "2",
          "poster": "incorrigble_maverick",
          "comments": [
            {
              "timestamp": "1714677240.0",
              "upvote_count": "1",
              "poster": "zain1258",
              "content": "D is clearly wrong. You are running your lambda function to get details in management account. The IAM role should be in management account with sts:AssumeRole permission to assume IAM roles in member accounts",
              "comment_id": "1060917"
            }
          ],
          "content": "BCE is wrong. They want to programmatically therefore B is definitenly wrong. The Lambda function IAM Role ARN in the management account needs to be able to assume a role in the member account that has the AmazonEC2ReadOnlyAccess attached to it. Therefore, I will go with C, D, E",
          "timestamp": "1708383600.0"
        },
        {
          "poster": "DavidPham",
          "content": "BCE correct",
          "timestamp": "1706171760.0",
          "comment_id": "962404",
          "upvote_count": "1"
        },
        {
          "upvote_count": "1",
          "comment_id": "924226",
          "content": "Selected Answer: BCE\nBCE is right.",
          "poster": "madperro",
          "timestamp": "1702653660.0"
        },
        {
          "comment_id": "910455",
          "upvote_count": "1",
          "content": "B, C and E",
          "poster": "bcx",
          "timestamp": "1701370980.0"
        },
        {
          "upvote_count": "2",
          "comment_id": "902550",
          "timestamp": "1700488260.0",
          "content": "Selected Answer: BCE\nB, C and E",
          "poster": "PhuocT"
        },
        {
          "comment_id": "899577",
          "poster": "2pk",
          "upvote_count": "2",
          "timestamp": "1700177520.0",
          "content": "Selected Answer: ACE\nA:By creating a trust relationship that allows users in the member accounts to assume the IAM role in the management account, they will have the necessary permissions to access resources and retrieve the required information.\n\nC:To grant the necessary permissions for retrieving information about EC2 security groups, an IAM role should be created in each member account. This role should have the AmazonEC2ReadOnlyAccess managed policy attached, which provides the required permissions.\n\nE:In the management account, an IAM role should be created that allows assuming the IAM role in the member accounts. This role should have the necessary permissions to perform the sts:AssumeRole action against the ARN of the IAM roles in the member accounts."
        },
        {
          "content": "Selected Answer: BCE\nBCE will create correct cross account permission",
          "upvote_count": "1",
          "poster": "ele",
          "timestamp": "1699874400.0",
          "comment_id": "896540"
        },
        {
          "timestamp": "1699094640.0",
          "comment_id": "889305",
          "poster": "vherman",
          "upvote_count": "1",
          "content": "Selected Answer: ACD\nacd looks good)"
        },
        {
          "poster": "marcoforexam",
          "timestamp": "1698934920.0",
          "comment_id": "887320",
          "content": "Selected Answer: ACD\nACE I guess\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-account-with-roles.html",
          "upvote_count": "1"
        },
        {
          "upvote_count": "1",
          "content": "Selected Answer: BCE\nCorrect answer",
          "timestamp": "1698898320.0",
          "comment_id": "886965",
          "poster": "haazybanj"
        },
        {
          "comment_id": "871404",
          "timestamp": "1697421240.0",
          "poster": "alce2020",
          "content": "Selected Answer: BCE\nIll go with BCF",
          "upvote_count": "2"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:18:52.444Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "ISfwDyrdOIqzbMRNrA97",
      "question_number": 296,
      "page": 60,
      "question_text": "A space exploration company receives telemetry data from multiple satellites. Small packets of data are received through Amazon API Gateway and are placed directly into an Amazon Simple Queue Service (Amazon SQS) standard queue. A custom application is subscribed to the queue and transforms the data into a standard format.\nBecause of inconsistencies in the data that the satellites produce, the application is occasionally unable to transform the data. In these cases, the messages remain in the SQS queue. A DevOps engineer must develop a solution that retains the failed messages and makes them available to scientists for review and future processing.\nWhich solution will meet these requirements?",
      "choices": {
        "D": "Configure API Gateway to send messages to different SQS virtual queues that are named for each of the satellites. Update the application to use a new virtual queue for any data that it cannot transform, and send the message to the new virtual queue. Instruct the scientists to use the virtual queue to review the data that is not valid. Reprocess this data at a later time.",
        "A": "Configure AWS Lambda to poll the SQS queue and invoke a Lambda function to check whether the queue messages are valid. If validation fails, send a copy of the data that is not valid to an Amazon S3 bucket so that the scientists can review and correct the data. When the data is corrected, amend the message in the SQS queue by using a replay Lambda function with the corrected data.",
        "B": "Convert the SQS standard queue to an SQS FIFO queue. Configure AWS Lambda to poll the SQS queue every 10 minutes by using an Amazon EventBridge schedule. Invoke the Lambda function to identify any messages with a SentTimestamp value that is older than 5 minutes, push the data to the same location as the application's output location, and remove the messages from the queue.",
        "C": "Create an SQS dead-letter queue. Modify the existing queue by including a redrive policy that sets the Maximum Receives setting to 1 and sets the dead-letter queue ARN to the ARN of the newly created queue. Instruct the scientists to use the dead-letter queue to review the data that is not valid. Reprocess this data at a later time."
      },
      "correct_answer": "C",
      "answer_ET": "C",
      "answers_community": [
        "C (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/105438-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-04-06 18:30:00",
      "unix_timestamp": 1680798600,
      "discussion_count": 23,
      "discussion": [
        {
          "upvote_count": "2",
          "timestamp": "1725794400.0",
          "content": "Selected Answer: C\nCreate an SQS dead-letter queue. Modify the existing queue by including a re-drive policy that sets the Maximum Receives setting to 1 and sets the dead-letter queue ARN to the ARN of the newly created queue. Instruct the scientists to use the dead-letter queue to review the data that is not valid. Reprocess this data at a later time.",
          "comment_id": "1168822",
          "poster": "4555894"
        },
        {
          "upvote_count": "1",
          "comment_id": "1168226",
          "poster": "zijo",
          "content": "Answer is A. Lambda function is required for automated fixing of the invalid message data and hence A is the right choice here.",
          "comments": [
            {
              "comment_id": "1168607",
              "timestamp": "1725771780.0",
              "upvote_count": "1",
              "poster": "dzn",
              "content": "This is not a good approach because it requires unifying the validation logic of the custom application and Lambda function, requires updating both the custom application and Lambda when data specifications change, and requires that the timing of those updates be the same from the SQS perspective, making the deployment process more complex and devops cost expensive. BTW, failed messages are reviewed by scientists, and there is no requirement that they be automatically fix by the program."
            }
          ],
          "timestamp": "1725723000.0"
        },
        {
          "timestamp": "1724231700.0",
          "poster": "dzn",
          "content": "Selected Answer: C\nA Dead Letter Queue (DLQ) can be the destination queue for messages that cannot be successfully processed by other queues. DLQs are used to analyze why a message failed or to isolate problem messages.",
          "comment_id": "1155459",
          "upvote_count": "2"
        },
        {
          "poster": "thanhnv142",
          "content": "C is correct: Use dead letter queue and config maximum receives is the right way",
          "comment_id": "1135474",
          "upvote_count": "1",
          "timestamp": "1722303420.0"
        },
        {
          "content": "definitely C",
          "poster": "Bans",
          "timestamp": "1720371360.0",
          "upvote_count": "1",
          "comment_id": "1116112"
        },
        {
          "comment_id": "1092440",
          "upvote_count": "3",
          "poster": "harithzainudin",
          "timestamp": "1718010180.0",
          "content": "Selected Answer: C\nThis is DLQ use case. So, its 100% C"
        },
        {
          "content": "Selected Answer: C\neveryone votes C but answer seems as A. which one correct? should we trust to voters or examtopic? :D",
          "timestamp": "1717237140.0",
          "poster": "SafranboluLokumu",
          "comment_id": "1085199",
          "upvote_count": "4"
        },
        {
          "comment_id": "1056979",
          "upvote_count": "2",
          "content": "The correct answer is C . This is a use case for Dead Letter Queue",
          "timestamp": "1714401480.0",
          "poster": "xhi158"
        },
        {
          "timestamp": "1710952860.0",
          "comment_id": "1012421",
          "content": "Selected Answer: C\nclassic DLQ usecase",
          "poster": "bugincloud",
          "upvote_count": "1"
        },
        {
          "comment_id": "991404",
          "timestamp": "1709039220.0",
          "upvote_count": "1",
          "poster": "Skshitiz",
          "content": "Selected Answer: C\nC - DLQ"
        },
        {
          "timestamp": "1708952400.0",
          "upvote_count": "1",
          "poster": "FEEREWMWKA",
          "content": "C - DLQ",
          "comment_id": "990656"
        },
        {
          "timestamp": "1706368920.0",
          "upvote_count": "1",
          "content": "DevOps is about automation! Variant C says: \"Instruct scientists... \" :D\nSo variant A is the best among other",
          "comment_id": "964756",
          "poster": "andriit"
        },
        {
          "poster": "Just_Ninja",
          "comment_id": "949526",
          "timestamp": "1705048020.0",
          "upvote_count": "4",
          "content": "Selected Answer: C\nDLQ is the right solution. SQS is one to one! So Lambda make no sense."
        },
        {
          "timestamp": "1704713220.0",
          "comment_id": "946350",
          "content": "Selected Answer: C\nAlways do with DLQ for failed deliveries. C all the way",
          "poster": "habros",
          "upvote_count": "1"
        },
        {
          "comment_id": "924229",
          "content": "Selected Answer: C\nC answer with DLQ is a right solution.",
          "timestamp": "1702654020.0",
          "upvote_count": "1",
          "poster": "madperro"
        },
        {
          "content": "Selected Answer: C\nThe perfect case for a dead-letter queue",
          "poster": "bcx",
          "upvote_count": "1",
          "timestamp": "1701371040.0",
          "comment_id": "910459"
        },
        {
          "upvote_count": "1",
          "timestamp": "1700812320.0",
          "comment_id": "905518",
          "content": "Selected Answer: C\nSQS DLQ needed",
          "poster": "Akaza"
        },
        {
          "poster": "mgonblan",
          "comment_id": "899271",
          "content": "sure is C, but the parameter maximum receives looks too small and could create false negatives.\n https://aws.amazon.com/es/blogs/compute/using-amazon-sqs-dead-letter-queues-to-control-message-failure/",
          "upvote_count": "2",
          "timestamp": "1700152140.0"
        },
        {
          "upvote_count": "1",
          "comment_id": "886969",
          "poster": "haazybanj",
          "content": "Selected Answer: C\nC is it",
          "timestamp": "1698898740.0"
        },
        {
          "content": "C is the correct answer",
          "comment_id": "870503",
          "poster": "alce2020",
          "upvote_count": "2",
          "timestamp": "1697321400.0"
        },
        {
          "upvote_count": "2",
          "content": "Selected Answer: C\nOption C is the correct solution. Creating a dead-letter queue and modifying the queue's redrive policy enables the messages that failed to be moved to the dead-letter queue instead of being discarded. Scientists can then review the data in the dead-letter queue and reprocess it later.\n\nOption A is incorrect because it involves the creation of a separate mechanism to replay failed messages. Although the use of Amazon S3 bucket allows scientists to review and correct the data, it doesn't provide a means of retaining failed messages.",
          "comment_id": "870440",
          "timestamp": "1697312880.0",
          "poster": "jqso234"
        },
        {
          "comment_id": "867971",
          "timestamp": "1697091780.0",
          "upvote_count": "1",
          "content": "Selected Answer: C\nCreate an Amazon DynamoDB table and configure an SQS dead letter queue to send failed messages to the DynamoDB table.",
          "poster": "ma_rio"
        },
        {
          "timestamp": "1696609800.0",
          "content": "Selected Answer: C\nС for sure",
          "upvote_count": "1",
          "comment_id": "863145",
          "poster": "Dimidrol"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:19:04.034Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "FRIvkeEvpTuWpRXFI68l",
      "question_number": 297,
      "page": 60,
      "question_text": "A company wants to use AWS CloudFormation for infrastructure deployment. The company has strict tagging and resource requirements and wants to limit the deployment to two Regions. Developers will need to deploy multiple versions of the same application.\nWhich solution ensures resources are deployed in accordance with company policy?",
      "choices": {
        "D": "Create AWS Service Catalog products with approved CloudFormation templates.",
        "B": "Create a Cloud Formation drift detection operation to find and remediate unapproved CloudFormation StackSets.",
        "C": "Create CloudFormation StackSets with approved CloudFormation templates.",
        "A": "Create AWS Trusted Advisor checks to find and remediate unapproved CloudFormation StackSets."
      },
      "correct_answer": "D",
      "answer_ET": "D",
      "answers_community": [
        "D (78%)",
        "C (22%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/106215-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-04-15 00:13:00",
      "unix_timestamp": 1681510380,
      "discussion_count": 26,
      "discussion": [
        {
          "upvote_count": "9",
          "comment_id": "1092442",
          "poster": "harithzainudin",
          "timestamp": "1702206420.0",
          "content": "Selected Answer: D\n100% D.\n\nAWS Service Catalog lets you centrally manage your cloud resources to achieve governance at scale of your infrastructure as code (IaC) templates, written in CloudFormation or Terraform configurations. With AWS Service Catalog, you can meet your compliance requirements while making sure your customers can quickly deploy the cloud resources they need.\nhttps://aws.amazon.com/servicecatalog/\n\nall other service in other answer is not related."
        },
        {
          "comment_id": "1267882",
          "upvote_count": "1",
          "content": "Service Catalogue is a like a internal marketplace for an organization in that accounts in that organization are limited to using only the resources describe in the product catalogue. For the use case described the best choice is using Service Catalogue.",
          "poster": "flaacko",
          "timestamp": "1723939140.0"
        },
        {
          "content": "Selected Answer: D\nkeywords: strict tagging, resource requirements a, limit the deployment\nAWS Service Catalog",
          "poster": "jamesf",
          "timestamp": "1722498780.0",
          "upvote_count": "1",
          "comment_id": "1259250"
        },
        {
          "content": "D would be a better option, especially for developers, to abstract configuring the CloudFormation StackSets when launching applications with diverse versions. In AWS Service Catalog, they would just pick up the version and deploy. Everything would be set for them in the background including the CloudFormation StackSet with the version parameter and the tagging enforcement.",
          "timestamp": "1722059760.0",
          "poster": "shammous",
          "upvote_count": "1",
          "comment_id": "1256074"
        },
        {
          "timestamp": "1717203600.0",
          "poster": "Gomer",
          "comment_id": "1222404",
          "content": "Selected Answer: D\nI'd argue that the correct answer is to use Service Catalog and StackSets. Option \"D:\" doesn't preclude using StackSets, it just doesn't mention it as part of the solution. ServiceCatalog is the formal method to distribute standard solutions (such as CloudFormation StackSets)",
          "upvote_count": "1",
          "comments": [
            {
              "content": "\"AWS Service Catalog enables you to launch a product in one or more accounts and AWS Regions. To do this, administrators must apply a stack set constraint to the product with the accounts and Regions, where it can launch as a stack set.\"\nhttps://docs.aws.amazon.com/servicecatalog/latest/userguide/launch-stacksets.html",
              "comment_id": "1222426",
              "timestamp": "1717206900.0",
              "poster": "Gomer",
              "upvote_count": "1"
            }
          ]
        },
        {
          "poster": "stoy123",
          "comments": [
            {
              "comment_id": "1205373",
              "timestamp": "1714637820.0",
              "upvote_count": "1",
              "content": "https://docs.aws.amazon.com/servicecatalog/latest/adminguide/catalogs_constraints_template-constraints.html Please check topic this , correct answer is C",
              "poster": "vn_thanhtung"
            }
          ],
          "comment_id": "1183949",
          "content": "\"A provisioned Service Catalog product is an AWS CloudFormation stack\"\nReally confusing. I go with D...",
          "timestamp": "1711529700.0",
          "upvote_count": "1"
        },
        {
          "poster": "zijo",
          "timestamp": "1709833560.0",
          "comment_id": "1168239",
          "content": "AWS Service Catalog can be used to deploy resources to two regions (or even more) with the help of AWS CloudFormation StackSets. So Answer is C",
          "upvote_count": "3"
        },
        {
          "comment_id": "1159617",
          "upvote_count": "3",
          "poster": "Shasha1",
          "timestamp": "1708945080.0",
          "content": "Answer C\nIf rules are applied across multiple accounts, the StackSets feature is more suitable. The service catalog is used for provisioning new accounts under the AWS control tower."
        },
        {
          "content": "Selected Answer: D\nIt's D",
          "upvote_count": "2",
          "poster": "Cert1Magic2",
          "comment_id": "1156826",
          "timestamp": "1708652460.0"
        },
        {
          "poster": "dzn",
          "upvote_count": "1",
          "content": "Selected Answer: C\nService Catalog cannot ensures that the same application can be deployed to multiple regions.",
          "timestamp": "1708515240.0",
          "comment_id": "1155467"
        },
        {
          "comment_id": "1135482",
          "upvote_count": "1",
          "poster": "thanhnv142",
          "comments": [
            {
              "upvote_count": "2",
              "poster": "thanhnv142",
              "comment_id": "1147302",
              "timestamp": "1707663660.0",
              "content": "Correction: should be C, not D. The question mentions <deployment to two Regions>. Only stacksets can do this. Even AWS Service Catalog products. It cannot be used cross-region unless it is deployed by stackset"
            }
          ],
          "content": "D is correct: Catalog product impose strict requirements for app deloyment. If using stacksets, devs can deploy app to everywhere without any restrictions",
          "timestamp": "1706586360.0"
        },
        {
          "poster": "EricFu",
          "content": "To restrict regions or accounts where catalog products can be deployed, refer to AWS Service Catalog Stack Set Constraints (https://docs.aws.amazon.com/servicecatalog/latest/adminguide/constraints-stackset.html),",
          "comment_id": "1122996",
          "timestamp": "1705282380.0",
          "upvote_count": "1"
        },
        {
          "comment_id": "1072458",
          "upvote_count": "2",
          "poster": "robertohyena",
          "content": "Selected Answer: D\nI will go with D.\nReference here:\nhttps://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/manage-aws-service-catalog-products-in-multiple-aws-accounts-and-aws-regions.html",
          "timestamp": "1700142960.0"
        },
        {
          "comment_id": "1063822",
          "timestamp": "1699273140.0",
          "content": "Selected Answer: D\nWhile stacksets and catalog can use in this case catalog can restrict the strick policy than stacksets. \nThe company has strict tagging and resource requirements.. So it's product catalog. in stacksets developers can modify the resources since they can get hold the template.",
          "poster": "2pk",
          "upvote_count": "4"
        },
        {
          "comment_id": "1056338",
          "upvote_count": "3",
          "poster": "rlf",
          "content": "Answer is D.\n\"Developers will need to deploy\" multiple versions of the same application. So service catalog products will be best for developers.",
          "timestamp": "1698511560.0"
        },
        {
          "comment_id": "1046953",
          "upvote_count": "2",
          "content": "Selected Answer: C\nThe correct answer is: C. Create CloudFormation StackSets with approved CloudFormation templates.\n\nCloudFormation StackSets enable you to deploy stacks across multiple accounts and Regions using a single template. This allows you to enforce company policy by only allowing developers to use approved templates.\n\nAWS Service Catalog products can be used to launch approved CloudFormation templates, but they do not enforce the use of approved templates.",
          "timestamp": "1697635980.0",
          "poster": "BaburTurk"
        },
        {
          "comment_id": "1013792",
          "content": "Selected Answer: C\nThe best solution to ensure that resources are deployed in accordance with company policy is to create CloudFormation StackSets with approved CloudFormation templates.\n\nCloudFormation StackSets allow you to create and manage stacks across multiple AWS accounts and Regions. You can specify the template to use when creating a StackSet, as well as any parameters and capabilities that the template requires.\n\nBy using approved CloudFormation templates, you can ensure that all resources deployed by the StackSet meet your company's tagging and resource requirements. You can also use StackSets to limit the deployment to two Regions.\n\nThe other options are not as effective:",
          "upvote_count": "2",
          "timestamp": "1695369900.0",
          "poster": "Dushank"
        },
        {
          "upvote_count": "4",
          "poster": "ProfXsamson",
          "comment_id": "1000026",
          "content": "D\nService Catalog allows organizations to centrally manage commonly deployed IT services, and helps organizations achieve consistent governance and meet compliance requirements. End users can quickly deploy only the approved IT services they need, following the constraints set by your organization.\n\nService Catalog provides the following benefit:\n\n----Standardization:: Administer and manage approved assets by restricting where the product can be launched, the type of instance that can be used, and many other configuration options. The result is a standardized landscape for product provisioning for your entire organization.",
          "timestamp": "1693960620.0"
        },
        {
          "poster": "cocegas",
          "content": "Selected Answer: D\nI think D is the correct. \nD: because of the specific requirements. Service catalogs allow to setup fixed products that will comply with the mandates. It is possible to its portfolios across accounts and regions, so de multi-account requirement would be met as well. (https://aws.amazon.com/blogs/mt/simplify-sharing-your-aws-service-catalog-portfolios-in-an-aws-organizations-setup/)\n\nThe option C: Yet stacksets allow to select the target account and regions, nothing blocks you select otherwise and dont follow the mandates. You can build anything and deploy as stackset",
          "upvote_count": "3",
          "timestamp": "1693437420.0",
          "comment_id": "994542"
        },
        {
          "upvote_count": "4",
          "poster": "Radeeka",
          "timestamp": "1692509400.0",
          "comment_id": "985565",
          "content": "Selected Answer: C\nI think it should be C.\n\"strict tagging and resource requirements\" is covered by the approved CFTs. Stacksets is the only options that covers multi region deployment."
        },
        {
          "timestamp": "1691920440.0",
          "content": "I think C is right.\nOption D involves AWS Service Catalog, which is used for creating and managing a catalog of approved IT services and resources that developers can deploy, but it doesn't inherently address the requirement of deploying across specific regions, and it might be overcomplicating the solution for this specific scenario.",
          "comment_id": "979908",
          "poster": "ixdb",
          "upvote_count": "2"
        },
        {
          "comment_id": "924231",
          "upvote_count": "3",
          "content": "Selected Answer: D\nD is the right answer.",
          "poster": "madperro",
          "timestamp": "1686835680.0"
        },
        {
          "content": "answer is C",
          "upvote_count": "2",
          "poster": "Flyingdagger",
          "timestamp": "1685379720.0",
          "comments": [
            {
              "upvote_count": "3",
              "poster": "Aja1",
              "timestamp": "1691929680.0",
              "comment_id": "980011",
              "content": "However, it would not limit the deployment to two Regions, and it would not prevent developers from deploying unapproved versions of the application."
            }
          ],
          "comment_id": "909565"
        },
        {
          "comment_id": "908919",
          "content": "The correct answer is C.",
          "poster": "youonebe",
          "upvote_count": "2",
          "timestamp": "1685315880.0"
        },
        {
          "content": "Selected Answer: D\nC. Create CloudFormation StackSets with approved CloudFormation templates.\n\nCloudFormation StackSets allows you to create, update, or delete stacks across multiple accounts and Regions with a single CloudFormation template. By creating approved CloudFormation templates and deploying them through StackSets, you can ensure that resources are deployed in accordance with company policy. Additionally, you can limit the deployment to two Regions by specifying the Regions in the StackSet configuration. This approach provides a centralized and automated solution for deploying resources and can help enforce strict tagging and resource requirements.\n\n\nOption D, creating AWS Service Catalog products with approved CloudFormation templates, can help ensure that developers are using pre-approved templates for their deployments. However, it does not address the requirement to limit the deployment to two Regions",
          "upvote_count": "4",
          "poster": "haazybanj",
          "timestamp": "1682994120.0",
          "comment_id": "886971",
          "comments": [
            {
              "comment_id": "979906",
              "content": "According to you words, it seems you prefer C",
              "timestamp": "1691920320.0",
              "upvote_count": "3",
              "poster": "ixdb"
            }
          ]
        },
        {
          "upvote_count": "3",
          "content": "Selected Answer: D\nD is the correct but Stacksets could work too",
          "comment_id": "870504",
          "timestamp": "1681510380.0",
          "poster": "alce2020"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:19:04.034Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "xFW9XS4nIYcbH7G7Ilrd",
      "question_number": 298,
      "page": 60,
      "question_text": "A company requires its developers to tag all Amazon Elastic Block Store (Amazon EBS) volumes in an account to indicate a desired backup frequency. This requirement Includes EBS volumes that do not require backups. The company uses custom tags named Backup_Frequency that have values of none, dally, or weekly that correspond to the desired backup frequency. An audit finds that developers are occasionally not tagging the EBS volumes.\nA DevOps engineer needs to ensure that all EBS volumes always have the Backup_Frequency tag so that the company can perform backups at least weekly unless a different value is specified.\nWhich solution will meet these requirements?",
      "choices": {
        "C": "Turn on AWS CloudTrail in the account. Create an Amazon EventBridge rule that reacts to EBS CreateVolume events. Configure a custom AWS Systems Manager Automation runbook to apply the Backup_Frequency tag with a value of weekly. Specify the runbook as the target of the rule.",
        "D": "Turn on AWS CloudTrail in the account. Create an Amazon EventBridge rule that reacts to EBS CreateVolume events or EBS ModifyVolume events. Configure a custom AWS Systems Manager Automation runbook to apply the Backup_Frequency tag with a value of weekly. Specify the runbook as the target of the rule.",
        "A": "Set up AWS Config in the account. Create a custom rule that returns a compliance failure for all Amazon EC2 resources that do not have a Backup Frequency tag applied. Configure a remediation action that uses a custom AWS Systems Manager Automation runbook to apply the Backup_Frequency tag with a value of weekly.",
        "B": "Set up AWS Config in the account. Use a managed rule that returns a compliance failure for EC2::Volume resources that do not have a Backup Frequency tag applied. Configure a remediation action that uses a custom AWS Systems Manager Automation runbook to apply the Backup_Frequency tag with a value of weekly."
      },
      "correct_answer": "B",
      "answer_ET": "B",
      "answers_community": [
        "B (95%)",
        "5%"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/105332-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-04-05 21:45:00",
      "unix_timestamp": 1680723900,
      "discussion_count": 15,
      "discussion": [
        {
          "comments": [
            {
              "timestamp": "1740603540.0",
              "upvote_count": "1",
              "content": "what will happen if dev decide to change tag to dally?\nAWS Systems Manager Automation runbook to apply the Backup_Frequency tag with a value of weekly",
              "comment_id": "1362296",
              "poster": "dark4igi"
            }
          ],
          "content": "Selected Answer: D\nOption D:\nReal-Time Enforcement: Ensures tagging compliance as soon as a volume is created or modified.\nComprehensive Coverage: Captures both CreateVolume and ModifyVolume events.\nMinimal Overhead: Automates tagging without requiring manual audits or remediation actions",
          "timestamp": "1735919640.0",
          "poster": "bb4f13b",
          "comment_id": "1336072",
          "upvote_count": "1"
        },
        {
          "upvote_count": "3",
          "comment_id": "901454",
          "poster": "lunt",
          "timestamp": "1727165460.0",
          "content": "Selected Answer: B\nOnly takes few minutes to login > Config > Managed rulename = BACKUP_PLAN_MIN_FREQUENCY_AND_MIN_RETENTION_CHECK\nA = tags everything in EC2, thats EC2::* which includes ELB/EIP/etc. Nope.\nOption B you can specify the tags to match & expected values = answer."
        },
        {
          "upvote_count": "4",
          "comment_id": "1274403",
          "poster": "CristianoRosa",
          "timestamp": "1727165460.0",
          "content": "Selected Answer: B\nA: It works, but it uses a custom rule.\nB: It is simpler than option A as it uses a managed rule which already exists.\nC: It only applies to new volumes and does not address existing resources.\nD: It is better than C but still does not fully meet the requirement to check all EBS volumes and enforce compliance.\nBest Answer is B.",
          "comments": [
            {
              "upvote_count": "1",
              "content": "[A] will enforce the rule on all EC2 resources, including EC2 instance which isn't a requirement and will unnecessary be tagged",
              "comment_id": "1563289",
              "timestamp": "1745478600.0",
              "poster": "arinaho_muleba"
            }
          ]
        },
        {
          "timestamp": "1727165460.0",
          "comment_id": "1209014",
          "upvote_count": "1",
          "content": "Selected Answer: B\nBy leveraging the AWS Config managed rule and automated remediation action, the DevOps engineer can ensure that all EBS volumes in the account always have the required Backup_Frequency tag, enabling the company to perform backups at least weekly unless a different value is explicitly specified. This solution provides continuous monitoring and automated remediation, reducing the risk of human error and ensuring compliance with the company's backup policy.",
          "poster": "c3518fc"
        },
        {
          "content": "Selected Answer: B\nOption B --> AWS config managed rule on EC2::Volume resource + custom SSM automation document\nNot Option A --> because it says custom config rule on all EC2::Instance + Managed SSM automation document\nNot options C & D --> As it says cloudtrail which is for logging API actions",
          "comment_id": "1239821",
          "timestamp": "1727165460.0",
          "upvote_count": "2",
          "poster": "ajeeshb",
          "comments": [
            {
              "poster": "ajeeshb",
              "timestamp": "1719778740.0",
              "upvote_count": "1",
              "content": "sorry, a typo.. Option A also says custom SSM automation document, but it is wrong where it says custom config rule on all Ec2::Instance",
              "comment_id": "1239823"
            }
          ]
        },
        {
          "upvote_count": "1",
          "poster": "Diego1414",
          "timestamp": "1707860580.0",
          "comment_id": "1149607",
          "comments": [
            {
              "poster": "Hizumi",
              "upvote_count": "1",
              "timestamp": "1708261620.0",
              "content": "We don't need to create a custom AWS Config rule, we can utilize the managed rule to detect for non-compliance on the EBS volumes. Otherwise the options indicate to use a custom runbook for AWS Systems Manager to remediate the missing tags.",
              "comment_id": "1153280"
            }
          ],
          "content": "Answer is A.\nChecks if your resources have the tags that you specify. For example, you can check whether your Amazon EC2 instances have the CostCenter tag, while also checking if all your RDS instance have one set of Keys tag. Separate multiple values with commas. You can check up to 6 tags at a time.\n\nThe AWS-managed AWS Systems Manager automation document AWS-SetRequiredTags does not work as a remediation with this rule. You will need to create your own custom Systems Manager automation documentation for remediation\n\nhttps://docs.aws.amazon.com/config/latest/developerguide/required-tags.html"
        },
        {
          "upvote_count": "4",
          "poster": "thanhnv142",
          "content": "B is correct: We should use AWS config for this task \nC and D: cloud trail is for auditing account activities, which is irrelevant\nA: <returns a compliance failure for all Amazon EC2 resources> : we need to remediate EC2 volumes only, not all EC2 resources",
          "timestamp": "1707458760.0",
          "comment_id": "1145264"
        },
        {
          "comment_id": "1131357",
          "content": "A is the correct answer \"The AWS-managed AWS Systems Manager automation document AWS-SetRequiredTags does not work as a remediation with this rule. You will need to create your own custom Systems Manager automation documentation for remediation.\" from this link : https://docs.aws.amazon.com/config/latest/developerguide/required-tags.html",
          "upvote_count": "1",
          "poster": "Sisanda_giiven",
          "timestamp": "1706163540.0"
        },
        {
          "comment_id": "1082927",
          "upvote_count": "3",
          "timestamp": "1701203520.0",
          "poster": "zijo",
          "content": "B is the best choice. If you look at Config Managed Rules you can find - ebs-in-backup-plan - Check if Amazon Elastic Block Store (Amazon EBS) volumes are added in backup plans of AWS Backup. The rule is NON_COMPLIANT if Amazon EBS volumes are not included in backup plans."
        },
        {
          "poster": "SanChan",
          "comment_id": "921051",
          "content": "Selected Answer: B\nB is the most straightforward and efficient solution to ensure that all EBS volumes always have the Backup_Frequency tag applied with the least amount of effort.\n\nA This approach requires more effort than using a managed rule provided by AWS.",
          "comments": [
            {
              "comment_id": "973599",
              "upvote_count": "4",
              "poster": "Aja1",
              "timestamp": "1691305500.0",
              "content": "https://docs.aws.amazon.com/config/latest/developerguide/required-tags.html"
            }
          ],
          "timestamp": "1686530160.0",
          "upvote_count": "3"
        },
        {
          "upvote_count": "1",
          "content": "Selected Answer: B\nB makes sense, you can use managed rule \"required-tags\" to identify non-compliant volumes and custom SSM document to fix it.",
          "comment_id": "918046",
          "poster": "madperro",
          "timestamp": "1686214560.0"
        },
        {
          "comment_id": "888278",
          "poster": "vherman",
          "content": "Selected Answer: B\nB makes sense",
          "timestamp": "1683100560.0",
          "upvote_count": "1"
        },
        {
          "poster": "alce2020",
          "comment_id": "870441",
          "timestamp": "1681501800.0",
          "content": "B. Set up AWS Config in the account. Use a managed rule that returns a compliance failure for EC2::Volume resources that do not have a Backup Frequency tag applied. Configure a remediation action that uses a custom AWS Systems Manager Automation runbook to apply the Backup_Frequency tag with a value of weekly.",
          "upvote_count": "1"
        },
        {
          "timestamp": "1680858600.0",
          "poster": "ele",
          "upvote_count": "1",
          "comment_id": "863678",
          "content": "Selected Answer: B\nAnswer B: Config has a managed rule for type AWS EC2 Volume for tag compliance check."
        },
        {
          "upvote_count": "2",
          "poster": "Dimidrol",
          "comment_id": "862433",
          "content": "Selected Answer: B\nB for me. https://aws.amazon.com/ru/blogs/mt/build-an-aws-config-custom-rule-to-optimize-amazon-ebs-volume-types/",
          "comments": [
            {
              "comments": [
                {
                  "comment_id": "862436",
                  "content": "But very strange that custom rule for all ec2 instances , it should be only ec2 volumes",
                  "comments": [
                    {
                      "comment_id": "870525",
                      "poster": "jqso234",
                      "upvote_count": "4",
                      "timestamp": "1681514580.0",
                      "content": "Option A creates a custom rule that applies to all EC2 resources, not just volumes, which may create additional overhead. The custom AWS Systems Manager Automation runbook is used to apply the Backup_Frequency tag with a value of weekly, but this approach can result in inconsistent tagging if the developers specify a different desired backup frequency. Therefore, Option A is not the correct answer.\n\nOption B is the correct answer because it uses a managed rule specifically for EC2 volumes, which simplifies the configuration effort and ensures that all volumes have the Backup_Frequency tag applied consistently. The custom AWS Systems Manager Automation runbook is used to automatically apply the Backup_Frequency tag with a value of weekly, which reduces the risk of data loss due to missing backups. Your comment that the managed rule should only apply to volumes is correct, and Option B addresses that requirement."
                    }
                  ],
                  "poster": "Dimidrol",
                  "timestamp": "1680724080.0",
                  "upvote_count": "1"
                }
              ],
              "poster": "Dimidrol",
              "timestamp": "1680723960.0",
              "upvote_count": "2",
              "content": "Sorry A is the answer. This is custom rule",
              "comment_id": "862434"
            }
          ],
          "timestamp": "1680723900.0"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:19:04.034Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "2B8aLUAytCcYcKM2WIzs",
      "question_number": 299,
      "page": 60,
      "question_text": "A company requires that its internally facing web application be highly available. The architecture is made up of one Amazon EC2 web server instance and one NAT instance that provides outbound internet access for updates and accessing public data.\nWhich combination of architecture adjustments should the company implement to achieve high availability? (Choose two.)",
      "choices": {
        "C": "Configure an Application Load Balancer in front of the EC2 instance. Configure Amazon CloudWatch alarms to recover the EC2 instance upon host failure.",
        "E": "Replace the NAT instance with a NAT gateway that spans multiple Availability Zones. Update the route tables.",
        "B": "Create additional EC2 instances spanning multiple Availability Zones. Add an Application Load Balancer to split the load between them.",
        "D": "Replace the NAT instance with a NAT gateway in each Availability Zone. Update the route tables.",
        "A": "Add the NAT instance to an EC2 Auto Scaling group that spans multiple Availability Zones. Update the route tables."
      },
      "correct_answer": "BD",
      "answer_ET": "BD",
      "answers_community": [
        "BD (84%)",
        "BE (16%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/106309-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-04-16 03:59:00",
      "unix_timestamp": 1681610340,
      "discussion_count": 23,
      "discussion": [
        {
          "content": "B&D\n\nNAT Gateway does not span multiple AZ. you must create foreach AZ for HA",
          "timestamp": "1698320280.0",
          "comment_id": "1054505",
          "poster": "Karamen",
          "upvote_count": "9"
        },
        {
          "upvote_count": "9",
          "comment_id": "1081364",
          "content": "BD \nE Is incorrect see NAT gateway basics in https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html",
          "timestamp": "1701078240.0",
          "poster": "HugoFM",
          "comments": [
            {
              "upvote_count": "3",
              "content": "Quoting the above link: \"If you have resources in multiple Availability Zones and they share one NAT gateway, and if the NAT gateway’s Availability Zone is down, resources in the other Availability Zones lose internet access. To improve resiliency, create a NAT gateway in each Availability Zone, and configure your routing to ensure that resources use the NAT gateway in the same Availability Zone.\"",
              "poster": "kaushald",
              "timestamp": "1710069720.0",
              "comment_id": "1170275"
            }
          ]
        },
        {
          "timestamp": "1714940700.0",
          "poster": "krishhhhhhhh",
          "comment_id": "1207057",
          "content": "Selected Answer: BD\nhttps://aws.amazon.com/blogs/networking-and-content-delivery/using-nat-gateways-with-multiple-amazon-vpcs-at-scale/\nNAT Gateways within an AZ are automatically implemented with redundancy. However, while Amazon VPCs can span multiple AZs, each NAT Gateway operates within a single AZ. If the NAT Gateway fails, then connections with resources using that NAT Gateway also fail. Therefore, it's recommended to deploy one NAT Gateway in each AZ and routing traffic locally within the same AZ.",
          "upvote_count": "2"
        },
        {
          "comment_id": "1168280",
          "content": "Both NAT Gateway and NAT instance are regional resources. But NAT Gateway offers automatic deployment across Availability Zones, you might need to manually configure redundancy across Availability Zones for NAT Instances.",
          "poster": "zijo",
          "upvote_count": "1",
          "timestamp": "1709837340.0"
        },
        {
          "content": "B and D are correct: We need to span EC2 to multiple avai zones and replace nat instance with nat gateway in each zone\nB: span EC2 to multiple avai zones\nD: replace nat instance with nat gateway",
          "upvote_count": "2",
          "timestamp": "1706596320.0",
          "poster": "thanhnv142",
          "comment_id": "1135545"
        },
        {
          "content": "Answer is B and D",
          "timestamp": "1704653340.0",
          "upvote_count": "1",
          "comment_id": "1116105",
          "poster": "Bans"
        },
        {
          "comment_id": "1092447",
          "poster": "harithzainudin",
          "upvote_count": "5",
          "timestamp": "1702206720.0",
          "content": "Selected Answer: BD\nAsnwer is B and D, \nNAT gateways are regional services and do not span across Availability Zones. So, E is completely wrong."
        },
        {
          "content": "Selected Answer: BD\nNAT Gateway can't spans in multiple regions, only in one subnet, I just tried it using the AWS Console",
          "timestamp": "1700509740.0",
          "comments": [
            {
              "poster": "harithzainudin",
              "upvote_count": "1",
              "content": "yes ure correct! i can confirm this. So BD is the correct answer",
              "timestamp": "1702206600.0",
              "comment_id": "1092445"
            }
          ],
          "comment_id": "1075798",
          "poster": "zolthar_z",
          "upvote_count": "6"
        },
        {
          "comment_id": "1021857",
          "content": "B&D https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-comparison.html",
          "timestamp": "1696104480.0",
          "upvote_count": "1",
          "poster": "DaddyDee"
        },
        {
          "upvote_count": "3",
          "timestamp": "1695221100.0",
          "content": "Selected Answer: BD\nB & D is correct\nNAT GW does not span across AZ, And has to be created in multi AZ for HA.",
          "poster": "bugincloud",
          "comment_id": "1012424"
        },
        {
          "comment_id": "1003997",
          "timestamp": "1694350380.0",
          "content": "Selected Answer: BD\nE is wrong. Natgatway cannot multiple subnets/zones",
          "upvote_count": "4",
          "poster": "RVivek"
        },
        {
          "poster": "cocegas",
          "timestamp": "1693437660.0",
          "comment_id": "994547",
          "upvote_count": "5",
          "content": "Selected Answer: BD\nBD correct.\nE: incorrect because NAT Gateway does not span multi AZ, you need to deploy it to different AZs. Not like LB that spans multiAZ automatically."
        },
        {
          "comment_id": "990662",
          "poster": "FEEREWMWKA",
          "timestamp": "1693047900.0",
          "content": "Defo BD - Cannot be E as Nat Gateways sit in one subnet",
          "upvote_count": "2"
        },
        {
          "content": "B,D.\nE is wrong because NAT Gateway is deployed to a single public subnet (Cannot span multiple AZs)",
          "timestamp": "1692006900.0",
          "comment_id": "980663",
          "upvote_count": "3",
          "poster": "lakescix"
        },
        {
          "content": "Selected Answer: BD\nE is wrong, NAT Gateway is a zonal resource.",
          "poster": "mamila",
          "upvote_count": "4",
          "timestamp": "1690736040.0",
          "comment_id": "967313"
        },
        {
          "timestamp": "1689501240.0",
          "poster": "Certified101",
          "comment_id": "953219",
          "upvote_count": "2",
          "content": "Selected Answer: BD\nA NAT Gateway is spun up in a single subnet that lives in a AZ. So you cannot build a NAT GW that spans mulitple AZ's. You will need to build a NAT GW in EACH AZ to succeed. BD are the correct answers."
        },
        {
          "poster": "Kiroo",
          "upvote_count": "4",
          "timestamp": "1689279180.0",
          "comment_id": "950973",
          "comments": [
            {
              "comment_id": "1267884",
              "timestamp": "1723939620.0",
              "upvote_count": "1",
              "content": "B is correct because it says you should create instances in multiple AZs and then set up a load balancer to split traffic between them. E is wrong because NAT gateways cannot span availability zones.",
              "poster": "flaacko"
            }
          ],
          "content": "Selected Answer: BE\nBeing honest DE are really similar \nBut BE looks more correct due to use the same language"
        },
        {
          "comments": [
            {
              "upvote_count": "1",
              "poster": "zain1258",
              "comment_id": "1060922",
              "content": "The NAT gateway can not be deployed across availability zones. You need create separate NAT gateways for each AZ. So E is wrong",
              "timestamp": "1698960240.0"
            }
          ],
          "poster": "Snape",
          "comment_id": "948805",
          "upvote_count": "3",
          "content": "Selected Answer: BE\nD is wrong - as this will make the NAT instance highly available, but it is not as effective as option E. A NAT gateway in each Availability Zone will provide redundancy, but it will not provide fault tolerance. If one Availability Zone fails, the NAT gateway in that Availability Zone will be unavailable.",
          "timestamp": "1689062820.0"
        },
        {
          "comment_id": "924233",
          "poster": "madperro",
          "content": "Selected Answer: BD\nBD is right.",
          "timestamp": "1686835800.0",
          "upvote_count": "2"
        },
        {
          "content": "Selected Answer: BD\nB and D are the best answers",
          "upvote_count": "2",
          "comment_id": "912804",
          "poster": "Bassel",
          "timestamp": "1685707800.0"
        },
        {
          "comment_id": "908922",
          "poster": "youonebe",
          "content": "BD, NAT gateways in each Availability Zone are implemented with redundancy. Create a NAT gateway in each Availability Zone to ensure zone-independent architecture.",
          "timestamp": "1685316180.0",
          "upvote_count": "2"
        },
        {
          "content": "Selected Answer: BD\nBD is the answer",
          "comment_id": "885823",
          "poster": "haazybanj",
          "upvote_count": "1",
          "timestamp": "1682911380.0"
        },
        {
          "poster": "alce2020",
          "upvote_count": "1",
          "comment_id": "871409",
          "content": "Selected Answer: BD\nB,D are correct",
          "timestamp": "1681610340.0"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:19:04.034Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "18Qv9lMuM07VeTRZytHc",
      "question_number": 300,
      "page": 60,
      "question_text": "A DevOps engineer is building a multistage pipeline with AWS CodePipeline to build, verify, stage, test, and deploy an application. A manual approval stage is required between the test stage and the deploy stage. The development team uses a custom chat tool with webhook support that requires near-real-time notifications.\nHow should the DevOps engineer configure status updates for pipeline activity and approval requests to post to the chat tool?",
      "choices": {
        "B": "Create an AWS Lambda function that is invoked by AWS CloudTrail events. When a CodePipeline Pipeline Execution State Change event is detected, send the event details to the chat webhook URL.",
        "D": "Modify the pipeline code to send the event details to the chat webhook URL at the end of each stage. Parameterize the URL so that each pipeline can send to a different URL based on the pipeline environment.",
        "A": "Create an Amazon CloudWatch Logs subscription that filters on CodePipeline Pipeline Execution State Change. Publish subscription events to an Amazon Simple Notification Service (Amazon SNS) topic. Subscribe the chat webhook URL to the SNS topic, and complete the subscription validation.",
        "C": "Create an Amazon EventBridge rule that filters on CodePipeline Pipeline Execution State Change. Publish the events to an Amazon Simple Notification Service (Amazon SNS) topic. Create an AWS Lambda function that sends event details to the chat webhook URL. Subscribe the function to the SNS topic."
      },
      "correct_answer": "C",
      "answer_ET": "C",
      "answers_community": [
        "C (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/106310-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-04-16 04:02:00",
      "unix_timestamp": 1681610520,
      "discussion_count": 7,
      "discussion": [
        {
          "comment_id": "885826",
          "content": "Selected Answer: C\nThe DevOps engineer should configure status updates for pipeline activity and approval requests to post to the chat tool by creating an Amazon EventBridge rule that filters on CodePipeline Pipeline Execution State Change. The events should be published to an Amazon Simple Notification Service (Amazon SNS) topic, and an AWS Lambda function should be created to send event details to the chat webhook URL. The function should be subscribed to the SNS topic. Option C is the correct answer.\n\nOption A is incorrect because it suggests using CloudWatch Logs instead of EventBridge, which is not the optimal solution for this use case. Option B is incorrect because it suggests using CloudTrail instead of CodePipeline events, which is not relevant. Option D is incorrect because modifying the pipeline code is not necessary and adds unnecessary complexity.",
          "upvote_count": "14",
          "poster": "haazybanj",
          "timestamp": "1682911680.0"
        },
        {
          "poster": "YucelFuat",
          "content": "Selected Answer: C\nExam Tip : If you see that question is related to an Event or Action --> EventBridge",
          "upvote_count": "4",
          "timestamp": "1725475380.0",
          "comment_id": "1278422"
        },
        {
          "comment_id": "1155488",
          "timestamp": "1708516680.0",
          "upvote_count": "1",
          "content": "Selected Answer: C\nAPI calls related to AWS CodePipeline are logged by CloudTrail. However, changes to the execution state of CodePipeline are events, not API calls. These events can be captured via Amazon EventBridge.",
          "poster": "dzn"
        },
        {
          "comment_id": "1135561",
          "timestamp": "1706598300.0",
          "upvote_count": "2",
          "content": "C is correct: Use lambda to send event detail to the chat webhook url. Subcribe lambda to SNS topic\nA: Log subscription filer is for logging, not event\nB: Should not use lamda with cloudtrail events\nD: no need to modify pipeline code to send event at the end of each stage",
          "poster": "thanhnv142",
          "comments": [
            {
              "upvote_count": "1",
              "content": "cloudtrail event cannot trigger lambda",
              "timestamp": "1706598420.0",
              "poster": "thanhnv142",
              "comment_id": "1135565"
            },
            {
              "content": "A: no way to collect log from code pipeline",
              "poster": "thanhnv142",
              "upvote_count": "1",
              "timestamp": "1707664080.0",
              "comment_id": "1147309"
            }
          ]
        },
        {
          "timestamp": "1686836160.0",
          "poster": "madperro",
          "content": "Selected Answer: C\nC makes most sene.",
          "comment_id": "924236",
          "upvote_count": "1"
        },
        {
          "poster": "ele",
          "timestamp": "1683970680.0",
          "upvote_count": "1",
          "content": "Selected Answer: C\nC right",
          "comment_id": "896554"
        },
        {
          "poster": "alce2020",
          "content": "Selected Answer: C\nC it is",
          "timestamp": "1681610520.0",
          "comment_id": "871410",
          "upvote_count": "2"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:19:04.034Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "7mcbaWUAtMYjLkkBwRE5",
      "question_number": 301,
      "page": 61,
      "question_text": "A company's application development team uses Linux-based Amazon EC2 instances as bastion hosts. Inbound SSH access to the bastion hosts is restricted to specific IP addresses, as defined in the associated security groups. The company's security team wants to receive a notification if the security group rules are modified to allow SSH access from any IP address.\nWhat should a DevOps engineer do to meet this requirement?",
      "choices": {
        "B": "Enable Amazon GuardDuty and check the findings for security groups in AWS Security Hub. Configure an Amazon EventBridge rule with a custom pattern that matches GuardDuty events with an output of NON_COMPLIANT. Define an Amazon Simple Notification Service (Amazon SNS) topic as the target.",
        "D": "Enable Amazon Inspector. Include the Common Vulnerabilities and Exposures-1.1 rules package to check the security groups that are associated with the bastion hosts. Configure Amazon Inspector to publish a message to an Amazon Simple Notification Service (Amazon SNS) topic.",
        "A": "Create an Amazon EventBridge rule with a source of aws.cloudtrail and the event name AuthorizeSecurityGroupIngress. Define an Amazon Simple Notification Service (Amazon SNS) topic as the target.",
        "C": "Create an AWS Config rule by using the restricted-ssh managed rule to check whether security groups disallow unrestricted incoming SSH traffic. Configure automatic remediation to publish a message to an Amazon Simple Notification Service (Amazon SNS) topic."
      },
      "correct_answer": "C",
      "answer_ET": "C",
      "answers_community": [
        "C (70%)",
        "A (30%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/106311-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-04-16 04:09:00",
      "unix_timestamp": 1681610940,
      "discussion_count": 54,
      "discussion": [
        {
          "content": "A is right.\nThe Config rule restricted-ssh will not check the ingress rule that use the CIDR other than 0.0.0.0/0 and not notify anyone.",
          "upvote_count": "18",
          "comment_id": "979938",
          "timestamp": "1691922780.0",
          "comments": [
            {
              "poster": "GripZA",
              "comment_id": "1561903",
              "timestamp": "1745059080.0",
              "comments": [
                {
                  "timestamp": "1745059200.0",
                  "poster": "GripZA",
                  "upvote_count": "2",
                  "content": "Why not A: this could catch changes to security groups, it wouldn't analyze the actual rule content to determine if it's an unrestricted SSH rule (0.0.0.0/0 on port 22).you’d need extra custom logic to parse events and check the CIDR and port range.",
                  "comment_id": "1561905"
                }
              ],
              "upvote_count": "1",
              "content": "Exactly why it should be C, not A."
            },
            {
              "poster": "csG13",
              "timestamp": "1702414440.0",
              "content": "A would send a notification for ANY change in the security group. The question clearly states that wants only when 0.0.0.0/0 is allowed. Therefore, should be C.",
              "comment_id": "1094956",
              "upvote_count": "10",
              "comments": [
                {
                  "comment_id": "1194822",
                  "timestamp": "1712999400.0",
                  "poster": "hoazgazh",
                  "content": "\"a notification if the security group rules are modified to allow SSH access from any IP address\" \nfrom any IP address => so A is correct, any change in SG should send noti",
                  "upvote_count": "1"
                }
              ]
            }
          ],
          "poster": "ixdb"
        },
        {
          "timestamp": "1687200780.0",
          "poster": "MarDog",
          "content": "Selected Answer: A\nI'm going to have to go with A on this one:\nhttps://aws.plainenglish.io/detecting-modifications-to-aws-ec2-security-groups-2ef8989a3350\n\nhttps://repost.aws/knowledge-center/monitor-security-group-changes-ec2",
          "upvote_count": "8",
          "comment_id": "927823"
        },
        {
          "upvote_count": "1",
          "content": "Selected Answer: A\nA is right",
          "timestamp": "1752964920.0",
          "comment_id": "1588536",
          "poster": "5fa1a40"
        },
        {
          "comment_id": "1344204",
          "content": "Selected Answer: C\nThe key point here is \"allow SSH access from any IP address\" which is exactly \"the restricted-ssh managed rule\", said that, it's C",
          "timestamp": "1737472200.0",
          "upvote_count": "3",
          "poster": "teo2157"
        },
        {
          "timestamp": "1732701480.0",
          "content": "Selected Answer: C\nVery, very, very hard question. I think the key point here is the ANY, based on that, it's C",
          "comment_id": "1318561",
          "upvote_count": "1",
          "poster": "teo2157"
        },
        {
          "poster": "steli0",
          "timestamp": "1732484580.0",
          "comment_id": "1317226",
          "content": "Selected Answer: C\nA would be right if the \"ANY\" word describing all IPs (0.0.0.0/0) wasn't there. CloudTrail will notify you for any SG rule change.",
          "upvote_count": "2"
        },
        {
          "content": "Option C (Incorrect):\n\n AWS Config rules are good for ongoing compliance checks, but they don't provide real-time notifications for changes.\n Config rules run periodically, which could result in a delay between the change and the notification.\n The automatic remediation aspect is not required in this scenario and could potentially interfere with legitimate changes.\n\nHence, it is Option A.",
          "upvote_count": "1",
          "comment_id": "1315319",
          "timestamp": "1732114140.0",
          "poster": "BrusingWayne"
        },
        {
          "upvote_count": "3",
          "poster": "Impromptu",
          "timestamp": "1732111620.0",
          "content": "Selected Answer: C\nA: Would send a message to SNS for every change, so not only SSH but all other ports/services. This would be too much.\n\nI do get the other comments that C would only notify for 0.0.0.0/0 but I think that is what the question is trying to state with \"any IP\".",
          "comment_id": "1315293"
        },
        {
          "timestamp": "1728427620.0",
          "upvote_count": "3",
          "comment_id": "1294888",
          "poster": "anuvindhs",
          "content": "C is the answer : https://docs.aws.amazon.com/config/latest/developerguide/notifications-for-AWS-Config.html"
        },
        {
          "upvote_count": "3",
          "timestamp": "1722859740.0",
          "poster": "jamesf",
          "content": "Selected Answer: C\nkeywords: Inbound SSH access\nC restricted for SSH port (22) only from ANY address",
          "comment_id": "1261057"
        },
        {
          "comment_id": "1256090",
          "upvote_count": "1",
          "content": "A! \"AWS Config provides rules such as restricted-ssh that can be used to detect Security Groups that have SSH access open for any IP\".",
          "timestamp": "1722060420.0",
          "poster": "shammous"
        },
        {
          "content": "Selected Answer: A\nA is right",
          "comment_id": "1242199",
          "timestamp": "1720113060.0",
          "poster": "TioChico",
          "upvote_count": "1"
        },
        {
          "upvote_count": "2",
          "comments": [
            {
              "timestamp": "1723946880.0",
              "upvote_count": "1",
              "comment_id": "1267906",
              "content": "In the context of AWS when you see ANY IP address, it is probably referring to the 0.0.0.0/0 CIDR block which allows traffic from all or any IP address from the internet. When you use the restricteded-ssh managed rule, Security Groups will be labelled as NON_COMPLIANT when they allow unrestricted SSH traffic from anywhere or any IP address (0.0.0.0/0).",
              "poster": "flaacko"
            }
          ],
          "comment_id": "1214792",
          "content": "Selected Answer: A\nI think keyword for C must be \"ALL\".\nANY means when new IP is added to security group, so SNS will be triggered",
          "poster": "Sodev",
          "timestamp": "1716277800.0"
        },
        {
          "upvote_count": "2",
          "poster": "liuyomz",
          "content": "Selected Answer: C\nC makes way more sense from the way AWS wants us to do it",
          "timestamp": "1715587560.0",
          "comment_id": "1210762"
        },
        {
          "comment_id": "1205169",
          "upvote_count": "2",
          "timestamp": "1714580640.0",
          "poster": "seetpt",
          "content": "Selected Answer: C\ni vote for c"
        },
        {
          "content": "Selected Answer: A\nA. This is the correct solution because it leverages Amazon EventBridge to monitor for changes to the security group rules, specifically the AuthorizeSecurityGroupIngress event, which indicates that the security group rules have been modified to allow SSH access from any IP address.\n\nBy creating an EventBridge rule with the appropriate event pattern and defining an Amazon SNS topic as the target, the DevOps engineer can ensure that the security team receives a notification whenever the security group rules are modified in an undesirable way.",
          "poster": "c3518fc",
          "timestamp": "1712930700.0",
          "upvote_count": "2",
          "comment_id": "1194380"
        },
        {
          "poster": "zijo",
          "timestamp": "1710964860.0",
          "upvote_count": "2",
          "comments": [
            {
              "content": "Yeah, but has nothing to do with anyone changing it. A is your answer because it detects changes and sends out an email notification",
              "comment_id": "1194378",
              "upvote_count": "1",
              "timestamp": "1712930640.0",
              "poster": "c3518fc"
            }
          ],
          "content": "Answer is C \nThe restricted-ssh managed rule in AWS Config helps ensure your bastion host security groups are locked down for SSH access. It specifically checks if incoming SSH traffic is accessible for the security groups.\nThe rule is considered COMPLIANT if:\nSSH access is not open to the public (meaning the rule doesn't find a security group allowing 0.0.0.0/0 for port 22).\nSSH access is restricted to specific IP addresses or security groups using CIDR notation (e.g., 10.0.0.0/16).\nIf the rule detects a security group allowing SSH access from anywhere (0.0.0.0/0), it triggers a NON_COMPLIANT status.",
          "comment_id": "1178655"
        },
        {
          "content": "Selected Answer: C\nrestricted-ssh : The rule is COMPLIANT if the IP addresses of the incoming SSH traffic in the security groups are restricted (CIDR other than 0.0.0.0/0 or ::/0). Otherwise, NON_COMPLIANT.\n\nhttps://docs.aws.amazon.com/config/latest/developerguide/restricted-ssh.html\n\nThat addresses exactly the requirement !",
          "poster": "Cervus18",
          "upvote_count": "3",
          "comment_id": "1177766",
          "timestamp": "1710885060.0"
        },
        {
          "timestamp": "1710473460.0",
          "content": "Selected Answer: C\nA is not meet the following requirements:\n`if the security group rules are modified to allow ssh access FROM ANY IP ADDRESS`",
          "upvote_count": "5",
          "poster": "dzn",
          "comment_id": "1174018"
        },
        {
          "timestamp": "1709904780.0",
          "poster": "4555894",
          "content": "Selected Answer: A\nhttps://aws.amazon.com/premiumsupport/knowledge-center/monitor-security-group-changes-ec2/",
          "comment_id": "1168830",
          "upvote_count": "2"
        },
        {
          "timestamp": "1709234340.0",
          "poster": "Diego1414",
          "upvote_count": "5",
          "content": "Selected Answer: C\nAnswer : C\nKeyword \"allow SSH access from any IP address\"\nA will send notification for any change made to the SG not just SSH",
          "comment_id": "1162951"
        },
        {
          "upvote_count": "4",
          "content": "Selected Answer: C\nI'll go with C. AWS config can help with that SG change detection. Then, we can just send a notification.",
          "timestamp": "1709220420.0",
          "poster": "CloudHandsOn",
          "comment_id": "1162757"
        },
        {
          "poster": "kyuhuck",
          "timestamp": "1708403400.0",
          "upvote_count": "2",
          "content": "a is right~~~~\nhttps://aws.amazon.com/premiumsupport/knowlege-center/monitor-security-group-cahnge-ec2/",
          "comment_id": "1154488"
        },
        {
          "content": "Selected Answer: C\nonly send notification if allow traffic from any address.",
          "comment_id": "1136978",
          "timestamp": "1706720820.0",
          "poster": "vortegon",
          "upvote_count": "4"
        },
        {
          "comment_id": "1135595",
          "poster": "thanhnv142",
          "content": "C is correct: only send noti if allow traffic from any address.",
          "comments": [
            {
              "upvote_count": "1",
              "content": "A: is an event type about changing security group rules. It would send noti if there is any changes in security group",
              "comment_id": "1135596",
              "poster": "thanhnv142",
              "timestamp": "1706601720.0"
            }
          ],
          "upvote_count": "1",
          "timestamp": "1706601480.0"
        },
        {
          "upvote_count": "3",
          "poster": "a54b16f",
          "comment_id": "1123433",
          "timestamp": "1705328280.0",
          "content": "Selected Answer: C\nkeyword: \"ANY\""
        },
        {
          "upvote_count": "1",
          "comment_id": "1105743",
          "poster": "helloworld_2024",
          "timestamp": "1703570940.0",
          "content": "Selected Answer: A\na is right"
        },
        {
          "comment_id": "1079405",
          "content": "Selected Answer: A\nA should be right\nC would not change to non-compliant if someone modifies the IP address to their own IP address, it only makes non-compliant when IP chosen to be 0.0.0.0/0",
          "upvote_count": "1",
          "poster": "shehenshah14",
          "timestamp": "1700839620.0"
        },
        {
          "upvote_count": "3",
          "timestamp": "1700510820.0",
          "content": "Selected Answer: C\nC: Because this link shows the changes with Event Bridge but the source is EC2 no cloudtrail https://repost.aws/knowledge-center/monitor-security-group-changes-ec2",
          "comment_id": "1075804",
          "poster": "zolthar_z"
        },
        {
          "content": "C. Formulation and wording points to ANY IP adress (0.0.0.0/0) CIDR block. So A will notify on all changes.",
          "timestamp": "1699551360.0",
          "upvote_count": "2",
          "comment_id": "1066573",
          "poster": "Coffeinerd"
        },
        {
          "poster": "MozzyRZA",
          "upvote_count": "5",
          "timestamp": "1698351360.0",
          "content": "Selected Answer: C\nUsing an Amazon EventBridge rule with a source of aws.cloudtrail and event name AuthorizeSecurityGroupIngress would indeed detect changes to security group rules. However, it would notify for any change, not specifically when the rule allows SSH access from any IP address. \n\nAWS Config allows you to assess, audit, and evaluate the configurations of your AWS resources. The restricted-ssh managed rule specifically checks whether security groups disallow unrestricted incoming SSH traffic, which is exactly what the requirement is. Configuring automatic remediation to publish a message to an Amazon SNS topic would notify the security team when this non-compliant change is detected.",
          "comment_id": "1054898"
        },
        {
          "comment_id": "1051907",
          "timestamp": "1698068220.0",
          "content": "Selected Answer: C\nC, \"any\" means 0.0.0.0/0",
          "poster": "YR4591",
          "upvote_count": "3"
        },
        {
          "content": "Selected Answer: C\nConfig does this out of the box. It seems some people don't get what 0.0.0.0/0 means in networking terms. It is the cidr for 'any' IP address.",
          "poster": "Geetar",
          "upvote_count": "3",
          "comment_id": "1047767",
          "timestamp": "1697708040.0"
        },
        {
          "timestamp": "1697707620.0",
          "content": "I'm going with C, as the config rule checks for ssh access from 'any' aka 0.0.0.0/0 which is the requirement. A will notify on any action on a security group.",
          "poster": "Geetar",
          "comment_id": "1047765",
          "upvote_count": "1"
        },
        {
          "upvote_count": "2",
          "timestamp": "1695370740.0",
          "poster": "Dushank",
          "content": "Selected Answer: A\n1\nThe best solution to meet this requirement is to create an Amazon EventBridge rule with a source of aws.cloudtrail and the event name AuthorizeSecurityGroupIngress. This will allow you to receive a notification whenever a security group ingress rule is created or modified.",
          "comment_id": "1013820"
        },
        {
          "poster": "RVivek",
          "upvote_count": "3",
          "content": "Selected Answer: C\nPlease ginore my earlier comments\nThe anwer is C. \n1. A will send email notification for any security group change. Requirement is notification of unrestricted ssh access\n2. SSH access from any IP address means entire internet ( 0.0.0.0/0)",
          "timestamp": "1695279780.0",
          "comment_id": "1012873"
        },
        {
          "upvote_count": "2",
          "timestamp": "1695223020.0",
          "poster": "bugincloud",
          "content": "Selected Answer: A\nIMO A should be the answer,\nhttps://docs.aws.amazon.com/config/latest/developerguide/restricted-ssh.html\nOption C is only correct if the CIDR for ssh is 0.0.0.0/0.\n(Although in option A the eventbridge rule source should be aws.ec2)",
          "comment_id": "1012447"
        },
        {
          "content": "Selected Answer: C\nI think it is C.\n\nThis statement \"The company's security team wants to receive a notification if the security group rules are modified to allow SSH access from any IP address.\" means an entry lfrom 0.0.0.0/0, which the AWS config checks.",
          "timestamp": "1693440540.0",
          "poster": "cocegas",
          "comment_id": "994577",
          "upvote_count": "3"
        },
        {
          "comments": [
            {
              "upvote_count": "1",
              "content": "Sorry The anwer is C. \n1. A will send email notification for any security group change. Requirement is notification of unrestricted ssh access\n2. SSH access from any IP address means entire internet ( 0.0.0.0/0)",
              "comment_id": "1012862",
              "timestamp": "1695279300.0",
              "poster": "RVivek"
            }
          ],
          "timestamp": "1693218240.0",
          "comment_id": "992046",
          "content": "Selected Answer: A\nA alerts if any IP address is added to allowed IP address.\nC- Alerts only if SSH is allowed to entire internet 0.0.0.0/0",
          "poster": "RVivek",
          "upvote_count": "2"
        },
        {
          "poster": "jason7",
          "comment_id": "990558",
          "content": "Selected Answer: A\nOption C: AWS Config is a service that can be used to audit AWS resources for compliance with security and compliance policies. However, the restricted-ssh managed rule only checks whether security groups disallow unrestricted incoming SSH traffic. It does not check whether security groups are modified to allow SSH access from any IP address.",
          "upvote_count": "4",
          "timestamp": "1693029720.0"
        },
        {
          "poster": "s50600822",
          "comment_id": "969775",
          "upvote_count": "3",
          "content": "From a security perspective, C is not safe enough since it's looking for 0.0.0.0/0\nhttps://docs.aws.amazon.com/config/latest/developerguide/restricted-ssh.html\nThough A potentially is better if the check look for IPs that are outside of allowed address, it doesn't clearly say that either... so no idea.",
          "timestamp": "1690957380.0"
        },
        {
          "comment_id": "927937",
          "upvote_count": "5",
          "content": "Selected Answer: C\nExplanation:\n\nAWS Config provides a wide range of predefined rules that evaluate whether your AWS resources comply with common best practices. The restricted-ssh managed rule checks whether security groups that are in use disallow unrestricted incoming SSH traffic.",
          "poster": "tartarus23",
          "timestamp": "1687208340.0"
        },
        {
          "upvote_count": "4",
          "comment_id": "924238",
          "poster": "madperro",
          "content": "Selected Answer: C\nC, AWS Config is the tool to check and manage configuration.",
          "timestamp": "1686836460.0"
        },
        {
          "upvote_count": "2",
          "poster": "rhinozD",
          "timestamp": "1686665040.0",
          "comment_id": "922286",
          "content": "Selected Answer: C\nI think A could work but AuthorizeSecurityGroupIngress is a little bit wider. if someone modifies(or adds a rule) the inbound rule to non-0.0.0.0/0, it'll send a notification too.\nSo, I'll go with C."
        },
        {
          "timestamp": "1685537040.0",
          "comment_id": "911266",
          "upvote_count": "3",
          "content": "Selected Answer: C\nIt is definitely C: https://docs.aws.amazon.com/config/latest/developerguide/restricted-ssh.html#:~:text=Checks%20if%20the%20incoming%20SSH%20traffic%20for%20the%20security%20groups%20is%20accessible.%20The%20rule%20is%20COMPLIANT%20when%20IP%20addresses%20of%20the%20incoming%20SSH%20traffic%20in%20the%20security%20groups%20are%20restricted%20(CIDR%20other%20than%200.0.0.0/0).%20This%20rule%20applies%20only%20to%20IPv4.",
          "poster": "rdoty"
        },
        {
          "poster": "bcx",
          "comment_id": "910816",
          "content": "Selected Answer: C\nI think C is the correnct answer, let's see the restricted-ssh rule documentation:\n\"Checks if the incoming SSH traffic for the security groups is accessible. The rule is COMPLIANT when IP addresses of the incoming SSH traffic in the security groups are restricted (CIDR other than 0.0.0.0/0). This rule applies only to IPv4.\n\nIdentifier: INCOMING_SSH_DISABLED\n\nResource Types: AWS::EC2::SecurityGroup\n\nTrigger type: Configuration changes\"\n\nWhy not A? well, A would catch an event and trigger it on any modification of a security group inbound rules. The question wants to be notified only about SSH unrestricted. While there are ways to do it with EventBridge, the answer does not contain any detail. However, C is a complete solution to the problem.",
          "upvote_count": "3",
          "timestamp": "1685510100.0"
        },
        {
          "poster": "hanbj",
          "content": "Selected Answer: C\naws config's restricted-ssh is check 0.0.0.0/0 any open ssh port.\nhttps://docs.aws.amazon.com/config/latest/developerguide/restricted-ssh.html",
          "timestamp": "1685167920.0",
          "upvote_count": "4",
          "comment_id": "907799"
        },
        {
          "poster": "TroyMcLure",
          "comment_id": "906938",
          "timestamp": "1685057520.0",
          "upvote_count": "4",
          "content": "Selected Answer: A\nI'm not sure of what's wrong with C, but I'd go with A because of this:\n\"Create an EventsBridge rule to trigger when an API call is made to modify your security groups. Then, configure an Amazon SNS notification for events that match your rule.\"\n\nhttps://repost.aws/knowledge-center/monitor-security-group-changes-ec2"
        },
        {
          "comment_id": "905564",
          "upvote_count": "3",
          "content": "I'll go A\nC creating an AWS Config rule using the restricted-ssh managed rule would help check if security groups disallow unrestricted incoming SSH traffic, but it does not specifically notify when changes occur to allow SSH access from any IP address",
          "poster": "Akaza",
          "timestamp": "1684911060.0"
        },
        {
          "upvote_count": "4",
          "content": "Selected Answer: A\nOption A is the correct solution, Creating an AWS Config rule using the restricted-ssh managed rule (option C) focuses on checking whether security groups disallow unrestricted incoming SSH traffic. So not C",
          "poster": "2pk",
          "timestamp": "1684255260.0",
          "comment_id": "899360"
        },
        {
          "poster": "ele",
          "comment_id": "896558",
          "upvote_count": "2",
          "timestamp": "1683971100.0",
          "content": "Selected Answer: C\nC works better, as A will trigger for any inbound rule, while asks is for 22 only"
        },
        {
          "timestamp": "1683192360.0",
          "upvote_count": "2",
          "content": "Selected Answer: C\nC is correct",
          "comment_id": "889328",
          "poster": "vherman"
        },
        {
          "upvote_count": "3",
          "timestamp": "1681675320.0",
          "poster": "herohiro",
          "content": "Selected Answer: A\nA is correct. C is incorrect because the restricted-ssh managed rule checks for the presence of unrestricted incoming SSH traffic, not changes to security group rules.\nhttps://docs.aws.amazon.com/config/latest/developerguide/restricted-ssh.html",
          "comment_id": "872117"
        },
        {
          "poster": "alce2020",
          "upvote_count": "4",
          "timestamp": "1681610940.0",
          "content": "Selected Answer: C\nI'll go for C although A could work too",
          "comment_id": "871415"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:19:14.647Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "vACHOtLgIpYFqXzDebBn",
      "question_number": 302,
      "page": 61,
      "question_text": "A DevOps team manages an API running on-premises that serves as a backend for an Amazon API Gateway endpoint. Customers have been complaining about high response latencies, which the development team has verified using the API Gateway latency metrics in Amazon CloudWatch. To identify the cause, the team needs to collect relevant data without introducing additional latency.\nWhich actions should be taken to accomplish this? (Choose two.)",
      "choices": {
        "B": "Enable AWS X-Ray tracing in API Gateway, modify the application to capture request segments, and upload those segments to X-Ray during each request.",
        "A": "Install the CloudWatch agent server side and configure the agent to upload relevant logs to CloudWatch.",
        "D": "Modify the on-premises application to send log information back to API Gateway with each request.",
        "E": "Modify the on-premises application to calculate and upload statistical data relevant to the API service requests to CloudWatch metrics.",
        "C": "Enable AWS X-Ray tracing in API Gateway, modify the application to capture request segments, and use the X-Ray daemon to upload segments to X-Ray."
      },
      "correct_answer": "AC",
      "answer_ET": "AC",
      "answers_community": [
        "AC (88%)",
        "12%"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/106312-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-04-16 04:16:00",
      "unix_timestamp": 1681611360,
      "discussion_count": 9,
      "discussion": [
        {
          "poster": "madperro",
          "upvote_count": "8",
          "content": "Selected Answer: AC\nAC is using standard parts of the solution.",
          "comment_id": "924245",
          "timestamp": "1686837000.0"
        },
        {
          "comments": [
            {
              "timestamp": "1707017580.0",
              "comment_id": "1139749",
              "content": "- <API Gateway latency metrics> means dev team have collect APM. They need to collect app log as well, which indicates option A. \nD and E: modifing the app introduces latencies",
              "poster": "thanhnv142",
              "upvote_count": "2"
            }
          ],
          "content": "A and C: use cloudwatch log agent to collect app log and use AWS X-ray to collect information about requests (traces). \nB is incorrect because modifying app to send message directly to X-RAY introduces more latency to the app. Use X-RAY daemon to do that task is a better idea",
          "comment_id": "1135627",
          "timestamp": "1706605260.0",
          "upvote_count": "6",
          "poster": "thanhnv142"
        },
        {
          "timestamp": "1721962080.0",
          "comment_id": "1255350",
          "content": "Selected Answer: AC\nAC is less impact to Application Latencies. \nKeywords: without additional latencies, cloudwatch\n\nB will provide more latencies\nDE will require modify the app and give more latencies.",
          "poster": "jamesf",
          "upvote_count": "2"
        },
        {
          "content": "Selected Answer: AC\nThe X-Ray daemon batches and uploads the data in the background, which helps to avoid introducing additional latency.",
          "upvote_count": "3",
          "poster": "TEC1",
          "comment_id": "1202159",
          "timestamp": "1714063860.0"
        },
        {
          "poster": "yorkicurke",
          "content": "Selected Answer: AC\nthe reason i am not so sure about is that API Gateway have built-in integration with X-Ray. This means that they automatically send trace data to X-Ray without needing a separate X-Ray daemon. and i dont think we have the option of installing one or using one, unless someone shows me the official link.",
          "timestamp": "1700820420.0",
          "comment_id": "1079170",
          "upvote_count": "3"
        },
        {
          "upvote_count": "3",
          "timestamp": "1685708640.0",
          "comment_id": "912810",
          "content": "Selected Answer: BE\nInstalling the CloudWatch agent server-side (option A) is not directly related to collecting latency data from API Gateway. The CloudWatch agent is typically used to collect and monitor system-level metrics from the server itself.\n\nEnabling AWS X-Ray tracing in API Gateway and using the X-Ray daemon (option C) is not necessary in this scenario. The X-Ray daemon is primarily used when you have applications running on EC2 instances or on-premises servers that need to send trace data to X-Ray.\n\nModifying the on-premises application to send log information back to API Gateway with each request (option D) is not an optimal solution for collecting latency data. It may introduce additional latency and overhead to the API requests and could be challenging to implement efficiently and accurately.",
          "comments": [
            {
              "content": "Do you think that doing B or E doesn't bring any latency?\nI think C is necessary because you could trace the performance of the application.\nAnd even the team can look into app logs on its server, but sending logs to Cloudwatch logs and then making a further investigation with AWS tools is not too bad.",
              "comment_id": "922294",
              "poster": "rhinozD",
              "timestamp": "1686665760.0",
              "upvote_count": "2"
            }
          ],
          "poster": "Bassel"
        },
        {
          "comment_id": "902383",
          "poster": "EricZhang",
          "upvote_count": "2",
          "timestamp": "1684563000.0",
          "content": "Why A? The team still can check logs without uploading to CloudWatch? I'd prefer E over A.",
          "comments": [
            {
              "upvote_count": "3",
              "content": "I thought the same but E might cause additional latency which is NOT what we want.",
              "timestamp": "1690586280.0",
              "poster": "NivNZ",
              "comment_id": "965914"
            }
          ]
        },
        {
          "timestamp": "1683971280.0",
          "comment_id": "896560",
          "content": "Selected Answer: AC\nAC less impact on app",
          "poster": "ele",
          "upvote_count": "4"
        },
        {
          "content": "Selected Answer: AC\nA, C, correct",
          "poster": "alce2020",
          "timestamp": "1681611360.0",
          "upvote_count": "2",
          "comment_id": "871416"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:19:14.647Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "kmfZj45v4D3VDXJyUJk4",
      "question_number": 303,
      "page": 61,
      "question_text": "A company has an application that is using a MySQL-compatible Amazon Aurora Multi-AZ DB cluster as the database. A cross-Region read replica has been created for disaster recovery purposes. A DevOps engineer wants to automate the promotion of the replica so it becomes the primary database instance in the event of a failure.\nWhich solution will accomplish this?",
      "choices": {
        "B": "Create an Aurora custom endpoint to point to the primary database instance. Configure the application to use this endpoint. Configure AWS CloudTrail to run an AWS Lambda function to promote the replica instance and modify the custom endpoint to point to the newly promoted instance.",
        "D": "Store the Aurora endpoint in AWS Systems Manager Parameter Store. Create an Amazon EventBridge event that detects the database failure and runs an AWS Lambda function to promote the replica instance and update the endpoint URL stored in AWS Systems Manager Parameter Store. Code the application to reload the endpoint from Parameter Store if a database connection fails.",
        "C": "Create an AWS Lambda function to modify the application's AWS CloudFormation template to promote the replica, apply the template to update the stack, and point the application to the newly promoted instance. Create an Amazon CloudWatch alarm to invoke this Lambda function after the failure event occurs.",
        "A": "Configure a latency-based Amazon Route 53 CNAME with health checks so it points to both the primary and replica endpoints. Subscribe an Amazon SNS topic to Amazon RDS failure notifications from AWS CloudTrail and use that topic to invoke an AWS Lambda function that will promote the replica instance as the primary."
      },
      "correct_answer": "D",
      "answer_ET": "D",
      "answers_community": [
        "D (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/106274-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-04-15 18:31:00",
      "unix_timestamp": 1681576260,
      "discussion_count": 12,
      "discussion": [
        {
          "poster": "haazybanj",
          "comment_id": "888059",
          "content": "Selected Answer: D\nD is the correct answer.\n\nExplanation:\n\nTo automate the promotion of a read replica to the primary instance in the event of a failure, we need to detect the failure and then invoke an AWS Lambda function to promote the replica instance. This can be achieved using Amazon EventBridge.\n\nOption A is incorrect because using a CNAME with health checks doesn't provide an automated way to promote the read replica. Additionally, subscribing an Amazon SNS topic to Amazon RDS failure notifications from AWS CloudTrail doesn't help to promote the replica.\n\nOption B is incorrect because a custom endpoint is not required to promote the read replica. Additionally, using AWS CloudTrail to run an AWS Lambda function to promote the replica instance doesn't provide an automated way to update the application endpoint to point to the newly promoted instance.",
          "timestamp": "1683078120.0",
          "upvote_count": "8"
        },
        {
          "timestamp": "1698502200.0",
          "comments": [
            {
              "poster": "VrilianVirgil",
              "content": "Aurora supports automated failover for a single cluster. [Be it a global Aurora cluster or a multi AZ/region deployment]\nIn this case it's implied that the read-replica is not part of the cluster.\n\nthat's my best guess.",
              "upvote_count": "2",
              "comment_id": "1150839",
              "timestamp": "1707989160.0"
            }
          ],
          "upvote_count": "7",
          "poster": "nlw",
          "comment_id": "1056246",
          "content": "doesnt failover happen automatically in aurora?"
        },
        {
          "timestamp": "1745059920.0",
          "upvote_count": "1",
          "content": "Selected Answer: D\neventbridge detects failure events from RDS.\nIt triggers a almbda function to:\nPromote the cross-region read replica.\nUpdate the endpoint in parameter store.\n\nthe app is designed to reload the DB endpoint from parameter store if it detects a connection issues\n\nthis supports automated failover with minimal downtime and makes the endpoint configurable rather than hardcoded",
          "poster": "GripZA",
          "comment_id": "1561911"
        },
        {
          "poster": "jamesf",
          "comment_id": "1255351",
          "upvote_count": "1",
          "timestamp": "1721962620.0",
          "content": "Selected Answer: D\nD is correct. \n\nOption B is wrong as AWS CloudTrail to run an AWS Lambda function to promote the replica instance doesn't provide an automated way.",
          "comments": [
            {
              "content": "Option B is wrong also due to: \n- Custom Endpoint Management: Extra complexity in managing and updating endpoints dynamically.\n- Lag in Promotion: Possible delays due to CloudTrail event delivery and Lambda invocation.\n- Reliance on CloudTrail: Lag in event processing can cause potential downtime or data inconsistency.",
              "comment_id": "1259261",
              "upvote_count": "1",
              "timestamp": "1722500340.0",
              "poster": "jamesf"
            }
          ]
        },
        {
          "timestamp": "1720673100.0",
          "upvote_count": "1",
          "content": "Correct answer is B\nHere is why.\nPreviously, you might have used the CNAMES mechanism to set up Domain Name Service (DNS) aliases from your own domain to achieve similar results. By using custom endpoints, you can avoid updating CNAME records when your cluster grows or shrinks. Custom endpoints also mean that you can use encrypted Transport Layer Security/Secure Sockets Layer (TLS/SSL) connections.\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Overview.Endpoints.html#Aurora.Endpoints.Custom",
          "poster": "hkh2",
          "comment_id": "1245884"
        },
        {
          "timestamp": "1706607480.0",
          "comment_id": "1135650",
          "content": "A is correct: Using Amazon Route 53 CNAME with health checks is the way for failover recommended by AWS: https://aws.amazon.com/blogs/database/cross-region-disaster-recovery-using-amazon-aurora-global-database-for-amazon-aurora-postgresql/",
          "poster": "thanhnv142",
          "upvote_count": "1"
        },
        {
          "poster": "Ffida",
          "comment_id": "1022463",
          "timestamp": "1696177560.0",
          "upvote_count": "1",
          "content": "option D is not either providing seemless solution, in option D application needed to be reload and that will cause downtime."
        },
        {
          "timestamp": "1686837600.0",
          "poster": "madperro",
          "content": "Selected Answer: D\nD make most sense.",
          "upvote_count": "2",
          "comment_id": "924253"
        },
        {
          "comment_id": "888062",
          "timestamp": "1683078240.0",
          "content": "Selected Answer: D\nOption D is the correct solution\n\nOption C is incorrect because modifying the AWS CloudFormation template requires manual intervention and cannot be automated. Additionally, creating an Amazon CloudWatch alarm to invoke the Lambda function after the failure event occurs doesn't provide an automated way to promote the replica instance.\n\nTherefore, Option D is the correct solution.",
          "upvote_count": "3",
          "poster": "haazybanj"
        },
        {
          "poster": "haazybanj",
          "comment_id": "885835",
          "timestamp": "1682912520.0",
          "content": "Selected Answer: D\nD is the answer",
          "upvote_count": "2"
        },
        {
          "timestamp": "1682695800.0",
          "comment_id": "883625",
          "content": "D: Refference:https://aws.amazon.com/es/blogs/database/cross-region-cross-account-disaster-recovery-using-amazon-aurora-global-database/",
          "upvote_count": "3",
          "poster": "mgonblan"
        },
        {
          "content": "Selected Answer: D\nD it is",
          "comment_id": "871097",
          "poster": "alce2020",
          "upvote_count": "2",
          "timestamp": "1681576260.0"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:19:14.647Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "ddkyRfsPxAYl8gsVxdYH",
      "question_number": 304,
      "page": 61,
      "question_text": "A company hosts its staging website using an Amazon EC2 instance backed with Amazon EBS storage. The company wants to recover quickly with minimal data losses in the event of network connectivity issues or power failures on the EC2 instance.\nWhich solution will meet these requirements?",
      "choices": {
        "C": "Create an Amazon CloudWatch alarm for the StatusCheckFailed System metric and select the EC2 action to recover the instance.",
        "B": "Add the instance to an EC2 Auto Scaling group with a lifecycle hook to detach the EBS volume when the EC2 instance shuts down or terminates.",
        "A": "Add the instance to an EC2 Auto Scaling group with the minimum, maximum, and desired capacity set to 1.",
        "D": "Create an Amazon CloudWatch alarm for the StatusCheckFailed Instance metric and select the EC2 action to reboot the instance."
      },
      "correct_answer": "C",
      "answer_ET": "C",
      "answers_community": [
        "C (95%)",
        "3%"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/105584-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-04-08 13:43:00",
      "unix_timestamp": 1680954180,
      "discussion_count": 15,
      "discussion": [
        {
          "timestamp": "1687251480.0",
          "content": "Selected Answer: C\nC is the right answer.\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-recover.html",
          "poster": "madperro",
          "comment_id": "928291",
          "upvote_count": "11"
        },
        {
          "timestamp": "1687208580.0",
          "upvote_count": "10",
          "poster": "tartarus23",
          "comment_id": "927945",
          "content": "Selected Answer: C\nExplanation:\n\nAmazon CloudWatch provides system-wide visibility into resource utilization, application performance, and operational health. If a system status check fails, this implies there's a problem with the underlying EC2 system that may require AWS involvement to repair. The \"Recover this instance\" action for the system status check automatically recovers the instance if it becomes impaired due to an underlying issue."
        },
        {
          "content": "Selected Answer: C\nC\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-recover.html\n\nIn the event that AWS determines an instance is unavailable due to an underlying hardware issue, there are two mechanisms that you can configure for instance resiliency which can restore availability—simplified automatic recovery and Amazon CloudWatch action based recovery. This process is called instance recovery.\n\nThe following are examples of underlying hardware issues that might require instance recovery:\n- Loss of network connectivity\n- Loss of system power\n- Software issues on the physical host\n- Hardware issues on the physical host that impact network reachability",
          "timestamp": "1721962800.0",
          "upvote_count": "2",
          "poster": "jamesf",
          "comment_id": "1255352"
        },
        {
          "poster": "c3518fc",
          "content": "Selected Answer: C\nC. This is the correct solution. By creating a CloudWatch alarm for the StatusCheckFailed System metric and configuring the alarm to trigger the \"Recover this instance\" action, the EC2 instance will be automatically recovered in the event of a system failure or power outage. This ensures the instance can be quickly recovered with minimal data loss, as the EBS volume remains attached during the recovery process.\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-recover.html",
          "comment_id": "1194404",
          "upvote_count": "1",
          "timestamp": "1712935980.0"
        },
        {
          "comment_id": "1168839",
          "poster": "4555894",
          "timestamp": "1709905620.0",
          "upvote_count": "1",
          "content": "Selected Answer: C\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-recover.html"
        },
        {
          "timestamp": "1706607720.0",
          "comment_id": "1135652",
          "poster": "thanhnv142",
          "content": "C is correct: recover is the right way\nA and B are irrelevant\nD: should not reboot in case of power failures",
          "upvote_count": "1"
        },
        {
          "content": "Selected Answer: C\nMy Reason:\nStatusCheckFailed_System: This check monitors the AWS systems on which your instance runs1. For Example loss of network connectivity, loss of system power, software issues on the physical host, and hardware issues on the physical host that impact network reachability\n\nStatusCheckFailed_Instance: This check monitors the software and network configuration of your individual instance. These checks detect problems that require your involvement to repair. If an instance status check fails, it typically means that there’s an issue with the instance, such as a misconfigured network or a problem with the instance’s file system.",
          "timestamp": "1700821380.0",
          "comment_id": "1079178",
          "poster": "yorkicurke",
          "upvote_count": "3"
        },
        {
          "poster": "bakamon",
          "comment_id": "925227",
          "upvote_count": "2",
          "timestamp": "1686923220.0",
          "content": "Selected Answer: C\nCorrect Answer is C"
        },
        {
          "upvote_count": "4",
          "timestamp": "1685323380.0",
          "content": "Selected Answer: C\nA is incorrect. \nSimplified automatic recovery is not initiated for instances in an Auto Scaling group. If your instance is part of an Auto Scaling group with health checks enabled, then the instance is replaced when it becomes impaired.\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-recover.html",
          "comment_id": "908954",
          "poster": "qan1257"
        },
        {
          "content": "Selected Answer: C\nC with recover action creates identical instance",
          "comment_id": "896571",
          "upvote_count": "1",
          "timestamp": "1683972120.0",
          "poster": "ele"
        },
        {
          "content": "C\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-recover.html\n\nThere are only 2 ways to recover EC2 instances. Since this instance has EBS volumes, only the CloudWatch action based recovery is applicable.",
          "poster": "Zoe_zoe",
          "comment_id": "889016",
          "timestamp": "1683154020.0",
          "upvote_count": "3"
        },
        {
          "timestamp": "1683078180.0",
          "comment_id": "888061",
          "poster": "haazybanj",
          "upvote_count": "1",
          "content": "Selected Answer: D\nOption D is the correct solution\nOption C is incorrect because modifying the AWS CloudFormation template requires manual intervention and cannot be automated. Additionally, creating an Amazon CloudWatch alarm to invoke the Lambda function after the failure event occurs doesn't provide an automated way to promote the replica instance.\n\nTherefore, Option D is the correct solution."
        },
        {
          "upvote_count": "1",
          "timestamp": "1681575900.0",
          "content": "C is correct",
          "comment_id": "871089",
          "poster": "alce2020"
        },
        {
          "poster": "jqso234",
          "content": "Selected Answer: A\noption A is a better choice for this scenario because it ensures that there is always an EC2 instance running to serve the staging website, and the new instance will have the same configuration as the original instance, including the EBS volume, so there will be minimal data loss. Option C may result in some data loss since a new EBS volume will be created, and it may take longer to recover the instance since the EC2 action to recover the instance will need to be triggered by the Amazon CloudWatch alarm.",
          "comment_id": "870446",
          "timestamp": "1681502580.0",
          "comments": [
            {
              "poster": "bcx",
              "timestamp": "1685511000.0",
              "content": "You would lose the contents on the EBS volume.",
              "upvote_count": "1",
              "comment_id": "910840",
              "comments": [
                {
                  "content": "How would you launch a new instance if they is a power outage? So, C is correct as you will have to recover and hopefully quickly.",
                  "comments": [
                    {
                      "timestamp": "1702360740.0",
                      "poster": "Ffida2214",
                      "content": "If there is power outage than it is considered as instance failure and cloudwatch alarm can recover from system failure but can't recover from instance failure.\nhttps://repost.aws/knowledge-center/automatic-recovery-ec2-cloudwatch\n\nI believe option A is quickest recovery, and if Data needed to be backup then option B",
                      "comment_id": "1094165",
                      "upvote_count": "1"
                    }
                  ],
                  "upvote_count": "1",
                  "timestamp": "1689686880.0",
                  "poster": "ogwu2000",
                  "comment_id": "955479"
                }
              ]
            }
          ],
          "upvote_count": "1"
        },
        {
          "poster": "Dimidrol",
          "upvote_count": "3",
          "timestamp": "1680954180.0",
          "content": "Selected Answer: C\nC for me",
          "comment_id": "864655"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:19:14.647Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "jM7pigKPCW63KLDun5Nj",
      "question_number": 305,
      "page": 61,
      "question_text": "A company wants to use AWS development tools to replace its current bash deployment scripts. The company currently deploys a LAMP application to a group of Amazon EC2 instances behind an Application Load Balancer (ALB). During the deployments, the company unit tests the committed application, stops and starts services, unregisters and re-registers instances with the load balancer, and updates file permissions. The company wants to maintain the same deployment functionality through the shift to using AWS services.\nWhich solution will meet these requirements?",
      "choices": {
        "B": "Use AWS CodePipeline to move the application from the AWS CodeCommit repository to AWS CodeDeploy. Use CodeDeploy's deployment group to test the application, unregister and re-register instances with the ALand restart services. Use the appspec.yml file to update file permissions without a custom script.",
        "A": "Use AWS CodeBuild to test the application. Use bash scripts invoked by AWS CodeDeploy's appspec.yml file to restart services, and deregister and register instances with the ALB. Use the appspec.yml file to update file permissions without a custom script.",
        "C": "Use AWS CodePipeline to move the application source code from the AWS CodeCommit repository to AWS CodeDeploy. Use CodeDeploy to test the application. Use CodeDeploy's appspec.yml file to restart services and update permissions without a custom script. Use AWS CodeBuild to unregister and re-register instances with the ALB.",
        "D": "Use AWS CodePipeline to trigger AWS CodeBuild to test the application. Use bash scripts invoked by AWS CodeDeploy's appspec.yml file to restart services. Unregister and re-register the instances in the AWS CodeDeploy deployment group with the ALB. Update the appspec.yml file to update file permissions without a custom script."
      },
      "correct_answer": "D",
      "answer_ET": "D",
      "answers_community": [
        "D (88%)",
        "8%"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/106272-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-04-15 18:22:00",
      "unix_timestamp": 1681575720,
      "discussion_count": 12,
      "discussion": [
        {
          "poster": "madperro",
          "comment_id": "928297",
          "timestamp": "1687251840.0",
          "upvote_count": "9",
          "content": "Selected Answer: D\nD is better than A. You need to include CodePipeline to move execution from CodeBuild to CodeDeploy."
        },
        {
          "timestamp": "1683078900.0",
          "comment_id": "888072",
          "poster": "haazybanj",
          "content": "Selected Answer: D\nOption D is also a viable solution. It suggests using AWS CodePipeline to trigger AWS CodeBuild to test the application, and then use bash scripts invoked by AWS CodeDeploy's appspec.yml file to restart services, unregister and re-register instances with the ALB, and update file permissions. This approach also covers all the deployment functionality required by the company",
          "upvote_count": "5"
        },
        {
          "poster": "92a2133",
          "timestamp": "1747755360.0",
          "content": "Selected Answer: D\nAlong with everyone else's reasoning the answer is also D because the mention of deployment groups in CodeDeploy, since its multiple instances it needs to be attached to a deployment group instead of just the ALB",
          "upvote_count": "1",
          "comment_id": "1570630"
        },
        {
          "timestamp": "1745061120.0",
          "upvote_count": "1",
          "comment_id": "1561917",
          "content": "Selected Answer: D\nwhy not:\na - need some way orchestrating source - build - deploy steps.\nb - says codedeploys deployment group does testing — not true. Testing should happen in codebuild\nc - suggests using codebuild to unregister/reregister instances — not the right service for that.that’s something codedeploy handles natively via its ALB integration",
          "poster": "GripZA"
        },
        {
          "content": "Selected Answer: B\nAnswer is B. company want to replace its bash deployment scripts so option D is not suitable",
          "poster": "rk0509",
          "timestamp": "1723693800.0",
          "comments": [
            {
              "content": "Options D is suitable since it includes \"Update the appspec.yml file to update file permissions without a custom script\".",
              "comment_id": "1326625",
              "timestamp": "1734216180.0",
              "poster": "SabeloM",
              "upvote_count": "1"
            }
          ],
          "comment_id": "1266179",
          "upvote_count": "1"
        },
        {
          "upvote_count": "3",
          "timestamp": "1721963340.0",
          "comment_id": "1255356",
          "content": "Selected Answer: D\nShould be D\n\nCodePipeline - execute from CodeBuild to CodeDeploy\nCodeBuild - test the application\nCodeDeploy - deploy app, restart services, Unregister and re-register instance\n\nNot Option A: not using CodePipeline\nNot Option BC: using CodeCommit repo, not relevant with question.",
          "poster": "jamesf"
        },
        {
          "poster": "zijo",
          "upvote_count": "1",
          "timestamp": "1711036080.0",
          "comment_id": "1179372",
          "content": "codebuild to test not codedeploy D is correct"
        },
        {
          "upvote_count": "4",
          "comment_id": "1135659",
          "content": "D: is correct: need codepipeline for a seamless deployment. Need codebuild to test and codedeploy to deploy the app on EC2\nA: no mention of codepipeline\nB and C both mention AWS CodeCommit repository, which is irrelevant",
          "timestamp": "1706608680.0",
          "comments": [
            {
              "comment_id": "1148888",
              "upvote_count": "1",
              "poster": "thanhnv142",
              "timestamp": "1707797100.0",
              "content": "A: <deregister and register instances with the ALB>: we need to unregister, not deregister it"
            },
            {
              "content": "B and C: The question doesnt mention the need for a source code repository. \nB: move the application from the AWS CodeCommit repository to AWS CodeDeploy -> Cannot do this, codecommit does not store apps, only code\nC: move the application source code from the AWS CodeCommit repository to AWS CodeDeploy -> cannot do this, code deploy does not store code",
              "timestamp": "1707017220.0",
              "comment_id": "1139745",
              "poster": "thanhnv142",
              "upvote_count": "1"
            }
          ],
          "poster": "thanhnv142"
        },
        {
          "timestamp": "1687209120.0",
          "upvote_count": "2",
          "poster": "tartarus23",
          "content": "Selected Answer: A\nExplanation:\n\nAWS CodeBuild is a fully managed build service that compiles source code, runs tests, and produces software packages that are ready to deploy, which is perfect for unit testing the application.\n\nAWS CodeDeploy is a deployment service that automates application deployments to Amazon EC2 instances. You can specify scripts to be run at set points during a deployment lifecycle, such as deregistering and registering instances with a load balancer, stopping and starting services, or changing file permissions, by defining them in the appspec.yml file.",
          "comment_id": "927949",
          "comments": [
            {
              "upvote_count": "1",
              "poster": "c3518fc",
              "content": "but it says to deregister",
              "comment_id": "1194419",
              "timestamp": "1712938200.0"
            }
          ]
        },
        {
          "poster": "bakamon",
          "timestamp": "1686923460.0",
          "comment_id": "925232",
          "upvote_count": "2",
          "content": "Selected Answer: D\nD is the correct bubamon"
        },
        {
          "upvote_count": "1",
          "content": "D for sure",
          "comment_id": "905595",
          "poster": "Akaza",
          "timestamp": "1684912920.0"
        },
        {
          "poster": "alce2020",
          "upvote_count": "2",
          "timestamp": "1681575720.0",
          "content": "Selected Answer: D\nD it is",
          "comment_id": "871082"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:19:14.647Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "MRB1AoGmk4bZeo04Qbs4",
      "question_number": 306,
      "page": 62,
      "question_text": "A company runs an application with an Amazon EC2 and on-premises configuration. A DevOps engineer needs to standardize patching across both environments. Company policy dictates that patching only happens during non-business hours.\nWhich combination of actions will meet these requirements? (Choose three.)",
      "choices": {
        "F": "Use AWS Systems Manager Maintenance Windows to schedule a patch window.",
        "E": "Use Amazon EventBridge scheduled events to schedule a patch window.",
        "A": "Add the physical machines into AWS Systems Manager using Systems Manager Hybrid Activations.",
        "C": "Create IAM access keys for the on-premises machines to interact with AWS Systems Manager.",
        "B": "Attach an IAM role to the EC2 instances, allowing them to be managed by AWS Systems Manager.",
        "D": "Run an AWS Systems Manager Automation document to patch the systems every hour"
      },
      "correct_answer": "ABF",
      "answer_ET": "ABF",
      "answers_community": [
        "ABF (81%)",
        "Other"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/106271-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-04-15 18:17:00",
      "unix_timestamp": 1681575420,
      "discussion_count": 9,
      "discussion": [
        {
          "comment_id": "1135662",
          "poster": "thanhnv142",
          "upvote_count": "9",
          "content": "ABF are the right answers:\nA: enable hybrid on AWS system manager\nB: create IAM role for System manager to manage EC2 instances\nF: use maintenance windows to schedule patching on non-business hours\n\nC: incorrect because there is no IAM access keys for on-prem \nD: should not run patching every hour\nE: should not use Eventbridge because AWS has its own service to schedule patching",
          "timestamp": "1706609040.0"
        },
        {
          "content": "Selected Answer: ABF\nABF is correct",
          "timestamp": "1690270800.0",
          "upvote_count": "5",
          "poster": "DavidPham",
          "comment_id": "962463"
        },
        {
          "timestamp": "1735053180.0",
          "content": "Selected Answer: ACF\nTo create IAM access keys for on-premises machines to interact with AWS Systems Manager, you need to: create a dedicated IAM user with the necessary permissions for Systems Manager actions, then generate access keys for that user and securely store them on the on-premises machine; ensure you follow best practices like rotating access keys regularly and using a secure method to distribute them.",
          "poster": "spring21",
          "upvote_count": "2",
          "comment_id": "1331163"
        },
        {
          "comment_id": "1255357",
          "content": "Selected Answer: ABF\nABF are correct\n\nhttps://docs.aws.amazon.com/systems-manager/latest/userguide/activations.html\n\nTo configure non-EC2 machines for use with AWS Systems Manager in a hybrid and multicloud environment, you create a hybrid activation. Non-EC2 machine types supported as managed nodes include the following:\n- Servers on your own premises (on-premises servers)\n- AWS IoT Greengrass core devices\n- AWS IoT and non-AWS edge devices\n- Virtual machines (VMs), including VMs in other cloud environments",
          "timestamp": "1721963520.0",
          "poster": "jamesf",
          "upvote_count": "2"
        },
        {
          "upvote_count": "1",
          "timestamp": "1717752660.0",
          "content": "ABF is correct",
          "comment_id": "1226012",
          "poster": "HarryLy"
        },
        {
          "upvote_count": "2",
          "comment_id": "953594",
          "content": "Selected Answer: AF\nhttps://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-managedinstances.html\n\nAF are right but the letter B is wrong the role is for non EC2 instances",
          "timestamp": "1689536640.0",
          "poster": "Kiroo"
        },
        {
          "upvote_count": "4",
          "content": "Selected Answer: ABF\nABF is correct.\nhttps://docs.aws.amazon.com/systems-manager/latest/userguide/activations.html",
          "timestamp": "1687252020.0",
          "poster": "madperro",
          "comment_id": "928298"
        },
        {
          "comment_id": "888074",
          "upvote_count": "3",
          "poster": "haazybanj",
          "timestamp": "1683078960.0",
          "content": "Selected Answer: ABF\nABF is right"
        },
        {
          "poster": "alce2020",
          "comment_id": "871076",
          "upvote_count": "3",
          "content": "Selected Answer: ABF\nABF it is",
          "timestamp": "1681575420.0"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:19:25.420Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "AV7acPn2JNyo7qBRkGkX",
      "question_number": 307,
      "page": 62,
      "question_text": "A company has chosen AWS to host a new application. The company needs to implement a multi-account strategy. A DevOps engineer creates a new AWS account and an organization in AWS Organizations. The DevOps engineer also creates the OU structure for the organization and sets up a landing zone by using AWS Control Tower.\nThe DevOps engineer must implement a solution that automatically deploys resources for new accounts that users create through AWS Control Tower Account Factory. When a user creates a new account, the solution must apply AWS CloudFormation templates and SCPs that are customized for the OU or the account to automatically deploy all the resources that are attached to the account. All the OUs are enrolled in AWS Control Tower.\nWhich solution will meet these requirements in the MOST automated way?",
      "choices": {
        "A": "Use AWS Service Catalog with AWS Control Tower. Create portfolios and products in AWS Service Catalog. Grant granular permissions to provision these resources. Deploy SCPs by using the AWS CLI and JSON documents.",
        "B": "Deploy CloudFormation stack sets by using the required templates. Enable automatic deployment. Deploy stack instances to the required accounts. Deploy a CloudFormation stack set to the organization’s management account to deploy SCPs.",
        "D": "Deploy the Customizations for AWS Control Tower (CfCT) solution. Use an AWS CodeCommit repository as the source. In the repository, create a custom package that includes the CloudFormation templates and the SCP JSON documents.",
        "C": "Create an Amazon EventBridge rule to detect the CreateManagedAccount event. Configure AWS Service Catalog as the target to deploy resources to any new accounts. Deploy SCPs by using the AWS CLI and JSON documents."
      },
      "correct_answer": "D",
      "answer_ET": "D",
      "answers_community": [
        "D (92%)",
        "8%"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/106270-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-04-15 18:11:00",
      "unix_timestamp": 1681575060,
      "discussion_count": 10,
      "discussion": [
        {
          "comment_id": "927951",
          "upvote_count": "9",
          "timestamp": "1687209240.0",
          "poster": "tartarus23",
          "content": "Selected Answer: D\nThe CfCT solution is designed for the exact purpose stated in the question. It extends the capabilities of AWS Control Tower by providing you with a way to automate resource provisioning and apply custom configurations across all AWS accounts created in the Control Tower environment. This enables the company to implement additional account customizations when new accounts are provisioned via the Control Tower Account Factory.\n\nThe CloudFormation templates and SCPs can be added to a CodeCommit repository and will be automatically deployed to new accounts when they are created. This provides a highly automated solution that does not require manual intervention to deploy resources and SCPs to new accounts."
        },
        {
          "upvote_count": "6",
          "comment_id": "928304",
          "timestamp": "1687252380.0",
          "poster": "madperro",
          "content": "Selected Answer: D\nCfCT is designed for the purpose stated in the question. So D.\nhttps://docs.aws.amazon.com/controltower/latest/userguide/cfct-overview.html"
        },
        {
          "comments": [
            {
              "comment_id": "1255360",
              "upvote_count": "1",
              "content": "keywords: \"sets up a landing zone by using AWS Control Tower\"",
              "timestamp": "1721963820.0",
              "poster": "jamesf"
            }
          ],
          "upvote_count": "1",
          "content": "Selected Answer: D\nD\nhttps://docs.aws.amazon.com/controltower/latest/userguide/cfct-overview.html\nCustomizations for AWS Control Tower (CfCT) helps you customize your AWS Control Tower landing zone and stay aligned with AWS best practices. Customizations are implemented with AWS CloudFormation templates and service control policies (SCPs).",
          "poster": "jamesf",
          "comment_id": "1255358",
          "timestamp": "1721963700.0"
        },
        {
          "upvote_count": "1",
          "content": "Selected Answer: D\n\"This CfCT capability is integrated with AWS Control Tower lifecycle events, so that your resource deployments remain synchronized with your landing zone.\"\n\"For example, when a new account is created through account factory, all resources attached to the account are deployed automatically.\"\n\"You can deploy the custom templates and policies to individual accounts and organizational units (OUs) within your organization.\"\nhttps://docs.aws.amazon.com/controltower/latest/userguide/cfct-overview.html",
          "comment_id": "1223847",
          "poster": "Gomer",
          "timestamp": "1717462800.0"
        },
        {
          "upvote_count": "4",
          "content": "D is correct: Use CfCT is the correct solution: it utilizes both CloudFormation template and SCP\nA and C: no mention of AWS CloudFormation\nB: No mention of AWS control tower",
          "timestamp": "1706628720.0",
          "comment_id": "1135923",
          "poster": "thanhnv142"
        },
        {
          "comment_id": "1123490",
          "content": "Selected Answer: D\nD. B is wrong because StackSets doesn't deploy stack instances to the organization management account.",
          "upvote_count": "3",
          "poster": "khchan123",
          "timestamp": "1705332900.0"
        },
        {
          "upvote_count": "2",
          "timestamp": "1685711460.0",
          "comment_id": "912837",
          "content": "Selected Answer: B\nB. Deploying CloudFormation stack sets is the most automated way to deploy resources for new accounts created through AWS Control Tower Account Factory. With stack sets, you can define a CloudFormation template and deploy it to multiple accounts automatically. By enabling automatic deployment and deploying stack instances to the required accounts, you can ensure that the resources specified in the CloudFormation templates are automatically provisioned for each account. Additionally, by deploying a CloudFormation stack set to the organization's management account, you can deploy Service Control Policies (SCPs) across all accounts in the organization.",
          "poster": "Bassel"
        },
        {
          "timestamp": "1685359140.0",
          "content": "Customizations for AWS Control Tower combines AWS Control Tower and other highly-available, trusted AWS services to help customers more quickly set up a secure, multi-account AWS environment using AWS best practices. You can easily add customizations to your AWS Control Tower landing zone using an AWS CloudFormation template and service control policies (SCPs). You can deploy the custom template and policies to individual accounts and organizational units (OUs) within your organization. It also integrates with AWS Control Tower lifecycle events to ensure that resource deployments stay in sync with your landing zone. For example, when a new account is created using the AWS Control Tower account factory, Customizations for AWS Control Tower ensures that all resources attached to the account's OUs will be automatically deployed.",
          "poster": "youonebe",
          "comment_id": "909323",
          "upvote_count": "2"
        },
        {
          "comment_id": "885843",
          "content": "Selected Answer: D\nD is it",
          "timestamp": "1682913540.0",
          "poster": "haazybanj",
          "upvote_count": "2"
        },
        {
          "upvote_count": "2",
          "content": "Selected Answer: D\nD it is",
          "timestamp": "1681575060.0",
          "comment_id": "871071",
          "poster": "alce2020"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:19:25.420Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "1YXCEF9Mm4h0ewxDO7E9",
      "question_number": 308,
      "page": 62,
      "question_text": "An online retail company based in the United States plans to expand its operations to Europe and Asia in the next six months. Its product currently runs on Amazon EC2 instances behind an Application Load Balancer. The instances run in an Amazon EC2 Auto Scaling group across multiple Availability Zones. All data is stored in an Amazon Aurora database instance.\nWhen the product is deployed in multiple regions, the company wants a single product catalog across all regions, but for compliance purposes, its customer information and purchases must be kept in each region.\nHow should the company meet these requirements with the LEAST amount of application changes?",
      "choices": {
        "B": "Use Amazon DynamoDB global tables for the product catalog and regional tables for the customer information and purchases.",
        "A": "Use Amazon Redshift for the product catalog and Amazon DynamoDB tables for the customer information and purchases.",
        "D": "Use Aurora for the product catalog and Amazon DynamoDB global tables for the customer information and purchases.",
        "C": "Use Aurora with read replicas for the product catalog and additional local Aurora instances in each region for the customer information and purchases."
      },
      "correct_answer": "C",
      "answer_ET": "C",
      "answers_community": [
        "C (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/106269-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-04-15 18:06:00",
      "unix_timestamp": 1681574760,
      "discussion_count": 9,
      "discussion": [
        {
          "upvote_count": "9",
          "poster": "Bassel",
          "content": "Selected Answer: C\nC. Using Aurora with read replicas for the product catalog allows for a single product catalog across all regions. Aurora read replicas can be set up in different regions to provide low-latency access to the product catalog from each region. Additionally, by deploying additional local Aurora instances in each region for customer information and purchases, the company can comply with the requirement of keeping customer data and purchases in each region.",
          "timestamp": "1685711700.0",
          "comment_id": "912843"
        },
        {
          "content": "Selected Answer: C\nC. Use Aurora with read replicas for the product catalog and additional local Aurora instances in each region for the customer information and purchases. \nIt's very bad to use word \"local\" in answer about AWS",
          "poster": "dark4igi",
          "upvote_count": "1",
          "comment_id": "1362770",
          "timestamp": "1740698040.0"
        },
        {
          "comment_id": "1255362",
          "upvote_count": "3",
          "poster": "jamesf",
          "content": "Selected Answer: C\nC\nkeywords: ''the LEAST amount of application changes\"",
          "timestamp": "1721964000.0"
        },
        {
          "timestamp": "1708570800.0",
          "comment_id": "1156051",
          "content": "Selected Answer: C\nHow should the company meet these requirements with the LEAST amount of application changes? Anything option with DynamoDB is out since the all the data is stored Aurora(relational database).",
          "poster": "Sisanda_giiven",
          "upvote_count": "3"
        },
        {
          "upvote_count": "3",
          "timestamp": "1706630160.0",
          "comment_id": "1135937",
          "content": "C is correct: data is kept in each region and one product catalog for all regions \nA: Redshift is for data analysis, not for the need in the question\nB: DynamoDB is primarily used for session data in a web app\nD: Amazon DynamoDB global tables for the customer information is against the policy",
          "poster": "thanhnv142"
        },
        {
          "timestamp": "1703462400.0",
          "poster": "amrit1227",
          "upvote_count": "1",
          "comment_id": "1104929",
          "content": "C is correct"
        },
        {
          "content": "Selected Answer: C\nC makes most sense and minimizes application changes.",
          "timestamp": "1687252560.0",
          "upvote_count": "2",
          "comment_id": "928307",
          "poster": "madperro"
        },
        {
          "upvote_count": "3",
          "comment_id": "885847",
          "timestamp": "1682913720.0",
          "content": "Selected Answer: C\nThe best solution to meet the company's requirements with the LEAST amount of application changes is to use Aurora with read replicas for the product catalog and additional local Aurora instances in each region for the customer information and purchases. This will allow for a single product catalog across all regions, while still keeping customer information and purchases in each region for compliance purposes. Amazon Redshift is a data warehousing solution and is not appropriate for this use case. Amazon DynamoDB global tables may be used, but they require application changes to support them. Using local Aurora instances in each region for customer information and purchases could also work, but this would require more configuration and management than using Aurora with read replicas. Therefore, option C is the best solution.",
          "poster": "haazybanj"
        },
        {
          "timestamp": "1681574760.0",
          "poster": "alce2020",
          "content": "Selected Answer: C\nC is correct",
          "comment_id": "871064",
          "upvote_count": "3"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:19:25.420Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "E8xEFfOkpjTClPP4LcIz",
      "question_number": 309,
      "page": 62,
      "question_text": "A company is using an Amazon Aurora cluster as the data store for its application. The Aurora cluster is configured with a single DB instance. The application performs read and write operations on the database by using the cluster's instance endpoint.\nThe company has scheduled an update to be applied to the cluster during an upcoming maintenance window. The cluster must remain available with the least possible interruption during the maintenance window.\nWhat should a DevOps engineer do to meet these requirements?",
      "choices": {
        "B": "Add a reader instance to the Aurora cluster. Create a custom ANY endpoint for the cluster. Update the application to use the Aurora cluster's custom ANY endpoint for read and write operations.",
        "C": "Turn on the Multi-AZ option on the Aurora cluster. Update the application to use the Aurora cluster endpoint for write operations. Update the Aurora cluster’s reader endpoint for reads.",
        "A": "Add a reader instance to the Aurora cluster. Update the application to use the Aurora cluster endpoint for write operations. Update the Aurora cluster's reader endpoint for reads.",
        "D": "Turn on the Multi-AZ option on the Aurora cluster. Create a custom ANY endpoint for the cluster. Update the application to use the Aurora cluster's custom ANY endpoint for read and write operations"
      },
      "correct_answer": "A",
      "answer_ET": "A",
      "answers_community": [
        "A (77%)",
        "13%",
        "5%"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/105457-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-04-06 21:24:00",
      "unix_timestamp": 1680809040,
      "discussion_count": 44,
      "discussion": [
        {
          "poster": "junrun3",
          "upvote_count": "15",
          "timestamp": "1688485260.0",
          "content": "Selected Answer: A\nB and D are incorrect because Aurora cluster provides cluster and read endpoints, but does not support creating custom ANY endpoints.\n\n\n\nC and D are incorrect because Amazon Aurora's multi-AZ option must be set when the DB instance is created.\n\n\n\nTherefore, A is correct.",
          "comment_id": "942938"
        },
        {
          "timestamp": "1688555820.0",
          "poster": "Just_Ninja",
          "content": "Option A is the right choise for an existing Cluster without Multi-AZ!\nRefer to: https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Concepts.AuroraHighAvailability.html\n \n#Read the Tip Box#\n\"You can set up a Multi-AZ cluster by making a simple choice when you create the cluster. The choice is simple whether you use the AWS Management Console, the AWS CLI, or the Amazon RDS API. You can also make an existing Aurora cluster into a Multi-AZ cluster by adding a new reader instance and specifying a different Availability Zone.\"",
          "comment_id": "943640",
          "upvote_count": "12"
        },
        {
          "comment_id": "1561464",
          "content": "Selected Answer: A\naurora cluster turn on multi az by default. for limit downtime, we create a new read instance. When an update window in the primary instance, the read instance become a new primary instance",
          "upvote_count": "1",
          "timestamp": "1744901580.0",
          "poster": "life1991"
        },
        {
          "upvote_count": "1",
          "content": "For me option C is the correct!",
          "timestamp": "1744757820.0",
          "comment_id": "1560996",
          "poster": "MarcosSantos"
        },
        {
          "content": "Selected Answer: C\nC. Turn on the Multi-AZ option on the Aurora cluster. Update the application to use the Aurora cluster endpoint for write operations. Update the Aurora cluster’s reader endpoint for reads.\n\nEnabling Multi-AZ ensures that the data in the Aurora cluster is replicated across multiple Availability Zones (AZs), providing high availability and durability. During maintenance, the update will be applied to one AZ at a time, allowing the cluster to remain available. Updating the application to use the cluster endpoint for write operations ensures that writes will continue to be directed to the primary instance in the cluster, while updating the reader endpoint for reads allows read traffic to be routed to the appropriate instance. Adding a reader instance or creating a custom ANY endpoint are not necessary for meeting the requirement of minimizing interruption during maintenance.",
          "timestamp": "1727165580.0",
          "poster": "haazybanj",
          "upvote_count": "3",
          "comment_id": "896069"
        },
        {
          "content": "Selected Answer: C\nTo meet the requirement of keeping the Aurora cluster available with the least possible interruption during the maintenance window, the DevOps engineer should choose option C: Turn on the Multi-AZ option on the Aurora cluster. Update the application to use the Aurora cluster endpoint for write operations. Update the Aurora cluster’s reader endpoint for reads.\n\nOption A is incorrect because adding a reader instance alone does not provide high availability during maintenance, and updating the application to use the reader endpoint for reads is unnecessary when the Multi-AZ option is enabled",
          "upvote_count": "1",
          "comment_id": "1075192",
          "timestamp": "1727165580.0",
          "poster": "koenigParas2324"
        },
        {
          "poster": "alexleely",
          "content": "Selected Answer: A\nOption A is the correct choice, adding a reader instance after provisioning is the same as setting a Multi-AZ during creation. In the event that the primary fails, the reader instance will be promoted to do both reading and writing automatically. \n\nAdditionally, reader instance can also be used for read activity if you use the cluster reader endpoint which you can serve to user/application closer to the region for better performance.",
          "comment_id": "1176201",
          "timestamp": "1727165520.0",
          "upvote_count": "2"
        },
        {
          "upvote_count": "1",
          "poster": "vietnguyen2",
          "content": "Selected Answer: A\n\"You can set up a Multi-AZ DB cluster by making a simple choice when you create the cluster. You can use the AWS Management Console, the AWS CLI, or the Amazon RDS API. You can also convert an existing Aurora DB cluster into a Multi-AZ DB cluster by adding a new reader DB instance and specifying a different Availability Zone.\"",
          "comment_id": "1195752",
          "timestamp": "1727165520.0"
        },
        {
          "timestamp": "1727165520.0",
          "comment_id": "1204766",
          "content": "Selected Answer: A\nTo meet the requirements of the given scenario, the DevOps engineer should do the following:\n\nAdd a reader instance to the Aurora cluster:\n\nThis will allow the application to offload read operations to the reader instance, reducing the load on the primary instance.\nThe application should be updated to use the Aurora cluster endpoint for write operations and the reader endpoint for read operations.\nThe engineer should not turn on the Multi-AZ option on the Aurora cluster.\n\nMulti-AZ is used to provide high availability and failover capabilities, but it does not necessarily minimize interruption during a maintenance window.\nAdding a reader instance is a more appropriate solution to maintain availability and distribute the read workload.\nTherefore, the correct option is A. Add a reader instance to the Aurora cluster and update the application to use the appropriate endpoints for read and write operations.",
          "poster": "Rizwan_Shaukat",
          "upvote_count": "2"
        },
        {
          "comment_id": "1233520",
          "poster": "Rahul369",
          "upvote_count": "1",
          "content": "Selected Answer: A\nYou cannot change the az option after creation but can deploy a reader instance in another az and use it for reading and writing in your main instance.",
          "timestamp": "1718870880.0"
        },
        {
          "comment_id": "1232342",
          "content": "There isn't a specific Multi-AZ mode for Aurora it's multi-AZ by default (it uses all three AZs). So I think A or B. A is the most commonly used method but I think B offers less disruption because requests are always routed to the instance with greater availability.",
          "upvote_count": "1",
          "timestamp": "1718700720.0",
          "poster": "Malcnorth59"
        },
        {
          "timestamp": "1715277240.0",
          "comment_id": "1209023",
          "content": "Selected Answer: D\nBy enabling Multi-AZ deployment, creating a custom ANY endpoint, and updating the application to use this endpoint for all read and write operations, the DevOps engineer can ensure that the Aurora cluster remains available during the maintenance window with minimal interruption. The application will be able to transparently connect to the available instances (primary or read-only replica), and Aurora will automatically fail over to the read-only replica if the primary instance becomes unavailable during the maintenance process.",
          "poster": "c3518fc",
          "upvote_count": "1"
        },
        {
          "timestamp": "1714885620.0",
          "content": "Selected Answer: B\nCan't tell the difference between A and C except Multi-AZ, both should be working if only read is needed during maintenance window.\nAnd also not understand why only read is needed when people choose them.\nSome say there is no custom ANY endpoint, I think it only means you can choose any instance or instances to that endpoint.\nSo I go with B",
          "comment_id": "1206778",
          "poster": "01037",
          "upvote_count": "1"
        },
        {
          "comment_id": "1180469",
          "upvote_count": "1",
          "poster": "Mackn",
          "content": "answer A).\nthe question doesn't mention what DB is behind the Aurora.\nMulti-AZ config avoids downtime EXCEPT MySQL/MariaDB.\nSo the question mentions \"the least possible interruption\", then A) is the appropriate one",
          "timestamp": "1711160160.0"
        },
        {
          "comment_id": "1154839",
          "content": "is corret is ='c' - > 'a' is not \nThis option leverages Aurora's built-in high availability and failover mechanisms to ensure minimal interruption. By using the cluster endpoint for writes, the application automatically writes to the primary instance. In case of maintenance or failure, Aurora handles failover to another instance with minimal downtime. The reader endpoint distributes read traffic across available replicas, enhancing read scalability and availability without affecting write operations. This setup ensures that the application remains as available as possible during maintenance",
          "upvote_count": "1",
          "timestamp": "1708445580.0",
          "poster": "kyuhuck"
        },
        {
          "timestamp": "1706363340.0",
          "upvote_count": "1",
          "poster": "thanhnv142",
          "comment_id": "1133390",
          "content": "A is good"
        },
        {
          "content": "A is correct.",
          "upvote_count": "1",
          "comment_id": "1127649",
          "timestamp": "1705816440.0",
          "poster": "Jonalb"
        },
        {
          "comment_id": "1115912",
          "content": "Agree answer A. There is no ANY custom endpoint and multy-AZ can be set up during cluster creation",
          "timestamp": "1704637920.0",
          "upvote_count": "1",
          "poster": "yuliaqwerty"
        },
        {
          "timestamp": "1704233640.0",
          "comment_id": "1112319",
          "poster": "n00b2023",
          "comments": [
            {
              "timestamp": "1710912780.0",
              "poster": "phu0298",
              "comment_id": "1177941",
              "content": "I agree with you. C D is the wrong selection. because we can't enable Multi-AZ after cluster is created",
              "upvote_count": "1"
            }
          ],
          "content": "'A' - since Multi AZ has to be setup when the cluster is created. It cannot be updated later.",
          "upvote_count": "2"
        },
        {
          "poster": "wem",
          "comment_id": "1080723",
          "content": "A. Add a Reader Instance; Use Cluster Endpoint for Writes, Reader Endpoint for Reads:\n-during the maintenance of the primary instance, write operations might be affected.\n*B. Add a Reader Instance; Use Custom ANY Endpoint for Reads and Writes:\n- A custom ANY endpoint in Aurora can route read/write traffic based on custom criteria. It can be configured to send write traffic to the primary instance and read traffic to the reader instance.This setup allows for continuous read availability and write operations during maintenance, assuming the primary instance is available.\nC. Enable Multi-AZ; Use Cluster Endpoint for Writes, Reader Endpoint for Reads:\n-Amazon Aurora is inherently designed for high availability and does not have a \"Multi-AZ\" option like RDS.\nD. Enable Multi-AZ; Use Custom ANY Endpoint for Reads and Writes:\n-Aurora does not have a \"Multi-AZ\" option to turn on",
          "upvote_count": "1",
          "timestamp": "1701010860.0"
        },
        {
          "timestamp": "1700398500.0",
          "comment_id": "1074619",
          "upvote_count": "1",
          "content": "Option C, turning on the Multi-AZ option and using the cluster endpoint for writes and the reader endpoint for reads, seems to be the best fit. It directly addresses the need for high availability during maintenance without overcomplicating the setup with a custom ANY endpoint, which may not be necessary in this context. The Multi-AZ feature in Aurora is specifically designed for scenarios like maintenance windows, where minimal downtime and high availability are crucial.",
          "poster": "wem"
        },
        {
          "comment_id": "1069641",
          "upvote_count": "2",
          "poster": "HeyBlinkin",
          "timestamp": "1699900920.0",
          "content": "Selected Answer: A\nIt can only be A."
        },
        {
          "timestamp": "1691838660.0",
          "content": "why not B? https://awscli.amazonaws.com/v2/documentation/api/latest/reference/rds/create-db-cluster-endpoint.html\nCreates a new custom endpoint and associates it with an Amazon Aurora DB cluster.",
          "comment_id": "979329",
          "upvote_count": "3",
          "poster": "ixdb"
        },
        {
          "content": "Selected Answer: A\nOption A!",
          "comment_id": "933418",
          "poster": "FunkyFresco",
          "upvote_count": "2",
          "timestamp": "1687685340.0"
        },
        {
          "poster": "tartarus23",
          "content": "Selected Answer: A\n(A) Amazon Aurora is designed to distribute the read workload across multiple read replica instances when you connect to the reader endpoint. The cluster endpoint always points to the current primary instance for the DB cluster. Therefore, by adding a reader instance to the Aurora cluster and updating the application to use the cluster endpoint for writes and reader endpoint for reads, the application can continue to operate with minimal interruption during the maintenance window.",
          "timestamp": "1687596600.0",
          "upvote_count": "5",
          "comment_id": "932375"
        },
        {
          "content": "The correct answer is A. For option c and d, you cannot turn an existing cluster to multi-az by turning on the multi-az option. You can turn it to Multi-AZ by adding a reader instance. Refer to https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Concepts.AuroraHighAvailability.html\n\nFor option b, the custom endpoint is pointed to a group of db instances without considering read/write query. You should add writer and reader instance to the same endpoint. it works to replace reader endpoints normally.",
          "comments": [
            {
              "timestamp": "1706167380.0",
              "poster": "Sisanda_giiven",
              "comment_id": "1131445",
              "upvote_count": "1",
              "content": "You can turn an existing cluster to multi-az : https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Concepts.AuroraHighAvailability.html#Concepts.AuroraHighAvailability.Instances"
            }
          ],
          "upvote_count": "4",
          "comment_id": "925560",
          "timestamp": "1686950880.0",
          "poster": "allen_devops"
        },
        {
          "upvote_count": "3",
          "content": "The answers are ambiguous. Conform AWS, \"For Amazon Aurora adding a reader instance is how we provide multi-az recovery.\"\n\nhttps://repost.aws/questions/QUwJB61VWcRFiS1WtzEJNzqg/unable-to-configure-multi-az-deployment-for-existing-aurora-mysql\n\nIn this case, the \"A\" and \"C\" answers are the same",
          "poster": "kacsabacsi78",
          "comment_id": "921366",
          "timestamp": "1686567780.0"
        },
        {
          "content": "Selected Answer: A\noption C is the most straightforward and efficient solution to keep the Aurora cluster available with the least possible interruption during the maintenance window.\nBut According to the AWS documentation, Multi-AZ cannot be enabled after an Aurora cluster has been created. \nSo A is the answer",
          "poster": "SanChan",
          "comment_id": "921060",
          "timestamp": "1686534840.0",
          "upvote_count": "4"
        },
        {
          "poster": "rhinozD",
          "content": "Either A or C.\nAbout A, if the new read replica is created in the same AZ as the primary, this option will be wrong.\nAbout C, if “turn on Multi-AZ option in Aurora Cluster” means “make an existing Aurora cluster into a Multi-AZ cluster by adding a new reader instance and specifying a different Availability Zone”, then this option is right. Otherwise, it is wrong because cannot turn on the Multi-AZ option of an Aurora Cluster after creation.",
          "upvote_count": "2",
          "comment_id": "918737",
          "timestamp": "1686266100.0"
        },
        {
          "comment_id": "918062",
          "content": "Selected Answer: A\nA is the best option. C and D are not working for Aurora as you can't change it to Multi-AZ after creation (like other RDS). \nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Modifying.html#Aurora.Modifying.SettingsNotApplicable",
          "upvote_count": "2",
          "timestamp": "1686215940.0",
          "poster": "madperro"
        },
        {
          "poster": "Manny20",
          "upvote_count": "1",
          "content": "Option C is the correct answer. \nSelect the Aurora cluster: Locate and select the Aurora cluster for which you want to enable Multi-AZ.\nChoose \"Instance actions\": From the \"Actions\" dropdown menu, select \"Modify\".\n Enable Multi-AZ: In the \"Modify DB Cluster\" page, scroll down to the \"Availability & durability\" section. Check the box next to \"Enable Multi-AZ deployment\" to enable Multi-AZ.",
          "comment_id": "917384",
          "timestamp": "1686154140.0",
          "comments": [
            {
              "timestamp": "1686154560.0",
              "content": "I apologize. In Amazon Aurora, the Multi-AZ configuration is set at the time of cluster creation and cannot be modified for an existing cluster. Therefore, Option C is not a valid solution for meeting the given requirements. So answer A should be correct. By adding a reader instance to the Aurora cluster, you create a read replica that can handle read traffic and offload some of the read workload from the primary instance.",
              "upvote_count": "2",
              "comment_id": "917391",
              "poster": "Manny20"
            }
          ]
        },
        {
          "timestamp": "1685965500.0",
          "upvote_count": "2",
          "poster": "BasselBuzz",
          "comment_id": "915400",
          "content": "Selected Answer: A\nA- Add a reader instance to the Aurora cluster. Update the application to use the Aurora cluster endpoint for write operations. Update the Aurora cluster's reader endpoint for reads. MultiAZ is not available for existing DB"
        },
        {
          "timestamp": "1685453280.0",
          "poster": "bcx",
          "comment_id": "910277",
          "content": "Selected Answer: A\nFor the operation of the question to succeed, you need an extra instance (because one is supposed to go down for maintenance), you can add it in the same of different AZ.",
          "upvote_count": "3"
        },
        {
          "timestamp": "1685045580.0",
          "comment_id": "906862",
          "content": "Selected Answer: D\nBased on this document, the best option to handle há for the maintenance in the given case is to have a custom endpoint to “balance” bd connection to any avaliable instance so it seems that it’s D\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Overview.Endpoints.html",
          "upvote_count": "1",
          "poster": "Kiroo"
        },
        {
          "poster": "qan1257",
          "upvote_count": "2",
          "content": "Selected Answer: A\nThe answer is A.\n\nThis is an Aurora cluster, there is no option to enable Multi-AZ. Users can only enable Muti-AZ by adding a read replica (up to 15 replicas).",
          "comment_id": "905502",
          "comments": [
            {
              "comment_id": "906861",
              "content": "Is multi az not multi region.\nThe definition of the product is that it allows multi az \n\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Concepts.AuroraHighAvailability.html",
              "upvote_count": "1",
              "timestamp": "1685045220.0",
              "poster": "Kiroo"
            }
          ],
          "timestamp": "1684905900.0"
        },
        {
          "content": "Selected Answer: C\nI think C is better, necause enabling Multi-AZ (or Global database ) allows users accessing with high availability. https://aws.amazon.com/es/rds/aurora/features/",
          "poster": "mgonblan",
          "timestamp": "1682699580.0",
          "comment_id": "883733",
          "upvote_count": "2"
        },
        {
          "timestamp": "1682312640.0",
          "poster": "5aga",
          "upvote_count": "2",
          "content": "Selected Answer: C\nThe answer is C. \n\nEnabling Multi-AZ would provide high availability by automatically replicating data to a standby instance in a different availability zone. During the maintenance window, Amazon RDS would automatically failover to the standby instance with minimal downtime. Updating the application to use the Aurora cluster endpoint for write operations and the Aurora cluster's reader endpoint for reads would ensure that the application continues to function properly during the maintenance window.",
          "comment_id": "879035",
          "comments": [
            {
              "upvote_count": "2",
              "comment_id": "905508",
              "content": "The answer should be A.\n\nBecause there is no Multi-AZ option on the Aurora cluster.\n\n\"You can set up a Multi-AZ cluster by making a simple choice when you create the cluster. The choice is simple whether you use the AWS Management Console, the AWS CLI, or the Amazon RDS API. You can also make an existing Aurora cluster into a Multi-AZ cluster by adding a new reader instance and specifying a different Availability Zone.\"\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Concepts.AuroraHighAvailability.html",
              "timestamp": "1684906560.0",
              "poster": "qan1257"
            }
          ]
        },
        {
          "upvote_count": "2",
          "content": "Selected Answer: A\nOption A is correct \nOption B : Any endpoint is not required\nOption C : turning on Multi AZ does not create a read instance\nOption D : turning on multi az doesnt create a read instance",
          "comment_id": "877865",
          "timestamp": "1682224440.0",
          "poster": "bakamon"
        },
        {
          "content": "Option C is the correct solution to meet the company's requirements with the least possible interruption. By turning on the Multi-AZ option on the Aurora cluster, a standby replica of the primary instance is created in a different availability zone (AZ). During the maintenance window, the update is first applied to the standby replica. After the update is applied and the replica is promoted to become the new primary instance, the old primary instance is updated. This ensures that the Aurora cluster remains available throughout the maintenance window, with minimal interruption.\n\n\nAdding a reader instance to the Aurora cluster (Options A and B) is not necessary and will not provide any additional benefits in terms of availability during the maintenance window.\n\n\nCreating a custom ANY endpoint for the Aurora cluster (Options B and D) is also not necessary and does not provide any additional benefits in terms of availability during the maintenance window",
          "upvote_count": "3",
          "poster": "alce2020",
          "timestamp": "1681501980.0",
          "comment_id": "870442"
        },
        {
          "comment_id": "870354",
          "poster": "jqso234",
          "upvote_count": "3",
          "content": "Option C is the correct answer since enabling the Multi-AZ option on the Aurora cluster provides a highly available environment that can automatically failover in the event of an availability zone failure, making it a good choice to maintain high availability during maintenance windows. Additionally, the application can continue to use the same endpoint for both read and write operations, simplifying the configuration process.",
          "timestamp": "1681492320.0"
        },
        {
          "comment_id": "869484",
          "poster": "henryyvr",
          "upvote_count": "1",
          "timestamp": "1681395960.0",
          "content": "https://repost.aws/knowledge-center/rds-required-maintenance"
        },
        {
          "content": "Selected Answer: D\nYou should turn ON Multi-az",
          "timestamp": "1681395900.0",
          "comment_id": "869483",
          "poster": "henryyvr",
          "upvote_count": "1"
        },
        {
          "timestamp": "1680868860.0",
          "content": "Selected Answer: B\nB, as A will only work for read. C,D are not available for existing cluster.",
          "upvote_count": "1",
          "comment_id": "863804",
          "poster": "ele",
          "comments": [
            {
              "poster": "ele",
              "timestamp": "1682148360.0",
              "upvote_count": "1",
              "comment_id": "877112",
              "content": "Changing to C,\nB is wrong: https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Concepts.AuroraHighAvailability.html\nYou can also make an existing Aurora cluster into a Multi-AZ cluster by adding a new reader instance and specifying a different Availability Zone."
            }
          ]
        },
        {
          "timestamp": "1680809040.0",
          "upvote_count": "1",
          "content": "Selected Answer: B\nC and D are not working on existing cluster so it will be A or B, for me they are the same looking to downtime",
          "comment_id": "863259",
          "poster": "Dimidrol"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:19:25.420Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "DUc3vGrhqvDDlLbPEJSa",
      "question_number": 310,
      "page": 62,
      "question_text": "A company is implementing a well-architected design for its globally accessible API stack. The design needs to ensure both high reliability and fast response times for users located in North America and Europe.\nThe API stack contains the following three tiers:\n\nAmazon API Gateway -\n\nAWS Lambda -\n\nAmazon DynamoDB -\nWhich solution will meet the requirements?",
      "choices": {
        "B": "Configure Amazon Route 53 to point to API Gateway APIs in North America and Europe using latency-based routing and health checks. Configure the APIs to forward requests to a Lambda function in that Region. Configure the Lambda functions to retrieve and update the data in a DynamoDB global table.",
        "D": "Configure Amazon Route 53 to point to API Gateway API in North America using latency-based routing. Configure the API to forward requests to the Lambda function in the Region nearest to the user. Configure the Lambda function to retrieve and update the data in a DynamoDB table.",
        "C": "Configure Amazon Route 53 to point to API Gateway in North America, create a disaster recovery API in Europe, and configure both APIs to forward requests to the Lambda functions in that Region. Retrieve the data from a DynamoDB global table. Deploy a Lambda function to check the North America API health every 5 minutes. In the event of a failure, update Route 53 to point to the disaster recovery API.",
        "A": "Configure Amazon Route 53 to point to API Gateway APIs in North America and Europe using health checks. Configure the APIs to forward requests to a Lambda function in that Region. Configure the Lambda functions to retrieve and update the data in a DynamoDB table in the same Region as the Lambda function."
      },
      "correct_answer": "B",
      "answer_ET": "B",
      "answers_community": [
        "B (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/106268-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-04-15 18:04:00",
      "unix_timestamp": 1681574640,
      "discussion_count": 7,
      "discussion": [
        {
          "content": "Selected Answer: B\nB is the correct solution.\n\nThe requirement is to ensure both high reliability and fast response times for users located in North America and Europe. To meet this requirement, we can use Amazon Route 53 with latency-based routing to direct users to the closest API Gateway endpoint. Additionally, we can use health checks to monitor the health of each endpoint and direct traffic away from unhealthy endpoints.\n\nTo maintain high reliability, we can use AWS Lambda to handle the API requests. Since Lambda scales automatically, we don't need to worry about provisioning or maintaining infrastructure. We can also use DynamoDB as the database since it provides low latency access and automatic scaling.",
          "poster": "haazybanj",
          "comment_id": "888089",
          "upvote_count": "12",
          "timestamp": "1698985320.0"
        },
        {
          "upvote_count": "3",
          "timestamp": "1722348060.0",
          "content": "B is correct: using both latency-based routing and health checks ensures high reliability and fast response\nA: only health check doesnt ensure fast response\nC: All traffic would be routed to one location only (either NA or Europe if NA failed)\nD: All traffic would be routed to one NA only. There would be no entry point which is near Europe users.",
          "poster": "thanhnv142",
          "comment_id": "1135939"
        },
        {
          "upvote_count": "1",
          "comment_id": "1104931",
          "timestamp": "1719266700.0",
          "poster": "amrit1227",
          "content": "B is correct"
        },
        {
          "upvote_count": "2",
          "poster": "z_inderjot",
          "comment_id": "1103849",
          "timestamp": "1719106440.0",
          "content": "Selected Answer: B\nB , \nUsing latency based routing for better response time . Having api gateway in each region reduce requrest fligh time. Lambda and Dynamo being a managed serivce scale automatically and having them in same region just reduce latency."
        },
        {
          "content": "Selected Answer: B\nReliability is different that resiliency, hence A and C are out as they are focussing on health checks which is required for the resiliency. DR again for the resiliency",
          "comment_id": "949439",
          "timestamp": "1705034640.0",
          "poster": "Snape",
          "upvote_count": "3"
        },
        {
          "content": "Selected Answer: B\nB is the best solution.",
          "timestamp": "1703071140.0",
          "comment_id": "928310",
          "poster": "madperro",
          "upvote_count": "4"
        },
        {
          "content": "Selected Answer: B\nB is correct",
          "timestamp": "1697385840.0",
          "poster": "alce2020",
          "upvote_count": "2",
          "comment_id": "871059"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:19:25.420Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "gdOZLo1RF94fIVUY6L2X",
      "question_number": 311,
      "page": 63,
      "question_text": "A rapidly growing company wants to scale for developer demand for AWS development environments. Development environments are created manually in the AWS Management Console. The networking team uses AWS CloudFormation to manage the networking infrastructure, exporting stack output values for the Amazon VPC and all subnets. The development environments have common standards, such as Application Load Balancers, Amazon EC2 Auto Scaling groups, security groups, and Amazon DynamoDB tables.\nTo keep up with demand, the DevOps engineer wants to automate the creation of development environments. Because the infrastructure required to support the application is expected to grow, there must be a way to easily update the deployed infrastructure. CloudFormation will be used to create a template for the development environments.\nWhich approach will meet these requirements and quickly provide consistent AWS environments for developers?",
      "choices": {
        "D": "Use Fn::ImportValue intrinsic functions in the Parameters section of the root template to retrieve Virtual Private Cloud (VPC) and subnet values. Define the development resources in the order they need to be created in the CloudFormation nested stacks. Use the CreateChangeSet. and ExecuteChangeSet commands to update existing development environments.",
        "B": "Use nested stacks to define common infrastructure components. To access the exported values, use TemplateURL to reference the networking team’s template. To retrieve Virtual Private Cloud (VPC) and subnet values, use Fn::ImportValue intrinsic functions in the Parameters section of the root template. Use the CreateChangeSet and ExecuteChangeSet commands to update existing development environments.",
        "C": "Use nested stacks to define common infrastructure components. Use Fn::ImportValue intrinsic functions with the resources of the nested stack to retrieve Virtual Private Cloud (VPC) and subnet values. Use the CreateChangeSet and ExecuteChangeSet commands to update existing development environments.",
        "A": "Use Fn::ImportValue intrinsic functions in the Resources section of the template to retrieve Virtual Private Cloud (VPC) and subnet values. Use CloudFormation StackSets for the development environments, using the Count input parameter to indicate the number of environments needed. Use the UpdateStackSet command to update existing development environments."
      },
      "correct_answer": "C",
      "answer_ET": "C",
      "answers_community": [
        "C (78%)",
        "B (22%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/106436-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-04-16 22:20:00",
      "unix_timestamp": 1681676400,
      "discussion_count": 14,
      "discussion": [
        {
          "content": "C is Correct.\nB is WRONG because intrinsic functions can't be used in Parameter as per AWS documentation. \nhttps://repost.aws/knowledge-center/cloudformation-template-validation",
          "timestamp": "1700070960.0",
          "poster": "ipsingh",
          "comment_id": "898469",
          "upvote_count": "14",
          "comments": [
            {
              "timestamp": "1710475860.0",
              "poster": "aksliveswithaws",
              "content": "You can use intrinsic functions only in specific parts of a template. Currently, you can use intrinsic functions in resource properties, outputs, metadata attributes, and update policy attributes\nRefer \nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/intrinsic-function-reference.html",
              "comment_id": "1008042",
              "upvote_count": "2"
            }
          ]
        },
        {
          "poster": "seetpt",
          "upvote_count": "1",
          "timestamp": "1730486280.0",
          "comment_id": "1205171",
          "content": "Selected Answer: C\nC seems right"
        },
        {
          "comment_id": "1136443",
          "content": "C is correct: use nested stacks and Fn::ImportValue intrinsic functions with the resources of the nested stack\nA: no mention of nested stack\nB and D: Fn::ImportValue intrinsic function is used on child template to import values from parent template. So it should not be used on root template, which is the universal parent tempalte of all other templates",
          "upvote_count": "4",
          "poster": "thanhnv142",
          "timestamp": "1722399420.0"
        },
        {
          "upvote_count": "2",
          "poster": "madperro",
          "comment_id": "928312",
          "timestamp": "1703071500.0",
          "content": "Selected Answer: C\nC is the best answer. B is wrong as you need to use Fn::ImportValue in Resource section to import CFN template outputs."
        },
        {
          "content": "Selected Answer: C\nI will go with C",
          "comment_id": "928229",
          "timestamp": "1703063700.0",
          "poster": "ducluanxutrieu",
          "upvote_count": "3"
        },
        {
          "content": "Selected Answer: B\nB. Use nested stacks to define common infrastructure components. To access the exported values, use TemplateURL to reference the networking team’s template. To retrieve Virtual Private Cloud (VPC) and subnet values, use Fn::ImportValue intrinsic functions in the Parameters section of the root template. Use the CreateChangeSet and ExecuteChangeSet commands to update existing development environments.\n\nNested stacks allow you to modularize and reuse CloudFormation code. For this case, this is helpful because you have common infrastructure components that are shared across environments.\n\nThe Fn::ImportValue function is used to import values that have been exported in another stack. Since the networking team exports the VPC and subnet information, this can be used in the CloudFormation stack to reference those values.",
          "comment_id": "927956",
          "poster": "tartarus23",
          "upvote_count": "2",
          "timestamp": "1703027940.0",
          "comments": [
            {
              "upvote_count": "4",
              "timestamp": "1708517340.0",
              "poster": "fanq10",
              "content": "B is WRONG, you cannot use `TemplateURL` to retrieve Network Stack export values.",
              "comment_id": "986386"
            },
            {
              "timestamp": "1706509920.0",
              "comment_id": "966076",
              "content": "B is incorrect. One of the reasons is that intrinsic functions are not allowed in the Parameters section. https://repost.aws/knowledge-center/cloudformation-template-validation",
              "upvote_count": "2",
              "poster": "sb333"
            }
          ]
        },
        {
          "timestamp": "1702741380.0",
          "poster": "bakamon",
          "comment_id": "925222",
          "content": "Selected Answer: C\nC is the correct answer",
          "upvote_count": "2"
        },
        {
          "content": "Selected Answer: C\nC ipsingh is absolutely correct",
          "poster": "lunt",
          "upvote_count": "2",
          "comment_id": "905088",
          "timestamp": "1700765220.0"
        },
        {
          "comment_id": "898657",
          "comments": [
            {
              "timestamp": "1701330840.0",
              "content": "The template URL in B makes it wrong IMHO. You import the values from an exiting template importing the parameter exported by it.",
              "comment_id": "910876",
              "poster": "bcx",
              "upvote_count": "1"
            }
          ],
          "upvote_count": "2",
          "poster": "2pk",
          "content": "Selected Answer: C\nIm 50/50 C or B, but B doesn't provide a clear approach for retrieving the exported values and placing position of Parameters section of the root template, which is not required to place it there, it must declare inside resource. So i think Answer C make sense.",
          "timestamp": "1700084040.0"
        },
        {
          "upvote_count": "2",
          "content": "Selected Answer: C\nC makes more sense",
          "timestamp": "1699951140.0",
          "comment_id": "897318",
          "poster": "ParagSanyashiv"
        },
        {
          "timestamp": "1699551780.0",
          "comment_id": "893260",
          "content": "Selected Answer: C\nc is correct",
          "poster": "meisme",
          "upvote_count": "2"
        },
        {
          "comment_id": "885851",
          "timestamp": "1698819120.0",
          "content": "Selected Answer: B\nB. Use nested stacks to define common infrastructure components. To access the exported values, use TemplateURL to reference the networking team’s template. To retrieve Virtual Private Cloud (VPC) and subnet values, use Fn::ImportValue intrinsic functions in the Parameters section of the root template. Use the CreateChangeSet and ExecuteChangeSet commands to update existing development environments.\n\nThis approach is a good fit because it allows the developer to define reusable infrastructure components as nested stacks. To retrieve VPC and subnet values, the intrinsic function Fn::ImportValue is used in the Parameters section of the root template, which retrieves the values from the output of the networking team’s CloudFormation stack. To update existing environments, the CreateChangeSet and ExecuteChangeSet commands are used, which provides a way to easily update the deployed infrastructure. Additionally, the use of nested stacks helps to ensure consistency across environments.",
          "upvote_count": "1",
          "poster": "haazybanj",
          "comments": [
            {
              "poster": "sb333",
              "comment_id": "966080",
              "content": "B is incorrect. One of the reasons is that intrinsic functions are not allowed in the Parameters section. https://repost.aws/knowledge-center/cloudformation-template-validation",
              "upvote_count": "2",
              "timestamp": "1706509980.0"
            }
          ]
        },
        {
          "content": "Selected Answer: B\nOption B is correct. Using nested stacks, the common infrastructure components can be defined in separate templates that can be referenced by the root template. This allows for easy updates and maintenance of the common components. The networking team’s CloudFormation template can be used to export the VPC and subnet values, which can be referenced in the root template using Fn::ImportValue intrinsic functions in the Parameters section. The CreateChangeSet and ExecuteChangeSet commands can be used to update the existing development environments.\n\nOption C is not the best choice because using Fn::ImportValue intrinsic functions with the resources of the nested stack can lead to circular dependencies and make it difficult to manage the infrastructure.",
          "comment_id": "877664",
          "timestamp": "1698012180.0",
          "poster": "herohiro",
          "upvote_count": "2"
        },
        {
          "content": "Selected Answer: C\nC is correct",
          "poster": "alce2020",
          "upvote_count": "2",
          "comment_id": "872143",
          "timestamp": "1697487600.0"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:19:36.069Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "CTEh9iWiv2H3b7nmIGr9",
      "question_number": 312,
      "page": 63,
      "question_text": "A company uses AWS Organizations to manage multiple accounts. Information security policies require that all unencrypted Amazon EBS volumes be marked as non-compliant. A DevOps engineer needs to automatically deploy the solution and ensure that this compliance check is always present.\nWhich solution will accomplish this?",
      "choices": {
        "A": "Create an AWS CloudFormation template that defines an AWS Inspector rule to check whether EBS encryption is enabled. Save the template to an Amazon S3 bucket that has been shared with all accounts within the company. Update the account creation script pointing to the CloudFormation template in Amazon S3.",
        "D": "Deploy an IAM role to all accounts from a single trusted account. Build a pipeline with AWS CodePipeline with a stage in AWS Lambda to assume the IAM role, and list all EBS volumes in the account. Publish a report to Amazon S3.",
        "B": "Create an AWS Config organizational rule to check whether EBS encryption is enabled and deploy the rule using the AWS CLI. Create and apply an SCP to prohibit stopping and deleting AWS Config across the organization.",
        "C": "Create an SCP in Organizations. Set the policy to prevent the launch of Amazon EC2 instances without encryption on the EBS volumes using a conditional expression. Apply the SCP to all AWS accounts. Use Amazon Athena to analyze the AWS CloudTrail output, looking for events that deny an ec2:RunInstances action."
      },
      "correct_answer": "B",
      "answer_ET": "B",
      "answers_community": [
        "B (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/106267-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-04-15 17:53:00",
      "unix_timestamp": 1681573980,
      "discussion_count": 7,
      "discussion": [
        {
          "timestamp": "1725476760.0",
          "poster": "YucelFuat",
          "comment_id": "1278444",
          "content": "Selected Answer: B\nExam Tip -> Compliance = AWS Config",
          "upvote_count": "1"
        },
        {
          "comment_id": "1156201",
          "content": "Selected Answer: B\nDeploy CloudFormation template with encrypted-volumes in the ConfigRuleName property, AWS Config will automatically scan the environment and check for unencrypted EBS volumes.",
          "upvote_count": "4",
          "poster": "dzn",
          "timestamp": "1708585860.0"
        },
        {
          "upvote_count": "1",
          "comment_id": "1136517",
          "content": "B is correct",
          "poster": "thanhnv142",
          "timestamp": "1706688240.0"
        },
        {
          "comment_id": "928314",
          "poster": "madperro",
          "content": "Selected Answer: B\nB is the only solution meeting the criteria.",
          "timestamp": "1687253280.0",
          "upvote_count": "3"
        },
        {
          "poster": "haazybanj",
          "upvote_count": "3",
          "timestamp": "1683081240.0",
          "comment_id": "888093",
          "content": "Selected Answer: B\nB. Create an AWS Config organizational rule to check whether EBS encryption is enabled and deploy the rule using the AWS CLI. Create and apply an SCP to prohibit stopping and deleting AWS Config across the organization, will accomplish the compliance check on all accounts.\n\nOption A is incorrect because an AWS Inspector rule is used to analyze the behavior of the application on the EC2 instance, not to check the encryption of the EBS volume."
        },
        {
          "timestamp": "1682914380.0",
          "content": "Selected Answer: B\nB is right",
          "poster": "haazybanj",
          "upvote_count": "2",
          "comment_id": "885852"
        },
        {
          "content": "Selected Answer: B\nB is the answer",
          "poster": "alce2020",
          "upvote_count": "2",
          "comment_id": "871047",
          "timestamp": "1681573980.0"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:19:36.069Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "WVcxIaLq3JdmWptm726K",
      "question_number": 313,
      "page": 63,
      "question_text": "A company is performing vulnerability scanning for all Amazon EC2 instances across many accounts. The accounts are in an organization in AWS Organizations. Each account's VPCs are attached to a shared transit gateway. The VPCs send traffic to the internet through a central egress VPC. The company has enabled Amazon Inspector in a delegated administrator account and has enabled scanning for all member accounts.\nA DevOps engineer discovers that some EC2 instances are listed in the \"not scanning\" tab in Amazon Inspector.\nWhich combination of actions should the DevOps engineer take to resolve this issue? (Choose three.)",
      "choices": {
        "C": "Grant inspector:StartAssessmentRun permissions to the IAM role that the DevOps engineer is using.",
        "A": "Verify that AWS Systems Manager Agent is installed and is running on the EC2 instances that Amazon Inspector is not scanning.",
        "E": "Associate the target EC2 instances with instance profiles that grant permissions to communicate with AWS Systems Manager.",
        "D": "Configure EC2 Instance Connect for the EC2 instances that Amazon Inspector is not scanning.",
        "F": "Create a managed-instance activation. Use the Activation Code and the Activation ID to register the EC2 instances.",
        "B": "Associate the target EC2 instances with security groups that allow outbound communication on port 443 to the AWS Systems Manager service endpoint."
      },
      "correct_answer": "ABE",
      "answer_ET": "ABE",
      "answers_community": [
        "ABE (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/105446-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-04-06 20:03:00",
      "unix_timestamp": 1680804180,
      "discussion_count": 9,
      "discussion": [
        {
          "poster": "Dimidrol",
          "content": "Selected Answer: ABE\nA b e https://docs.aws.amazon.com/inspector/latest/user/scanning-ec2.html",
          "comment_id": "863212",
          "timestamp": "1696615380.0",
          "upvote_count": "7"
        },
        {
          "comment_id": "1156221",
          "upvote_count": "3",
          "content": "Selected Answer: ABE\nC is not a fundamental solution. Because Inspector is actually able to run, and it is not the same IAM role that DevOps uses.",
          "timestamp": "1724306040.0",
          "poster": "dzn"
        },
        {
          "poster": "thanhnv142",
          "content": "ABE are correct: Check if SSM agent is installed, check connection and permission of Ec2 that allows access to SSM\nC: no need to grant inspector:StartAssessmentRun permissions because the dev has already finish the scanning task\nD: There is not EC2 instance Connect, only need SSM agent\nF: there is no managed-instance activation",
          "upvote_count": "4",
          "comment_id": "1136558",
          "timestamp": "1722409320.0"
        },
        {
          "timestamp": "1717929420.0",
          "poster": "yorkicurke",
          "content": "Selected Answer: ABE\nthe following link explains it all;\nhttps://repost.aws/knowledge-center/systems-manager-ec2-instance-not-appear",
          "comment_id": "1091801",
          "upvote_count": "3"
        },
        {
          "content": "Selected Answer: ABE\nABE seem to be prerequisites to work with SSM and Inspector.",
          "comment_id": "928325",
          "upvote_count": "2",
          "poster": "madperro",
          "timestamp": "1703073420.0"
        },
        {
          "poster": "bcx",
          "comment_id": "910888",
          "timestamp": "1701331320.0",
          "content": "Selected Answer: ABE\nA B E is the correct one IMHO",
          "upvote_count": "2"
        },
        {
          "comment_id": "892221",
          "poster": "ParagSanyashiv",
          "content": "Selected Answer: ABE\nABE makes more sense.",
          "upvote_count": "2",
          "timestamp": "1699458780.0"
        },
        {
          "comment_id": "872165",
          "poster": "alce2020",
          "upvote_count": "3",
          "timestamp": "1697489040.0",
          "content": "A,B,E are correct https://docs.aws.amazon.com/inspector/latest/user/scanning-ec2.html"
        },
        {
          "comment_id": "870453",
          "upvote_count": "4",
          "timestamp": "1697314560.0",
          "poster": "jqso234",
          "content": "Selected Answer: ABE\nOption C suggests granting inspector:StartAssessmentRun permissions to the IAM role being used by the DevOps engineer. However, this may not be relevant to the issue of instances not being scanned by Amazon Inspector, as the IAM role may already have the necessary permissions by default.\n\nTherefore, A, B, E is a better choice in this case as it includes the necessary steps to ensure that the instances can communicate with AWS Systems Manager, which is required for Amazon Inspector to scan the instances."
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:19:36.069Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "UeJZxpoMxaqK55ITUNer",
      "question_number": 314,
      "page": 63,
      "question_text": "A development team uses AWS CodeCommit for version control for applications. The development team uses AWS CodePipeline, AWS CodeBuild. and AWS CodeDeploy for CI/CD infrastructure. In CodeCommit, the development team recently merged pull requests that did not pass long-running tests in the code base. The development team needed to perform rollbacks to branches in the codebase, resulting in lost time and wasted effort.\nA DevOps engineer must automate testing of pull requests in CodeCommit to ensure that reviewers more easily see the results of automated tests as part of the pull request review.\nWhat should the DevOps engineer do to meet this requirement?",
      "choices": {
        "A": "Create an Amazon EventBridge rule that reacts to the pullRequestStatusChanged event. Create an AWS Lambda function that invokes a CodePipeline pipeline with a CodeBuild action that runs the tests for the application. Program the Lambda function to post the CodeBuild badge as a comment on the pull request so that developers will see the badge in their code review.",
        "D": "Create an Amazon EventBridge rule that reacts to the pullRequestStatusChanged event. Create an AWS Lambda function that invokes a CodePipeline pipeline with a CodeBuild action that runs the tests for the application. Program the Lambda function to post the CodeBuild test results as a comment on the pull request when the test results are complete.",
        "C": "Create an Amazon EventBridge rule that reacts to pullRequestCreated and pullRequestSourceBranchUpdated events. Create an AWS Lambda function that invokes a CodePipeline pipeline with a CodeBuild action that runs the tests for the application. Program the Lambda function to post the CodeBuild badge as a comment on the pull request so that developers will see the badge in their code review.",
        "B": "Create an Amazon EventBridge rule that reacts to the pullRequestCreated event. Create an AWS Lambda function that invokes a CodePipeline pipeline with a CodeBuild action that runs the tests for the application. Program the Lambda function to post the CodeBuild test results as a comment on the pull request when the test results are complete."
      },
      "correct_answer": "C",
      "answer_ET": "C",
      "answers_community": [
        "C (65%)",
        "B (21%)",
        "14%"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/105492-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-04-07 11:26:00",
      "unix_timestamp": 1680859560,
      "discussion_count": 28,
      "discussion": [
        {
          "upvote_count": "16",
          "comments": [
            {
              "upvote_count": "1",
              "poster": "Gomer",
              "comment_id": "1225053",
              "timestamp": "1717629180.0",
              "content": "Link is dead"
            }
          ],
          "timestamp": "1688072880.0",
          "content": "C. Look at #3 in the below. \nhttps://container-devsecops.awssecworkshops.com/04-testing/",
          "comment_id": "938573",
          "poster": "MarDog"
        },
        {
          "upvote_count": "12",
          "timestamp": "1687255380.0",
          "comment_id": "928327",
          "poster": "madperro",
          "content": "Selected Answer: B\nB, we need to run tests only when pull request is created and we need to publish test results, not only badge."
        },
        {
          "comment_id": "1561942",
          "content": "Selected Answer: C\nC triggers on both pullRequestCreated and pullRequestSourceBranchUpdated events ensures tests are re-run when a PR is first made or when new commits are pushed to the PR branch. triggers on both creations and updates is just good practice IMO. not handling updated PRs isn't ideal.\n\nwhy not A: triggers only on pullRequestStatusChanged, which typically means opened/closed/merged — too late to catch new commits pushed to a PR\nB Similar to A, but at least uses pullRequestCreated but doesn't handle updated PRs (new commits to the source branch)which is a major gap\nD again, pullRequestStatusChanged misses the moment when new code is pushed to the PR",
          "upvote_count": "1",
          "poster": "GripZA",
          "timestamp": "1745066820.0"
        },
        {
          "comment_id": "1342284",
          "timestamp": "1737137880.0",
          "content": "Selected Answer: C\nC covers both event scenarios: PR creation and update (when updating the source branch, for example, with a new commit.",
          "upvote_count": "1",
          "poster": "lgallard"
        },
        {
          "timestamp": "1717629120.0",
          "upvote_count": "3",
          "comment_id": "1225052",
          "poster": "Gomer",
          "content": "Selected Answer: C\n\"Automated Code Review on Pull Requests using AWS CodeCommit and AWS CodeBuild\"\n\"The solution comprises of the following components:\"\n\"Amazon EventBridge: AWS service to receive pullRequestCreated and pullRequestSourceBranchUpdated events and trigger Amazon EventBridge rule.\"\nhttps://aws.amazon.com/blogs/devops/automated-code-review-on-pull-requests-using-aws-codecommit-and-aws-codebuild/"
        },
        {
          "upvote_count": "1",
          "content": "Selected Answer: C\ni go with C",
          "comment_id": "1205172",
          "poster": "seetpt",
          "timestamp": "1714581720.0"
        },
        {
          "comments": [
            {
              "upvote_count": "1",
              "poster": "shammous",
              "content": "I agree the C is the closed correct answer but it doesn't mention pullRequestStatusChanged (not sure why you mention it in your comment). \n\"The primary events in AWS CodeCommit that can trigger the pipeline are:\n- pullRequestCreated: This event occurs when a new pull request is created.\n- pullRequestSourceBranchUpdated: This event occurs when the source branch of an existing pull request is updated (e.g. when new commits are pushed to the branch).\"\nThe other events that might be considered but I would exclude are: \n- pullRequestStatusChanged: This event occurs when the status of a pull request changes, from closed to open (which we can consider), or from open to close (which we shouldn't consider in our case).\n- pullRequestMerged: This event occurs when a pull request is merged. I would exclude it because we are looking to test before merging.",
              "comment_id": "1256586",
              "timestamp": "1722142620.0"
            }
          ],
          "content": "C is the answer to ensure code reviewers more easily see the results of automated tests as part of the pull request review\npullRequestStatusChanged event is triggered whenever the status of a pull request changes. This could include transitions like: Open to Closed (pull request is merged or marked as closed)\nClosed to Open (pull request is reopened)\npullRequestCreated event is triggered whenever a new pull request is created in a CodeCommit repository.\npullRequestSourceBranchUpdated event is triggered whenever there are updates (new commits) pushed to the source branch of an open pull request",
          "poster": "zijo",
          "timestamp": "1711107180.0",
          "upvote_count": "4",
          "comment_id": "1180026"
        },
        {
          "content": "B is correct: we need to react when there is merge request (pullRequestCreated event)\nA: we need to react when there is merge request, not when the status of merge request is changed (pullRequestStatusChanged event)\nC: we only need to react when there is merge request, not when a sourcebranch is updated (pullRequestSourceBranchUpdated events)\nD: we need to react when there is merge request, not when the status of merge request is changed (pullRequestStatusChanged event)",
          "comment_id": "1136589",
          "poster": "thanhnv142",
          "upvote_count": "2",
          "timestamp": "1706693640.0",
          "comments": [
            {
              "upvote_count": "4",
              "poster": "gg_robin",
              "content": "If the source is updated after the PR is created, you don't run any tests against those changes.",
              "timestamp": "1720163400.0",
              "comment_id": "1242556"
            }
          ]
        },
        {
          "timestamp": "1703085780.0",
          "upvote_count": "2",
          "comment_id": "1101679",
          "content": "Why not B?",
          "poster": "DucSiu"
        },
        {
          "poster": "zolthar_z",
          "content": "Answer is C: the pullRequestStatusChanged only has two values (OPEN|CLOSED) so If there is any update in the code the tests will not run.\n\nhttps://docs.aws.amazon.com/codecommit/latest/APIReference/API_PullRequestStatusChangedEventMetadata.html",
          "comment_id": "1076286",
          "upvote_count": "2",
          "timestamp": "1700570280.0"
        },
        {
          "upvote_count": "2",
          "content": "Selected Answer: C\nI'll go for C. Tbh, I don't think we will need a lambda here as the event rule can definitely trigger the code pipeline & code build.",
          "timestamp": "1698910440.0",
          "poster": "DZ_Ben",
          "comment_id": "1060359"
        },
        {
          "content": "Selected Answer: C\nhttps://aws.amazon.com/blogs/devops/automated-code-review-on-pull-requests-using-aws-codecommit-and-aws-codebuild/",
          "timestamp": "1694391900.0",
          "comment_id": "1004371",
          "poster": "RVivek",
          "upvote_count": "4"
        },
        {
          "poster": "ggrodskiy",
          "upvote_count": "2",
          "timestamp": "1692014940.0",
          "content": "Correct C.",
          "comment_id": "980759"
        },
        {
          "poster": "vherman",
          "timestamp": "1690954740.0",
          "content": "Selected Answer: C\nС \nrun tests on pull requests created and when source branch receives new commits to re-run tests",
          "comment_id": "969738",
          "upvote_count": "2"
        },
        {
          "upvote_count": "2",
          "poster": "lunt",
          "content": "Selected Answer: C\nNot sure why so much discussion. Triggers Rule: A CloudWatch Event Rule is triggered based on the following events: pullRequestSourceBranchUpdated or pullRequestCreated.\nC is only viable option. I mean it even tells you the answer in the question \"development team needed to perform rollbacks to branches in the codebase\".\nAns is C.",
          "timestamp": "1690643160.0",
          "comment_id": "966489"
        },
        {
          "content": "Selected Answer: C\nThis approach allows testing whenever a pull request is created or the source branch of a pull request is updated. When the tests are complete, the AWS Lambda function posts the test status badge as a comment on the pull request, providing visual feedback to reviewers directly in the context of the pull request review.\n\nIt's important to note that CodeBuild creates a build badge that provides status about the last build, which might not directly reflect the test results of the specific pull request. Posting the test results would provide more accurate and relevant information but doing so might require additional scripting or tooling not described in the available options.",
          "upvote_count": "3",
          "comment_id": "927961",
          "timestamp": "1687209780.0",
          "poster": "tartarus23"
        },
        {
          "upvote_count": "3",
          "poster": "bcx",
          "content": "Selected Answer: C\nC is the correct IMHO.\n\nA pull request is just a branch that the requestor is asking to be merged in master/main. When you create a pull request you set the branch, that is the start, you have to use the current contents of the branch to execute the tests. When time passes developers add commits to that branch or force-push it, changing the contents of the PR's branch. That is the moment in which you have to trigger the tests. The PR comments and discussions may change, but that does not change the code so no need to perform new tests.\nYou only test when the PR is created and every time the branch is pushed (updated).",
          "timestamp": "1685513340.0",
          "comment_id": "910895"
        },
        {
          "content": "Why not B?",
          "poster": "youonebe",
          "upvote_count": "2",
          "timestamp": "1685360820.0",
          "comment_id": "909351"
        },
        {
          "comment_id": "908983",
          "content": "Selected Answer: C\nD is incorrect.\n\npullRequestStatusChanged event\nIn this example event, a user who assumed a role named Admin with a session name of Mary_Major closed a pull request with the ID of 1. The pull request was not merged.\nhttps://docs.aws.amazon.com/codecommit/latest/userguide/monitoring-events.html#pullRequestMergeStatusUpdated",
          "timestamp": "1685327220.0",
          "upvote_count": "2",
          "poster": "qan1257"
        },
        {
          "comment_id": "898493",
          "comments": [
            {
              "timestamp": "1684167300.0",
              "content": "C is right answer.",
              "upvote_count": "2",
              "poster": "ipsingh",
              "comment_id": "898494"
            }
          ],
          "timestamp": "1684167240.0",
          "poster": "ipsingh",
          "content": "D is Wring because pullRequestStatusChanged can trigger even when a PR is closed. Why should we run a test when a PR is closed. \n\nTest should run only when PR is raised or the source branch of the PR (yet to be merged) has changed.",
          "upvote_count": "3"
        },
        {
          "comment_id": "896686",
          "poster": "ele",
          "content": "Selected Answer: C\nC is right, because if source branch is updated, tests must be rerun. Badge is Faled/Success in codecommit in PR, to block PR if build failed.",
          "timestamp": "1683983640.0",
          "upvote_count": "4"
        },
        {
          "upvote_count": "1",
          "timestamp": "1683648060.0",
          "content": "Selected Answer: D\nD is correct",
          "poster": "meisme",
          "comment_id": "893276"
        },
        {
          "content": "Selected Answer: D\nTo automate testing of pull requests in CodeCommit and ensure that reviewers more easily see the results of automated tests as part of the pull request review, the DevOps engineer can use Amazon EventBridge to react to the pullRequestStatusChanged event and create an AWS Lambda function that invokes a CodePipeline pipeline with a CodeBuild action that runs the tests for the application. Then, the Lambda function can be programmed to post the CodeBuild test results as a comment on the pull request when the test results are complete. Therefore, the correct answer is option D.",
          "upvote_count": "2",
          "comment_id": "888098",
          "timestamp": "1683081900.0",
          "poster": "haazybanj"
        },
        {
          "timestamp": "1682914860.0",
          "poster": "haazybanj",
          "upvote_count": "2",
          "content": "Selected Answer: D\nD. Create an Amazon EventBridge rule that reacts to the pullRequestStatusChanged event. Create an AWS Lambda function that invokes a CodePipeline pipeline with a CodeBuild action that runs the tests for the application. Program the Lambda function to post the CodeBuild test results as a comment on the pull request when the test results are complete.",
          "comment_id": "885855"
        },
        {
          "upvote_count": "4",
          "content": "Selected Answer: C\nI'll go for C",
          "poster": "alce2020",
          "comment_id": "872169",
          "timestamp": "1681678080.0"
        },
        {
          "content": "go with D",
          "comment_id": "870481",
          "timestamp": "1681507140.0",
          "upvote_count": "1",
          "poster": "boledadian"
        },
        {
          "comments": [
            {
              "timestamp": "1682481720.0",
              "upvote_count": "1",
              "poster": "[Removed]",
              "comment_id": "881075",
              "content": "why not B?"
            }
          ],
          "comment_id": "870455",
          "content": "Selected Answer: D\nD is likely the best answer for this scenario because it addresses the original requirement of automating testing of pull requests in CodeCommit and ensuring that reviewers see the results of automated tests as part of the pull request review. \nOption C reacts to pullRequestCreated and pullRequestSourceBranchUpdated events, but it also only posts the CodeBuild badge as a comment, which may not provide enough detail for reviewers to make an informed decision about the pull request.\n\nbased on the requirements of automating testing of pull requests in CodeCommit and ensuring that reviewers see the results of automated tests as part of the pull request review, D is the most suitable answer.",
          "timestamp": "1681503540.0",
          "upvote_count": "3",
          "poster": "jqso234"
        },
        {
          "upvote_count": "5",
          "content": "Selected Answer: C\nC for me. https://aws.amazon.com/ru/blogs/devops/validating-aws-codecommit-pull-requests-with-aws-codebuild-and-aws-lambda/",
          "timestamp": "1680859560.0",
          "comment_id": "863683",
          "poster": "Dimidrol"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:19:36.069Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "jC0zo2YjuShUiYco5Dtu",
      "question_number": 315,
      "page": 63,
      "question_text": "A company has deployed an application in a production VPC in a single AWS account. The application is popular and is experiencing heavy usage. The company’s security team wants to add additional security, such as AWS WAF, to the application deployment. However, the application's product manager is concerned about cost and does not want to approve the change unless the security team can prove that additional security is necessary.\nThe security team believes that some of the application's demand might come from users that have IP addresses that are on a deny list. The security team provides the deny list to a DevOps engineer. If any of the IP addresses on the deny list access the application, the security team wants to receive automated notification in near real time so that the security team can document that the application needs additional security. The DevOps engineer creates a VPC flow log for the production VPC.\nWhich set of additional steps should the DevOps engineer take to meet these requirements MOST cost-effectively?",
      "choices": {
        "B": "Create an Amazon S3 bucket for log files. Configure the VPC flow log to capture all traffic and to send the data to the S3 bucket. Configure Amazon Athena to return all log files in the S3 bucket for IP addresses on the deny list. Configure Amazon QuickSight to accept data from Athena and to publish the data as a dashboard that the security team can access. Create a threshold alert of 1 for successful access. Configure the alert to automatically notify the security team as frequently as possible when the alert threshold is met.",
        "A": "Create a log group in Amazon CloudWatch Logs. Configure the VPC flow log to capture accepted traffic and to send the data to the log group. Create an Amazon CloudWatch metric filter for IP addresses on the deny list. Create a CloudWatch alarm with the metric filter as input. Set the period to 5 minutes and the datapoints to alarm to 1. Use an Amazon Simple Notification Service (Amazon SNS) topic to send alarm notices to the security team.",
        "C": "Create an Amazon S3 bucket for log files. Configure the VPC flow log to capture accepted traffic and to send the data to the S3 bucket. Configure an Amazon OpenSearch Service cluster and domain for the log files. Create an AWS Lambda function to retrieve the logs from the S3 bucket, format the logs, and load the logs into the OpenSearch Service cluster. Schedule the Lambda function to run every 5 minutes. Configure an alert and condition in OpenSearch Service to send alerts to the security team through an Amazon Simple Notification Service (Amazon SNS) topic when access from the IP addresses on the deny list is detected.",
        "D": "Create a log group in Amazon CloudWatch Logs. Create an Amazon S3 bucket to hold query results. Configure the VPC flow log to capture all traffic and to send the data to the log group. Deploy an Amazon Athena CloudWatch connector in AWS Lambda. Connect the connector to the log group. Configure Athena to periodically query for all accepted traffic from the IP addresses on the deny list and to store the results in the S3 bucket. Configure an S3 event notification to automatically notify the security team through an Amazon Simple Notification Service (Amazon SNS) topic when new objects are added to the S3 bucket."
      },
      "correct_answer": "A",
      "answer_ET": "A",
      "answers_community": [
        "A (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/106266-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-04-15 17:24:00",
      "unix_timestamp": 1681572240,
      "discussion_count": 7,
      "discussion": [
        {
          "timestamp": "1703074140.0",
          "upvote_count": "8",
          "poster": "madperro",
          "comment_id": "928334",
          "content": "Selected Answer: A\nA meets the requirements at the lowest cost."
        },
        {
          "content": "Selected Answer: A\nopensearch cost a lot of $$$. Athena got tons of things to do afterwards. It's used purely for interactive query. \n\nnatively, vpcflow sends logs to s3 or cloudwatch logs. no brainer answer",
          "upvote_count": "5",
          "comment_id": "946411",
          "poster": "habros",
          "timestamp": "1704718320.0"
        },
        {
          "poster": "thanhnv142",
          "comment_id": "1136790",
          "timestamp": "1722425820.0",
          "upvote_count": "2",
          "content": "A is correct: push all VPC flow log to cloudwatch logs. Create metric filter to find denied IP addresses. Create cloudwatch alarm with the metric filter as input. Alarm's action is send noti to Security team via SNS\nB: \"Configure the alert to automatically notify the security team\": alert cannot notify by itself. Must use SNS\nC: This option uses both S3 bucket and \"Amazon OpenSearch Service cluster\" to store log files, which would cost a lot of money and unnecessary \nD: This option uses both S3 bucket and VPC flow log to store log files, which is costly"
        },
        {
          "timestamp": "1699888920.0",
          "poster": "ele",
          "comment_id": "896691",
          "content": "Selected Answer: A\nA most cost effective",
          "upvote_count": "2"
        },
        {
          "content": "Selected Answer: A\nTo meet the requirements most cost-effectively, the DevOps engineer should create a log group in Amazon CloudWatch Logs and configure the VPC flow log to capture accepted traffic and to send the data to the log group. Then, create an Amazon CloudWatch metric filter for IP addresses on the deny list and create a CloudWatch alarm with the metric filter as input. Set the period to 5 minutes and the datapoints to alarm to 1. Finally, use an Amazon Simple Notification Service (Amazon SNS) topic to send alarm notices to the security team.\n\nOption A is the correct answer. It provides a cost-effective solution that meets the requirements. The CloudWatch alarm notifies the security team in near real-time when traffic from an IP address on the deny list is detected. This will help the security team document that the application needs additional security. This solution only requires the use of AWS services that the company is already using, and does not require any additional services or tools.",
          "poster": "haazybanj",
          "comment_id": "888105",
          "timestamp": "1698987300.0",
          "upvote_count": "4"
        },
        {
          "poster": "haazybanj",
          "content": "D. Create an Amazon EventBridge rule that reacts to the pullRequestStatusChanged event. Create an AWS Lambda function that invokes a CodePipeline pipeline with a CodeBuild action that runs the tests for the application. Program the Lambda function to post the CodeBuild test results as a comment on the pull request when the test results are complete.",
          "upvote_count": "1",
          "timestamp": "1698819720.0",
          "comment_id": "885858",
          "comments": [
            {
              "upvote_count": "1",
              "comment_id": "999800",
              "content": "Option D does not mention an event bridge rule",
              "poster": "BaburTurk",
              "comments": [
                {
                  "content": "This answer is for a previous question",
                  "upvote_count": "1",
                  "timestamp": "1716400920.0",
                  "poster": "zolthar_z",
                  "comment_id": "1077760"
                }
              ],
              "timestamp": "1709670720.0"
            }
          ]
        },
        {
          "poster": "alce2020",
          "content": "Selected Answer: A\nA seems correct",
          "timestamp": "1697383440.0",
          "comment_id": "871030",
          "upvote_count": "2"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:19:36.069Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "pG9ZlmDA2dvhFT1T7DJM",
      "question_number": 316,
      "page": 64,
      "question_text": "A DevOps engineer has automated a web service deployment by using AWS CodePipeline with the following steps:\n1. An AWS CodeBuild project compiles the deployment artifact and runs unit tests.\n2. An AWS CodeDeploy deployment group deploys the web service to Amazon EC2 instances in the staging environment.\n3. A CodeDeploy deployment group deploys the web service to EC2 instances in the production environment.\nThe quality assurance (QA) team requests permission to inspect the build artifact before the deployment to the production environment occurs. The QA team wants to run an internal penetration testing tool to conduct manual tests. The tool will be invoked by a REST API call.\nWhich combination of actions should the DevOps engineer take to fulfill this request? (Choose two.)",
      "choices": {
        "B": "Modify the buildspec.yml file for the compilation stage to require manual approval before completion.",
        "E": "Update the pipeline to invoke an AWS Lambda function that calls the REST API for the penetration testing tool.",
        "D": "Update the pipeline to directly call the REST API for the penetration testing tool.",
        "C": "Update the CodeDeploy deployment groups so that they require manual approval to proceed.",
        "A": "Insert a manual approval action between the test actions and deployment actions of the pipeline."
      },
      "correct_answer": "AE",
      "answer_ET": "AE",
      "answers_community": [
        "AE (81%)",
        "AD (19%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/106265-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-04-15 17:17:00",
      "unix_timestamp": 1681571820,
      "discussion_count": 20,
      "discussion": [
        {
          "comment_id": "927963",
          "content": "Selected Answer: AE\nExplanation:\nThe manual approval action (A) will allow the QA team to inspect the build artifact and run their internal penetration testing tool before the deployment to the production environment proceeds.\n\nUsing an AWS Lambda function (E) would provide an automated way to call the REST API of the penetration testing tool. This would allow for the tests to be conducted automatically within the pipeline. This is beneficial because it ensures consistency in the testing process and could be run programmatically, reducing manual steps.",
          "upvote_count": "8",
          "poster": "tartarus23",
          "timestamp": "1687209960.0"
        },
        {
          "upvote_count": "1",
          "timestamp": "1753175040.0",
          "comment_id": "1589254",
          "content": "Selected Answer: AE\nA E there is no way to call REST API directly in the code pipeline, it is possible invoke via Lambda function only",
          "poster": "Jonalb"
        },
        {
          "content": "Selected Answer: AE\nOption D (updating the pipeline to directly call the REST API for the penetration testing tool) is not recommended because it tightly couples the pipeline with the QA team's tool, making it less flexible and harder to maintain. Using a Lambda function as an intermediary provides better separation of concerns and easier maintainability.",
          "timestamp": "1722934920.0",
          "poster": "iulian0585",
          "comment_id": "1261567",
          "upvote_count": "2"
        },
        {
          "upvote_count": "1",
          "content": "Selected Answer: AE\nShould be AE\nAlthough there are limitation 15mins of Lambda function. \nBut Option D is wrong as CodePipeline does not have the ability to execute HTTP requests \"directly\".\n\nhttps://docs.aws.amazon.com/codepipeline/latest/userguide/actions-invoke-lambda-function.html",
          "poster": "jamesf",
          "comment_id": "1255431",
          "timestamp": "1721969820.0",
          "comments": [
            {
              "upvote_count": "1",
              "content": "For option A, keywords: conduct manual tests",
              "poster": "jamesf",
              "comment_id": "1259275",
              "timestamp": "1722502920.0"
            }
          ]
        },
        {
          "timestamp": "1711122660.0",
          "poster": "zijo",
          "upvote_count": "1",
          "comment_id": "1180141",
          "content": "This is tricky but AD should be a better choice because of the 15 min timeout of Lambda functions. To call REST API in CodePipeline these are the two options\nFor complex API calls, security requirements, and access to external resources, an AWS Lambda function is the recommended approach.\nFor simple API calls with limited requirements, consider the inline script approach within CodeBuild, but with caution due to security and maintainability limitations."
        },
        {
          "content": "AE there is no way to call REST API directly in the code pipeline, it is possible invoke via Lambda function only",
          "comment_id": "1161551",
          "timestamp": "1709118840.0",
          "poster": "Shasha1",
          "upvote_count": "4"
        },
        {
          "upvote_count": "3",
          "poster": "dzn",
          "timestamp": "1708594620.0",
          "content": "Selected Answer: AE\nCodePipeline does not have the ability to execute HTTP requests \"directly\".",
          "comment_id": "1156276"
        },
        {
          "content": "A and D are correct: a manual approval action between the test actions and deployment actions allows tester to verify and test built artifacts before allowing deploying to production \nB and C: no mentions of test and deployment env\nE: manual test take more than 15 minutes, which is the maximum execution time of lambda",
          "upvote_count": "1",
          "timestamp": "1706708880.0",
          "comment_id": "1136802",
          "poster": "thanhnv142"
        },
        {
          "comment_id": "1123483",
          "upvote_count": "2",
          "timestamp": "1705332060.0",
          "content": "Selected Answer: AE\nD is wrong, alternative option ( not using Lambda, for example, if the pen testing will take more than 15 minutes) is using codebuild, either add a new codebuild for pen testing, or update existing unit testing codebuild to include pen testing. You should never run Pen testing inside codepipeline directly , it lacks the hooks to collect test result, inform result, etc",
          "poster": "a54b16f"
        },
        {
          "upvote_count": "4",
          "comment_id": "1063758",
          "comments": [
            {
              "comment_id": "1256600",
              "timestamp": "1722146280.0",
              "content": "The lambda function would just invoke the REST API, it won't execute the pen test itself. An asynchronous mechanism involving SQS could handle the waiting time between the requesting sending and the response receiving, which can indeed last more than 15mn.",
              "poster": "shammous",
              "upvote_count": "1"
            }
          ],
          "poster": "2pk",
          "content": "Selected Answer: AD\nTricky one:\nCodeDeploy can't do actions directly like invoke REST API but code Build can.\ne.g. it's mentioned to test build artifacts.. So after the build artifact is created This means the solution uses Code Build even not from Code Build you can setup a python script and run it directly using Code Build Command: \nI'd not use Lambda as an alternative due to the time taken for penetration tests would take more than 15 mins. and the pipeline would failed with Lambda execution timeout.",
          "timestamp": "1699269000.0"
        },
        {
          "content": "conducting manual tests might takes more than 15m.",
          "poster": "Seoyong",
          "timestamp": "1692000480.0",
          "upvote_count": "1",
          "comment_id": "980593"
        },
        {
          "timestamp": "1690951380.0",
          "content": "A, \nE in practice is a cheap and handy off-switch, recommended, for some contributors to CI/CD that we don't control directly. However, no idea what the writer of the question wanted.",
          "upvote_count": "1",
          "comment_id": "969680",
          "poster": "s50600822"
        },
        {
          "poster": "DavidPham",
          "timestamp": "1690273500.0",
          "upvote_count": "1",
          "content": "why don't you choose D",
          "comment_id": "962508"
        },
        {
          "poster": "habros",
          "content": "Selected Answer: AE\nI’ll choose AE. I can tie up multiple REST calls in a Lambda and customize it as I wish. A web hook is not flexible in this aspect I feel.",
          "timestamp": "1689119880.0",
          "comment_id": "949371",
          "upvote_count": "2"
        },
        {
          "upvote_count": "1",
          "comment_id": "928337",
          "comments": [
            {
              "upvote_count": "1",
              "content": "But there is no option to invoke call an API directly = https://docs.aws.amazon.com/codepipeline/latest/userguide/integrations-action-type.html#integrations-invoke",
              "timestamp": "1693525500.0",
              "comment_id": "995543",
              "poster": "cocegas"
            }
          ],
          "poster": "madperro",
          "timestamp": "1687255920.0",
          "content": "Selected Answer: AD\nAD, Lambda is not needed, a webhook can call REST API directly."
        },
        {
          "upvote_count": "3",
          "poster": "bcx",
          "timestamp": "1685514180.0",
          "content": "Selected Answer: AE\n\"AWS Lambda is a compute service that lets you run code without provisioning or managing servers. You can create Lambda functions and add them as actions in your pipelines. Because Lambda allows you to write functions to perform almost any task, you can customize the way your pipeline works. \"\n\nhttps://docs.aws.amazon.com/codepipeline/latest/userguide/actions-invoke-lambda-function.html",
          "comment_id": "910906"
        },
        {
          "poster": "qsergii",
          "timestamp": "1685348460.0",
          "comment_id": "909192",
          "upvote_count": "2",
          "content": "Selected Answer: AD\nA & D, lambda (E) is extra and not needed."
        },
        {
          "poster": "Akaza",
          "content": "Selected Answer: AE\nYepp A, E for me",
          "comment_id": "905634",
          "timestamp": "1684916340.0",
          "upvote_count": "2"
        },
        {
          "content": "Selected Answer: AE\nA & E look right",
          "poster": "haazybanj",
          "upvote_count": "3",
          "comment_id": "885860",
          "timestamp": "1682915040.0"
        },
        {
          "content": "Selected Answer: AE\nA & E sound right",
          "timestamp": "1681571820.0",
          "comment_id": "871026",
          "upvote_count": "2",
          "poster": "alce2020"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:19:46.538Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "J0revAdsW7lNgmJNGsLw",
      "question_number": 317,
      "page": 64,
      "question_text": "A company is hosting a web application in an AWS Region. For disaster recovery purposes, a second region is being used as a standby. Disaster recovery requirements state that session data must be replicated between regions in near-real time and 1% of requests should route to the secondary region to continuously verify system functionality. Additionally, if there is a disruption in service in the main region, traffic should be automatically routed to the secondary region, and the secondary region must be able to scale up to handle all traffic.\nHow should a DevOps engineer meet these requirements?",
      "choices": {
        "A": "In both regions, deploy the application on AWS Elastic Beanstalk and use Amazon DynamoDB global tables for session data. Use an Amazon Route 53 weighted routing policy with health checks to distribute the traffic across the regions.",
        "D": "In both regions, launch the application in Auto Scaling groups and use DynamoDB global tables for session data. Enable an Amazon CloudFront weighted distribution across regions. Point the Amazon Route 53 DNS record at the CloudFront distribution.",
        "C": "In both regions, deploy the application in AWS Lambda, exposed by Amazon API Gateway, and use Amazon RDS for PostgreSQL with cross-region replication for session data. Deploy the web application with client-side logic to call the API Gateway directly.",
        "B": "In both regions, launch the application in Auto Scaling groups and use DynamoDB for session data. Use a Route 53 failover routing policy with health checks to distribute the traffic across the regions."
      },
      "correct_answer": "A",
      "answer_ET": "A",
      "answers_community": [
        "A (81%)",
        "D (19%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/106264-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-04-15 17:12:00",
      "unix_timestamp": 1681571520,
      "discussion_count": 21,
      "discussion": [
        {
          "upvote_count": "7",
          "poster": "davdan99",
          "timestamp": "1704646620.0",
          "content": "Selected Answer: A\nI think it is A, We can have failover with CloudFront, but it can't have weighted routing, Here is the link of how auromatic failover works in the CloudFront\n\nhttps://disaster-recovery.workshop.aws/en/services/networking/cloudfront/cloudfront-origin-group.html",
          "comment_id": "1116025"
        },
        {
          "upvote_count": "3",
          "comment_id": "1255436",
          "content": "A\nKeywords: web application - ElasticBeanstalk, weighted routing required. \nDynamoDB Global Table required.\n\nAs understand, Cloudfront not support weighted routing.",
          "poster": "jamesf",
          "timestamp": "1721970720.0"
        },
        {
          "upvote_count": "1",
          "poster": "auxwww",
          "timestamp": "1721786220.0",
          "content": "Selected Answer: A\nA - correct - Elasticbeanstalk - option of ALB to register route53 with active-active alias with health checks and weighted routing\n\"In active-active failover, all the records that have the same name, the same type (such as A or AAAA), and the same routing policy (such as weighted or latency) are active unless Route 53 considers them unhealthy. Route 53 can respond to a DNS query using any healthy record.\"\n\n\nD - incorrect because no ALB in front of ASG.",
          "comment_id": "1254047"
        },
        {
          "comments": [
            {
              "timestamp": "1717708080.0",
              "upvote_count": "1",
              "poster": "Gomer",
              "content": "Just to clarify, the scenarios is absolutely desrivinb \"weighted routing\" with 99% to 1% traffic split between regions for normal operation (unbalanced Active/Active).",
              "comment_id": "1225781"
            }
          ],
          "timestamp": "1717707240.0",
          "content": "Selected Answer: D\nIf the requirement is for \"1% of requests should route to the secondary region to continuously\", then that means the secondary region is in continuously in an Active state (Active/Active). A \"request\" is not a health check. You also have to have auto-scaling to dynamically pick up any extra traffic. The question is a little weird, in I don't know you you dynamically adjust the weighted routing policy to steer all traffice to the secondary region. I just know that \"D\" is the closest choice to meeting the specified requirements. This is absolutely and \"Active-Active\" design using weighted routing at some level, and auto-scaling just meets the demaind wherever it comes from. I think \"latency-based\" routing would make more sence, but the requirements are clearly describing \"weighted routing\" and Active-Active design.\nhttps://aws.amazon.com/blogs/networking-and-content-delivery/latency-based-routing-leveraging-amazon-cloudfront-for-a-multi-region-active-active-architecture/",
          "upvote_count": "1",
          "comment_id": "1225778",
          "poster": "Gomer"
        },
        {
          "comment_id": "1205174",
          "content": "Selected Answer: A\nA is ok",
          "timestamp": "1714582500.0",
          "poster": "seetpt",
          "upvote_count": "1"
        },
        {
          "timestamp": "1710673080.0",
          "comment_id": "1175736",
          "content": "Selected Answer: A\nA is correct\nB + C no DynamoDB Global Tables\nD - does not use Route53",
          "upvote_count": "2",
          "poster": "DanShone"
        },
        {
          "timestamp": "1706709000.0",
          "poster": "thanhnv142",
          "upvote_count": "2",
          "comment_id": "1136805",
          "content": "A is correct: beanstalk is literally designed for this specific purpose"
        },
        {
          "poster": "Jaguaroooo",
          "timestamp": "1704369180.0",
          "content": "It is A, 1% of the traffic should be going to the 2ndary site. so that's weighted routing.",
          "upvote_count": "2",
          "comment_id": "1113638"
        },
        {
          "comments": [
            {
              "timestamp": "1704646800.0",
              "content": "We have to use DynamoDB Global tables for make db acessable from 2 regions.",
              "poster": "davdan99",
              "upvote_count": "1",
              "comment_id": "1116028"
            }
          ],
          "content": "Why not B?",
          "comment_id": "1102696",
          "upvote_count": "1",
          "timestamp": "1703173980.0",
          "poster": "DucSiu"
        },
        {
          "poster": "denccc",
          "content": "It's A",
          "upvote_count": "2",
          "comment_id": "1061206",
          "timestamp": "1699002480.0"
        },
        {
          "content": "A is correct answer",
          "timestamp": "1698874440.0",
          "upvote_count": "1",
          "comment_id": "1060071",
          "poster": "rahulsingha2112"
        },
        {
          "comments": [
            {
              "upvote_count": "1",
              "comment_id": "1180151",
              "timestamp": "1711123260.0",
              "content": "The title of this page mentions Active-Active scenario and not Active-Passive as mentioned in this question.",
              "poster": "zijo"
            }
          ],
          "poster": "ekki",
          "comment_id": "1051128",
          "timestamp": "1698010260.0",
          "content": "Answer is D.\n\"Testing Regional failover\"\nhttps://aws.amazon.com/blogs/networking-and-content-delivery/latency-based-routing-leveraging-amazon-cloudfront-for-a-multi-region-active-active-architecture/",
          "upvote_count": "1"
        },
        {
          "timestamp": "1693939380.0",
          "upvote_count": "1",
          "poster": "BaburTurk",
          "comment_id": "999808",
          "comments": [
            {
              "upvote_count": "2",
              "comment_id": "1047961",
              "content": "A- Route 53 does offer the capability to automatically route traffic to the secondary region in case of a disruption. In the context of the requirements specified, option A seems to be a feasible solution as it involves the use of AWS Elastic Beanstalk for deployment, DynamoDB global tables for session data replication, and a weighted routing policy in Route 53 for traffic distribution across regions. The health checks can ensure that traffic is routed to the secondary region automatically in case of a disruption in the main region.\n\nTherefore, considering the ability of Route 53 to automatically reroute traffic, option A appears to be the most appropriate solution for meeting the specified disaster recovery requirements.",
              "poster": "BaburTurk",
              "timestamp": "1697722860.0"
            }
          ],
          "content": "Selected Answer: D\nOption A uses Elastic Beanstalk, which is not as scalable as Auto Scaling groups.\nD is correct"
        },
        {
          "poster": "mamila",
          "upvote_count": "1",
          "timestamp": "1690740060.0",
          "comment_id": "967381",
          "content": "The answer is NONE of them, none of them specified both weighted and failover routing policies."
        },
        {
          "comment_id": "953239",
          "upvote_count": "2",
          "poster": "totopopo",
          "timestamp": "1689504360.0",
          "content": "Selected Answer: A\nD is not offering scaling on DRP. A offers scaling by using BeanStalk."
        },
        {
          "content": "Selected Answer: A\nGoing for A given that DynamoDB global tables can replicate data across selected regions in near real-time. Clearly weighting and failover, thus Route53 should be selected. \n\nIt's not D because Cloudfront cannot do weighted routing.",
          "timestamp": "1688140140.0",
          "poster": "csG13",
          "comment_id": "939196",
          "upvote_count": "4"
        },
        {
          "content": "Selected Answer: D\nA and D are very similar but with using different services (BeanStalk vs CloudFront). However A is using R53 traffic distribution and D is using CF traffic distribution. I think D is better in this case. Note that not all applications will run easily on BeanStalk too.",
          "upvote_count": "3",
          "poster": "madperro",
          "timestamp": "1687322700.0",
          "comment_id": "929053"
        },
        {
          "content": "Answer seems to be B as we need to do automatic \nfailover",
          "timestamp": "1685416740.0",
          "comment_id": "909836",
          "upvote_count": "2",
          "poster": "Flyingdagger"
        },
        {
          "timestamp": "1684916640.0",
          "content": "How about D you guys? \nOption D provides a comprehensive solution that covers all the specified requirements:\n\nLaunching the application in Auto Scaling groups ensures scalability and resilience in both regions.\n\nUsing DynamoDB global tables allows for near-real-time replication of session data between the regions, ensuring data consistency.\n\nEnabling an Amazon CloudFront weighted distribution across regions allows for the routing of a percentage of requests to the secondary region for continuous verification of system functionality.\n\nPointing the Amazon Route 53 DNS record at the CloudFront distribution enables automatic traffic routing between the regions based on the weighted distribution configuration",
          "poster": "Akaza",
          "upvote_count": "1",
          "comment_id": "905639",
          "comments": [
            {
              "poster": "TroyMcLure",
              "content": "I saw absolutely no reason for ALB or Cloudfront, according to the requirements in the question. All the requirements for a DR can be fulfilled by the services that were described in Answer A.\nI hope it helps!",
              "timestamp": "1685069580.0",
              "upvote_count": "2",
              "comment_id": "907007"
            }
          ]
        },
        {
          "comment_id": "885862",
          "content": "Selected Answer: A\nA looks good",
          "timestamp": "1682915100.0",
          "poster": "haazybanj",
          "upvote_count": "3"
        },
        {
          "timestamp": "1681571520.0",
          "comment_id": "871022",
          "content": "Selected Answer: A\nA is correct",
          "upvote_count": "2",
          "poster": "alce2020"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:19:46.538Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "xuLLcgcL8C9U5rCnQ4y7",
      "question_number": 318,
      "page": 64,
      "question_text": "A company runs an application on Amazon EC2 instances. The company uses a series of AWS CloudFormation stacks to define the application resources. A developer performs updates by building and testing the application on a laptop and then uploading the build output and CloudFormation stack templates to Amazon S3. The developer's peers review the changes before the developer performs the CloudFormation stack update and installs a new version of the application onto the EC2 instances.\nThe deployment process is prone to errors and is time-consuming when the developer updates each EC2 instance with the new application. The company wants to automate as much of the application deployment process as possible while retaining a final manual approval step before the modification of the application or resources.\nThe company already has moved the source code for the application and the CloudFormation templates to AWS CodeCommit. The company also has created an AWS CodeBuild project to build and test the application.\nWhich combination of steps will meet the company’s requirements? (Choose two.)",
      "choices": {
        "A": "Create an application group and a deployment group in AWS CodeDeploy. Install the CodeDeploy agent on the EC2 instances.",
        "D": "Use AWS CodePipeline to invoke the CodeBuild job, create CloudFormation change sets for each of the application stacks, and pause for a manual approval step. After approval, run the CloudFormation change sets and start the AWS CodeDeploy deployment.",
        "C": "Use AWS CodePipeline to invoke the CodeBuild job, run the CloudFormation update, and pause for a manual approval step. After approval, start the AWS CodeDeploy deployment.",
        "B": "Create an application revision and a deployment group in AWS CodeDeploy. Create an environment in CodeDeploy. Register the EC2 instances to the CodeDeploy environment.",
        "E": "Use AWS CodePipeline to invoke the CodeBuild job, create CloudFormation change sets for each of the application stacks, and pause for a manual approval step. After approval, start the AWS CodeDeploy deployment."
      },
      "correct_answer": "AD",
      "answer_ET": "AD",
      "answers_community": [
        "AD (67%)",
        "BD (30%)",
        "2%"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/106441-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-04-16 22:57:00",
      "unix_timestamp": 1681678620,
      "discussion_count": 18,
      "discussion": [
        {
          "content": "Selected Answer: AD\n(A) This step sets up the environment to use AWS CodeDeploy for application deployments. CodeDeploy uses an agent installed on the EC2 instances to perform the deployment tasks.\n\n(D) This option uses CodePipeline to orchestrate the process. CodeBuild is used to build and test the application. CloudFormation is used to prepare the infrastructure updates as change sets. A manual approval step is inserted before applying the changes. After approval, the CloudFormation change sets are applied, and then CodeDeploy is invoked to deploy the new version of the application to the EC2 instances.",
          "comment_id": "927965",
          "upvote_count": "16",
          "poster": "tartarus23",
          "timestamp": "1687210140.0"
        },
        {
          "upvote_count": "6",
          "comments": [
            {
              "upvote_count": "8",
              "poster": "fanq10",
              "content": "- EC2 needs to install the CodeDeploy agent.\n- CodeDeploy does not need to register EC2 instances, instead of it uses tag filter.\nTherefore, B is incorrect, A is correct. Final answer: AD",
              "comments": [
                {
                  "timestamp": "1698368460.0",
                  "poster": "Karamen",
                  "content": "@fanq10 \nyou are right.\nCodeDeploy doesn't require registering EC2 instances, it fillters by tag",
                  "upvote_count": "3",
                  "comment_id": "1055023"
                }
              ],
              "comment_id": "987307",
              "timestamp": "1692697860.0"
            }
          ],
          "content": "Selected Answer: BD\nThere is no application group in CodeDeploy",
          "timestamp": "1692439200.0",
          "comment_id": "985101",
          "poster": "ky11223344"
        },
        {
          "poster": "Srikantha",
          "comment_id": "1411878",
          "timestamp": "1743285660.0",
          "upvote_count": "1",
          "content": "Selected Answer: CD\nBoth options automate the application deployment process, trigger manual approval, and ensure controlled updates and deployments to EC2 instances using CodePipeline, CodeBuild, CloudFormation, and CodeDeploy"
        },
        {
          "content": "Selected Answer: AD\nAD\nA - To run CodeDeploy on EC2, need agent. \nD - The approval step will trigger both CloudFormation and CodeDeploy\n\nB incorrect as no mention of installing agent on EC2 and CodeDeploy doesn't require registering EC2 instances, it filters by tag. \nC incorrect as The approval step does not affect CloudFormation updates, which is not accepted\nE incorrect as The approval step only allows CodeDeploy but no have CloudFormation updates",
          "comment_id": "1255452",
          "poster": "jamesf",
          "timestamp": "1721971200.0",
          "upvote_count": "2"
        },
        {
          "content": "Selected Answer: BD\nB:\nhttps://docs.aws.amazon.com/codedeploy/latest/userguide/codedeploy-agent.html\nYou can configure automatic installation and updates of the CodeDeploy agent when you create your deployment group in the console.\nhttps://docs.aws.amazon.com/codedeploy/latest/userguide/applications.html\nAfter you configure instances, but before you can deploy a revision, you must create an application in CodeDeploy. An application is simply a name or container used by CodeDeploy to ensure the correct revision, deployment configuration, and deployment group are referenced during a deployment.\nhttps://docs.aws.amazon.com/codedeploy/latest/userguide/application-revisions.html\nIn CodeDeploy, a revision contains a version of the source files CodeDeploy will deploy to your instances or scripts CodeDeploy will run on your instances.\nhttps://aws.amazon.com/ru/blogs/devops/using-codedeploy-environment-variables/",
          "upvote_count": "2",
          "comment_id": "1247143",
          "poster": "alex_heavy",
          "timestamp": "1720852200.0"
        },
        {
          "timestamp": "1719764760.0",
          "content": "Selected Answer: AD\nfor those who vote B: what is creating environment in code deploy?\n\nI think application group means application and registering instances with code deploy is basically creating deployment group, and instance is not registered manually it has to be tagged",
          "comment_id": "1239740",
          "upvote_count": "1",
          "poster": "xdkonorek2"
        },
        {
          "upvote_count": "1",
          "poster": "seetpt",
          "comment_id": "1205175",
          "timestamp": "1714582620.0",
          "content": "Selected Answer: AD\nAD is ok"
        },
        {
          "content": "Selected Answer: AD\nA- https://docs.aws.amazon.com/codedeploy/latest/userguide/codedeploy-agent.html \nD - This option correctly utilizes AWS CodePipeline to invoke the CodeBuild job and create CloudFormationchange sets. It adds a manual approval step before executing the change sets and starting the AWSCodeDeploy deployment. This ensures that the deployment process is automated while retaining the\nfinal manual approval step.",
          "timestamp": "1709908260.0",
          "upvote_count": "3",
          "comment_id": "1168869",
          "poster": "4555894"
        },
        {
          "timestamp": "1709124000.0",
          "content": "AD\nNeeds to install code deploy agent and give necessary permission for access S3 bucket where it will be stored the application revision. then EC2 instance will download the application revision from the S3 bucket. Therefore Answer A is correct. if we use System manager only the EC2 instance can be installed and updated automatically.",
          "upvote_count": "1",
          "poster": "Shasha1",
          "comment_id": "1161624"
        },
        {
          "comment_id": "1136813",
          "timestamp": "1706709660.0",
          "upvote_count": "2",
          "content": "A and D are correct: A - To run codedploy on EC2, need agent. D - The approval step will trigger both cloudformation and codedeploy\nB: no mention of installing agent on EC2\nC: The approval step doesnot affect CloudFormation updates, which is not accepted\nE: The approval step only allows codedeploy but not CloudFormation updates",
          "poster": "thanhnv142"
        },
        {
          "poster": "Jaguaroooo",
          "comment_id": "1113650",
          "upvote_count": "1",
          "timestamp": "1704370080.0",
          "content": "AD is the correct answer, you need the CD agent in order to use code deploy. And D is correct also because you can do your testing during codebuild and finally do a change set review and then approval."
        },
        {
          "content": "Selected Answer: BD\nCode deploy agent installation can be skipped when you setting up Code Deploy group .. e.g.\nYou can configure automatic installation and updates of the CodeDeploy agent when you create your deployment group in the console.\nWhy A is wrong - cause there is no application group only deployment group and when setting up deployment group you can setup agent installation automatically.\nhttps://docs.aws.amazon.com/codedeploy/latest/userguide/instances-ec2-create.html#:~:text=Note-,You%20can%20configure%20automatic%20installation%20and%20updates%20of%20the%20CodeDeploy%20agent%20when%20you%20create%20your%20deployment%20group%20in%20the%20console.,-Did%20this%20page",
          "timestamp": "1699267980.0",
          "poster": "2pk",
          "comments": [
            {
              "comment_id": "1113653",
              "poster": "Jaguaroooo",
              "timestamp": "1704370260.0",
              "content": "i agree with you on the explanation that there are no application group. but how can you use code deploy to push code to the EC2's if they have no agents on them?",
              "upvote_count": "1"
            }
          ],
          "comment_id": "1063749",
          "upvote_count": "4"
        },
        {
          "comment_id": "929059",
          "poster": "madperro",
          "timestamp": "1687323240.0",
          "upvote_count": "2",
          "content": "Selected Answer: AD\nAD is right."
        },
        {
          "content": "The CodeDeploy agent must be installed on your Amazon EC2 instance before using it in CodeDeploy deployments",
          "timestamp": "1686039540.0",
          "comment_id": "916041",
          "upvote_count": "1",
          "poster": "noriknic"
        },
        {
          "comment_id": "909839",
          "content": "There is nothing like application group in code deploy answer is BD",
          "timestamp": "1685417040.0",
          "upvote_count": "2",
          "poster": "Flyingdagger"
        },
        {
          "upvote_count": "2",
          "poster": "ele",
          "comment_id": "896714",
          "content": "Selected Answer: AD\nAD need codedeploy agent, and review CFN changes set, before run update",
          "timestamp": "1683985500.0"
        },
        {
          "timestamp": "1683444720.0",
          "content": "Selected Answer: AD\nA and D are correct",
          "comment_id": "891222",
          "upvote_count": "2",
          "poster": "PhuocT"
        },
        {
          "content": "Selected Answer: BD\nB and D are correct",
          "comment_id": "872175",
          "poster": "alce2020",
          "timestamp": "1681678620.0",
          "upvote_count": "1"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:19:46.538Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "4beuRuMsMGCPXn0qqJ30",
      "question_number": 319,
      "page": 64,
      "question_text": "A DevOps engineer manages a web application that runs on Amazon EC2 instances behind an Application Load Balancer (ALB). The instances run in an EC2 Auto Scaling group across multiple Availability Zones. The engineer needs to implement a deployment strategy that:\nLaunches a second fleet of instances with the same capacity as the original fleet.\nMaintains the original fleet unchanged while the second fleet is launched.\nTransitions traffic to the second fleet when the second fleet is fully deployed.\nTerminates the original fleet automatically 1 hour after transition.\nWhich solution will satisfy these requirements?",
      "choices": {
        "D": "Use AWS Elastic Beanstalk with the configuration set to Immutable. Create an .ebextension using the Resources key that sets the deletion policy of the ALB to 1 hour, and deploy the application.",
        "C": "Use AWS CodeDeploy with a deployment group configured with a blue/green deployment configuration Select the option Terminate the original instances in the deployment group with a waiting period of 1 hour.",
        "B": "Use two AWS Elastic Beanstalk environments to perform a blue/green deployment from the original environment to the new one. Create an application version lifecycle policy to terminate the original environment in 1 hour.",
        "A": "Use an AWS CloudFormation template with a retention policy for the ALB set to 1 hour. Update the Amazon Route 53 record to reflect the new ALB."
      },
      "correct_answer": "C",
      "answer_ET": "C",
      "answers_community": [
        "C (94%)",
        "6%"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/106262-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-04-15 16:45:00",
      "unix_timestamp": 1681569900,
      "discussion_count": 11,
      "discussion": [
        {
          "poster": "haazybanj",
          "upvote_count": "12",
          "comment_id": "885869",
          "content": "Option B, using two AWS Elastic Beanstalk environments to perform a blue/green deployment from the original environment to the new one, would not launch a second fleet of instances. Instead, it would create a new environment and deploy the application version to it. It also requires the use of an application version lifecycle policy to terminate the original environment in 1 hour.\n\nOption D, using AWS Elastic Beanstalk with the configuration set to Immutable and creating an .ebextension to set the deletion policy of the ALB to 1 hour, would not launch a second fleet of instances, and it would not maintain the original fleet unchanged while the second fleet is launched. Additionally, the .ebextension approach is not the recommended way to delete resources in AWS.\n\nTherefore, the correct option is C, using AWS CodeDeploy with a deployment group configured with a blue/green deployment configuration and selecting the option to Terminate the original instances in the deployment group with a waiting period of 1 hour.",
          "timestamp": "1682916060.0"
        },
        {
          "poster": "GripZA",
          "comment_id": "1561971",
          "content": "Selected Answer: C\nCodedploy:\nlaunches a second fleet (green environment) with the same capacity as the original\nit keeps the original fleet unchanged while launching the new one\nswitches traffic to the new fleet only after it's fully ready.\nuatomatically terminates the original (blue) fleet after a 1hour waiting period — exactly as needed",
          "timestamp": "1745072580.0",
          "upvote_count": "1"
        },
        {
          "upvote_count": "2",
          "poster": "hayjaykay",
          "comment_id": "1351248",
          "content": "Selected Answer: C\nKey words: EC2 for CodeDeploy, Elastic Beanstalk for CodePipeline.",
          "timestamp": "1738655820.0"
        },
        {
          "comment_id": "1292678",
          "timestamp": "1727937060.0",
          "upvote_count": "1",
          "poster": "newpotato",
          "content": "Option B would require more manual intervention and configuration, making it a less optimal solution compared to the seamless blue/green deployment and automatic fleet termination provided by Option C. \n\nOption D involves unnecessary complexity around setting ALB deletion policies, and while immutable deployments offer zero-downtime updates, they don't fully meet the core requirements of automatic traffic shifting and fleet termination"
        },
        {
          "timestamp": "1717273800.0",
          "upvote_count": "1",
          "comment_id": "1222891",
          "poster": "73d8cc9",
          "content": "Selected Answer: D\nImmutable strategy with Elastic Beanstalk involves deploying additional instance while Blue/Green strategy involves deploying another environment. The key difference is environment vs instances"
        },
        {
          "poster": "seetpt",
          "comment_id": "1205176",
          "content": "Selected Answer: C\nC is ok",
          "timestamp": "1714582680.0",
          "upvote_count": "1"
        },
        {
          "content": "Selected Answer: C\nhttps://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-groups-create-blue-green.html",
          "upvote_count": "3",
          "poster": "vmahilevskyi",
          "comment_id": "1164319",
          "timestamp": "1709404080.0"
        },
        {
          "comments": [
            {
              "upvote_count": "1",
              "content": "No C is the more accurate and logical",
              "poster": "jojom19980",
              "timestamp": "1708259040.0",
              "comment_id": "1153257"
            }
          ],
          "content": "A is correct: The question ask for a solution to automatic deployment of EC2 instances, which is the job of cloudFormation\n- B and D is irrelevant because it is use to deploy webapps only, not EC2 instances\n- C is also irrelevant because codedeploy (literlly by the name: CODEdeploy) is only used for deploying code, not EC2 instances, which is not code. Dont know why ChatGPT recommend this, but it is wrong definitely",
          "poster": "thanhnv142",
          "timestamp": "1706710140.0",
          "comment_id": "1136819",
          "upvote_count": "1"
        },
        {
          "content": "Selected Answer: C\nC looks like the best solution.",
          "poster": "madperro",
          "comment_id": "929072",
          "timestamp": "1687323780.0",
          "upvote_count": "3"
        },
        {
          "comment_id": "885867",
          "timestamp": "1682916000.0",
          "content": "Selected Answer: C\nTo satisfy the requirements of launching a second fleet of instances with the same capacity as the original fleet, maintaining the original fleet unchanged while the second fleet is launched, transitioning traffic to the second fleet when the second fleet is fully deployed, and terminating the original fleet automatically 1 hour after the transition, the best solution is to use AWS CodeDeploy with a blue/green deployment configuration, and selecting the option to Terminate the original instances in the deployment group with a waiting period of 1 hour.",
          "poster": "haazybanj",
          "upvote_count": "3"
        },
        {
          "comment_id": "871002",
          "poster": "alce2020",
          "upvote_count": "2",
          "timestamp": "1681569900.0",
          "content": "Selected Answer: C\nOption C"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:19:46.538Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "Dc8ZFzl5RSWIxkoWpWk4",
      "question_number": 320,
      "page": 64,
      "question_text": "A company must encrypt all AMIs that the company shares across accounts. A DevOps engineer has access to a source account where an unencrypted custom AMI has been built. The DevOps engineer also has access to a target account where an Amazon EC2 Auto Scaling group will launch EC2 instances from the AMI. The DevOps engineer must share the AMI with the target account.\nThe company has created an AWS Key Management Service (AWS KMS) key in the source account.\nWhich additional steps should the DevOps engineer perform to meet the requirements? (Choose three.)",
      "choices": {
        "F": "In the source account, share the encrypted AMI with the target account.",
        "E": "In the source account, share the unencrypted AMI with the target account.",
        "B": "In the source account, copy the unencrypted AMI to an encrypted AMI. Specify the default Amazon Elastic Block Store (Amazon EBS) encryption key in the copy action.",
        "C": "In the source account, create a KMS grant that delegates permissions to the Auto Scaling group service-linked role in the target account.",
        "D": "In the source account, modify the key policy to give the target account permissions to create a grant. In the target account, create a KMS grant that delegates permissions to the Auto Scaling group service-linked role.",
        "A": "In the source account, copy the unencrypted AMI to an encrypted AMI. Specify the KMS key in the copy action."
      },
      "correct_answer": "ADF",
      "answer_ET": "ADF",
      "answers_community": [
        "ADF (96%)",
        "2%"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/105265-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-04-05 12:27:00",
      "unix_timestamp": 1680690420,
      "discussion_count": 24,
      "discussion": [
        {
          "content": "Selected Answer: ADF\nADF seems to be the correct answer",
          "poster": "kacsabacsi78",
          "timestamp": "1686569460.0",
          "upvote_count": "11",
          "comment_id": "921400"
        },
        {
          "upvote_count": "9",
          "comment_id": "861977",
          "poster": "Dimidrol",
          "content": "Selected Answer: ADF\nA D F for me. https://jackiechen.blog/2020/01/29/share-encrypted-ami-across-aws-accounts/",
          "timestamp": "1680690420.0"
        },
        {
          "comment_id": "1561467",
          "upvote_count": "2",
          "poster": "life1991",
          "timestamp": "1744901820.0",
          "content": "Selected Answer: ADF\ni think so"
        },
        {
          "upvote_count": "1",
          "timestamp": "1722792120.0",
          "poster": "namtp",
          "comment_id": "1260740",
          "content": "Selected Answer: ADF\nADF for me,"
        },
        {
          "timestamp": "1709312040.0",
          "upvote_count": "1",
          "comment_id": "1163630",
          "poster": "martinarg2024",
          "content": "Selected Answer: ADF\nADF is correct"
        },
        {
          "upvote_count": "1",
          "poster": "Vitalydt",
          "comment_id": "1161558",
          "timestamp": "1709119560.0",
          "content": "Selected Answer: ADF\nA D F for me"
        },
        {
          "content": "ADF: \nA: cannot be B because using KMS\nD: Must share with the account because grant is only temp\nF: share the AMI with the target",
          "upvote_count": "2",
          "poster": "thanhnv142",
          "comment_id": "1133782",
          "timestamp": "1706410440.0"
        },
        {
          "comment_id": "1133414",
          "poster": "thanhnv142",
          "timestamp": "1706366520.0",
          "upvote_count": "1",
          "content": "AFD seem about right"
        },
        {
          "upvote_count": "1",
          "content": "ADF the correct answer",
          "poster": "Jonalb",
          "comment_id": "1127651",
          "timestamp": "1705816560.0"
        },
        {
          "upvote_count": "1",
          "comment_id": "1121239",
          "comments": [
            {
              "timestamp": "1705113840.0",
              "content": "Should be ADF",
              "upvote_count": "1",
              "comment_id": "1121241",
              "poster": "khchan123"
            }
          ],
          "timestamp": "1705113780.0",
          "poster": "khchan123",
          "content": "Selected Answer: ACF\nACF. For autoscaling to work a KMS grant is needed"
        },
        {
          "content": "Selected Answer: ADF\nADF is the right answer",
          "upvote_count": "1",
          "timestamp": "1702174020.0",
          "poster": "harithzainudin",
          "comment_id": "1092249"
        },
        {
          "timestamp": "1698960900.0",
          "upvote_count": "2",
          "comment_id": "1060931",
          "poster": "VrilianVirgil",
          "content": "Selected Answer: ADF\nC is incorrect as the AMI **MUST** be shared with the account.\nnot just the scaling group. So it would make sense for the target account to create the grant."
        },
        {
          "timestamp": "1695572640.0",
          "upvote_count": "1",
          "content": "Selected Answer: ADF\nADF is the right answer.",
          "poster": "ataince",
          "comment_id": "1016006"
        },
        {
          "timestamp": "1693648800.0",
          "poster": "BaburTurk",
          "comment_id": "996788",
          "upvote_count": "3",
          "content": "Selected Answer: ADF\nhttps://aws.amazon.com/blogs/security/how-to-create-a-custom-ami-with-encrypted-amazon-ebs-snapshots-and-share-it-with-other-accounts-and-regions/"
        },
        {
          "content": "Selected Answer: ADF\nADF is right",
          "timestamp": "1693073280.0",
          "poster": "Skshitiz",
          "comment_id": "990984",
          "upvote_count": "1"
        },
        {
          "content": "Selected Answer: ADF\nADF is correct",
          "upvote_count": "2",
          "timestamp": "1690184820.0",
          "poster": "DavidPham",
          "comment_id": "961291"
        },
        {
          "poster": "habros",
          "content": "Selected Answer: ADF\nStep 1: Always specify the KMS (CMK) key to encrypt with when creating/copying images\nStep 2: Modify the CMK key policy to allow trusted role to assume the key to decrypt image\nStep 3: Use cross-account trust policy to grant the other account access to the encrypted image",
          "upvote_count": "2",
          "timestamp": "1688470440.0",
          "comment_id": "942688"
        },
        {
          "timestamp": "1686535620.0",
          "comments": [
            {
              "upvote_count": "1",
              "timestamp": "1686535740.0",
              "comment_id": "921069",
              "content": "Sorry as my fault ADF is correct",
              "poster": "SanChan"
            }
          ],
          "poster": "SanChan",
          "upvote_count": "1",
          "comment_id": "921068",
          "content": "Selected Answer: ACD\nACD, The question is KMS Permission.\nOption F is not a valid solution because it shares the encrypted AMI with the target account, but it does not address the requirement of delegating permissions to the Auto Scaling group service-linked role to use the KMS key to launch instances from the encrypted AMI."
        },
        {
          "poster": "madperro",
          "comment_id": "918085",
          "timestamp": "1686217620.0",
          "upvote_count": "4",
          "content": "Selected Answer: ADF\nADF seems to be the correct answer.\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/key-policy-requirements-EBS-encryption.html"
        },
        {
          "content": "Selected Answer: ADF\nFor me is ADF",
          "poster": "bcx",
          "comment_id": "910284",
          "upvote_count": "1",
          "timestamp": "1685453760.0"
        },
        {
          "content": "Selected Answer: ADF\nADF makes more sense",
          "timestamp": "1683517980.0",
          "comment_id": "891787",
          "poster": "ParagSanyashiv",
          "upvote_count": "1"
        },
        {
          "timestamp": "1680868980.0",
          "content": "Selected Answer: ADF\nADF right",
          "comment_id": "863811",
          "poster": "ele",
          "upvote_count": "1"
        },
        {
          "comment_id": "862902",
          "poster": "911f9cf",
          "content": "Selected Answer: ADF\nA D F are good",
          "upvote_count": "1",
          "timestamp": "1680781080.0"
        },
        {
          "poster": "asfsdfsdf",
          "content": "Selected Answer: ADF\nThis is clearly stated in the document:\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/key-policy-requirements-EBS-encryption.html\nService linked role must get specific grant on the account it cannot use key policy",
          "timestamp": "1680765660.0",
          "upvote_count": "3",
          "comment_id": "862744"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:19:46.538Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "9Gyy9b18LlsQe4ZpRoGx",
      "question_number": 321,
      "page": 65,
      "question_text": "A video-sharing company stores its videos in Amazon S3. The company has observed a sudden increase in video access requests, but the company does not know which videos are most popular. The company needs to identify the general access pattern for the video files. This pattern includes the number of users who access a certain file on a given day, as well as the number of pull requests for certain files.\nHow can the company meet these requirements with the LEAST amount of effort?",
      "choices": {
        "D": "Record an Amazon CloudWatch Logs log message for every S3 object access event. Configure a CloudWatch Logs log stream to write the file access information, such as user, S3 bucket, and file key, to an Amazon Kinesis Data Analytics for SQL application. Perform a sliding window analysis.",
        "C": "Invoke an AWS Lambda function for every S3 object access event. Configure the Lambda function to write the file access information, such as user. S3 bucket, and file key, to an Amazon Aurora database. Use an Aurora SQL query to analyze the access patterns.",
        "B": "Activate S3 server access logging. Use Amazon Athena to create an external table with the log files. Use Athena to create a SQL query to analyze the access patterns.",
        "A": "Activate S3 server access logging. Import the access logs into an Amazon Aurora database. Use an Aurora SQL query to analyze the access patterns."
      },
      "correct_answer": "B",
      "answer_ET": "B",
      "answers_community": [
        "B (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/106260-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-04-15 16:38:00",
      "unix_timestamp": 1681569480,
      "discussion_count": 10,
      "discussion": [
        {
          "timestamp": "1727020440.0",
          "content": "Bis the Answer because Athena is designed for these type of use cases\nAWS Athena is a serverless interactive query service that lets you analyze data stored in Amazon Simple Storage Service (S3) using standard SQL.",
          "poster": "zijo",
          "comment_id": "1180224",
          "upvote_count": "4"
        },
        {
          "content": "B is correct:Use S3 in combination with Athena is the recommended way to analyze data\nA: setups of Aurora is complex and unnecessary. It also more costly than B\nC and D are both too complicated.",
          "timestamp": "1722428040.0",
          "comment_id": "1136824",
          "upvote_count": "2",
          "poster": "thanhnv142"
        },
        {
          "timestamp": "1704767400.0",
          "poster": "habros",
          "comment_id": "946802",
          "upvote_count": "4",
          "content": "Selected Answer: B\nB is so much simpler. Athena can do interactive queries on S3 data."
        },
        {
          "upvote_count": "2",
          "content": "Selected Answer: B\nB is correct and simplest.",
          "timestamp": "1703142360.0",
          "comment_id": "929075",
          "poster": "madperro"
        },
        {
          "upvote_count": "1",
          "timestamp": "1701423480.0",
          "content": "Selected Answer: B\nB is the answer",
          "comment_id": "911856",
          "poster": "tom_uk"
        },
        {
          "content": "Selected Answer: B\nB is the natural way to do it",
          "upvote_count": "1",
          "timestamp": "1701333420.0",
          "poster": "bcx",
          "comment_id": "910917"
        },
        {
          "comment_id": "895425",
          "content": "Selected Answer: B\nhttps://repost.aws/ja/knowledge-center/analyze-logs-athena",
          "poster": "gdtypk",
          "upvote_count": "1",
          "timestamp": "1699745040.0"
        },
        {
          "upvote_count": "1",
          "timestamp": "1698988380.0",
          "poster": "haazybanj",
          "comment_id": "888112",
          "content": "Selected Answer: B\nB is right"
        },
        {
          "poster": "Sazeka",
          "comment_id": "875969",
          "timestamp": "1697832420.0",
          "upvote_count": "1",
          "content": "I would go with B"
        },
        {
          "content": "Selected Answer: B\nB is correct",
          "upvote_count": "1",
          "poster": "alce2020",
          "comment_id": "870994",
          "timestamp": "1697380680.0"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:19:56.962Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "KKhcbkuwwY2N9qqcAUjU",
      "question_number": 322,
      "page": 65,
      "question_text": "A development team wants to use AWS CloudFormation stacks to deploy an application. However, the developer IAM role does not have the required permissions to provision the resources that are specified in the AWS CloudFormation template. A DevOps engineer needs to implement a solution that allows the developers to deploy the stacks. The solution must follow the principle of least privilege.\nWhich solution will meet these requirements?",
      "choices": {
        "B": "Create an IAM policy that allows full access to AWS CloudFormation. Attach the policy to the developer IAM role.",
        "C": "Create an AWS CloudFormation service role that has the required permissions. Grant the developer IAM role a cloudformation:* action. Use the new service role during stack deployments.",
        "D": "Create an AWS CloudFormation service role that has the required permissions. Grant the developer IAM role the iam:PassRole permission. Use the new service role during stack deployments.",
        "A": "Create an IAM policy that allows the developers to provision the required resources. Attach the policy to the developer IAM role."
      },
      "correct_answer": "D",
      "answer_ET": "D",
      "answers_community": [
        "D (82%)",
        "Other"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/106259-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-04-15 16:31:00",
      "unix_timestamp": 1681569060,
      "discussion_count": 18,
      "discussion": [
        {
          "upvote_count": "1",
          "timestamp": "1719506100.0",
          "comment_id": "1238285",
          "poster": "fuzzycom",
          "content": "D is totally correct"
        },
        {
          "comment_id": "1168872",
          "timestamp": "1709908380.0",
          "poster": "4555894",
          "content": "Selected Answer: D\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-iam-servicerole.html",
          "upvote_count": "2"
        },
        {
          "upvote_count": "4",
          "timestamp": "1706783640.0",
          "comment_id": "1137515",
          "content": "D is correct: Need to create a role for Cloud formation that has the required permissions. Then adding iam:PassRole permission to the dev IAM role to allow them to pass this role to CF\nA: no mention of creating the required permissions for ACF. Additionally, should not grant permissions for dev. \nB: grant full access is against the least privilege policy\nC: no mention of granting iam:PassRole permission to the dev",
          "poster": "thanhnv142"
        },
        {
          "comment_id": "1093983",
          "timestamp": "1702343940.0",
          "poster": "imymoco",
          "content": "A is incorrect; A would also allow resources to be used from outside of cfn.\nTherefore, D is correct.",
          "upvote_count": "1"
        },
        {
          "timestamp": "1693039860.0",
          "content": "Selected Answer: D\nOption D allows you to create a dedicated AWS CloudFormation service role with the precise permissions required for stack deployments. Then, you grant the developer IAM role the iam:PassRole permission, which enables it to pass the service role to AWS CloudFormation without granting it broad IAM permissions. This approach aligns best with the principle of least privilege and ensures developers can deploy stacks while maintaining control over their permissions.",
          "comment_id": "990601",
          "poster": "jason7",
          "upvote_count": "2"
        },
        {
          "timestamp": "1689415200.0",
          "comment_id": "952249",
          "comments": [
            {
              "comment_id": "987317",
              "timestamp": "1692698280.0",
              "content": "B is not best practice of using CloudFormation. \nD is correct, 100% sure. `iam:PassRole` to a CloudFormation Service Role (take a look at this: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-iam-servicerole.html#:~:text=you%20can%20use.-,Important,-When%20you%20specify)",
              "poster": "fanq10",
              "upvote_count": "1"
            },
            {
              "content": "Should be D! See here https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-iam-servicerole.html",
              "timestamp": "1698918480.0",
              "upvote_count": "1",
              "poster": "DZ_Ben",
              "comment_id": "1060446"
            }
          ],
          "poster": "ogwu2000",
          "content": "B is the answer. DC wrong - Nothing like CloudFormation service-role.",
          "upvote_count": "1"
        },
        {
          "poster": "madperro",
          "comment_id": "929081",
          "upvote_count": "1",
          "content": "Selected Answer: D\nD is the right answer.",
          "timestamp": "1687324740.0"
        },
        {
          "content": "Selected Answer: D\nThis solution follows the principle of least privilege by creating a specific AWS CloudFormation service role that only has the permissions required for the resources in the AWS CloudFormation stack. The developers are then granted permission to pass this role (iam:PassRole) to the AWS CloudFormation service when they initiate stack deployments, which allows the service to act on behalf of the developer to provision the specified resources.",
          "comment_id": "928778",
          "poster": "tartarus23",
          "timestamp": "1687292940.0",
          "upvote_count": "1"
        },
        {
          "poster": "bcx",
          "content": "Selected Answer: D\nD, you pass the role that can create the resources, the user does not have the right to create the resources himself but can pass the role to CloudFormation so CloudFormation assumes it. IMHO.",
          "comment_id": "910931",
          "upvote_count": "1",
          "timestamp": "1685515860.0"
        },
        {
          "poster": "2pk",
          "comment_id": "898253",
          "timestamp": "1684152420.0",
          "content": "Selected Answer: D\nThis allows them to provision the required resources specified in the CloudFormation template without granting them full access to AWS CloudFormation or the underlying resources.",
          "upvote_count": "1"
        },
        {
          "upvote_count": "1",
          "comment_id": "896724",
          "content": "Selected Answer: D\nD , passrole is right action",
          "poster": "ele",
          "timestamp": "1683986640.0"
        },
        {
          "comment_id": "895428",
          "timestamp": "1683840900.0",
          "poster": "gdtypk",
          "upvote_count": "3",
          "content": "Selected Answer: D\nhttps://docs.aws.amazon.com/ja_jp/AWSCloudFormation/latest/UserGuide/using-iam-servicerole.html"
        },
        {
          "upvote_count": "1",
          "comment_id": "892294",
          "timestamp": "1683559620.0",
          "content": "Selected Answer: D\nD is more suitable in this case.",
          "poster": "ParagSanyashiv"
        },
        {
          "timestamp": "1683495360.0",
          "poster": "Frodo_the_cat",
          "comments": [
            {
              "comment_id": "891691",
              "content": "Option B, creating an IAM policy that allows full access to AWS CloudFormation, is not a good solution either, as it grants excessive permissions to the developers.\n\nOption D, creating an AWS CloudFormation service role with the required permissions and granting the developer IAM role the iam:PassRole permission, allows the developers to assume the service role and deploy the stacks with the required resources. However, this option grants additional permissions to the developer IAM role, which could be abused by malicious actors\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-iam-template.html",
              "timestamp": "1683495420.0",
              "poster": "Frodo_the_cat",
              "upvote_count": "1"
            }
          ],
          "comment_id": "891689",
          "content": "C. Create an AWS CloudFormation service role that has the required permissions. Grant the developer IAM role a cloudformation:* action. Use the new service role during stack deployments.\nBy creating an AWS CloudFormation service role with the required permissions, the DevOps engineer can control the resources that the developers can access. This approach ensures that the developers have only the necessary permissions to deploy the stacks, without granting them excessive permissions that could be exploited by malicious actors. The IAM policy granting a cloudformation:* action to the developer IAM role allows the developers to use the AWS CloudFormation service role and deploy the stacks with the required resources.\nOption A, creating an IAM policy that allows the developers to provision the required resources, is not a good solution because it could potentially grant the developers too much access to resources they don't need. This violates the principle of least privilege.",
          "upvote_count": "1"
        },
        {
          "upvote_count": "1",
          "poster": "kassem77",
          "comment_id": "890296",
          "content": "D it is",
          "timestamp": "1683316200.0"
        },
        {
          "poster": "haazybanj",
          "timestamp": "1682916240.0",
          "upvote_count": "1",
          "comment_id": "885871",
          "content": "Selected Answer: D\nOption D is the recommended solution to meet the requirements because it follows the principle of least privilege. The IAM policy that allows the developers to provision the required resources should be created and associated with the IAM role, which should be assigned the iam:PassRole permission for the AWS CloudFormation service role. By doing so, the IAM role can only assume the specific AWS CloudFormation service role and deploy the stack with the required permissions, and not have full access to all resources or full access to AWS CloudFormation."
        },
        {
          "timestamp": "1681678980.0",
          "poster": "alce2020",
          "content": "Selected Answer: B\nB it is",
          "comment_id": "872182",
          "upvote_count": "2"
        },
        {
          "poster": "alce2020",
          "comment_id": "870991",
          "content": "Selected Answer: C\nC will follow the principle less privileged",
          "comments": [],
          "upvote_count": "1",
          "timestamp": "1681569060.0"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:19:56.962Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "jhYKyqC1fMsnEpaqmAiE",
      "question_number": 323,
      "page": 65,
      "question_text": "A production account has a requirement that any Amazon EC2 instance that has been logged in to manually must be terminated within 24 hours. All applications in the production account are using Auto Scaling groups with the Amazon CloudWatch Logs agent configured.\nHow can this process be automated?",
      "choices": {
        "C": "Create an Amazon CloudWatch alarm that will be invoked by the login event. Configure the alarm to send to an Amazon Simple Queue Service (Amazon SQS) queue. Use a group of worker instances to process messages from the queue, which then schedules an Amazon EventBridge rule to be invoked.",
        "A": "Create a CloudWatch Logs subscription to an AWS Step Functions application. Configure an AWS Lambda function to add a tag to the EC2 instance that produced the login event and mark the instance to be decommissioned. Create an Amazon EventBridge rule to invoke a second Lambda function once a day that will terminate all instances with this tag.",
        "B": "Create an Amazon CloudWatch alarm that will be invoked by the login event. Send the notification to an Amazon Simple Notification Service (Amazon SNS) topic that the operations team is subscribed to, and have them terminate the EC2 instance within 24 hours.",
        "D": "Create a CloudWatch Logs subscription to an AWS Lambda function. Configure the function to add a tag to the EC2 instance that produced the login event and mark the instance to be decommissioned. Create an Amazon EventBridge rule to invoke a daily Lambda function that terminates all instances with this tag."
      },
      "correct_answer": "D",
      "answer_ET": "D",
      "answers_community": [
        "D (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/106442-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-04-16 23:05:00",
      "unix_timestamp": 1681679100,
      "discussion_count": 7,
      "discussion": [
        {
          "poster": "Aesthet",
          "comment_id": "1264175",
          "timestamp": "1723383060.0",
          "upvote_count": "1",
          "content": "Opion D: \"with this tag\"\nSo, there will be one tag, like ShoudTerminate: true. But by doing so we will terminate ALL instances with a tag - even those created 10 minutes ago. It doesn't seem correct, or am I missing something?"
        },
        {
          "upvote_count": "1",
          "comment_id": "1238287",
          "poster": "fuzzycom",
          "timestamp": "1719506460.0",
          "content": "D is best answer.\nhint: question includes \"~~Amazon CloudWatch Logs agent configured\"\n Lambda function is keyword."
        },
        {
          "content": "D is correct:\nA: If using step function, no need to include \"Amazon EventBridge rule to invoke a second Lambda function\" \nB: With this method, policy-breaching Ec2 would be terminated manually, which cannot ensure that they are terminated within 24 hours\nC: no mention of terminating the instances",
          "comment_id": "1137678",
          "timestamp": "1706797560.0",
          "upvote_count": "4",
          "poster": "thanhnv142"
        },
        {
          "content": "D is correct; with B, SNS can cause delays.",
          "comment_id": "1093989",
          "poster": "imymoco",
          "timestamp": "1702344420.0",
          "upvote_count": "2"
        },
        {
          "poster": "madperro",
          "upvote_count": "1",
          "content": "Selected Answer: D\nD is the best answer.",
          "comment_id": "929087",
          "timestamp": "1687325100.0"
        },
        {
          "upvote_count": "3",
          "content": "Selected Answer: D\nD. Create a CloudWatch Logs subscription to an AWS Lambda function. Configure the function to add a tag to the EC2 instance that produced the login event and mark the instance to be decommissioned. Create an Amazon EventBridge rule to invoke a daily Lambda function that terminates all instances with this tag.",
          "timestamp": "1682916720.0",
          "comment_id": "885876",
          "poster": "haazybanj"
        },
        {
          "timestamp": "1681679100.0",
          "content": "Selected Answer: D\nD is the correct answer",
          "comment_id": "872185",
          "poster": "alce2020",
          "upvote_count": "3"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:19:56.962Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "0aa9DBEhC1wIXIbLZ2Dy",
      "question_number": 324,
      "page": 65,
      "question_text": "A company has enabled all features for its organization in AWS Organizations. The organization contains 10 AWS accounts. The company has turned on AWS CloudTrail in all the accounts. The company expects the number of AWS accounts in the organization to increase to 500 during the next year. The company plans to use multiple OUs for these accounts.\nThe company has enabled AWS Config in each existing AWS account in the organization. A DevOps engineer must implement a solution that enables AWS Config automatically for all future AWS accounts that are created in the organization.\nWhich solution will meet this requirement?",
      "choices": {
        "A": "In the organization's management account, create an Amazon EventBridge rule that reacts to a CreateAccount API call. Configure the rule to invoke an AWS Lambda function that enables trusted access to AWS Config for the organization.",
        "C": "In the organization's management account, create an SCP that allows the appropriate AWS Config API calls to enable AWS Config. Apply the SCP to the root-level OU.",
        "D": "In the organization's management account, create an Amazon EventBridge rule that reacts to a CreateAccount API call. Configure the rule to invoke an AWS Systems Manager Automation runbook to enable AWS Config for the account.",
        "B": "In the organization's management account, create an AWS CloudFormation stack set to enable AWS Config. Configure the stack set to deploy automatically when an account is created through Organizations."
      },
      "correct_answer": "B",
      "answer_ET": "B",
      "answers_community": [
        "B (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/106258-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-04-15 16:21:00",
      "unix_timestamp": 1681568460,
      "discussion_count": 7,
      "discussion": [
        {
          "upvote_count": "2",
          "comment_id": "1137720",
          "timestamp": "1722517800.0",
          "comments": [
            {
              "content": "A: trusted access to AWS Config: this is used by other services to access to AWS config, not for account\nD: enable AWS Config for the account: this means we only activate AWS config for the management account, not the newly created ones",
              "timestamp": "1722776160.0",
              "comment_id": "1140276",
              "poster": "thanhnv142",
              "upvote_count": "1"
            }
          ],
          "poster": "thanhnv142",
          "content": "B is correct: The question ask a solution to \"enables AWS Config automatically\" for all future accounts. In AWS org, to provision or configure resources on other accounts, we use ACF\nA, C and D: no mention of ACF"
        },
        {
          "poster": "2pk",
          "upvote_count": "1",
          "timestamp": "1714983300.0",
          "comment_id": "1063725",
          "content": "B:\nDetails the new feature with enable trusted access to new accounts in any region\nhttps://docs.aws.amazon.com/organizations/latest/userguide/services-that-can-integrate-cloudformation.html"
        },
        {
          "upvote_count": "1",
          "content": "B \nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/stacksets-sampletemplates.html",
          "comment_id": "987054",
          "timestamp": "1708581480.0",
          "poster": "hjey0329"
        },
        {
          "comment_id": "929157",
          "content": "Selected Answer: B\nB is the best solution.",
          "poster": "madperro",
          "timestamp": "1703150820.0",
          "upvote_count": "1"
        },
        {
          "comment_id": "903603",
          "content": "B\nhttps://aws.amazon.com/about-aws/whats-new/2020/02/aws-cloudformation-stacksets-introduces-automatic-deployments-across-accounts-and-regions-through-aws-organizations/",
          "poster": "samgyeopsal",
          "timestamp": "1700621880.0",
          "upvote_count": "2"
        },
        {
          "content": "Selected Answer: B\nThe correct solution to enable AWS Config automatically for all future AWS accounts created in the organization is Option B: In the organization's management account, create an AWS CloudFormation stack set to enable AWS Config. Configure the stack set to deploy automatically when an account is created through Organizations.\n\nOption C is incorrect because although it suggests creating an SCP that allows the appropriate AWS Config API calls to enable AWS Config and applying the SCP to the root-level OU, it does not specifically enable AWS Config automatically for all future AWS accounts that are created in the organization.",
          "timestamp": "1698821640.0",
          "poster": "haazybanj",
          "comment_id": "885878",
          "upvote_count": "4",
          "comments": [
            {
              "timestamp": "1718837580.0",
              "content": "In terms of Option C: SCP can only Deny access, not Allow",
              "poster": "Olelukoe",
              "upvote_count": "3",
              "comment_id": "1101097"
            }
          ]
        },
        {
          "comment_id": "870983",
          "upvote_count": "1",
          "timestamp": "1697379660.0",
          "poster": "alce2020",
          "content": "Selected Answer: B\nB is correct"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:19:56.962Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "dKDRFLHxVs7z1lgJYOjC",
      "question_number": 325,
      "page": 65,
      "question_text": "A company has many applications. Different teams in the company developed the applications by using multiple languages and frameworks. The applications run on premises and on different servers with different operating systems. Each team has its own release protocol and process. The company wants to reduce the complexity of the release and maintenance of these applications.\nThe company is migrating its technology stacks, including these applications, to AWS. The company wants centralized control of source code, a consistent and automatic delivery pipeline, and as few maintenance tasks as possible on the underlying infrastructure.\nWhat should a DevOps engineer do to meet these requirements?",
      "choices": {
        "D": "Create one AWS CodeCommit repository for each of the applications. Use AWS CodeBuild to build one Docker image for each application in Amazon Elastic Container Registry (Amazon ECR). Use AWS CodeDeploy to deploy the applications to Amazon Elastic Container Service (Amazon ECS) on infrastructure that AWS Fargate manages.",
        "A": "Create one AWS CodeCommit repository for all applications. Put each application's code in a different branch. Merge the branches, and use AWS CodeBuild to build the applications. Use AWS CodeDeploy to deploy the applications to one centralized application server.",
        "C": "Create one AWS CodeCommit repository for each of the applications. Use AWS CodeBuild to build the applications one at a time and to create one AMI for each server. Use AWS CloudFormation StackSets to automatically provision and decommission Amazon EC2 fleets by using these AMIs.",
        "B": "Create one AWS CodeCommit repository for each of the applications. Use AWS CodeBuild to build the applications one at a time. Use AWS CodeDeploy to deploy the applications to one centralized application server."
      },
      "correct_answer": "D",
      "answer_ET": "D",
      "answers_community": [
        "D (93%)",
        "7%"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/108077-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-05-01 06:57:00",
      "unix_timestamp": 1682917020,
      "discussion_count": 7,
      "discussion": [
        {
          "content": "Selected Answer: D\nOption D is the best choice to meet the requirements of centralized control of source code, a consistent and automatic delivery pipeline, and minimal maintenance tasks.\nOption D is the best choice because it allows each application to have its own repository and build process, but uses containerization to create a consistent and automatic delivery pipeline that can be easily deployed to Amazon ECS on infrastructure that AWS Fargate manages. This approach also provides scalability and ease of maintenance.",
          "comment_id": "885879",
          "poster": "haazybanj",
          "timestamp": "1698821820.0",
          "upvote_count": "12"
        },
        {
          "upvote_count": "2",
          "content": "D is correct: \"centralized control of source code\" = CodeCommit. \"Consistent and automatic delivery pipeline\" = codepipeline/codebuide/codedeploy. \"as few maintenance tasks as possible on the underlying infrastructur\" = containerization\nA: \"CodeCommit repository for all applications\": should not, need separate repos for each app\nB and C: no mention of containerization (fargate, ECS)",
          "timestamp": "1722518160.0",
          "comment_id": "1137726",
          "poster": "thanhnv142"
        },
        {
          "timestamp": "1712055240.0",
          "comment_id": "1023022",
          "upvote_count": "1",
          "poster": "DaddyDee",
          "content": "D is the Answer: https://aws.amazon.com/blogs/compute/building-deploying-and-operating-containerized-applications-with-aws-fargate/"
        },
        {
          "comment_id": "959427",
          "content": "Selected Answer: A\nA ISNT CORRECT?",
          "comments": [
            {
              "comment_id": "1116172",
              "poster": "davdan99",
              "upvote_count": "2",
              "timestamp": "1720379820.0",
              "content": "Of course no, it is the most wrong one, It saying to have all the applications code in one repo (bad thing to do), separated in branches, and after that merge them (second very bad thing to do)."
            }
          ],
          "timestamp": "1705928040.0",
          "upvote_count": "1",
          "poster": "ddedqdqw"
        },
        {
          "comment_id": "929667",
          "content": "Option D. \nI was torn between C and D, but the requirement for ease of maintenance on the underlying infrastructure clearly points to ECS.",
          "upvote_count": "3",
          "poster": "RickSk",
          "timestamp": "1703178780.0"
        },
        {
          "content": "Selected Answer: D\nD is the best option, there is virtually no infrastructure to manage.",
          "poster": "madperro",
          "upvote_count": "1",
          "timestamp": "1703151060.0",
          "comment_id": "929159"
        },
        {
          "content": "Selected Answer: D\nD is best option.",
          "comment_id": "896739",
          "upvote_count": "1",
          "timestamp": "1699892460.0",
          "poster": "ele"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:19:56.962Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "m9DmmsUq6Dl3V3J62DHr",
      "question_number": 326,
      "page": 66,
      "question_text": "A company's application is currently deployed to a single AWS Region. Recently, the company opened a new office on a different continent. The users in the new office are experiencing high latency. The company's application runs on Amazon EC2 instances behind an Application Load Balancer (ALB) and uses Amazon DynamoDB as the database layer. The instances run in an EC2 Auto Scaling group across multiple Availability Zones. A DevOps engineer is tasked with minimizing application response times and improving availability for users in both Regions.\nWhich combination of actions should be taken to address the latency issues? (Choose three.)",
      "choices": {
        "D": "Create Amazon Route 53 records, health checks, and latency-based routing policies to route to the ALB.",
        "B": "Create new ALB and Auto Scaling group global resources and configure the new ALB to direct traffic to the new Auto Scaling group.",
        "F": "Convert the DynamoDB table to a global table.",
        "E": "Create Amazon Route 53 aliases, health checks, and failover routing policies to route to the ALB.",
        "A": "Create a new DynamoDB table in the new Region with cross-Region replication enabled.",
        "C": "Create new ALB and Auto Scaling group resources in the new Region and configure the new ALB to direct traffic to the new Auto Scaling group."
      },
      "correct_answer": "CDF",
      "answer_ET": "CDF",
      "answers_community": [
        "CDF (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/106444-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-04-16 23:12:00",
      "unix_timestamp": 1681679520,
      "discussion_count": 8,
      "discussion": [
        {
          "poster": "haazybanj",
          "upvote_count": "10",
          "comment_id": "885880",
          "content": "Selected Answer: CDF\nC. Create new ALB and Auto Scaling group resources in the new Region and configure the new ALB to direct traffic to the new Auto Scaling group. This will allow users in the new Region to access the application with lower latency by reducing the network hops between the user and the application servers.\n\nD. Create Amazon Route 53 records, health checks, and latency-based routing policies to route to the ALB. This will enable Route 53 to route user traffic to the nearest healthy ALB, based on the latency between the user and the ALBs.\n\nF. Convert the DynamoDB table to a global table. This will enable reads and writes to the table in both Regions with low latency, improving the overall response time of the application",
          "timestamp": "1698822120.0"
        },
        {
          "timestamp": "1747327800.0",
          "poster": "Jonalb",
          "comment_id": "1569104",
          "content": "Selected Answer: CDF\nC,D,F }\n\nC. Create new ALB and Auto Scaling group resources in the new Region and configure the new ALB to direct traffic to the new Auto Scaling group. This will allow users in the new Region to access the application with lower latency by reducing the network hops between the user and the application servers.\n\nD. Create Amazon Route 53 records, health checks, and latency-based routing policies to route to the ALB. This will enable Route 53 to route user traffic to the nearest healthy ALB, based on the latency between the user and the ALBs.\n\nF. Convert the DynamoDB table to a global table. This will enable reads and writes to the table in both Regions with low latency, improving the overall response time of the application",
          "upvote_count": "1"
        },
        {
          "poster": "xdkonorek2",
          "comment_id": "1199494",
          "upvote_count": "2",
          "timestamp": "1729494420.0",
          "content": "Technically converting dynamodb table to global table requires creating replica in another region with cross-region replication and you don't \"convert\" you add a replica in \"global tables\" in specified region so this answers are a little bit misleading.\n\nProbably F is better than A since they name this operation as \"converting\" e.g. here https://aws.amazon.com/blogs/aws/new-convert-your-single-region-amazon-dynamodb-tables-to-global-tables/"
        },
        {
          "comment_id": "1137739",
          "timestamp": "1722519300.0",
          "content": "D is, of course, correct: <apply a core set of security controls to an existing set of AWS accounts> and <The accounts are in an organization in AWS Organizations> means we need ACF template to deploy these set of security controls. <Individual account administrators must not be able to edit or delete any of the baseline resources> means we need scp to deny permission\nA and B: no mention of SCP\nC: this option deploy the rules by AWS Config management account, which is not correct because we need ACF. Additionally, no mention of denying modification to CloudTrail trails",
          "poster": "thanhnv142",
          "upvote_count": "1"
        },
        {
          "content": "CDF: <opened a new office on a different continent> and <The users in the new office are experiencing high latency> means they need to replicate their existing site to the new region. <Amazon EC2 instances behind an Application Load Balancer (ALB)> means they need to replicate both these. <address the latency issues> means they need route53 with latency-based and health check\nA: <cross-Region replication> is used for backup only, not a live site. It would introduce a lot of latency\nB: <Auto Scaling group global resources>: there is no such thing\nE: No mention of latency-based.",
          "poster": "thanhnv142",
          "timestamp": "1722518820.0",
          "upvote_count": "2",
          "comment_id": "1137732"
        },
        {
          "content": "Selected Answer: CDF\nCDF very easy",
          "poster": "z_inderjot",
          "timestamp": "1719192240.0",
          "comment_id": "1104435",
          "upvote_count": "1"
        },
        {
          "content": "CDF is collect",
          "timestamp": "1713874500.0",
          "comment_id": "1051806",
          "upvote_count": "1",
          "poster": "IIIIIIIIIlllll"
        },
        {
          "comment_id": "872191",
          "content": "Selected Answer: CDF\nC,D,F are correct",
          "poster": "alce2020",
          "upvote_count": "2",
          "timestamp": "1697490720.0"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:20:07.610Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "2dG3F7kQCYL9iV1L4cUs",
      "question_number": 327,
      "page": 66,
      "question_text": "A DevOps engineer needs to apply a core set of security controls to an existing set of AWS accounts. The accounts are in an organization in AWS Organizations. Individual teams will administer individual accounts by using the AdministratorAccess AWS managed policy. For all accounts. AWS CloudTrail and AWS Config must be turned on in all available AWS Regions. Individual account administrators must not be able to edit or delete any of the baseline resources. However, individual account administrators must be able to edit or delete their own CloudTrail trails and AWS Config rules.\nWhich solution will meet these requirements in the MOST operationally efficient way?",
      "choices": {
        "B": "Enable AWS Control Tower. Enroll the existing accounts in AWS Control Tower. Grant the individual account administrators access to CloudTrail and AWS Config.",
        "A": "Create an AWS CloudFormation template that defines the standard account resources. Deploy the template to all accounts from the organization's management account by using CloudFormation StackSets. Set the stack policy to deny Update:Delete actions.",
        "C": "Designate an AWS Config management account. Create AWS Config recorders in all accounts by using AWS CloudFormation StackSets. Deploy AWS Config rules to the organization by using the AWS Config management account. Create a CloudTrail organization trail in the organization’s management account. Deny modification or deletion of the AWS Config recorders by using an SCP.",
        "D": "Create an AWS CloudFormation template that defines the standard account resources. Deploy the template to all accounts from the organization's management account by using Cloud Formation StackSets Create an SCP that prevents updates or deletions to CloudTrail resources or AWS Config resources unless the principal is an administrator of the organization's management account."
      },
      "correct_answer": "D",
      "answer_ET": "D",
      "answers_community": [
        "D (47%)",
        "C (44%)",
        "9%"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/106219-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-04-15 04:14:00",
      "unix_timestamp": 1681524840,
      "discussion_count": 36,
      "discussion": [
        {
          "comment_id": "885884",
          "comments": [
            {
              "upvote_count": "4",
              "comment_id": "1076928",
              "poster": "koenigParas2324",
              "content": "this solution lacks clarity on allowing individual account administrators control over their CloudTrail trails.",
              "timestamp": "1700630460.0",
              "comments": [
                {
                  "timestamp": "1702594320.0",
                  "poster": "bnagaraja9099",
                  "comment_id": "1096889",
                  "upvote_count": "1",
                  "content": "C is good. \nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html"
                }
              ]
            },
            {
              "comment_id": "1074061",
              "poster": "a1234321606",
              "upvote_count": "5",
              "content": "Why C? If you deny modification or deletion of the AWS Config recorders by using an SCP, how do individual account administrators edit or delete their own CloudTrail trails and AWS Config rules?",
              "timestamp": "1700317740.0"
            }
          ],
          "upvote_count": "20",
          "poster": "haazybanj",
          "content": "Selected Answer: C\nC\nThis solution meets the requirements in the most operationally efficient way. It uses AWS CloudFormation StackSets to deploy AWS Config recorders in all accounts and AWS Config rules to the organization, which can be centrally managed from an AWS Config management account. A CloudTrail organization trail can also be created in the organization’s management account to collect logs from all accounts. An SCP can be used to deny modification or deletion of the AWS Config recorders, ensuring that the baseline resources cannot be modified or deleted by individual account administrators. However, individual account administrators can still edit or delete their own CloudTrail trails and AWS Config rules.",
          "timestamp": "1682917620.0"
        },
        {
          "timestamp": "1708759320.0",
          "upvote_count": "6",
          "content": "Selected Answer: B\nWhen Control Tower is enabled, AWS-GR_CLOUDTRAIL_ENABLED and AWS-GR_CONFIG_ENABLED will enable CloudTrail and Config in all available regions. The guardrails are automatically set to disallow changes to baseline resources.\n\nA, C, D - No mention about baseline resource.",
          "poster": "dzn",
          "comment_id": "1157707"
        },
        {
          "timestamp": "1753299720.0",
          "upvote_count": "1",
          "content": "Selected Answer: C\nSCPs do not support conditions based on the principal's IAM role or policy (like \"administrator of management account\").",
          "poster": "toma",
          "comment_id": "1589788"
        },
        {
          "timestamp": "1750793640.0",
          "comment_id": "1580324",
          "content": "Selected Answer: D\nDefiantly D",
          "upvote_count": "1",
          "poster": "toma"
        },
        {
          "timestamp": "1745088060.0",
          "comment_id": "1562041",
          "poster": "GripZA",
          "content": "Selected Answer: C\nmy 2 cents: \"AWS CloudTrail and AWS Config must be turned on in all available AWS Regions\" these are the baseline resources which cannot be deleted. Definitely operatioanlly efficient to have a designated AWS config management account and organizational trail. Then the deny with SCP will prevent baseline resource deletion, but admins can still edit their own trails and config rules. \n\nWhy not D: SCP is used, which is good — but the condition in the SCP to allow updates only by \"administrators of the management account\" is hard to implement cleanly",
          "upvote_count": "1"
        },
        {
          "content": "Selected Answer: D\nFor the people fighting over Option C or D:\n\nOption D is a better choice because it deploys all the required baseline resources—both CloudTrail and AWS Config—across every account using CloudFormation StackSets, and then locks them down with an SCP. This setup makes sure that only the management account’s administrators can change or remove these core security controls. \nMeanwhile, individual account admins can still manage any extra CloudTrail trails or Config rules they create on their own. \n\nBut, Option C, mainly focuses on AWS Config recorders and doesn't clearly cover CloudTrail, so it does not offer us as complete a solution.",
          "timestamp": "1738589580.0",
          "upvote_count": "3",
          "comment_id": "1350892",
          "poster": "lovekiller"
        },
        {
          "upvote_count": "2",
          "comment_id": "1334000",
          "content": "Selected Answer: D\nOption D:\n\nCreate an AWS CloudFormation Template: Develop a template that defines the standard resources, including CloudTrail and AWS Config, configured to operate in all available AWS Regions.\n\nDeploy Using CloudFormation StackSets: Utilize AWS CloudFormation StackSets from the organization's management account to deploy the template across all member accounts. This approach ensures consistent configuration and simplifies management.\n\nImplement a Service Control Policy (SCP): Establish an SCP that restricts updates or deletions of CloudTrail and AWS Config resources. This policy should allow only the organization's management account administrators to perform such actions, preventing individual account administrators from making unauthorized changes.",
          "poster": "Slays",
          "timestamp": "1735548720.0"
        },
        {
          "comment_id": "1329591",
          "timestamp": "1734716340.0",
          "upvote_count": "2",
          "poster": "youonebe",
          "content": "Selected Answer: D\nThis is the most operationally efficient solution. Using CloudFormation StackSets ensures standard resources are consistently deployed, and SCPs provide the necessary restrictions and flexibility."
        },
        {
          "content": "Selected Answer: C\nI think C because the SCP defines the principal being an administrator from the management account, not the individual account.",
          "timestamp": "1732490580.0",
          "comment_id": "1317238",
          "upvote_count": "1",
          "comments": [
            {
              "timestamp": "1732490700.0",
              "poster": "steli0",
              "comment_id": "1317240",
              "content": "moreover Principals are not supported in SCPs https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps_syntax.html#scp-syntax-unsupported",
              "upvote_count": "1"
            }
          ],
          "poster": "steli0"
        },
        {
          "poster": "BrusingWayne",
          "upvote_count": "3",
          "comment_id": "1315518",
          "timestamp": "1732138860.0",
          "content": "D. CloudFormation StackSets + SCP with conditional permissions:\n\n Centralized deployment of resources\n SCP prevents modifications to core resources\n Allows admins to edit their own resources (by implication)\n Matches all requirements efficiently"
        },
        {
          "content": "Selected Answer: D\nD is correct",
          "timestamp": "1723705080.0",
          "poster": "rk0509",
          "upvote_count": "4",
          "comment_id": "1266275"
        },
        {
          "content": "Selected Answer: C\nI think should be C\nKeywords: \"an existing set of AWS accounts\"",
          "timestamp": "1721977560.0",
          "upvote_count": "1",
          "poster": "jamesf",
          "comment_id": "1255530"
        },
        {
          "content": "Selected Answer: D\nmust be D",
          "timestamp": "1721109840.0",
          "poster": "trungtd",
          "upvote_count": "2",
          "comment_id": "1248709"
        },
        {
          "content": "Selected Answer: C\nI agree with C",
          "comment_id": "1205444",
          "upvote_count": "1",
          "timestamp": "1714645560.0",
          "poster": "seetpt"
        },
        {
          "content": "Selected Answer: C\nOption C is the most operationally efficient and meets all the requirements: ensuring CloudTrail and AWS Config are enabled in all regions, preventing the deletion or editing of baseline resources by individual account administrators, while still allowing them the flexibility to manage their own specific resources. This approach uses centralized control mechanisms (AWS Config management account and organization trail for CloudTrail) and leverages SCPs for enforcement, aligning with best practices for security and governance in AWS Organizations.",
          "upvote_count": "2",
          "poster": "Mordans",
          "comment_id": "1180742",
          "timestamp": "1711185600.0"
        },
        {
          "poster": "CloudHandsOn",
          "comment_id": "1166529",
          "content": "Selected Answer: D\nIm going with D. SCPs is what helps us here",
          "upvote_count": "2",
          "comments": [
            {
              "content": "but SCP not support direct principal.",
              "upvote_count": "1",
              "poster": "vn_thanhtung",
              "comment_id": "1207650",
              "timestamp": "1715043300.0"
            }
          ],
          "timestamp": "1709648100.0"
        },
        {
          "content": "Selected Answer: D\nD for me. \nI think C is incorrect because \"However, individual account administrators must be able to edit or delete their own CloudTrail trails and AWS Config rules.\" requirement is not satisfied because this answer has nothing about individual account administrators are able to edit their own CloudTrail trails. Organisational trail can be edited only from management or delegated administrator account.",
          "upvote_count": "5",
          "timestamp": "1709462520.0",
          "poster": "vmahilevskyi",
          "comment_id": "1164707"
        },
        {
          "upvote_count": "2",
          "comment_id": "1149023",
          "poster": "thanhnv142",
          "content": "Selected Answer: D\nD is correct: This denies modifications to AWS config or cloudtrail unless the principal is the management account\nA: No explicitly mention of denying modifications to Config or cloudtrail\nB: No explicitly mention of denying modifications to Config or cloudtrail\nC: < Create a CloudTrail organization trail in the organization’s management account>: This means the deny rule only affects the management account",
          "timestamp": "1707815220.0"
        },
        {
          "content": "Selected Answer: D\nC is using AWS Config Recorder, AWS Config uses the configuration recorder to detect changes in your resource configurations and capture these changes as configuration items.\n\nIt is not used for prevent you doing something, it is detecting something",
          "timestamp": "1707493080.0",
          "upvote_count": "3",
          "comment_id": "1145598",
          "poster": "Chelseajcole"
        },
        {
          "poster": "vortegon",
          "upvote_count": "2",
          "timestamp": "1707232080.0",
          "comment_id": "1142294",
          "content": "Selected Answer: C\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html",
          "comments": [
            {
              "poster": "jilly",
              "upvote_count": "1",
              "timestamp": "1707262080.0",
              "comment_id": "1142833",
              "content": "how many questions are there in DOP-C02. It says 217, but i dont see that many",
              "comments": [
                {
                  "poster": "Ramdi1",
                  "timestamp": "1707754980.0",
                  "upvote_count": "1",
                  "comment_id": "1148350",
                  "content": "i only see 209 questions i think even though it says 217 not sure if its something that they have to wait 2 weeks to release the rest since they update it maybe"
                }
              ]
            }
          ]
        },
        {
          "comment_id": "1137738",
          "timestamp": "1706801700.0",
          "upvote_count": "1",
          "content": "CloudTrail trails",
          "poster": "thanhnv142"
        },
        {
          "comment_id": "1134071",
          "timestamp": "1706445060.0",
          "content": "Selected Answer: D\nCloudTrail resources we want todeny this not config recorder",
          "upvote_count": "1",
          "poster": "hotblooded"
        },
        {
          "upvote_count": "1",
          "content": "Selected Answer: D\nthe common practice is using stacksets to enable AWS config, so D make sense",
          "poster": "a54b16f",
          "timestamp": "1705336140.0",
          "comment_id": "1123520"
        },
        {
          "comment_id": "1123518",
          "timestamp": "1705336020.0",
          "content": "Selected Answer: D\nC mentioned using AWS Config recorders, which is for drift detection and has nothing to do with enable AWS config",
          "upvote_count": "1",
          "poster": "a54b16f"
        },
        {
          "content": "Selected Answer: D\nAnswer is D.\nC is wrong because:\n\"Deny modification or deletion of the AWS Config recorders by using an SCP.\"\nAWS Config recroders track resource configurations.\nWe need to ensure that the baseline resources CANNOT be modified or deleted by individual account administrators. \nWe don't need to track this modifications)",
          "upvote_count": "1",
          "poster": "AikAWS",
          "timestamp": "1704784440.0",
          "comment_id": "1117254"
        },
        {
          "poster": "Jaguaroooo",
          "content": "I think D is a better choice based on the following statement: Create an SCP that prevents updates or deletions to CloudTrail resources or AWS Config resources unless the principal is an administrator of the organization's management account.",
          "comment_id": "1114950",
          "upvote_count": "1",
          "timestamp": "1704511200.0"
        },
        {
          "timestamp": "1700630280.0",
          "poster": "koenigParas2324",
          "content": "Selected Answer: D\nC. Designate Config Management Account, SCP, and StackSets:\n\nDeploys AWS Config recorders and rules using CloudFormation StackSets.\nUtilizes an SCP to prevent modification or deletion of AWS Config recorders.\nHowever, this solution lacks clarity on allowing individual account administrators control over their CloudTrail trails.\n\noption D appears to be the most operationally efficient. It employs CloudFormation for deployment and an SCP to control access to baseline resources while ensuring that only administrators of the management account can modify or delete the baseline resources. This ensures the core security controls and allows individual account administrators control over their own CloudTrail trails and AWS Config rules.",
          "comment_id": "1076925",
          "upvote_count": "3"
        },
        {
          "comment_id": "1047688",
          "upvote_count": "2",
          "poster": "kacsabacsi78",
          "timestamp": "1697703120.0",
          "content": "Selected Answer: C\nD is wrong - This trail was created by the management account for your organization. You cannot edit or delete this trail unless you are logged in with that account"
        },
        {
          "upvote_count": "2",
          "poster": "bugincloud",
          "comment_id": "1015389",
          "content": "Selected Answer: D\nIf you have an SCP to deny update & delete the administrators themselves won't be able to update the resources.\nIn my opinion D is the answer.",
          "timestamp": "1695515220.0"
        },
        {
          "poster": "HoangGuru",
          "timestamp": "1695196380.0",
          "content": "Selected Answer: D\nchoose D",
          "comment_id": "1012009",
          "upvote_count": "1"
        },
        {
          "timestamp": "1694078520.0",
          "poster": "beanxyz",
          "upvote_count": "1",
          "comment_id": "1001370",
          "content": "Selected Answer: D\nA and B are obviously wrong.\nC Why did we create aws config recorder to detect drift? Shall we write another lambda function to remediate the drift? It's not efficient\nD We can deploy the cfn to enable cloudtrail and aws config in each account via stackset. And use scp as a guardtrail to prevent updating and deletion actions."
        },
        {
          "poster": "Just_Ninja",
          "content": "Selected Answer: D\nD is the right one!\nWhy?\nBecause you can ByPass a SCP...\n{\n \"Version\": \"2012-10-17\",\n \"Statement\": [\n {\n \"Sid\": \"ThisTemplateIsShortenForHere!\",\n \"Effect\": \"Deny\",\n \"NotAction\": [\n \"health:*\",\n \"iam:*\",\n\n ],\n \"Resource\": \"*\",\n \"Condition\": {\n \"StringNotEquals\": {\n \"aws:RequestedRegion\": [\n \"us-west-2\"\n ]\n },\n \"ArnNotLike\": {\n \"aws:PrincipalARN\": [\n \"arn:aws:iam::*:role/Role1AllowedToBypassThisSCP\",\n \"arn:aws:iam::*:role/Role2AllowedToBypassThisSCP\"\n ]\n }\n }\n }\n ]\n}",
          "timestamp": "1689144960.0",
          "comment_id": "949541",
          "upvote_count": "1"
        },
        {
          "poster": "Blueee",
          "content": "Selected Answer: C\nAWS Control Tower is a service that provides the easiest way to set up and govern a new, secure, multi-account AWS environment based on best practices established through AWS’ experience working with thousands of enterprise customers as they move to the cloud1.\n\nWhile AWS Control Tower can be used to manage multiple AWS accounts and control access and permissions, it is not the most operationally efficient way to apply a core set of security controls to an existing set of AWS accounts.\n\nOption C is more operationally efficient because it allows individual teams to administer individual accounts by using the AdministratorAccess AWS managed policy while ensuring that AWS CloudTrail and AWS Config are turned on in all available AWS Regions. Individual account administrators will not be able to edit or delete any of the baseline resources but will be able to edit or delete their own CloudTrail trails and AWS Config rules",
          "upvote_count": "3",
          "timestamp": "1688547000.0",
          "comment_id": "943498"
        },
        {
          "timestamp": "1688263920.0",
          "content": "Selected Answer: B\nThe correct answer is B. AWS Control Tower enables AWS Config and AWS CloudTrail in every member account, as well as aggregates the logs from those services in the Audit account and Log archive accounts that it creates. This is one of the benefits of deploying Control Tower.",
          "comment_id": "940434",
          "upvote_count": "2",
          "poster": "sb333"
        },
        {
          "upvote_count": "2",
          "poster": "Sazeka",
          "content": "Selected Answer: D\nThe answer is D. It's more efficient to use SCP than stack policies to deny permissions to update or delete resources that are provisioned using CloudFormation StackSets. A is also possible but create more overhead and is not a best practice.\nhttps://www.examtopics.com/exams/amazon/aws-devops-engineer-professional/view/3/",
          "comment_id": "877962",
          "timestamp": "1682233380.0"
        },
        {
          "comment_id": "870560",
          "timestamp": "1681524840.0",
          "content": "Selected Answer: C\nC is the correct answer",
          "upvote_count": "3",
          "poster": "alce2020"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:20:07.610Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "vf6o5bcjLYOXlHnx8FHp",
      "question_number": 328,
      "page": 66,
      "question_text": "A company has its AWS accounts in an organization in AWS Organizations. AWS Config is manually configured in each AWS account. The company needs to implement a solution to centrally configure AWS Config for all accounts in the organization The solution also must record resource changes to a central account.\nWhich combination of actions should a DevOps engineer perform to meet these requirements? (Choose two.)",
      "choices": {
        "A": "Configure a delegated administrator account for AWS Config. Enable trusted access for AWS Config in the organization.",
        "B": "Configure a delegated administrator account for AWS Config. Create a service-linked role for AWS Config in the organization’s management account.",
        "E": "Create an AWS Config organization aggregator in the delegated administrator account. Configure data collection from all AWS accounts in the organization and from all AWS Regions.",
        "D": "Create an AWS Config organization aggregator in the organization's management account. Configure data collection from all AWS accounts in the organization and from all AWS Regions.",
        "C": "Create an AWS CloudFormation template to create an AWS Config aggregator. Configure a CloudFormation stack set to deploy the template to all accounts in the organization."
      },
      "correct_answer": "AE",
      "answer_ET": "AE",
      "answers_community": [
        "AE (86%)",
        "14%"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/105450-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-04-06 20:30:00",
      "unix_timestamp": 1680805800,
      "discussion_count": 11,
      "discussion": [
        {
          "comment_id": "874034",
          "content": "Selected Answer: AE\nAE \nhttps://aws.amazon.com/blogs/mt/org-aggregator-delegated-admin/\nA - When enabling trust - the service-linked role will be created but not the other way around.\nE - the delegated account will be the account that manages AWS config so it should collect all data centrally.",
          "upvote_count": "18",
          "poster": "asfsdfsdf",
          "timestamp": "1681845420.0"
        },
        {
          "comment_id": "1255535",
          "content": "Selected Answer: AE\nA - You can enable trusted access using either the AWS Config console or the AWS Organizations console.\nhttps://docs.aws.amazon.com/organizations/latest/userguide/services-that-can-integrate-config.html",
          "upvote_count": "2",
          "timestamp": "1721977920.0",
          "poster": "jamesf"
        },
        {
          "poster": "zijo",
          "content": "AE is the answer\nAWS Config offers an organization-wide data aggregation capability called the Config organization aggregator. It allows you to collect and view configuration data from all member accounts within your AWS Organization in a single location. This centralizes your view of resource configurations and compliance posture across your entire AWS environment.",
          "upvote_count": "1",
          "timestamp": "1711381680.0",
          "comment_id": "1182607"
        },
        {
          "upvote_count": "1",
          "poster": "thanhnv142",
          "content": "A and E are correct: <AWS Config is manually configured in each AWS account> means we dont need ACF (only used for the deployment of AWS config). <centrally configure AWS Config for all accounts> means we need to allow a central account to control AWS config in all member accounts.\n- <record resource changes to a central account> means we need to collect data from all member accounts and push to the central account\nB: service-linked role only used for interacting with other AWS services\nC: no need ACF\nD: we need AWS Config organization aggregator in the delegated administrator account, not the organization's management account",
          "timestamp": "1706803380.0",
          "comment_id": "1137752"
        },
        {
          "content": "Selected Answer: AE\nAE is most correct",
          "upvote_count": "2",
          "comment_id": "1017492",
          "poster": "hoaile257",
          "timestamp": "1695711780.0"
        },
        {
          "comment_id": "949553",
          "timestamp": "1689145860.0",
          "upvote_count": "3",
          "poster": "Just_Ninja",
          "content": "Selected Answer: AE\nHere you have the Tutorial :) \nhttps://aws.amazon.com/blogs/mt/org-aggregator-delegated-admin/"
        },
        {
          "content": "Selected Answer: AE\nhttps://aws.amazon.com/blogs/mt/org-aggregator-delegated-admin/\nhttps://docs.aws.amazon.com/organizations/latest/userguide/services-that-can-integrate-config.html",
          "upvote_count": "3",
          "timestamp": "1686757440.0",
          "comment_id": "923349",
          "poster": "rhinozD"
        },
        {
          "upvote_count": "3",
          "content": "BE is the most efficient",
          "comment_id": "905213",
          "poster": "Kodoma",
          "timestamp": "1684873680.0"
        },
        {
          "upvote_count": "3",
          "poster": "ParagSanyashiv",
          "timestamp": "1683562380.0",
          "comments": [
            {
              "poster": "2pk",
              "content": "Why ? it says setup service linked role in management account not in Delegated account?",
              "timestamp": "1699264860.0",
              "upvote_count": "1",
              "comment_id": "1063717"
            }
          ],
          "comment_id": "892319",
          "content": "Selected Answer: BD\nBD is most suitable in this case"
        },
        {
          "comment_id": "870465",
          "upvote_count": "2",
          "timestamp": "1681504740.0",
          "content": "Selected Answer: BD\nThe correct answers are B and D. Option B is correct because it suggests configuring a delegated administrator account for AWS Config and creating a service-linked role for AWS Config in the organization’s management account. This allows AWS Config to perform supported operations within the accounts in the organization, and enables trusted access. Option D is correct because it suggests creating an AWS Config organization aggregator in the organization's management account and configuring data collection from all AWS accounts in the organization and from all AWS Regions, which enables multi-account, multi-region data aggregation. Options A and E are not correct because they do not suggest using a service-linked role for AWS Config or creating an AWS Config organization aggregator in the organization's management account.",
          "poster": "jqso234"
        },
        {
          "comment_id": "863231",
          "timestamp": "1680805800.0",
          "content": "Selected Answer: AE\nAE . https://docs.aws.amazon.com/organizations/latest/userguide/services-that-can-integrate-config.html",
          "poster": "Dimidrol",
          "upvote_count": "3"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:20:07.610Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "lkr24cTkRvr6R0rq6S6h",
      "question_number": 329,
      "page": 66,
      "question_text": "A company wants to migrate its content sharing web application hosted on Amazon EC2 to a serverless architecture. The company currently deploys changes to its application by creating a new Auto Scaling group of EC2 instances and a new Elastic Load Balancer, and then shifting the traffic away using an Amazon Route 53 weighted routing policy.\nFor its new serverless application, the company is planning to use Amazon API Gateway and AWS Lambda. The company will need to update its deployment processes to work with the new application. It will also need to retain the ability to test new features on a small number of users before rolling the features out to the entire user base.\nWhich deployment strategy will meet these requirements?",
      "choices": {
        "C": "Use AWS Elastic Beanstalk to deploy API Gateway and Lambda functions. When code needs to be changed, deploy a new version of the API and Lambda functions. Shift traffic gradually using an Elastic Beanstalk blue/green deployment.",
        "A": "Use AWS CDK to deploy API Gateway and Lambda functions. When code needs to be changed, update the AWS CloudFormation stack and deploy the new version of the APIs and Lambda functions. Use a Route 53 failover routing policy for the canary release strategy.",
        "B": "Use AWS CloudFormation to deploy API Gateway and Lambda functions using Lambda function versions. When code needs to be changed, update the CloudFormation stack with the new Lambda code and update the API versions using a canary release strategy. Promote the new version when testing is complete.",
        "D": "Use AWS OpsWorks to deploy API Gateway in the service layer and Lambda functions in a custom layer. When code needs to be changed, use OpsWorks to perform a blue/green deployment and shift traffic gradually."
      },
      "correct_answer": "B",
      "answer_ET": "B",
      "answers_community": [
        "B (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/108354-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-05-03 06:20:00",
      "unix_timestamp": 1683087600,
      "discussion_count": 6,
      "discussion": [
        {
          "upvote_count": "13",
          "comment_id": "888135",
          "timestamp": "1683087600.0",
          "poster": "haazybanj",
          "content": "Selected Answer: B\nThe deployment strategy that will meet the company's requirements is B. Use AWS CloudFormation to deploy API Gateway and Lambda functions using Lambda function versions. When code needs to be changed, update the CloudFormation stack with the new Lambda code and update the API versions using a canary release strategy. Promote the new version when testing is complete.\n\nExplanation:\n\nOption B provides a deployment strategy for the company's new serverless architecture, allowing the company to retain the ability to test new features on a small number of users before rolling the features out to the entire user base. Using AWS CloudFormation, the company can deploy API Gateway and Lambda functions using Lambda function versions. When code needs to be changed, the company can update the CloudFormation stack with the new Lambda code and update the API versions using a canary release strategy. Once testing is complete, the new version can be promoted."
        },
        {
          "comments": [
            {
              "timestamp": "1694081460.0",
              "comment_id": "1001418",
              "poster": "beanxyz",
              "upvote_count": "3",
              "content": "Your answer is correct but the explanation is not.\nA is wrong because we can't use Route 53 failover routing for canary release. If it says Route53 weighted routing, then it is a possible option.\nD is wrong because when you use blue/green mode, the switch from blue to green is all done at once, not like a granular canary change"
            }
          ],
          "upvote_count": "7",
          "comment_id": "950277",
          "poster": "Snape",
          "timestamp": "1689215460.0",
          "content": "Selected Answer: B\nA Wrong: not using Canary, or Blue/green\nC Wrong: Beanstalk is not serverless deployment platform\nD Wrong: Irrelevant, OpsWork is configuration management platform and situation is requesting application deployment /AWS resource provisioning platform"
        },
        {
          "comment_id": "1319297",
          "content": "Selected Answer: B\nIt could be either A or B but the key here is that \"Use a Route 53 failover routing policy for the canary release strategy.\" is a wrong statement, it should be a Route 53 weighted routing policy so B is the correct answer",
          "poster": "teo2157",
          "timestamp": "1732806120.0",
          "upvote_count": "1"
        },
        {
          "content": "B is correct: <serverless architecture> means ECS, lambda, Beanstalk. < It will also need to retain the ability to test new features on a small number of users before rolling the features out to the entire user base> means canary deployment\nA: <Route 53 failover routing policy for the canary release strategy>: there is no such thing\nC and D: no mention of canary deployment",
          "timestamp": "1706804100.0",
          "upvote_count": "1",
          "comment_id": "1137756",
          "poster": "thanhnv142"
        },
        {
          "timestamp": "1706804040.0",
          "poster": "thanhnv142",
          "comment_id": "1137755",
          "content": "B is correct: <serverless architecture> means ECS, lambda, Beanstalk. < It will also need to retain the ability to test new features on a small number of users before rolling the features out to the entire user base> means canary deployment\nA: <Route 53 failover routing policy for the canary release strategy>: there is no such thing\nC: no mention of canary deployment",
          "upvote_count": "1"
        },
        {
          "poster": "Jeanphi72",
          "timestamp": "1683124620.0",
          "upvote_count": "1",
          "comment_id": "888617",
          "content": "Selected Answer: B\nONly B is possible"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:20:07.610Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "DXin7hPsdsfYvSExXKYW",
      "question_number": 330,
      "page": 66,
      "question_text": "A development team uses AWS CodeCommit, AWS CodePipeline, and AWS CodeBuild to develop and deploy an application. Changes to the code are submitted by pull requests. The development team reviews and merges the pull requests, and then the pipeline builds and tests the application.\nOver time, the number of pull requests has increased. The pipeline is frequently blocked because of failing tests. To prevent this blockage, the development team wants to run the unit and integration tests on each pull request before it is merged.\nWhich solution will meet these requirements?",
      "choices": {
        "A": "Create a CodeBuild project to run the unit and integration tests. Create a CodeCommit approval rule template. Configure the template to require the successful invocation of the CodeBuild project. Attach the approval rule to the project's CodeCommit repository.",
        "B": "Create an Amazon EventBridge rule to match pullRequestCreated events from CodeCommit Create a CodeBuild project to run the unit and integration tests. Configure the CodeBuild project as a target of the EventBridge rule that includes a custom event payload with the CodeCommit repository and branch information from the event.",
        "D": "Create a CodeBuild project to run the unit and integration tests. Create a CodeCommit notification rule that matches when a pull request is created or updated. Configure the notification rule to invoke the CodeBuild project.",
        "C": "Create an Amazon EventBridge rule to match pullRequestCreated events from CodeCommit. Modify the existing CodePipeline pipeline to not run the deploy steps if the build is started from a pull request. Configure the EventBridge rule to run the pipeline with a custom payload that contains the CodeCommit repository and branch information from the event."
      },
      "correct_answer": "B",
      "answer_ET": "B",
      "answers_community": [
        "B (70%)",
        "D (15%)",
        "Other"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/108078-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-05-01 07:23:00",
      "unix_timestamp": 1682918580,
      "discussion_count": 11,
      "discussion": [
        {
          "timestamp": "1698823380.0",
          "comment_id": "885893",
          "upvote_count": "13",
          "content": "Selected Answer: B\nTo run the unit and integration tests on each pull request before it is merged, a solution that listens to pullRequestCreated events and runs a CodeBuild project to execute tests would be the most appropriate option.\n\nOption B describes a solution that creates an Amazon EventBridge rule to match pullRequestCreated events from CodeCommit and configures a CodeBuild project to run the unit and integration tests, passing the CodeCommit repository and branch information from the event as a custom payload.\n\nTherefore, option B is the correct answer.",
          "poster": "haazybanj"
        },
        {
          "poster": "nickp84",
          "content": "Selected Answer: A\nThe main requirement is: \"To prevent this blockage, the development team wants to run the unit and integration tests on each pull request before it is merged.\"\n\n Option B doesn’t stop someone from merging a PR even if the tests fail — it's informational only unless you add more enforcement logic manually (which Option A includes by design).\n\nNo Approval Integration:\n\n There is no mechanism in B to integrate CodeBuild results with CodeCommit's approval rules or to block the merge.\n\n That means a developer can still manually merge a PR before the tests complete or even if they fail.\n\nManual Enforcement Needed:\n\n You'd need to custom build additional logic (e.g., using AWS Lambda or webhooks) to actually enforce that tests must pass before allowing a merge — which Option A handles automatically with approval rules.",
          "comment_id": "1568900",
          "timestamp": "1747244940.0",
          "upvote_count": "1"
        },
        {
          "upvote_count": "1",
          "poster": "that1guy",
          "content": "Selected Answer: C\nThese days it would be C instead of B, it's very common to reuse the same pipeline but with conditions to skip certain steps depending on the branch.\n\nhttps://aws.amazon.com/blogs/devops/aws-codepipeline-adds-support-for-branch-based-development-and-monorepos/",
          "comment_id": "1210955",
          "timestamp": "1731521700.0"
        },
        {
          "timestamp": "1724006940.0",
          "comments": [
            {
              "upvote_count": "1",
              "poster": "vn_thanhtung",
              "content": "The development team reviews and merges the pull requests, and then the pipeline builds and tests the application.",
              "comments": [
                {
                  "poster": "Jon_aws",
                  "comment_id": "1350398",
                  "content": "shouldn't review be after build and test?",
                  "timestamp": "1738497900.0",
                  "upvote_count": "1"
                }
              ],
              "comment_id": "1214296",
              "timestamp": "1732108620.0"
            }
          ],
          "poster": "jojom19980",
          "content": "Selected Answer: A\nThe Answer should Be A because this option is allows test the code and the approval is depending on test's result",
          "comment_id": "1153561",
          "upvote_count": "2"
        },
        {
          "upvote_count": "2",
          "comment_id": "1137762",
          "poster": "thanhnv142",
          "content": "A is definitely correct: <The development team reviews and merges the pull requests> and <the development team wants to run the unit and integration tests on each pull request before it is merged> means the dev team always review all pull requests and they need a solution to test committed code before merging to main. option A allow them to do tests and manually approve it before allow merging\nB C and D: no mention of the step that allow the dev team to manually approve the merge.",
          "timestamp": "1722522420.0"
        },
        {
          "content": "B is the answer . D is wrong. Code commit only can setup notification rule to SNS topics or Chatbot.",
          "comment_id": "1067515",
          "timestamp": "1715365860.0",
          "upvote_count": "2",
          "poster": "2pk"
        },
        {
          "upvote_count": "3",
          "poster": "BaburTurk",
          "comment_id": "1002561",
          "timestamp": "1709913960.0",
          "content": "Selected Answer: B\nhttps://aws.amazon.com/blogs/devops/validating-aws-codecommit-pull-requests-with-aws-codebuild-and-aws-lambda/"
        },
        {
          "timestamp": "1708509720.0",
          "content": "Selected Answer: D\nOnly D covers create pull request and update pull request",
          "poster": "Seoyong",
          "comments": [
            {
              "timestamp": "1716469260.0",
              "comment_id": "1078502",
              "upvote_count": "2",
              "content": "Can't be D because you can trigger a Codebuild project with CodeCommit notification rules",
              "poster": "zolthar_z"
            }
          ],
          "comment_id": "986312",
          "upvote_count": "1"
        },
        {
          "timestamp": "1706346540.0",
          "poster": "Aja1",
          "content": "Option B is the most appropriate solution as it uses Amazon EventBridge rules to automatically trigger a CodeBuild project for running tests on each pull request, enabling early testing and preventing pipeline blockages due to failing tests after merging.",
          "upvote_count": "1",
          "comment_id": "964458"
        },
        {
          "upvote_count": "3",
          "content": "Selected Answer: D\nNothing in requirements says that the team wants a blockage of the pull request merge. \nAnd the B solution talks only about the \"PullRequestCreated\" which is not enough, it has to be reexecuted at any event on the pull request.",
          "timestamp": "1705414320.0",
          "comment_id": "953291",
          "poster": "totopopo"
        },
        {
          "timestamp": "1705121040.0",
          "comment_id": "950281",
          "upvote_count": "3",
          "content": "Selected Answer: B\nA wrong: Dev team will need to manually approve each pull request before merging, this can be time consuming and error-prone.\nC Wrong: Modifying the exisiting codepipeline is not necessary\nD wrong: No preventiong from pipeline being blocked by failing tests",
          "poster": "Snape"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:20:07.610Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "92pr4RALY335Q3T40X1o",
      "question_number": 331,
      "page": 67,
      "question_text": "A company uses AWS CodePipeline pipelines to automate releases of its application A typical pipeline consists of three stages build, test, and deployment. The company has been using a separate AWS CodeBuild project to run scripts for each stage. However, the company now wants to use AWS CodeDeploy to handle the deployment stage of the pipelines.\nThe company has packaged the application as an RPM package and must deploy the application to a fleet of Amazon EC2 instances. The EC2 instances are in an EC2 Auto Scaling group and are launched from a common AMI.\nWhich combination of steps should a DevOps engineer perform to meet these requirements? (Choose two.)",
      "choices": {
        "B": "Create a new version of the common AMI with the CodeDeploy agent installed. Create an AppSpec file that contains application deployment scripts and grants access to CodeDeploy.",
        "E": "Create an application in CodeDeploy. Configure an in-place deployment type. Specify the EC2 instances that are launched from the common AMI as the deployment target. Update the CodePipeline pipeline to use the CodeDeploy action to deploy the application.",
        "A": "Create a new version of the common AMI with the CodeDeploy agent installed. Update the IAM role of the EC2 instances to allow access to CodeDeploy.",
        "D": "Create an application in CodeDeploy. Configure an in-place deployment type. Specify the Auto Scaling group as the deployment target. Update the CodePipeline pipeline to use the CodeDeploy action to deploy the application.",
        "C": "Create an application in CodeDeploy. Configure an in-place deployment type. Specify the Auto Scaling group as the deployment target. Add a step to the CodePipeline pipeline to use EC2 Image Builder to create a new AMI. Configure CodeDeploy to deploy the newly created AMI."
      },
      "correct_answer": "AD",
      "answer_ET": "AD",
      "answers_community": [
        "AD (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/105504-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-04-07 14:12:00",
      "unix_timestamp": 1680869520,
      "discussion_count": 13,
      "discussion": [
        {
          "upvote_count": "11",
          "poster": "bcx",
          "timestamp": "1685454240.0",
          "content": "A and D are the correct ones.\n\nE is wrong because it says that the instances are on an ASG.\nC is qrong. You deploy the new RPM on the AMI, you do not create a new AMI every time to install the RPM.\nB is wrong, the appspec has nothing to do with permissions",
          "comment_id": "910286"
        },
        {
          "comment_id": "1145277",
          "timestamp": "1727165700.0",
          "poster": "thanhnv142",
          "content": "Selected Answer: AD\nA and D are correct: \nA: rebuild the AMI and Update the IAM role of the EC2 instances to allow access to CodeDeploy is necessary \nB: no need to grants AppSpec file access to code CodeDeploy\nC: <Add a step to the CodePipeline pipeline to use EC2 Image Builder to create a new AMI> : this is unnecessary, we have already done it in option A. Addtionally, recreating AIM each time running the CICD pipiline is unnecessary\nD: ok\nE: <Specify the EC2 instances that are launched from the common AMI as the deployment targe>: this is time-consumming. There might by hundreds of EC2 instances and targeting them individually is time-consuming and not effective.",
          "upvote_count": "6"
        },
        {
          "content": "the ans is A and D \n\nD not E because in deployment the best practise deploy in group not instance",
          "comment_id": "1310565",
          "upvote_count": "1",
          "timestamp": "1731413700.0",
          "poster": "Saudis"
        },
        {
          "poster": "thanhnv142",
          "comment_id": "1133435",
          "timestamp": "1706368680.0",
          "content": "A and D: \nB is incorrect: AppSpec file does not need to be granted access to code deploy. It is code deploy that need the permission to get acess to Appspec file",
          "upvote_count": "1"
        },
        {
          "poster": "z_inderjot",
          "upvote_count": "3",
          "content": "Selected Answer: AD\nA - instances need code deploy agent and role \nD - target as ASG",
          "comment_id": "1100272",
          "timestamp": "1702960140.0"
        },
        {
          "upvote_count": "2",
          "timestamp": "1699362360.0",
          "content": "A D. \nIAM role/instance profile requirement for EC2 is to allow EC2 access to S3 buckets used by CodeDeploy.\n\nhttps://docs.aws.amazon.com/codedeploy/latest/userguide/getting-started-create-iam-instance-profile.html",
          "comment_id": "1064803",
          "poster": "robertohyena"
        },
        {
          "comments": [
            {
              "content": "A is right \nyou have to attach IAM role to EC2 instance , for them to be controlled by Code deploy . The agent running in the ec2 instance needs to talk with code deploy .",
              "comment_id": "1100271",
              "timestamp": "1702960080.0",
              "poster": "z_inderjot",
              "upvote_count": "3"
            }
          ],
          "content": "Should be BD. EC2 doesn't need a permission to access CodeDeploy. Instead an IAM role associated with Code Deployment Group should have an permission to launch instances in that autoscaling group.",
          "comment_id": "1057349",
          "timestamp": "1698641520.0",
          "upvote_count": "2",
          "poster": "DZ_Ben"
        },
        {
          "poster": "sivre",
          "comment_id": "1041869",
          "upvote_count": "3",
          "timestamp": "1697123700.0",
          "content": "Why EC2 instance need access to CodeDeploy??, in the doc is mentioned only S3: \"Create or locate an IAM instance profile that allows the Amazon EC2 Auto Scaling group to work with Amazon S3\" https://docs.aws.amazon.com/codedeploy/latest/userguide/integrations-aws-auto-scaling.html",
          "comments": [
            {
              "comment_id": "1092254",
              "poster": "harithzainudin",
              "content": "As the codedeploy agent is being installed inside the EC2, This agent facilitates the deployment process by coordinating with the CodeDeploy service. Hence, \nThe EC2 instances must have an IAM role that grants them the necessary permissions to interact with CodeDeploy. This step is critical to ensure that the deployment process can be executed securely and successfully",
              "timestamp": "1702174500.0",
              "upvote_count": "3"
            }
          ]
        },
        {
          "comment_id": "976742",
          "timestamp": "1691591400.0",
          "upvote_count": "1",
          "poster": "Ja13",
          "content": "Selected Answer: AD\nAD as explained en the comments"
        },
        {
          "timestamp": "1686218340.0",
          "content": "Selected Answer: AD\nAD are correct.\nhttps://docs.aws.amazon.com/codedeploy/latest/userguide/integrations-aws-auto-scaling.html",
          "comment_id": "918096",
          "upvote_count": "2",
          "comments": [
            {
              "timestamp": "1691307300.0",
              "comment_id": "973617",
              "content": "An in-place deployment allows you to deploy your application without creating new infrastructure.\nThe deployment type that is specific to the deployment's compute platform or deployments initiated by a CloudFormation stack update.https://docs.aws.amazon.com/codedeploy/latest/userguide/integrations-aws-auto-scaling.html",
              "poster": "Aja1",
              "upvote_count": "2"
            }
          ],
          "poster": "madperro"
        },
        {
          "upvote_count": "1",
          "content": "A, D; B is wrong, because the AppSpec file cannot grant permissions for CodeDeploy",
          "comment_id": "878573",
          "poster": "tycho",
          "timestamp": "1682265360.0"
        },
        {
          "timestamp": "1681577700.0",
          "upvote_count": "1",
          "content": "A&D it is",
          "comment_id": "871123",
          "poster": "alce2020"
        },
        {
          "comment_id": "863828",
          "poster": "ele",
          "timestamp": "1680869520.0",
          "upvote_count": "2",
          "content": "Selected Answer: AD\nA,D. \nB - wrong, the The 'permissions' section specifies how special permissions, should be applied to the files and directories/folders in the 'files' section\nC wrong, no need as AMI was already built.\nE wrong, as ASG is the target."
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:20:18.300Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "9AdawXWHucbSMb13ekxj",
      "question_number": 332,
      "page": 67,
      "question_text": "A company has an application that runs on a fleet of Amazon EC2 instances. The application requires frequent restarts. The application logs contain error messages when a restart is required. The application logs are published to a log group in Amazon CloudWatch Logs.\nAn Amazon CloudWatch alarm notifies an application engineer through an Amazon Simple Notification Service (Amazon SNS) topic when the logs contain a large number of restart-related error messages. The application engineer manually restarts the application on the instances after the application engineer receives a notification from the SNS topic.\nA DevOps engineer needs to implement a solution to automate the application restart on the instances without restarting the instances.\nWhich solution will meet these requirements in the MOST operationally efficient manner?",
      "choices": {
        "B": "Create an AWS Lambda function that restarts the application on the instances. Configure the Lambda function as an event destination of the SNS topic.",
        "C": "Configure an AWS Systems Manager Automation runbook that runs a script to restart the application on the instances. Create an AWS Lambda function to invoke the runbook. Configure the Lambda function as an event destination of the SNS topic.",
        "D": "Configure an AWS Systems Manager Automation runbook that runs a script to restart the application on the instances. Configure an Amazon EventBridge rule that reacts when the CloudWatch alarm enters ALARM state. Specify the runbook as a target of the rule.",
        "A": "Configure an AWS Systems Manager Automation runbook that runs a script to restart the application on the instances. Configure the SNS topic to invoke the runbook."
      },
      "correct_answer": "D",
      "answer_ET": "D",
      "answers_community": [
        "D (71%)",
        "C (16%)",
        "13%"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/108414-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-05-03 16:25:00",
      "unix_timestamp": 1683123900,
      "discussion_count": 26,
      "discussion": [
        {
          "upvote_count": "23",
          "comment_id": "1010215",
          "poster": "daburahjail",
          "content": "Selected Answer: D\nIt is debatable, as both C and D are correct and simple in their own ways, however, take a look at the number of components in each approach:\n\nC: CW -> SNS -> LAMBDA -> SSM (4)\nD: CW -> EVENTBRIDGE -> SSM (3)\n\nThere is an extra component (SNS) to maintain on C, also, there is some coding involved on this option, which also needs to be maintained.\nEven if we already have the SNS created on option C, we still have to go there to remove the notification and configure the lambda invocation.\n\nOption D has fewer components, and require less customization.",
          "timestamp": "1695008100.0"
        },
        {
          "poster": "ParagSanyashiv",
          "comment_id": "892669",
          "upvote_count": "10",
          "content": "Selected Answer: C\nC makes more sense here",
          "timestamp": "1683607260.0"
        },
        {
          "upvote_count": "1",
          "content": "Selected Answer: D\nD is more simpler solution than C.",
          "poster": "jamesf",
          "comment_id": "1255558",
          "timestamp": "1721979480.0"
        },
        {
          "poster": "xdkonorek2",
          "comment_id": "1239780",
          "upvote_count": "1",
          "content": "Selected Answer: D\nD)\n\nB is wrong since it's way easier to use SSM automation runbook to execute logic inside instance using \"run command\" action within automation runbook than doing this with lambda",
          "timestamp": "1719770940.0"
        },
        {
          "upvote_count": "1",
          "timestamp": "1711387560.0",
          "poster": "zijo",
          "content": "A is not possible - AWS Systems Manager (SSM) Run Command or Automation runbooks cannot be directly triggered by an Amazon SNS topic.\nThen C and D are the next best options. C is flexible but D is the most simple solution",
          "comment_id": "1182681"
        },
        {
          "timestamp": "1708291260.0",
          "upvote_count": "3",
          "content": "Selected Answer: D\nBoth C and D are valid answers. However, D is less complicated.",
          "comment_id": "1153582",
          "poster": "Diego1414"
        },
        {
          "poster": "jojom19980",
          "upvote_count": "2",
          "content": "Selected Answer: D\nC is correct , But D is more easy to implement , cost saving, managed services by AWS ^_^",
          "comment_id": "1153565",
          "timestamp": "1708289940.0"
        },
        {
          "upvote_count": "1",
          "poster": "thanhnv142",
          "comment_id": "1137771",
          "content": "B is correct: <implement a solution to automate the application restart on the instances> means we need to automate the restart step. We can use lambda, AWS system manager. <CloudWatch alarm notifies an application engineer through an Amazon Simple Notification Service> means we already have the alarm. We just need to simply trigger the restart process with lambda\nA, C and D are all too complicated compared to B. They ask for \"the MOST operationally efficient manner\", not the most complicated one",
          "comments": [
            {
              "upvote_count": "1",
              "content": "Option B not correct.\nhttps://docs.aws.amazon.com/systems-manager/latest/userguide/running-automations-event-bridge.html",
              "timestamp": "1715587320.0",
              "poster": "vn_thanhtung",
              "comment_id": "1210761"
            }
          ],
          "timestamp": "1706805540.0"
        },
        {
          "upvote_count": "2",
          "poster": "z_inderjot",
          "comment_id": "1104441",
          "content": "Selected Answer: D\nFor me D is the answer , because we use lamba for the custom operations , if we already have SSM automation to perform that same action then why writing our custom logic in lambda ?",
          "timestamp": "1703390880.0"
        },
        {
          "comment_id": "1099173",
          "content": "Selected Answer: D\nIt’s D. Here is a reference:\n\nhttps://aws.amazon.com/blogs/mt/use-amazon-eventbridge-rules-to-run-aws-systems-manager-automation-in-response-to-cloudwatch-alarms/",
          "upvote_count": "4",
          "timestamp": "1702841040.0",
          "poster": "csG13"
        },
        {
          "content": "Selected Answer: D\nD It's the most simples approach. But C its also a solution, but why build and mantain a lambda?",
          "upvote_count": "3",
          "poster": "HugoFM",
          "comment_id": "1082715",
          "timestamp": "1701187980.0"
        },
        {
          "comment_id": "1078521",
          "content": "Selected Answer: B\nI think is B, you only need to create the lambda and update the SNS to the lambda,",
          "poster": "zolthar_z",
          "timestamp": "1700752680.0",
          "upvote_count": "2"
        },
        {
          "upvote_count": "2",
          "content": "Selected Answer: B\nB seems like the shortest number of steps given that SNS already exists",
          "poster": "nlw",
          "comment_id": "1064697",
          "timestamp": "1699351500.0"
        },
        {
          "content": "Selected Answer: D\nIll go with D too, less components, less configurations",
          "poster": "AWSdeveloper08",
          "comment_id": "1020600",
          "timestamp": "1695974700.0",
          "upvote_count": "3"
        },
        {
          "poster": "beanxyz",
          "content": "Selected Answer: D\nB is wrong because SSM document is used to run on managed instances so definitely more efficient than lambda.\n\nC is wrong because although this solution should work, we need to write a lambda script to invoke the runbook, while in D we don't need to do it",
          "timestamp": "1694083740.0",
          "upvote_count": "2",
          "comment_id": "1001436"
        },
        {
          "upvote_count": "2",
          "comment_id": "987312",
          "poster": "beanxyz",
          "timestamp": "1692698100.0",
          "content": "I think both C and D will work, but the question is to chose the most efficient way, so I pickup D."
        },
        {
          "upvote_count": "7",
          "comment_id": "973889",
          "timestamp": "1691329860.0",
          "content": "Selected Answer: B\nThe target of SNS can be a lamda function. The question is asking for most efficient and lease overhead. Why go extra effort of creating additional stuff when already the SNS is being sent to the engineer. Just add extra subscription to that SNS topic to a lamda function. in the lamda function.\n\nThe target of SNS can be a lamda function: https://docs.aws.amazon.com/sns/latest/dg/sns-event-destinations.html",
          "poster": "Chetantest07"
        },
        {
          "upvote_count": "1",
          "comment_id": "964557",
          "poster": "Aja1",
          "timestamp": "1690450860.0",
          "content": "OPTION C:\n D involves using Amazon Event Bridge, which might introduce additional complexity compared to using the direct integration of SNS topic and Lambda function."
        },
        {
          "timestamp": "1689785880.0",
          "content": "Selected Answer: D\nAn EC2 instance running the Apache HTTP Server and the CloudWatch agent proctstat plugin that monitors the httpd process.\nWhen the httpd process is stopped, CloudWatch raises an alarm and sends an event to EventBridge.\nEventBridge receives the CloudWatch event that matches the event pattern in the predefined rule. EventBridge sends the event to the specified target (Systems Manager) and triggers the action defined in the rule.\nThe executeAwsApi automation action calls the SendCommand API action that includes the EC2 instance ID and the SSM document (runbook) to the SSM Agent running on the EC2 instance.\nSSM Agent executes the automation (runbook) on the EC2 instance to restart the httpd process.",
          "comment_id": "956836",
          "upvote_count": "3",
          "poster": "haazybanj"
        },
        {
          "timestamp": "1688547420.0",
          "content": "Selected Answer: D\nD agree with sb333 as Lamnda creates extra complexity while EventBridge triggers an AWS Systems Manager Automation runbook that runs a script to restart the application on the instances when the CloudWatch alarm enters ALARM state",
          "comment_id": "943502",
          "upvote_count": "2",
          "poster": "Blueee"
        },
        {
          "comment_id": "941000",
          "poster": "csG13",
          "content": "Why not B?",
          "timestamp": "1688310600.0",
          "upvote_count": "1"
        },
        {
          "upvote_count": "1",
          "comment_id": "940928",
          "timestamp": "1688304960.0",
          "content": "Selected Answer: C\nC make sense",
          "poster": "pepecastr0"
        },
        {
          "poster": "rhinozD",
          "upvote_count": "3",
          "timestamp": "1686832440.0",
          "comments": [
            {
              "upvote_count": "1",
              "content": "SNS is already created\n\n\"The application engineer manually restarts the application on the instances after the application engineer receives a notification from the SNS topic.\"",
              "poster": "pepecastr0",
              "comments": [
                {
                  "upvote_count": "1",
                  "content": "But Lambda is not. You would have to program a Lambda function. Using EventBridge does not require any coding at all. Simple to remove the SNS (or still use it if you want to be notified) than to create a Lambda function to do something that EventBridge can do natively.",
                  "timestamp": "1688349720.0",
                  "comment_id": "941370",
                  "poster": "sb333"
                }
              ],
              "timestamp": "1688304900.0",
              "comment_id": "940923"
            }
          ],
          "comment_id": "924175",
          "content": "Selected Answer: D\nI don't see any need for lambda or SNS here unless you have to stick with them.\nD meets the requirements."
        },
        {
          "timestamp": "1684176900.0",
          "upvote_count": "3",
          "content": "Selected Answer: D\nThe reason why not C : it introduces an additional layer of complexity and may not be as operationally efficient as option D. The AWS Lambda function serves as an intermediary between the SNS topic and the Systems Manager Automation runbook. The SNS topic sends a notification to the Lambda function, which then invokes the runbook. This indirect invocation introduces an extra step and potential latency in the application restart process. But as per Answer D, we can create an EventBridge rule that reacts to the CloudWatch alarm entering the ALARM state directly.",
          "poster": "2pk",
          "comment_id": "898640",
          "comments": []
        },
        {
          "content": "Selected Answer: D\nD. Configure an AWS Systems Manager Automation runbook that runs a script to restart the application on the instances. Configure an Amazon EventBridge rule that reacts when the CloudWatch alarm enters ALARM state. Specify the runbook as a target of the rule.\n\nThis solution meets the requirements in the most operationally efficient manner by automating the application restart process on the instances without restarting them. When the CloudWatch alarm enters the ALARM state, the EventBridge rule is triggered, which in turn invokes the Systems Manager Automation runbook that contains the script to restart the application on the instances.",
          "poster": "PhuocT",
          "upvote_count": "4",
          "timestamp": "1683453240.0",
          "comment_id": "891281"
        },
        {
          "timestamp": "1683123900.0",
          "comment_id": "888605",
          "content": "Selected Answer: C\nC for me",
          "poster": "Jeanphi72",
          "upvote_count": "2"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:20:18.300Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "XVEEvzIJ601s4JBqrSgC",
      "question_number": 333,
      "page": 67,
      "question_text": "A DevOps engineer at a company is supporting an AWS environment in which all users use AWS IAM Identity Center (AWS Single Sign-On). The company wants to immediately disable credentials of any new IAM user and wants the security team to receive a notification.\nWhich combination of steps should the DevOps engineer take to meet these requirements? (Choose three.)",
      "choices": {
        "F": "Create an Amazon Simple Queue Service (Amazon SQS) queue that is a target of the Lambda function. Subscribe the security team's group email address to the queue.",
        "D": "Create an AWS Lambda function that is a target of the EventBridge rule. Configure the Lambda function to delete the login profiles that are associated with the IAM user.",
        "E": "Create an Amazon Simple Notification Service (Amazon SNS) topic that is a target of the EventBridge rule. Subscribe the security team's group email address to the topic.",
        "B": "Create an Amazon EventBridge rule that reacts to an IAM GetLoginProfile API call in AWS CloudTrail.",
        "C": "Create an AWS Lambda function that is a target of the EventBridge rule. Configure the Lambda function to disable any access keys and delete the login profiles that are associated with the IAM user.",
        "A": "Create an Amazon EventBridge rule that reacts to an IAM CreateUser API call in AWS CloudTrail."
      },
      "correct_answer": "ACE",
      "answer_ET": "ACE",
      "answers_community": [
        "ACE (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/108355-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-05-03 06:32:00",
      "unix_timestamp": 1683088320,
      "discussion_count": 8,
      "discussion": [
        {
          "poster": "mrjaehong",
          "comment_id": "1311128",
          "timestamp": "1731480120.0",
          "content": "The IAM user that was created cannot have an access key from the beginning. You need to log in and get an access key.",
          "upvote_count": "1"
        },
        {
          "upvote_count": "1",
          "content": "Took the test 4/15 and passed. Almost all of the questions appeared. \nACE is correct.",
          "timestamp": "1713383580.0",
          "poster": "0b005fc",
          "comment_id": "1197433"
        },
        {
          "upvote_count": "4",
          "content": "ACE are correct: <disable credentials of any new IAM user> means disable all access key and profile related to the user. <the security team to receive a notification> means SNS\nB: GetLoginProfile API is not equal to creating new user\nD: we should delete all access key and profile related to the user, not just profile\nF: we need SNS, not SQS",
          "poster": "thanhnv142",
          "timestamp": "1706865480.0",
          "comment_id": "1138359"
        },
        {
          "poster": "khchan123",
          "comment_id": "1125084",
          "timestamp": "1705504500.0",
          "content": "Selected Answer: ACE\nAnswer ACE",
          "upvote_count": "2"
        },
        {
          "comment_id": "1117411",
          "poster": "yuliaqwerty",
          "upvote_count": "1",
          "timestamp": "1704797280.0",
          "content": "Answer ACE"
        },
        {
          "content": "Selected Answer: ACE\nNo Brainer",
          "poster": "Snape",
          "comment_id": "950312",
          "timestamp": "1689221280.0",
          "upvote_count": "3"
        },
        {
          "comment_id": "888602",
          "content": "Selected Answer: ACE\nMy answer ACE",
          "timestamp": "1683123720.0",
          "poster": "Jeanphi72",
          "upvote_count": "4"
        },
        {
          "comment_id": "888141",
          "poster": "haazybanj",
          "timestamp": "1683088320.0",
          "upvote_count": "3",
          "content": "Selected Answer: ACE\nACE is the right answer"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:20:18.300Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "LCydM7wtgQYQrWlN2jX1",
      "question_number": 334,
      "page": 67,
      "question_text": "A company wants to set up a continuous delivery pipeline. The company stores application code in a private GitHub repository. The company needs to deploy the application components to Amazon Elastic Container Service (Amazon ECS). Amazon EC2, and AWS Lambda. The pipeline must support manual approval actions.\nWhich solution will meet these requirements?",
      "choices": {
        "C": "Use AWS CodePipeline with AWS Elastic Beanstalk as the deploy provider.",
        "B": "Use AWS CodePipeline with AWS CodeDeploy as the deploy provider.",
        "A": "Use AWS CodePipeline with Amazon ECS. Amazon EC2, and Lambda as deploy providers.",
        "D": "Use AWS CodeDeploy with GitHub integration to deploy the application."
      },
      "correct_answer": "B",
      "answer_ET": "B",
      "answers_community": [
        "B (80%)",
        "A (17%)",
        "3%"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/108356-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-05-03 06:33:00",
      "unix_timestamp": 1683088380,
      "discussion_count": 12,
      "discussion": [
        {
          "timestamp": "1683088380.0",
          "upvote_count": "9",
          "content": "Selected Answer: B\nB is correct",
          "poster": "haazybanj",
          "comment_id": "888142"
        },
        {
          "upvote_count": "7",
          "comment_id": "949583",
          "content": "Selected Answer: B\nBecause the Term \"The pipeline must support manual approval actions.\" \nThat is not possible without a pipeline :)",
          "poster": "Just_Ninja",
          "timestamp": "1689149760.0"
        },
        {
          "upvote_count": "1",
          "timestamp": "1735649820.0",
          "poster": "1rob",
          "content": "Selected Answer: B\nLambda is not defined as a deployment provider. Only as an \"invoke\" option. Amazon ECS is possible as deploy provider , check https://docs.aws.amazon.com/codedeploy/latest/userguide/welcome.html where it gives: CodeDeploy is a deployment service that automates application deployments to Amazon EC2 instances, on-premises instances, serverless Lambda functions, or Amazon ECS services. . So I go for B.",
          "comment_id": "1334809"
        },
        {
          "upvote_count": "1",
          "content": "Selected Answer: A\nOption B is partially correct but lacks native support for ECS deployments. AWS CodeDeploy is excellent for deploying to EC2 and Lambda, but it doesn't natively handle ECS deployments without additional configuration.",
          "timestamp": "1735087740.0",
          "poster": "Zdujgfr567783ff",
          "comment_id": "1331292"
        },
        {
          "comment_id": "1331291",
          "timestamp": "1735087620.0",
          "content": "Selected Answer: A\nasked chat GPT says a",
          "poster": "Zdujgfr567783ff",
          "upvote_count": "2"
        },
        {
          "timestamp": "1725192000.0",
          "upvote_count": "1",
          "content": "A is correct, CodeDeploy can't deploy the ECS",
          "poster": "hzaki",
          "comment_id": "1276039"
        },
        {
          "upvote_count": "1",
          "content": "A is correct",
          "comment_id": "1138388",
          "poster": "thanhnv142",
          "timestamp": "1706866800.0",
          "comments": [
            {
              "poster": "thanhnv142",
              "upvote_count": "1",
              "timestamp": "1707735000.0",
              "content": "Correction: B is correct",
              "comment_id": "1147995"
            }
          ]
        },
        {
          "comment_id": "1070137",
          "timestamp": "1699948680.0",
          "upvote_count": "4",
          "content": "Selected Answer: B\nThe solution for deploy ECS by codePipeline and codeDeploy\n\nCreate your CodeDeploy application and deployment group (ECS compute platform\nhttps://docs.aws.amazon.com/codepipeline/latest/userguide/tutorials-ecs-ecr-codedeploy.html#tutorials-ecs-ecr-codedeploy-deployment",
          "poster": "due"
        },
        {
          "timestamp": "1695110400.0",
          "upvote_count": "3",
          "poster": "RVivek",
          "comments": [
            {
              "poster": "z_inderjot",
              "upvote_count": "3",
              "timestamp": "1703391540.0",
              "content": "using codedeploy we can deploy to ecs and even can perform blue / green deployment. Codedeploy support all there of the deployment strategies",
              "comment_id": "1104443"
            }
          ],
          "content": "Selected Answer: A\nWhy not A ?\nB (Code depoly providr does not support ECS)\nD does not have \"codepipeleine\" and the question says \"\"The pipeline must support manual approval actions.\" \n\nSo A is the only feasible option",
          "comment_id": "1011087"
        },
        {
          "comments": [
            {
              "upvote_count": "1",
              "timestamp": "1693015140.0",
              "poster": "Radeeka",
              "comment_id": "990471",
              "content": "My bad, Above only support EC2 and OnPrem."
            }
          ],
          "upvote_count": "1",
          "content": "Selected Answer: D\nAnswer D.\nSource: https://docs.aws.amazon.com/codedeploy/latest/userguide/integrations-partners-github.html\nThe question is asking for a application code stored in a GitHub repository.",
          "comment_id": "985739",
          "timestamp": "1692530820.0",
          "poster": "Radeeka"
        },
        {
          "comment_id": "964576",
          "content": "option A with AWS CodePipeline and individual deployment actions for Amazon ECS, Amazon EC2, and AWS Lambda, along with support for manual approval actions, is the most suitable solution to meet the requirements of the continuous delivery pipeline.\nB mentions using AWS CodeDeploy as the deploy provider, but it does not explicitly mention support for deploying to Amazon ECS, Amazon EC2, and AWS Lambda. AWS CodeDeploy primarily focuses on deploying applications to Amazon EC2 instances, and while it does have support for AWS Lambda, it might not be as straightforward to use for deploying to Amazon ECS.",
          "poster": "Aja1",
          "timestamp": "1690452420.0",
          "upvote_count": "2",
          "comments": [
            {
              "comment_id": "964678",
              "content": "i think B is correct",
              "poster": "Aja1",
              "upvote_count": "1",
              "timestamp": "1690459200.0"
            }
          ]
        },
        {
          "content": "Selected Answer: B\nYou will need a deployment tool (CodeDeploy) for this. You cannot directly deploy via CodePipeline. Hence, B.",
          "comment_id": "946823",
          "upvote_count": "7",
          "poster": "habros",
          "timestamp": "1688867220.0"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:20:18.300Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "xA3KpQ2Rp578tw1rAxjC",
      "question_number": 335,
      "page": 67,
      "question_text": "A company has an application that runs on Amazon EC2 instances that are in an Auto Scaling group. When the application starts up. the application needs to process data from an Amazon S3 bucket before the application can start to serve requests.\nThe size of the data that is stored in the S3 bucket is growing. When the Auto Scaling group adds new instances, the application now takes several minutes to download and process the data before the application can serve requests. The company must reduce the time that elapses before new EC2 instances are ready to serve requests.\nWhich solution is the MOST cost-effective way to reduce the application startup time?",
      "choices": {
        "A": "Configure a warm pool for the Auto Scaling group with warmed EC2 instances in the Stopped state. Configure an autoscaling:EC2_INSTANCE_LAUNCHING lifecycle hook on the Auto Scaling group. Modify the application to complete the lifecycle hook when the application is ready to serve requests.",
        "D": "Increase the maximum instance count of the Auto Scaling group. Configure an autoscaling:EC2_INSTANCE_LAUNCHING lifecycle hook on the Auto Scaling group. Modify the application to complete the lifecycle hook and to place the new instance in the Standby state when the application is ready to serve requests.",
        "B": "Increase the maximum instance count of the Auto Scaling group. Configure an autoscaling:EC2_INSTANCE_LAUNCHING lifecycle hook on the Auto Scaling group. Modify the application to complete the lifecycle hook when the application is ready to serve requests.",
        "C": "Configure a warm pool for the Auto Scaling group with warmed EC2 instances in the Running state. Configure an autoscaling:EC2_INSTANCE_LAUNCHING lifecycle hook on the Auto Scaling group. Modify the application to complete the lifecycle hook when the application is ready to serve requests."
      },
      "correct_answer": "A",
      "answer_ET": "A",
      "answers_community": [
        "A (88%)",
        "13%"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/105586-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-04-08 14:33:00",
      "unix_timestamp": 1680957180,
      "discussion_count": 17,
      "discussion": [
        {
          "poster": "haazybanj",
          "comment_id": "885896",
          "content": "Selected Answer: A\nOption A is the most cost-effective solution. By configuring a warm pool of EC2 instances in the Stopped state, the company can reduce the time it takes for new instances to be ready to serve requests. When the Auto Scaling group launches a new instance, it can attach the stopped EC2 instance from the warm pool. The instance can then be started up immediately, rather than having to wait for the data to be downloaded and processed. This reduces the overall startup time for the application.\n\nOption C is also a solution that involves a warm pool of EC2 instances, but the instances are in the Running state. This means that they are already running and incurring costs, even though they are not currently serving requests. This is not a cost-effective solution.",
          "upvote_count": "18",
          "timestamp": "1682919420.0"
        },
        {
          "content": "Selected Answer: A\nkeywords: MOST cost-effective way to reduce the application startup time",
          "poster": "jamesf",
          "upvote_count": "2",
          "timestamp": "1721979900.0",
          "comment_id": "1255565"
        },
        {
          "timestamp": "1711373040.0",
          "poster": "stoy123",
          "comment_id": "1182493",
          "upvote_count": "1",
          "comments": [
            {
              "comment_id": "1204961",
              "timestamp": "1714553400.0",
              "poster": "Jay_2pt0_1",
              "content": "I thought this, as well, but A appears to be correct. See https://aws.amazon.com/blogs/compute/scaling-your-applications-faster-with-ec2-auto-scaling-warm-pools/",
              "upvote_count": "1"
            }
          ],
          "content": "Selected Answer: C\nC \" The company must reduce the time that elapses before new EC2 instances are ready to serve requests.\"!!!!!!!!!! this cannot happen with a stopped instance as it will still need to read the data from S3 upon startup,"
        },
        {
          "timestamp": "1709214840.0",
          "comment_id": "1162638",
          "poster": "Shasha1",
          "upvote_count": "1",
          "content": "A \nfor warm pool in the hibernated or stop status we will pay only for the attached EBS volume, therefore its much cost effective rather than running instance"
        },
        {
          "content": "Selected Answer: A\nWarm Pool allows instances to be set to a stopped state after performing any process (e.g., running initialization scripts, warm-up tasks, etc.).",
          "poster": "dzn",
          "timestamp": "1708831140.0",
          "upvote_count": "1",
          "comment_id": "1158319"
        },
        {
          "poster": "thanhnv142",
          "content": "A is correct: the question says <the application needs to process data from an Amazon S3 bucket before the application can start to serve requests> but <The size of the data that is stored in the S3 bucket is growing>. This means we should maintain a warm pool for EC2 so that they are always ready to process data (reduce the time that elapses before new EC2 instances are ready)\nB and D: no mention of warmpool\nC: If the instance is up and running, no need to configure warm pool",
          "timestamp": "1706867100.0",
          "comment_id": "1138394",
          "upvote_count": "1"
        },
        {
          "content": "Selected Answer: A\nAnswer is A, the question is cost-effective, and even with A you will have less wait time to download the S3 data, it will download the delta from the warm up process to ready to join to ASG",
          "comments": [
            {
              "upvote_count": "1",
              "poster": "Jaguaroooo",
              "timestamp": "1704513180.0",
              "comment_id": "1114960",
              "content": "A&C are both good in terms of solutions, however, the caveat here is the \"cost-effective\" solution and that's why I agree with A. \nhttps://aws.amazon.com/blogs/compute/scaling-your-applications-faster-with-ec2-auto-scaling-warm-pools/"
            }
          ],
          "poster": "zolthar_z",
          "timestamp": "1700753460.0",
          "upvote_count": "2",
          "comment_id": "1078531"
        },
        {
          "content": "Selected Answer: A\nexcerpt from the url: https://aws.amazon.com/blogs/compute/scaling-your-applications-faster-with-ec2-auto-scaling-warm-pools/\nEC2 Auto Scaling Warm Pools works by launching a configured number of EC2 instances in the background, allowing any lengthy application initialization processes to run as necessary, and then stopping those instances until they are needed",
          "poster": "RVivek",
          "upvote_count": "1",
          "comment_id": "1005554",
          "timestamp": "1694507340.0"
        },
        {
          "content": "Selected Answer: A\nA is the most cost-effective solution. Besides, when the warm EC2 was created, it already downloaded the contents from S3 so the next time when it started, it would just download any new files from S3. ( e.g s3 sync )",
          "timestamp": "1692698760.0",
          "upvote_count": "1",
          "comment_id": "987326",
          "poster": "beanxyz"
        },
        {
          "timestamp": "1692066960.0",
          "comment_id": "981230",
          "content": "C is right.\nplease carefully check the question:\nThe company must reduce the time that elapses before new EC2 instances are ready to serve requests.\nWhen the application starts up. the application needs to process data from an Amazon S3 bucket before the application can start to serve requests.",
          "upvote_count": "2",
          "poster": "ixdb"
        },
        {
          "comment_id": "973959",
          "upvote_count": "3",
          "content": "Selected Answer: C\nI understand the question is asking for the most cost-effective. keeping it stopped state is most cost efficient but it would not work because in the question it also states that, \"When the application starts up. the application needs to process data\" and to process that data takes time. If the Ec2 instance is stopped then started at the time of need, then again it will take time to process the data, right? so in this scenario, the EC2 instance need to be running.",
          "timestamp": "1691334900.0",
          "poster": "Chetantest07"
        },
        {
          "timestamp": "1690306680.0",
          "upvote_count": "2",
          "comment_id": "962963",
          "content": "I think it should be C, as the A option would not be effective. Coming from the instance stop state the application will start up again and need to process the data from S3 bucket.",
          "poster": "Suyx"
        },
        {
          "content": "Selected Answer: A\nWarm pool with stopped state is most cost efficient option",
          "timestamp": "1689222600.0",
          "upvote_count": "3",
          "comment_id": "950325",
          "poster": "Snape"
        },
        {
          "poster": "pepecastr0",
          "comment_id": "940931",
          "content": "Selected Answer: A\nA - Keep it stopped until you need it to save money",
          "timestamp": "1688305260.0",
          "upvote_count": "1"
        },
        {
          "upvote_count": "1",
          "comment_id": "870470",
          "content": "Selected Answer: C\nWhile A can also be a cost-effective solution, C is the MOST cost-effective solution because it utilizes Amazon S3 Transfer Acceleration, which is a feature that enables fast, easy, and secure transfers of files over the internet between Amazon S3 buckets and EC2 instances located in different regions or across the internet. By using S3 Transfer Acceleration, the data transfer speed can be increased significantly, which can reduce the time that elapses before new EC2 instances are ready to serve requests.\n\nIn contrast, A suggests using a larger instance size with more CPU and network capacity, which can be more expensive than the current instance size. Moreover, this approach may not be scalable in the long run since as the data in the S3 bucket continues to grow, the instance size may need to be further increased, which can incur more costs. Therefore, while A can also be a viable solution, C is the most cost-effective and scalable solution.",
          "poster": "jqso234",
          "timestamp": "1681505340.0"
        },
        {
          "poster": "ma_rio",
          "comment_id": "869166",
          "timestamp": "1681368840.0",
          "content": "Selected Answer: A\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-warm-pools.html\nKeeping instances in a Stopped state is an effective way to minimize costs.",
          "upvote_count": "4"
        },
        {
          "timestamp": "1680957180.0",
          "content": "Selected Answer: A\nA for me to decrease costs",
          "poster": "Dimidrol",
          "comment_id": "864680",
          "upvote_count": "2"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:20:18.300Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "4iYxGkalLztpSP7RJAWq",
      "question_number": 336,
      "page": 68,
      "question_text": "A company is using an AWS CodeBuild project to build and package an application. The packages are copied to a shared Amazon S3 bucket before being deployed across multiple AWS accounts.\nThe buildspec.yml file contains the following:\n//IMG//\n\nThe DevOps engineer has noticed that anybody with an AWS account is able to download the artifacts.\nWhat steps should the DevOps engineer take to stop this?",
      "choices": {
        "C": "Create an S3 bucket policy that grants read access to the relevant AWS accounts and denies read access to the principal “*”.",
        "D": "Modify the post_build command to remove --acl authenticated-read and configure a bucket policy that allows read access to the relevant AWS accounts only.",
        "B": "Configure a default ACL for the S3 bucket that defines the set of authenticated users as the relevant AWS accounts only and grants read-only access.",
        "A": "Modify the post_build command to use --acl public-read and configure a bucket policy that grants read access to the relevant AWS accounts only."
      },
      "correct_answer": "D",
      "answer_ET": "D",
      "answers_community": [
        "D (82%)",
        "A (18%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/108079-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [
        "https://img.examtopics.com/aws-certified-devops-engineer-professional-dop-c02/image5.png"
      ],
      "answer_images": [],
      "timestamp": "2023-05-01 07:39:00",
      "unix_timestamp": 1682919540,
      "discussion_count": 7,
      "discussion": [
        {
          "comment_id": "885897",
          "poster": "haazybanj",
          "content": "Selected Answer: D\nD is correct",
          "upvote_count": "14",
          "timestamp": "1682919540.0"
        },
        {
          "comment_id": "987341",
          "content": "Selected Answer: A\n--acl authenticated-read means any authenticated users can read the S3 bucket. We should remove it and configure the bucket policy to explicitly grant access",
          "upvote_count": "5",
          "timestamp": "1692700260.0",
          "comments": [
            {
              "comment_id": "987344",
              "upvote_count": "6",
              "timestamp": "1692700260.0",
              "poster": "beanxyz",
              "content": "I mean D..."
            }
          ],
          "poster": "beanxyz"
        },
        {
          "poster": "jamesf",
          "comment_id": "1255567",
          "content": "Selected Answer: D\n\"--acl authenticated-read\" means any authenticated users can read the S3 bucket. We should remove it and configure the bucket policy to explicitly grant access",
          "timestamp": "1721980080.0",
          "upvote_count": "3"
        },
        {
          "comment_id": "1196756",
          "poster": "zijo",
          "upvote_count": "2",
          "timestamp": "1713294000.0",
          "content": "D is the answer\n\nACL-authenticated users: This refers to any user who has successfully authenticated with AWS credentials, including IAM users and federated users. It does not include anonymous users (public access).\nIt's generally recommended to use bucket policies for access control in S3 rather than ACLs. Bucket policies offer more granular control and better security practices. You can achieve \"acl-authenticated reads\" access using a bucket policy as well."
        },
        {
          "content": "Selected Answer: D\n`remove --acl authenticated-read` is required to fulfill the requirement.",
          "comment_id": "1158329",
          "upvote_count": "4",
          "poster": "dzn",
          "timestamp": "1708833480.0"
        },
        {
          "upvote_count": "1",
          "poster": "thanhnv142",
          "timestamp": "1706869560.0",
          "content": "B is correct: In the \"buildspec.yml file\", we see that there is \"--acl authenticated-read\". This allow all aws users who successfully authen to AWS can download the file. To restrict access, we need to modify ACL that only grant access to some specific users. \nNote that we should not use bucket policy because it will affect all ojbects in the bucket (that is why it is called BUCKET policy). We only need to restrict acess to an object, then ACL is the right choice.\nA is incorrect: Use use --acl public-read means we allow all user to access the object \nC and D: Use bucket policy, which is incorrect",
          "comment_id": "1138423"
        },
        {
          "content": "Selected Answer: D\nD is correct",
          "comment_id": "1078533",
          "poster": "zolthar_z",
          "timestamp": "1700753580.0",
          "upvote_count": "2"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:20:29.019Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "T9fpOVO3kpp1AYqfcSbH",
      "question_number": 337,
      "page": 68,
      "question_text": "A company has developed a serverless web application that is hosted on AWS. The application consists of Amazon S3. Amazon API Gateway, several AWS Lambda functions, and an Amazon RDS for MySQL database. The company is using AWS CodeCommit to store the source code. The source code is a combination of AWS Serverless Application Model (AWS SAM) templates and Python code.\nA security audit and penetration test reveal that user names and passwords for authentication to the database are hardcoded within CodeCommit repositories. A DevOps engineer must implement a solution to automatically detect and prevent hardcoded secrets.\nWhat is the MOST secure solution that meets these requirements?",
      "choices": {
        "B": "Associate the CodeCommit repository with Amazon CodeGuru Reviewer. Manually check the code review for any recommendations. Choose the option to protect the secret. Update the SAM templates and the Python code to pull the secret from AWS Secrets Manager.",
        "C": "Enable Amazon CodeGuru Profiler. Decorate the handler function with @with_lambda_profiler(). Manually review the recommendation report. Choose the option to protect the secret. Update the SAM templates and the Python code to pull the secret from AWS Secrets Manager.",
        "D": "Associate the CodeCommit repository with Amazon CodeGuru Reviewer. Manually check the code review for any recommendations. Write the secret to AWS Systems Manager Parameter Store as a string. Update the SAM templates and the Python code to pull the secret from Parameter Store.",
        "A": "Enable Amazon CodeGuru Profiler. Decorate the handler function with @with_lambda_profiler(). Manually review the recommendation report. Write the secret to AWS Systems Manager Parameter Store as a secure string. Update the SAM templates and the Python code to pull the secret from Parameter Store."
      },
      "correct_answer": "B",
      "answer_ET": "B",
      "answers_community": [
        "B (96%)",
        "4%"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/108080-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-05-01 07:42:00",
      "unix_timestamp": 1682919720,
      "discussion_count": 6,
      "discussion": [
        {
          "poster": "haazybanj",
          "upvote_count": "12",
          "content": "Selected Answer: B\nB\nThe MOST secure solution that meets the requirement of automatically detecting and preventing hardcoded secrets is to use AWS CodeGuru Reviewer to check the code for any hardcoded secrets, and then update the SAM templates and Python code to retrieve the secrets from AWS Secrets Manager.\n\nOption B is the correct answer. By associating the CodeCommit repository with Amazon CodeGuru Reviewer, the code can be checked for any hardcoded secrets during code reviews. When a hardcoded secret is detected, CodeGuru Reviewer will recommend updating the code to retrieve the secret from a secure storage service like AWS Secrets Manager. The DevOps engineer can choose the option to protect the secret and then update the SAM templates and Python code to retrieve the secret from AWS Secrets Manager instead of hardcoding it in the code.",
          "timestamp": "1698824520.0",
          "comment_id": "885900"
        },
        {
          "timestamp": "1702652220.0",
          "content": "Selected Answer: B\nB is correct.\nCodeGuru Reviewer for security problems.\nAmazon CodeGuru Profiler is for performance.",
          "upvote_count": "8",
          "poster": "rhinozD",
          "comment_id": "924199"
        },
        {
          "poster": "thanhnv142",
          "timestamp": "1722589380.0",
          "upvote_count": "1",
          "comment_id": "1138461",
          "content": "B is correct: <implement a solution to automatically detect and prevent hardcoded secrets> means we need CodeGuru reviewer to analyze the code and uncover hardcoded credentials. \nA and C: no mention of CodeGuru reviewer\nD: using System Manager Parameter store is a good method to avoid hardcoded credentials. However, the question requires <the MOST secure solution>, so we should use AWS secret manager (option B). It costs more than Para store, but more secure."
        },
        {
          "content": "Selected Answer: C\nI'd say it's C, because the system to examine includes Python code and CodeGuru profiles for Python needs the decorator: https://docs.aws.amazon.com/codeguru/latest/profiler-ug/python-lambda.html",
          "poster": "a16a848",
          "upvote_count": "1",
          "timestamp": "1719290880.0",
          "comment_id": "1105066"
        },
        {
          "timestamp": "1706362320.0",
          "poster": "Aja1",
          "comment_id": "964652",
          "comments": [
            {
              "poster": "Aja1",
              "comment_id": "975793",
              "upvote_count": "6",
              "timestamp": "1707412320.0",
              "content": "Sorry B\n\nAmazon CodeGuru Reviewer and Amazon CodeGuru Profiler are both tools that can be used to improve the quality and security of your code. However, they have different strengths and weaknesses.\n\nCodeGuru Reviewer is a static code analysis tool that can be used to find potential defects in your code. It can scan your code for hardcoded secrets, security vulnerabilities, and other potential problems. CodeGuru Reviewer can also provide recommendations on how to fix the problems that it finds.\n\nCodeGuru Profiler is a dynamic code analysis tool that can be used to understand how your code performs. It can track the performance of your code, identify bottlenecks, and suggest ways to improve performance. CodeGuru Profiler can also be used to find potential memory leaks and other performance problems."
            }
          ],
          "content": "option C\nhttps://docs.aws.amazon.com/codeguru/latest/profiler-ug/python-lambda-command-line.html",
          "upvote_count": "1"
        },
        {
          "comment_id": "927948",
          "upvote_count": "2",
          "poster": "MarDog",
          "content": "Selected Answer: B\nDefinitely B.",
          "timestamp": "1703027220.0"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:20:29.019Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "vd9qdNYskI6Rdbxi9D5u",
      "question_number": 338,
      "page": 68,
      "question_text": "A company is using Amazon S3 buckets to store important documents. The company discovers that some S3 buckets are not encrypted. Currently, the company’s IAM users can create new S3 buckets without encryption. The company is implementing a new requirement that all S3 buckets must be encrypted.\n\nA DevOps engineer must implement a solution to ensure that server-side encryption is enabled on all existing S3 buckets and all new S3 buckets. The encryption must be enabled on new S3 buckets as soon as the S3 buckets are created. The default encryption type must be 256-bit Advanced Encryption Standard (AES-256).\n\nWhich solution will meet these requirements?",
      "choices": {
        "D": "Configure an IAM policy that denies the s3:CreateBucket action if the s3:x-amz-server-side-encryption condition key has a value that is not AES-256. Create an IAM group for all the company’s IAM users. Associate the IAM policy with the IAM group.",
        "A": "Create an AWS Lambda function that is invoked periodically by an Amazon EventBridge scheduled rule. Program the Lambda function to scan all current S3 buckets for encryption status and to set AES-256 as the default encryption for any S3 bucket that does not have an encryption configuration.",
        "C": "Create an AWS Lambda function that is invoked by an Amazon EventBridge event rule. Define the rule with an event pattern that matches the creation of new S3 buckets. Program the Lambda function to parse the EventBridge event, check the configuration of the S3 buckets from the event, and set AES-256 as the default encryption.",
        "B": "Set up and activate the s3-bucket-server-side-encryption-enabled AWS Config managed rule. Configure the rule to use the AWS-EnableS3BucketEncryption AWS Systems Manager Automation runbook as the remediation action. Manually run the re-evaluation process to ensure that existing S3 buckets are compliant."
      },
      "correct_answer": "B",
      "answer_ET": "B",
      "answers_community": [
        "B (88%)",
        "13%"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/108608-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-05-06 07:00:00",
      "unix_timestamp": 1683349200,
      "discussion_count": 12,
      "discussion": [
        {
          "upvote_count": "13",
          "content": "B caters to both existing and new buckets. \nC is triggered on when new bucket is created, existing buckets are not handled by the event.",
          "poster": "paali",
          "timestamp": "1685194320.0",
          "comment_id": "908018"
        },
        {
          "poster": "Zoe_zoe",
          "content": "Selected Answer: B\nB to me",
          "upvote_count": "10",
          "timestamp": "1683349200.0",
          "comment_id": "890488"
        },
        {
          "comment_id": "1228782",
          "upvote_count": "2",
          "poster": "Gomer",
          "content": "Selected Answer: C\nI think neither \"B\" or \"C\" is complete solution. They both need to be done to deal with both existing and new buckets.\nA carefull reading of the question doesn't preclude the need to do both.\nHowever, the specific and emphasized criteria of enabling encryption \"as soon as the S3 buckets are created\" can only be done by \"C\" (event driven action)\nI think this may be a trick question. I'm very confident they are defining an event driven action as part of the solution, and only \"C\" provides that.\n\nB: (NO) \"Manually run the re-evaluation process to ensure that existing S3 buckets are compliant.\"\nComment: Doesn't achieve \"encryption must be enabled on new S3 buckets as soon as the S3 buckets are created.\"",
          "timestamp": "1718158920.0"
        },
        {
          "poster": "dzn",
          "upvote_count": "1",
          "comment_id": "1158334",
          "timestamp": "1708834560.0",
          "content": "Selected Answer: B\n`s3-bucket-server-side-encryption-enabled` checks if your Amazon S3 bucket either has the Amazon S3 default encryption enabled or that the Amazon S3 bucket policy explicitly denies put-object requests without server side encryption that uses AES-256 or AWS Key Management Service."
        },
        {
          "content": "A is correct: <implement a solution to ensure that server-side encryption is enabled on all existing S3 buckets and all new S3 buckets>: We can use lambda to configure all S3. Use Eventbridge to schedule-run lambda. \nB: This option uses AWS config rule to activate AWS-EnableS3BucketEncryption AWS Systems Manager Automation runbook, which is incorrect. Remember that AWS config have no action and cannot trigger anything. It only collect data and report. Additionally, this option does not mention actions to new S3 bucket\nC: <define the rule with an event pattern that matches the creation of new S3 buckets> means that this only affect newly-created bucket, not existing ones. \nD: No mention of enforcing encryption on S3\n\nNote: Should not use chatgpt for this exam, its answers are mostly wrong",
          "comment_id": "1138480",
          "comments": [
            {
              "upvote_count": "1",
              "poster": "thanhnv142",
              "comment_id": "1149042",
              "timestamp": "1707816780.0",
              "content": "Correct: D"
            }
          ],
          "timestamp": "1706873100.0",
          "upvote_count": "2",
          "poster": "thanhnv142"
        },
        {
          "upvote_count": "5",
          "comment_id": "1116589",
          "timestamp": "1704715440.0",
          "poster": "davdan99",
          "content": "Selected Answer: B\nAnswer is B\n\nhttps://docs.aws.amazon.com/config/latest/developerguide/s3-bucket-server-side-encryption-enabled.html"
        },
        {
          "timestamp": "1704545460.0",
          "poster": "Jaguaroooo",
          "comment_id": "1115175",
          "content": "I would have chose B over D because aws config can do this with lambda.",
          "upvote_count": "1"
        },
        {
          "content": "A has automation. I didn't like B: because of this statement: Manually run the re-evaluation process to ensure that existing S3 buckets are compliant.",
          "timestamp": "1704545400.0",
          "poster": "Jaguaroooo",
          "comment_id": "1115174",
          "upvote_count": "1"
        },
        {
          "upvote_count": "1",
          "content": "Amazon S3 Encrypts New Objects By Default\nhttps://aws.amazon.com/blogs/aws/amazon-s3-encrypts-new-objects-by-default/#:~:text=At%20AWS%2C%20security%20is%20the,specify%20a%20different%20encryption%20option.",
          "timestamp": "1703521080.0",
          "comment_id": "1105390",
          "poster": "Jamshif01"
        },
        {
          "upvote_count": "1",
          "poster": "zenith_cloud",
          "comment_id": "1054472",
          "content": "Selected Answer: B\nB to me.\nAWS Config can monitor resource compliance against desired configurations. The managed rule s3-bucket-server-side-encryption-enabled checks whether Amazon S3 buckets have server-side encryption enabled. The AWS Systems Manager Automation runbook, AWS-EnableS3BucketEncryption, can be used as a remediation action to enable default encryption. This solution would also work for new buckets as soon as they're created, making it an effective solution.",
          "timestamp": "1698317160.0"
        },
        {
          "content": "Selected Answer: B\nB is right.\nDoable solution for new buckets as well as existing buckets.",
          "poster": "rhinozD",
          "comment_id": "924203",
          "timestamp": "1686834060.0",
          "upvote_count": "4"
        },
        {
          "poster": "marcoforexam",
          "content": "Selected Answer: C\nOption C meets the requirement of modifying the policy immediately after creating the bucket.",
          "upvote_count": "1",
          "comments": [],
          "comment_id": "891257",
          "timestamp": "1683449760.0"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:20:29.019Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "0OrByeJUjcm2TlSguV93",
      "question_number": 339,
      "page": 68,
      "question_text": "A DevOps engineer is architecting a continuous development strategy for a company’s software as a service (SaaS) web application running on AWS. For application and security reasons, users subscribing to this application are distributed across multiple Application Load Balancers (ALBs), each of which has a dedicated Auto Scaling group and fleet of Amazon EC2 instances. The application does not require a build stage, and when it is committed to AWS CodeCommit, the application must trigger a simultaneous deployment to all ALBs, Auto Scaling groups, and EC2 fleets.\n\nWhich architecture will meet these requirements with the LEAST amount of configuration?",
      "choices": {
        "B": "Create a single AWS CodePipeline pipeline that deploys the application using a single AWS CodeDeploy application and single deployment group.",
        "D": "Create an AWS CodePipeline pipeline for each ALB-Auto Scaling group pair that deploys the application using an AWS CodeDeploy application and deployment group created for the same ALB-Auto Scaling group pair.",
        "A": "Create a single AWS CodePipeline pipeline that deploys the application in parallel using unique AWS CodeDeploy applications and deployment groups created for each ALB-Auto Scaling group pair.",
        "C": "Create a single AWS CodePipeline pipeline that deploys the application in parallel using a single AWS CodeDeploy application and unique deployment group for each ALB-Auto Scaling group pair."
      },
      "correct_answer": "C",
      "answer_ET": "C",
      "answers_community": [
        "C (77%)",
        "B (19%)",
        "4%"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/108676-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-05-07 11:30:00",
      "unix_timestamp": 1683451800,
      "discussion_count": 11,
      "discussion": [
        {
          "content": "Selected Answer: C\nYou can just use one CodeDeploy application and multiple deployment groups in this case.\nso C.",
          "timestamp": "1686834360.0",
          "upvote_count": "14",
          "comment_id": "924210",
          "poster": "rhinozD"
        },
        {
          "upvote_count": "1",
          "poster": "nickp84",
          "timestamp": "1747247400.0",
          "content": "Selected Answer: C\nB. Single deployment group:\n Won’t support multiple Auto Scaling groups and ALBs unless they’re all in one group, which is not the case here.\n Cannot handle independent ALB/ASG pairs cleanly.\nDoesn’t scale well",
          "comment_id": "1568907"
        },
        {
          "comment_id": "1317420",
          "upvote_count": "1",
          "content": "Selected Answer: C\nI was about to vote B since the link from xdkonorek2 shows that one deployment can include up to 10 ELBs. Nevertheless the question says multiple instead of 10.",
          "poster": "steli0",
          "timestamp": "1732529640.0"
        },
        {
          "content": "Selected Answer: C\nOption B not feasible as it assumes a single deployment group can manage deployments across multiple ALBs and Auto Scaling groups simultaneously, which is not supported.",
          "upvote_count": "1",
          "comment_id": "1244621",
          "poster": "trungtd",
          "timestamp": "1720484700.0"
        },
        {
          "content": "Selected Answer: B\nhttps://aws.amazon.com/about-aws/whats-new/2023/10/aws-codedeploy-multiple-load-balancers-amazon-ec2-applications/",
          "upvote_count": "2",
          "comment_id": "1232548",
          "poster": "xdkonorek2",
          "timestamp": "1718732220.0"
        },
        {
          "comments": [
            {
              "timestamp": "1713979080.0",
              "poster": "that1guy",
              "content": "Also B for me, you can target multiple ASGs as part of one deployment: https://docs.aws.amazon.com/codedeploy/latest/APIReference/API_TargetInstances.html#CodeDeploy-Type-TargetInstances-autoScalingGroups",
              "comment_id": "1201537",
              "upvote_count": "2"
            }
          ],
          "upvote_count": "3",
          "comment_id": "1199630",
          "timestamp": "1713700380.0",
          "poster": "xdkonorek2",
          "content": "Selected Answer: B\nB is the simplest :)\n\nDuring creation of deployment group:\n1. select \"Amazon EC2 Auto Scaling groups\"\n2. tip appears: \"You can select up to 10 Amazon EC2 Auto Scaling groups to deploy your application revision to.\""
        },
        {
          "poster": "thanhnv142",
          "timestamp": "1706889360.0",
          "upvote_count": "4",
          "content": "C is correct: <the application must trigger a simultaneous deployment> means deployment in parallel \nB and D: no mention of deployment in parallel\nA: <unique AWS CodeDeploy applications and deployment groups created for each ALB-Auto Scaling group pair> means there are multiple AWS CodeDeploy applications and deployment groups for each site, which is unnecessary",
          "comment_id": "1138695"
        },
        {
          "poster": "Aja1",
          "comment_id": "964683",
          "content": "Option C\ndeployed in parallel to all ALB-Auto Scaling group pairs simultaneously. This means that the deployment process is efficient and fast, and all ALBs and Auto Scaling groups receive updates at the same time.",
          "upvote_count": "1",
          "timestamp": "1690459500.0"
        },
        {
          "timestamp": "1684040580.0",
          "upvote_count": "3",
          "content": "C is the correct answer.",
          "comment_id": "897257",
          "poster": "devnv"
        },
        {
          "content": "Selected Answer: C\nC is correct.",
          "comment_id": "893103",
          "upvote_count": "3",
          "poster": "ParagSanyashiv",
          "timestamp": "1683636960.0"
        },
        {
          "timestamp": "1683451800.0",
          "comment_id": "891271",
          "poster": "marcoforexam",
          "content": "Selected Answer: A\nA\nAWS CodePipeline can target multiple AWS CodeDeploy applications.",
          "upvote_count": "1"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:20:29.019Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "LanZ4Lesl7nNaj5qqfLv",
      "question_number": 340,
      "page": 68,
      "question_text": "A company is hosting a static website from an Amazon S3 bucket. The website is available to customers at example.com. The company uses an Amazon Route 53 weighted routing policy with a TTL of 1 day. The company has decided to replace the existing static website with a dynamic web application. The dynamic web application uses an Application Load Balancer (ALB) in front of a fleet of Amazon EC2 instances.\n\nOn the day of production launch to customers, the company creates an additional Route 53 weighted DNS record entry that points to the ALB with a weight of 255 and a TTL of 1 hour. Two days later, a DevOps engineer notices that the previous static website is displayed sometimes when customers navigate to example.com.\n\nHow can the DevOps engineer ensure that the company serves only dynamic content for example.com?",
      "choices": {
        "B": "Update the weighted DNS record entry that points to the S3 bucket. Apply a weight of 0. Specify the domain reset option to propagate changes immediately.",
        "C": "Configure webpage redirect requests on the S3 bucket with a hostname that redirects to the ALB.",
        "D": "Remove the weighted DNS record entry that points to the S3 bucket from the example.com hosted zone. Wait for DNS propagation to become complete.",
        "A": "Delete all objects, including previous versions, from the S3 bucket that contains the static website content."
      },
      "correct_answer": "D",
      "answer_ET": "D",
      "answers_community": [
        "D (80%)",
        "B (20%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/108678-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-05-07 11:47:00",
      "unix_timestamp": 1683452820,
      "discussion_count": 22,
      "discussion": [
        {
          "timestamp": "1699260960.0",
          "content": "Selected Answer: D\nD is the answer \nB wrong because \nRoute 53 initially considers only the nonzero weighted records, if any.\n\nIf all the records that have a weight greater than 0 are unhealthy, then Route 53 considers the zero-weighted records.",
          "poster": "2pk",
          "comment_id": "1063658",
          "upvote_count": "9"
        },
        {
          "poster": "92a2133",
          "upvote_count": "1",
          "comment_id": "1571305",
          "content": "Selected Answer: D\nWent with D because correct me if I'm wrong almost all changes to anything DNS related does not happen instantly and usually takes some time to propagate",
          "timestamp": "1747915320.0"
        },
        {
          "timestamp": "1745504460.0",
          "comment_id": "1563370",
          "upvote_count": "1",
          "poster": "Rs123x",
          "content": "Selected Answer: B\nChatGPT suggests B..."
        },
        {
          "upvote_count": "1",
          "poster": "steli0",
          "content": "Selected Answer: D\nB would be correct if instead of domain reset option (which doesn't exist) there was a TTL decrease or nothing.",
          "comment_id": "1317426",
          "timestamp": "1732530300.0"
        },
        {
          "upvote_count": "2",
          "timestamp": "1722520380.0",
          "comment_id": "1259406",
          "content": "Selected Answer: D\nD is correct\n\nNot B as \n- Route 53 initially considers only the nonzero weighted records, if any.\n- If all the records that have a weight greater than 0 are unhealthy, then Route 53 considers the zero-weighted records.\nhttps://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy-weighted.html\n\nBesides, in B, <domain reset option to propagate changes immediately>, i don't think DNS record will update immediately",
          "poster": "jamesf"
        },
        {
          "content": "Selected Answer: B\nD is correct but I will go with B because is more save to the customers, some clients have the old record (TTL 1 day) so after 1 day I can confirm that all the clients have the the new DNs record so I can delete the record",
          "upvote_count": "2",
          "comment_id": "1153587",
          "poster": "jojom19980",
          "timestamp": "1708291920.0"
        },
        {
          "poster": "thanhnv142",
          "timestamp": "1706890500.0",
          "upvote_count": "4",
          "content": "D is correct: \nA: should not delete all objects from S3, they change nothing\nC: should not do this, we have a more efficient method\nB: < domain reset option to propagate changes immediately>: there is no such thing. DNS record will expire after TTL. Cannot force DNS resolvers to query for DNS record before TTL expire",
          "comment_id": "1138709"
        },
        {
          "content": "Selected Answer: D\nB- is incorrect , as gigi_devops has metioned setting 0 weight is not enough. Also reset domain option is not available.",
          "poster": "RVivek",
          "comment_id": "1005650",
          "timestamp": "1694515080.0",
          "upvote_count": "3"
        },
        {
          "comment_id": "981521",
          "timestamp": "1692094260.0",
          "content": "D is right.\nJust setting the weight to 0 does not ensure that traffic will not go to example.com.",
          "upvote_count": "2",
          "poster": "ixdb"
        },
        {
          "upvote_count": "3",
          "comment_id": "969559",
          "content": "Selected Answer: D\nSetting the weight to 0 is not enough. So C is the best answer. https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy-weighted.html#:~:text=Si%20tous%20les%20enregistrements%20dont%20le%20poids%20est%20sup%C3%A9rieur%20%C3%A0%200%20ne%20sont%20pas%20sains%2C%20Route%2053%20prend%20en%20compte%20les%20enregistrements%20pond%C3%A9r%C3%A9s%20%C3%A0%20z%C3%A9ro",
          "poster": "gigi_devops",
          "timestamp": "1690941000.0"
        },
        {
          "content": "Setting the weight to 0 is not enough. So C is the best answer. https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy-weighted.html#:~:text=Si%20tous%20les%20enregistrements%20dont%20le%20poids%20est%20sup%C3%A9rieur%20%C3%A0%200%20ne%20sont%20pas%20sains%2C%20Route%2053%20prend%20en%20compte%20les%20enregistrements%20pond%C3%A9r%C3%A9s%20%C3%A0%20z%C3%A9ro",
          "timestamp": "1690940820.0",
          "comments": [],
          "poster": "gigi_devops",
          "comment_id": "969557",
          "upvote_count": "1"
        },
        {
          "comments": [
            {
              "content": "D.\n\nWe can remove the Weighted DNS as it is not necessary",
              "upvote_count": "2",
              "poster": "Aja1",
              "comment_id": "976199",
              "timestamp": "1691551260.0"
            }
          ],
          "poster": "Aja1",
          "timestamp": "1690459800.0",
          "content": "option B is the most appropriate choice as it immediately redirects all traffic away from the S3 bucket and ensures that only the dynamic content from the ALB is served for example.com.",
          "upvote_count": "1",
          "comment_id": "964688"
        },
        {
          "upvote_count": "1",
          "comment_id": "946846",
          "content": "Selected Answer: D\nD. Also, do check the weight of the CNAME record of the ALB. It might be conflicting.",
          "poster": "habros",
          "timestamp": "1688869680.0"
        },
        {
          "timestamp": "1688547720.0",
          "upvote_count": "1",
          "comment_id": "943509",
          "poster": "Blueee",
          "content": "Selected Answer: D\nagree with D"
        },
        {
          "upvote_count": "4",
          "comment_id": "938579",
          "poster": "MarDog",
          "content": "In reference to B, I don't think the \"domain reset option\" exists. So, it's D.",
          "timestamp": "1688073780.0"
        },
        {
          "timestamp": "1687691580.0",
          "upvote_count": "2",
          "content": "Selected Answer: D\nOption D.",
          "comment_id": "933506",
          "poster": "FunkyFresco"
        },
        {
          "poster": "ducluanxutrieu",
          "comment_id": "925701",
          "content": "Selected Answer: D\nTo ensure that the company only serves dynamic content for example.com, the DevOps engineer should remove the weighted DNS record entry that points to the S3 bucket from the example.com hosted zone. This will immediately remove the static website from the DNS resolution pool.\nSolution B would only update the weight of the record entry, but it would still take 24 hours for the changes to propagate.",
          "upvote_count": "4",
          "timestamp": "1686967620.0"
        },
        {
          "poster": "rhinozD",
          "timestamp": "1686834780.0",
          "upvote_count": "2",
          "content": "Selected Answer: B\nB\nTo disable routing to a resource, set Weight to 0",
          "comment_id": "924217"
        },
        {
          "upvote_count": "2",
          "comment_id": "916000",
          "timestamp": "1686035580.0",
          "poster": "nocinfra",
          "content": "Selected Answer: B\nnocinfra 0 minutes ago Awaiting moderator approval\nB for me , You can gradually change the balance by changing the weights. If you want to stop sending traffic to a resource, you can change the weight for that record to 0."
        },
        {
          "comment_id": "915999",
          "content": "B for me , You can gradually change the balance by changing the weights. If you want to stop sending traffic to a resource, you can change the weight for that record to 0.",
          "upvote_count": "2",
          "poster": "nocinfra",
          "timestamp": "1686035520.0"
        },
        {
          "comment_id": "915424",
          "content": "Answer is B. If you want to stop sending traffic to a resource, you can change the weight for that record to 0.\nhttps://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy-weighted.html",
          "poster": "youonebe",
          "timestamp": "1685966880.0",
          "upvote_count": "3"
        },
        {
          "comment_id": "891278",
          "content": "Selected Answer: D\nsounds D",
          "timestamp": "1683452820.0",
          "upvote_count": "1",
          "poster": "marcoforexam"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:20:29.019Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "8cWo83CWul9W4vGEQt6y",
      "question_number": 341,
      "page": 69,
      "question_text": "A company is implementing AWS CodePipeline to automate its testing process. The company wants to be notified when the execution state fails and used the following custom event pattern in Amazon EventBridge:\n\n//IMG//\n\n\nWhich type of events will match this event pattern?",
      "choices": {
        "C": "All the events across all pipelines",
        "B": "All rejected or failed approval actions across all the pipelines",
        "A": "Failed deploy and build actions across all the pipelines",
        "D": "Approval actions across all the pipelines"
      },
      "correct_answer": "B",
      "answer_ET": "B",
      "answers_community": [
        "B (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/108679-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [
        "https://img.examtopics.com/aws-certified-devops-engineer-professional-dop-c02/image6.png"
      ],
      "answer_images": [],
      "timestamp": "2023-05-07 11:54:00",
      "unix_timestamp": 1683453240,
      "discussion_count": 4,
      "discussion": [
        {
          "comment_id": "934941",
          "poster": "willhsien",
          "upvote_count": "14",
          "content": "Selected Answer: B\nUse this sample event pattern to capture all rejected or failed approval actions across all the pipelines.\nhttps://docs.aws.amazon.com/codepipeline/latest/userguide/detect-state-changes-cloudwatch-events.html",
          "timestamp": "1703646840.0"
        },
        {
          "comment_id": "1138715",
          "upvote_count": "3",
          "poster": "thanhnv142",
          "content": "B is correct: <state:failed and category:approval> means failed approval\nA, C and D: no mention of approval",
          "timestamp": "1722608700.0"
        },
        {
          "timestamp": "1700783040.0",
          "poster": "gdtypk",
          "content": "Selected Answer: B\nhttps://docs.aws.amazon.com/ja_jp/codepipeline/latest/userguide/detect-state-changes-cloudwatch-events.html",
          "comment_id": "905259",
          "upvote_count": "3"
        },
        {
          "timestamp": "1699358040.0",
          "poster": "marcoforexam",
          "comment_id": "891282",
          "upvote_count": "1",
          "content": "Selected Answer: B\ncategory: approval"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:20:39.591Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "0o8ktG8Eq5dnUyZHXr5Q",
      "question_number": 342,
      "page": 69,
      "question_text": "A company’s security team requires that all external Application Load Balancers (ALBs) and Amazon API Gateway APIs are associated with AWS WAF web ACLs. The company has hundreds of AWS accounts, all of which are included in a single organization in AWS Organizations. The company has configured AWS Config for the organization. During an audit, the company finds some externally facing ALBs that are not associated with AWS WAF web ACLs.\nWhich combination of steps should a DevOps engineer take to prevent future violations? (Choose two.)",
      "choices": {
        "B": "Delegate Amazon GuardDuty to a security account.",
        "C": "Create an AWS Firewall Manager policy to attach AWS WAF web ACLs to any newly created ALBs and API Gateway APIs.",
        "A": "Delegate AWS Firewall Manager to a security account.",
        "E": "Configure an AWS Config managed rule to attach AWS WAF web ACLs to any newly created ALBs and API Gateway APIs.",
        "D": "Create an Amazon GuardDuty policy to attach AWS WAF web ACLs to any newly created ALBs and API Gateway APIs."
      },
      "correct_answer": "AC",
      "answer_ET": "AC",
      "answers_community": [
        "AC (96%)",
        "4%"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/105266-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-04-05 12:30:00",
      "unix_timestamp": 1680690600,
      "discussion_count": 13,
      "discussion": [
        {
          "upvote_count": "12",
          "content": "Selected Answer: AC\nIf you see WAF you have to think AWS Firewall Manager.",
          "poster": "ataince",
          "timestamp": "1695573120.0",
          "comment_id": "1016018"
        },
        {
          "timestamp": "1681578060.0",
          "upvote_count": "10",
          "poster": "alce2020",
          "comment_id": "871133",
          "content": "A and C"
        },
        {
          "timestamp": "1727165760.0",
          "content": "Selected Answer: AC\nIf instead you want to automatically apply the policy to existing in-scope resources, choose Auto remediate any noncompliant resources. This option creates a web ACL in each applicable account within the AWS organization and associates the web ACL with the resources in the accounts.\nWhen you choose Auto remediate any noncompliant resources, you can also choose to remove existing web ACL associations from in-scope resources, for the web ACLs that aren't managed by another active Firewall Manager policy. If you choose this option, Firewall Manager first associates the policy's web ACL with the resources, and then removes the prior associations. If a resource has an association with another web ACL that's managed by a different active Firewall Manager policy, this choice doesn't affect that association.",
          "upvote_count": "1",
          "poster": "ele",
          "comment_id": "863841"
        },
        {
          "comment_id": "1260748",
          "upvote_count": "1",
          "poster": "namtp",
          "timestamp": "1722792840.0",
          "content": "Selected Answer: AC\nI think that is best way to centralize manage firewall config"
        },
        {
          "content": "Selected Answer: AC\nAs my understanding, WAF related with AWS Firewall Manager.",
          "poster": "jamesf",
          "comment_id": "1254710",
          "upvote_count": "1",
          "timestamp": "1721882640.0"
        },
        {
          "upvote_count": "2",
          "comments": [
            {
              "poster": "Gomer",
              "upvote_count": "1",
              "timestamp": "1716494820.0",
              "comment_id": "1216987",
              "content": "In reading a little further, I suspect that Config may be being used in the background (since Config must be enabled to use WAF. However, I believe that is totally transparent to the Organization WAF Administrator. The administration of WAF and enforcement of WAF policies is ALL handled with the Web Application Firewall service."
            }
          ],
          "poster": "Gomer",
          "timestamp": "1716494340.0",
          "content": "Selected Answer: AC\nThese references indicate this can all be handled within Firewall manager (w/no references to Config or GuardDuty)\nhttps://aws.amazon.com/blogs/security/how-to-enforce-a-security-baseline-for-an-aws-waf-acl-across-your-organization-using-aws-firewall-manager/\nhttps://aws.amazon.com/solutions/implementations/automations-for-aws-firewall-manager/",
          "comment_id": "1216981"
        },
        {
          "comments": [
            {
              "poster": "GripZA",
              "comment_id": "1561747",
              "content": "AWS Config can't auto remediate unless there's additional integration, eg with Lambda.",
              "upvote_count": "1",
              "timestamp": "1745004600.0"
            }
          ],
          "comment_id": "1206810",
          "upvote_count": "1",
          "timestamp": "1714898160.0",
          "content": "Selected Answer: AC\nI think E works, but Firewall manager is designed for the purpose.",
          "poster": "01037"
        },
        {
          "upvote_count": "1",
          "comment_id": "1171590",
          "content": "Selected Answer: AC\nA and C: AWS Config rules are primarily used for monitoring and evaluating the configurations of your AWS resources for compliance with desired configurations. However, AWS Config also supports remediation actions through AWS Systems Manager Automation documents.",
          "timestamp": "1710238020.0",
          "poster": "Cervus18"
        },
        {
          "comments": [
            {
              "comment_id": "1206808",
              "upvote_count": "1",
              "content": "I think E works, but Firewall manager is designed for the purpose.",
              "poster": "01037",
              "timestamp": "1714898100.0"
            },
            {
              "content": "AWS Config rules are primarily used for monitoring and evaluating the configurations of AWS resources for compliance with desired configurations. However, AWS Config also supports remediation actions through AWS Systems Manager Automation documents or lambda. Firewall manager is used to apply and enforce WebACLs to all ALBs at an organizational level to all your AWS Organization's accounts, and you can configure auto remidation for any non-compliant resource in any account.",
              "comment_id": "1171587",
              "timestamp": "1710237960.0",
              "poster": "Cervus18",
              "upvote_count": "1"
            }
          ],
          "content": "Selected Answer: CE\nWhy not E?",
          "upvote_count": "1",
          "comment_id": "1161566",
          "poster": "Vitalydt",
          "timestamp": "1709120160.0"
        },
        {
          "comment_id": "1133444",
          "content": "A and C: Config does not have any action, only notifications",
          "upvote_count": "2",
          "poster": "thanhnv142",
          "timestamp": "1706369040.0"
        },
        {
          "comment_id": "974466",
          "upvote_count": "2",
          "timestamp": "1691392320.0",
          "poster": "Fco_Javier",
          "content": "A) is a prerequisites: AWS Firewall Manager prerequisites\nhttps://docs.aws.amazon.com/es_es/waf/latest/developerguide/join-aws-orgs.html"
        },
        {
          "poster": "habros",
          "upvote_count": "2",
          "content": "Selected Answer: AC\nGuardDuty only posts findings, hence they can be eliminated.\nFrom my knowledge, Config only notifies.\nHence, A and C.",
          "timestamp": "1688476980.0",
          "comment_id": "942801"
        },
        {
          "timestamp": "1680690600.0",
          "comment_id": "861978",
          "upvote_count": "1",
          "poster": "Dimidrol",
          "content": "Selected Answer: AC\nA C for me"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:20:39.591Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "7Yv3J1F0lkaCzANapT4u",
      "question_number": 343,
      "page": 69,
      "question_text": "An application running on a set of Amazon EC2 instances in an Auto Scaling group requires a configuration file to operate. The instances are created and maintained with AWS CloudFormation. A DevOps engineer wants the instances to have the latest configuration file when launched, and wants changes to the configuration file to be reflected on all the instances with a minimal delay when the CloudFormation template is updated. Company policy requires that application configuration files be maintained along with AWS infrastructure configuration files in source control.\n\nWhich solution will accomplish this?",
      "choices": {
        "B": "In the CloudFormation template, add an EC2 launch template resource. Place the configuration file content in the launch template. Configure the cfn-init script to run when the instance is launched, and configure the cfn-hup script to poll for updates to the configuration.",
        "C": "In the CloudFormation template, add an EC2 launch template resource. Place the configuration file content in the launch template. Add an AWS Systems Manager Resource Data Sync resource to the template to poll for updates to the configuration.",
        "D": "In the CloudFormation template, add CloudFormation init metadata. Place the configuration file content in the metadata. Configure the cfn-init script to run when the instance is launched, and configure the cfn-hup script to poll for updates to the configuration.",
        "A": "In the CloudFormation template, add an AWS Config rule. Place the configuration file content in the rule’s InputParameters property, and set the Scope property to the EC2 Auto Scaling group. Add an AWS Systems Manager Resource Data Sync resource to the template to poll for updates to the configuration."
      },
      "correct_answer": "D",
      "answer_ET": "D",
      "answers_community": [
        "D (79%)",
        "B (21%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/108586-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-05-05 22:14:00",
      "unix_timestamp": 1683317640,
      "discussion_count": 19,
      "discussion": [
        {
          "comment_id": "1168939",
          "poster": "4555894",
          "timestamp": "1709914680.0",
          "upvote_count": "7",
          "content": "Selected Answer: D\nUse the AWS::CloudFormation::Init type to include metadata on an Amazon EC2 instance for the cfn-init helper script. If your template calls the cfn-init script, the script looks for resource metadata rooted in the AWS::CloudFormation::Init metadata key. Reference:\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-init.html"
        },
        {
          "comment_id": "1317429",
          "poster": "steli0",
          "upvote_count": "1",
          "content": "Selected Answer: D\nIt's D",
          "timestamp": "1732530600.0"
        },
        {
          "timestamp": "1721983500.0",
          "content": "Selected Answer: D\nRequire CloudFormation init for cfn-init and cfn-hup\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-hup.html\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-init.html",
          "upvote_count": "2",
          "poster": "jamesf",
          "comment_id": "1255602"
        },
        {
          "comment_id": "1138722",
          "timestamp": "1706891820.0",
          "content": "D is correct: <The instances are created and maintained with AWS CloudFormation> means we will only use ACF to satisfy the requirements of this question. <changes to the configuration file to be reflected on all the instances with a minimal delay when the CloudFormation template is updated> means we nead cfn-init, which is a daemon that check for updates and update the changes\nA and C: no mention of cfn-init\nB: no mention of CloudFormation init. we need CloudFormation init because cfn-init is specified in CloudFormation init key.",
          "poster": "thanhnv142",
          "upvote_count": "3"
        },
        {
          "poster": "khchan123",
          "comment_id": "1125094",
          "content": "Selected Answer: D\nD. cfn-hup poll for cloudformation metadata. B is wrong because putting the config content in launch template instead of metadata, where cfn-hub is not able to poll.",
          "upvote_count": "4",
          "timestamp": "1705506000.0"
        },
        {
          "content": "Selected Answer: D\ncfn-init is defined inside AWS::CloudFormation::Init",
          "poster": "a54b16f",
          "comment_id": "1120776",
          "timestamp": "1705063620.0",
          "upvote_count": "2"
        },
        {
          "comment_id": "1117426",
          "poster": "yuliaqwerty",
          "timestamp": "1704798660.0",
          "upvote_count": "1",
          "content": "I vote for B"
        },
        {
          "upvote_count": "4",
          "comment_id": "1115181",
          "timestamp": "1704546420.0",
          "poster": "Jaguaroooo",
          "content": "But what happened to the aspect of using source control?"
        },
        {
          "timestamp": "1703488320.0",
          "content": "Selected Answer: B\nGoogle Bart says it is B. \nBy using an EC2 launch template resource, the configuration file will be installed and configured on all instances when they are launched. The cfn-init script will also poll for updates to the configuration, so that all instances will have the latest configuration file as soon as it is updated.\n\nIn addition, the solution will comply with company policy by storing the configuration file in source control along with the AWS infrastructure configuration files. This will ensure that changes to the configuration file are tracked and managed in a consistent way.\nOption C: Using an AWS Systems Manager Resource Data Sync resource alone is not enough to ensure that all instances have the latest configuration file. The cfn-init script is needed to install and configure the configuration file on the instance, and the cfn-hup script is needed to poll for updates to the configuration.",
          "comment_id": "1105078",
          "upvote_count": "2",
          "poster": "a16a848"
        },
        {
          "content": "Selected B:\nYou can have cfn on Launch Config and Launch Template\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-hup.html\nhttps://stackoverflow.com/questions/54691327/cfn-init-for-cloudformation-launchtemplate",
          "comment_id": "1086968",
          "poster": "svjl",
          "upvote_count": "3",
          "timestamp": "1701624300.0"
        },
        {
          "poster": "HugoFM",
          "timestamp": "1701233280.0",
          "comment_id": "1083112",
          "content": "Selected Answer: B\nI choos B because the config file must be mantained along teh insfrastructure configuration files in source control",
          "upvote_count": "1"
        },
        {
          "timestamp": "1698317940.0",
          "poster": "zenith_cloud",
          "comment_id": "1054482",
          "upvote_count": "1",
          "content": "Selected Answer: B\nB and D are similar. I will go for B, because D doesn't involve EC2 launch templates"
        },
        {
          "timestamp": "1694516640.0",
          "content": "Selected Answer: D\ncfn-init and cfn-hup are used to update metadata. https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-hup.html",
          "poster": "RVivek",
          "comment_id": "1005659",
          "upvote_count": "2"
        },
        {
          "upvote_count": "1",
          "comment_id": "965449",
          "poster": "Aja1",
          "comments": [
            {
              "comment_id": "976209",
              "timestamp": "1691552100.0",
              "content": "D\n\ncfn-hup is a daemon that detects changes in resource metadata and runs user-specified actions when a change is detected. This allows you to automatically update the configuration of your Amazon EC2 instances when you make changes to your AWS CloudFormation stacks.",
              "upvote_count": "4",
              "poster": "Aja1"
            }
          ],
          "timestamp": "1690537320.0",
          "content": "option C provides a reliable and scalable solution to manage the configuration file for the application running on the EC2 instances, while also adhering to company policies regarding source control for configuration files."
        },
        {
          "content": "Selected Answer: D\nD\nMetadata:\n \"AWS::CloudFormation::Init\":",
          "timestamp": "1686835620.0",
          "upvote_count": "4",
          "comment_id": "924230",
          "poster": "rhinozD"
        },
        {
          "content": "Selected Answer: B\nBy using an EC2 launch template, you can include the configuration file content directly in the template. The cfn-init script can be configured to run when the instance is launched.\n\nas CloudFormation init metadata is more suitable for configuring instances during stack creation rather than for dynamically updating configuration files.",
          "poster": "2pk",
          "comment_id": "898238",
          "upvote_count": "3",
          "timestamp": "1684151400.0"
        },
        {
          "timestamp": "1683460320.0",
          "comment_id": "891353",
          "poster": "PhuocT",
          "upvote_count": "2",
          "content": "Selected Answer: D\nD it is"
        },
        {
          "content": "Selected Answer: D\nSounds D\nIt uses managed services.",
          "comment_id": "891320",
          "poster": "marcoforexam",
          "timestamp": "1683457320.0",
          "upvote_count": "3"
        },
        {
          "timestamp": "1683317640.0",
          "upvote_count": "2",
          "poster": "kassem77",
          "comment_id": "890308",
          "content": "D it is"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:20:39.591Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "zhS78aETCFCdEzcsVXud",
      "question_number": 344,
      "page": 69,
      "question_text": "A company manages an application that stores logs in Amazon CloudWatch Logs. The company wants to archive the logs to an Amazon S3 bucket. Logs are rarely accessed after 90 days and must be retained for 10 years.\n\nWhich combination of steps should a DevOps engineer take to meet these requirements? (Choose two.)",
      "choices": {
        "B": "Configure a CloudWatch Logs subscription filter to use Amazon Kinesis Data Firehose to stream all logs to an S3 bucket.",
        "E": "Configure the S3 bucket lifecycle policy to transition logs to Reduced Redundancy after 90 days and to expire logs after 3.650 days.",
        "C": "Configure a CloudWatch Logs subscription filter to stream all logs to an S3 bucket.",
        "A": "Configure a CloudWatch Logs subscription filter to use AWS Glue to transfer all logs to an S3 bucket.",
        "D": "Configure the S3 bucket lifecycle policy to transition logs to S3 Glacier after 90 days and to expire logs after 3.650 days."
      },
      "correct_answer": "BD",
      "answer_ET": "BD",
      "answers_community": [
        "BD (85%)",
        "CD (15%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/109258-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-05-15 09:07:00",
      "unix_timestamp": 1684134420,
      "discussion_count": 15,
      "discussion": [
        {
          "comment_id": "898096",
          "timestamp": "1684134420.0",
          "poster": "2pk",
          "content": "Selected Answer: BD\nAmazon Kinesis Data Firehose simplifies the process of loading streaming data into S3 and provides automatic scaling, buffering, and retries.",
          "upvote_count": "6"
        },
        {
          "timestamp": "1752155100.0",
          "comment_id": "1585299",
          "content": "Selected Answer: BD\nI vote B/D",
          "upvote_count": "1",
          "poster": "xxxdolorxxx"
        },
        {
          "upvote_count": "1",
          "timestamp": "1722559320.0",
          "comment_id": "1259615",
          "poster": "jamesf",
          "content": "Selected Answer: BD\nB - keywords: continue stream but not one time task\nD - keywords: S3 Glacier"
        },
        {
          "comment_id": "1256699",
          "poster": "ericphl",
          "timestamp": "1722161940.0",
          "content": "Selected Answer: BD\nvote B and D. \nI initially thought the C is better than B, because Amazon Kinesis Data Firehose is primarily used for time-sensitive tasks, which is not suitable for this case, But when I read the C. I found the Directly streaming logs from cloudwatch log to s3 is not a feature provided by Cloudwatch. \nSo, I will go with B and D.",
          "upvote_count": "3"
        },
        {
          "poster": "Gomer",
          "comment_id": "1229621",
          "content": "Selected Answer: BD\nYou can absolutly directly \"export log data from your log groups to an Amazon S3 bucket\"\nHowever, this is a one time export, and NOT an ongoing stream.\nIf you want to steam continuously you have to use \"subscription filter with Kinesis Data Streams, Lambda, or Firehose.\"\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/S3Export.html\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/SubscriptionFilters.html\nhttps://dev.to/aws-builders/automate-export-of-cloudwatch-logs-to-s3-bucket-using-lambda-with-eventbridge-trigger-2ieg",
          "timestamp": "1718253720.0",
          "upvote_count": "3"
        },
        {
          "comment_id": "1212920",
          "content": "Looks like creating subscription filters in AWS cloudwatch logs, there are only limited destination options. There is no S3 as a direct destination. You have to either create Elasticsearch or Kinesis or Kinesis Firehose or Lambda subscription filters. Given the choices we have, we need to pick B & D",
          "timestamp": "1715954160.0",
          "poster": "zijo",
          "upvote_count": "1"
        },
        {
          "timestamp": "1714638660.0",
          "comment_id": "1205382",
          "content": "Selected Answer: CD\nC & D for the reasons that thanhnv142 mentioned.",
          "poster": "Jay_2pt0_1",
          "upvote_count": "1",
          "comments": [
            {
              "upvote_count": "2",
              "timestamp": "1715595060.0",
              "poster": "vn_thanhtung",
              "content": "https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/SubscriptionFilters.html\nPls check link. You can use a subscription filter with Kinesis Data Streams, Lambda, or Firehose. Logs that are sent to a receiving service through a subscription filter are base64 encoded and compressed with the gzip format. correct is B and D",
              "comment_id": "1210806"
            }
          ]
        },
        {
          "timestamp": "1709029740.0",
          "content": "CD，The question does not mention trying to switch to S3 in real time. C is more cost-effective.\nhttps://docs.aws.amazon.com/zh_cn/AmazonCloudWatch/latest/logs/S3ExportTasksConsole.html",
          "comment_id": "1160431",
          "poster": "Heyang",
          "upvote_count": "2"
        },
        {
          "poster": "dzn",
          "content": "Selected Answer: BD\nAmazon S3 Glacier is a secure, durable, and very low-cost cloud storage service that can be used for data archiving and long-term backup.",
          "timestamp": "1708838160.0",
          "upvote_count": "3",
          "comment_id": "1158399"
        },
        {
          "timestamp": "1708320120.0",
          "upvote_count": "2",
          "poster": "jojom19980",
          "content": "Selected Answer: BD\nYou can use a subscription filter with Kinesis Data Streams, Lambda, or Firehose.\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/SubscriptionFilters.html",
          "comment_id": "1153723"
        },
        {
          "comments": [
            {
              "upvote_count": "1",
              "poster": "thanhnv142",
              "timestamp": "1706893380.0",
              "content": "A: AWS Glue is used primarily to integrate data from multiple data sources (up to 70) for data analysis. Of course it works well with one data source only (CW logs in this case). But it costs a lot of money and using it with only one data source is a waste of corporate budget. Should not use this. \nB: Amazon Kinesis Data Firehose is primarily used for time-sensitive tasks, such as video streaming. It is very powerful that it can handle data in near realtime. However, this premium feature comes with a big expense. We only need to archive data, not video streaming it. \nE: we need to transition it to S3 Glacier not Reduced Redundancy after 90 days",
              "comment_id": "1138737"
            }
          ],
          "poster": "thanhnv142",
          "timestamp": "1706893380.0",
          "content": "Selected Answer: CD\nC and D is correct: <archive the logs to an Amazon S3 bucket> means we need to transport logs from CloudWatch Logs to S3. CloudWatch Logs can directly transport log data to S3. Logs are rarely accessed after 90 days means we need S3 bucket lifecycle policy",
          "comment_id": "1138736",
          "upvote_count": "4"
        },
        {
          "upvote_count": "4",
          "poster": "habros",
          "timestamp": "1688869980.0",
          "comment_id": "946848",
          "content": "Selected Answer: BD\nB to shift logs out using Kinesis Firehose to S3. Then D to set S3 bucket storage class to Glacier Flexible."
        },
        {
          "comments": [
            {
              "content": "Option C is incorrect because streaming all logs to an S3 bucket is not a good solution for archiving logs",
              "upvote_count": "2",
              "timestamp": "1691552880.0",
              "poster": "Aja1",
              "comment_id": "976215"
            }
          ],
          "comment_id": "936798",
          "upvote_count": "4",
          "timestamp": "1687964520.0",
          "content": "Selected Answer: BD\nC would make sense, but subscription filters don't go to S3 directly\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/SubscriptionFilters.html",
          "poster": "YXXt55"
        },
        {
          "upvote_count": "1",
          "timestamp": "1687320060.0",
          "comment_id": "929028",
          "poster": "haazybanj",
          "content": "Selected Answer: BD\nBD is right"
        },
        {
          "upvote_count": "1",
          "poster": "OrganizedChaos25",
          "comment_id": "899145",
          "content": "Selected Answer: BD",
          "timestamp": "1684237860.0"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:20:39.591Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "JNK0RCHwdoy8vDrOfT4S",
      "question_number": 345,
      "page": 69,
      "question_text": "A company is developing a new application. The application uses AWS Lambda functions for its compute tier. The company must use a canary deployment for any changes to the Lambda functions. Automated rollback must occur if any failures are reported.\n\nThe company’s DevOps team needs to create the infrastructure as code (IaC) and the CI/CD pipeline for this solution.\n\nWhich combination of steps will meet these requirements? (Choose three.)",
      "choices": {
        "D": "Create an AWS CodeCommit repository. Create an AWS CodePipeline pipeline. Use the CodeCommit repository in a new source stage that starts the pipeline. Create an AWS CodeDeploy deployment group that is configured for canary deployments with a DeploymentPreference type of Canary10Percent10Minutes. Upload the AWS CloudFormation template and source code to the CodeCommit repository. In the CodeCommit repository, create an appspec.yml file that includes the commands to deploy the CloudFormation template.",
        "C": "Create an AWS CodeCommit repository. Create an AWS CodePipeline pipeline. Use the CodeCommit repository in a new source stage that starts the pipeline. Create an AWS CodeBuild project to deploy the AWS Serverless Application Model (AWS SAM) template. Upload the template and source code to the CodeCommit repository. In the CodeCommit repository, create a buildspec.yml file that includes the commands to build and deploy the SAM application.",
        "E": "Create an Amazon CloudWatch composite alarm for all the Lambda functions. Configure an evaluation period and dimensions for Lambda. Configure the alarm to enter the ALARM state if any errors are detected or if there is insufficient data.",
        "B": "Create an AWS Serverless Application Model (AWS SAM) template for the application. Define each Lambda function in the template by using the AWS::Serverless::Function resource type. For each function, include configurations for the AutoPublishAlias property and the DeploymentPreference property. Configure the deployment configuration type to LambdaCanary10Percent10Minutes.",
        "F": "Create an Amazon CloudWatch alarm for each Lambda function. Configure the alarms to enter the ALARM state if any errors are detected. Configure an evaluation period, dimensions for each Lambda function and version, and the namespace as AWS/Lambda on the Errors metric.",
        "A": "Create an AWS CloudFormation template for the application. Define each Lambda function in the template by using the AWS::Lambda::Function resource type. In the template, include a version for the Lambda function by using the AWS::Lambda::Version resource type. Declare the CodeSha256 property. Configure an AWS::Lambda::Alias resource that references the latest version of the Lambda function."
      },
      "correct_answer": "BCF",
      "answer_ET": "BCF",
      "answers_community": [
        "BCF (91%)",
        "9%"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/109261-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-05-15 09:27:00",
      "unix_timestamp": 1684135620,
      "discussion_count": 17,
      "discussion": [
        {
          "timestamp": "1700040420.0",
          "comment_id": "898103",
          "upvote_count": "5",
          "poster": "2pk",
          "content": "Selected Answer: BCF\nBCF correct"
        },
        {
          "upvote_count": "3",
          "timestamp": "1722649500.0",
          "content": "Selected Answer: BCF\nBCF is my choice",
          "comment_id": "1138988",
          "poster": "thanhnv142"
        },
        {
          "comment_id": "1138986",
          "content": "BCF are correct:\nA is not correct: <needs to create the infrastructure as code (IaC)> means we prefer AWS SAM over ACF. ACF is used to deploy AWS instances, not for IaC\nD is wrong: no mention of AWS SAM\nE is wrong: <Amazon CloudWatch composite alarm for all the Lambda functions>, but we need alarm for each lambda func, not one alarm for all of them",
          "upvote_count": "2",
          "poster": "thanhnv142",
          "timestamp": "1722649440.0"
        },
        {
          "upvote_count": "2",
          "content": "it should be BDF because code deploy can be configured for canary\nhttps://docs.aws.amazon.com/serverless-application-model/latest/developerguide/automating-updates-to-serverless-apps.html",
          "poster": "sarlos",
          "comment_id": "1109404",
          "timestamp": "1719714780.0"
        },
        {
          "content": "Selected Answer: BCF\nBCF, E is not correct you need to monitor each lambda to do a rollback of a particular deploy",
          "timestamp": "1716951900.0",
          "poster": "HugoFM",
          "comment_id": "1083118",
          "upvote_count": "3"
        },
        {
          "poster": "AzureDP900",
          "comment_id": "1080992",
          "upvote_count": "1",
          "timestamp": "1716744840.0",
          "content": "BCF is right"
        },
        {
          "comment_id": "1057850",
          "timestamp": "1714479060.0",
          "upvote_count": "1",
          "content": "Answer is BCF.\nhttps://docs.aws.amazon.com/serverless-application-model/latest/developerguide/automating-updates-to-serverless-apps.html",
          "poster": "rlf"
        },
        {
          "timestamp": "1713718260.0",
          "content": "Can someone please explain why not A and D? seem the same of B C without using SAM",
          "upvote_count": "4",
          "poster": "sivre",
          "comment_id": "1049679"
        },
        {
          "comment_id": "1005699",
          "poster": "RVivek",
          "upvote_count": "2",
          "timestamp": "1710251100.0",
          "comments": [
            {
              "content": "Why we cannot use code deploy for canary there are already few deployment percentage for codedeploy BDF is correct",
              "upvote_count": "1",
              "comment_id": "1135567",
              "timestamp": "1722316020.0",
              "poster": "hotblooded"
            }
          ],
          "content": "Selected Answer: BCF\nA is wrong beacuse of AWS::Lambda::Function\nB - Can work\nC: is correct\nD: SAM or Lambda deployment in Codedeploy cannot be canary deployment. canaray deployment should be included in the Lambda code as mentioned in option B.\nE: Composite Alram is not required. if any Lambda fails , it should generate alarm\nF: works\nBasically select B from AB which is for lambda coding, Select C from CD for deploying and F from EF for monitoring and alerting"
        },
        {
          "comments": [
            {
              "comment_id": "946852",
              "poster": "habros",
              "content": "I’ll stick with BCF still. Composite alarms does not apply in this context. https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Create_Composite_Alarm.html",
              "timestamp": "1704775200.0",
              "upvote_count": "2"
            }
          ],
          "timestamp": "1704775080.0",
          "upvote_count": "2",
          "comment_id": "946851",
          "content": "Selected Answer: BCF\nLeaning to BCF. Lambda errors are standard although BCE is possible too.\n\nFor server less it is giveaway question. Stick with SAM if possible",
          "poster": "habros"
        },
        {
          "comment_id": "946711",
          "poster": "sb333",
          "upvote_count": "2",
          "timestamp": "1704750060.0",
          "content": "Selected Answer: BCF\nBCF is correct."
        },
        {
          "poster": "Blueee",
          "content": "Selected Answer: BCF\nBC and F",
          "upvote_count": "3",
          "timestamp": "1704452880.0",
          "comment_id": "943514"
        },
        {
          "upvote_count": "3",
          "poster": "Manny20",
          "content": "Composite Alarm requires underlying metric alarms which requires one CloudWatch alarm for each lambda functions and then tie them back to a composite alarm. So BC and F makes sense.",
          "comment_id": "938504",
          "timestamp": "1703885640.0"
        },
        {
          "timestamp": "1703510280.0",
          "comment_id": "933510",
          "content": "Selected Answer: BCE\nI think BCE makes more sense.",
          "upvote_count": "1",
          "poster": "FunkyFresco"
        },
        {
          "content": "Selected Answer: BCE\nF creates an Amazon CloudWatch alarm for each Lambda function. However, it is not necessary to create an alarm for each Lambda function. A single composite alarm can be used to monitor all the Lambda functions.",
          "comments": [],
          "poster": "ducluanxutrieu",
          "comment_id": "925779",
          "upvote_count": "1",
          "timestamp": "1702797420.0"
        },
        {
          "poster": "OrganizedChaos25",
          "comment_id": "899156",
          "timestamp": "1700143140.0",
          "upvote_count": "1",
          "content": "BCF are correct"
        },
        {
          "poster": "2pk",
          "comment_id": "898109",
          "upvote_count": "2",
          "timestamp": "1700040600.0",
          "content": "Cannot deploy the canary deployemnt in a pipeline for lambda creation, it has to be created in lambda resource file."
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:20:39.591Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "sMxgwfpjOqTQumI1KzRd",
      "question_number": 346,
      "page": 70,
      "question_text": "A DevOps engineer is deploying a new version of a company’s application in an AWS CodeDeploy deployment group associated with its Amazon EC2 instances. After some time, the deployment fails. The engineer realizes that all the events associated with the specific deployment ID are in a Skipped status, and code was not deployed in the instances associated with the deployment group.\n\nWhat are valid reasons for this failure? (Choose two.)",
      "choices": {
        "A": "The networking configuration does not allow the EC2 instances to reach the internet via a NAT gateway or internet gateway, and the CodeDeploy endpoint cannot be reached.",
        "E": "The appspec.yml file was not included in the application revision.",
        "C": "The target EC2 instances were not properly registered with the CodeDeploy endpoint.",
        "D": "An instance profile with proper permissions was not attached to the target EC2 instances.",
        "B": "The IAM user who triggered the application deployment does not have permission to interact with the CodeDeploy endpoint."
      },
      "correct_answer": "AD",
      "answer_ET": "AD",
      "answers_community": [
        "AD (92%)",
        "8%"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/108810-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-05-09 15:56:00",
      "unix_timestamp": 1683640560,
      "discussion_count": 12,
      "discussion": [
        {
          "comment_id": "962906",
          "upvote_count": "11",
          "poster": "haazybanj",
          "content": "Selected Answer: AD\nA.\n\nExplanation: For CodeDeploy to work, the EC2 instances need to reach the CodeDeploy endpoint to download the deployment artifacts. If the networking configuration of the EC2 instances does not allow them to access the internet via a NAT gateway or internet gateway, they won't be able to reach the CodeDeploy endpoint, leading to deployment failure.\nD\n\nExplanation: When EC2 instances are part of a CodeDeploy deployment group, they need to have an associated IAM instance profile with the necessary permissions to interact with CodeDeploy and download the deployment artifacts. If the instance profile with proper permissions is not attached to the target EC2 instances, the deployment will fail as the instances won't have the required permissions to complete the deployment process.",
          "timestamp": "1690303800.0"
        },
        {
          "timestamp": "1686836820.0",
          "poster": "rhinozD",
          "comments": [
            {
              "comments": [
                {
                  "poster": "sejar",
                  "upvote_count": "2",
                  "comment_id": "1171022",
                  "content": "No registration required, once agent is installed it should be sufficient. However permissions and network connectivity to S3 or code deploy would be must. Since that takes priority, A&D should be right.",
                  "timestamp": "1710161640.0"
                }
              ],
              "comment_id": "1097024",
              "timestamp": "1702611540.0",
              "poster": "bnagaraja9099",
              "upvote_count": "1",
              "content": "C is correct. \nthe first reason for skipped errors on the link. \n The CodeDeploy agent might not be installed or running on the instance. To determine if the CodeDeploy agent is running:"
            }
          ],
          "upvote_count": "7",
          "content": "Selected Answer: AD\nAD\nhttps://docs.aws.amazon.com/codedeploy/latest/userguide/troubleshooting-deployments.html\nSearch with: Troubleshooting all lifecycle events skipped errors",
          "comment_id": "924241"
        },
        {
          "timestamp": "1725617520.0",
          "content": "My question is \"skipped\" situation doesn't sound like a network error. There is no 4xx error or fail status",
          "poster": "YucelFuat",
          "upvote_count": "1",
          "comment_id": "1279520"
        },
        {
          "timestamp": "1715960760.0",
          "content": "The user needs to create a service role and attach the AWSCodeDeployRole policy to it to grant the correct permissions for CodeDeploy to access EC2 instances. The role chosen should allow access to start and stop EC2 instances.\nIf the IAM role used by CodeDeploy doesn't have the necessary permissions to access the deployment artifacts or interact with the EC2 instances, the deployment may be skipped.\nSo it is not the IAM permissions of the user invoking the CodeDeploy.",
          "upvote_count": "1",
          "poster": "zijo",
          "comment_id": "1212948"
        },
        {
          "comment_id": "1138993",
          "upvote_count": "2",
          "timestamp": "1706933100.0",
          "content": "Selected Answer: AD\nA and D are correct: the deployment process might be skipped because of codedeploy agent\nA: no connection means skipped deployment\nD: insufficient permission means skipped deployment",
          "poster": "thanhnv142"
        },
        {
          "upvote_count": "1",
          "content": "Selected Answer: AD\nA and D. See https://docs.aws.amazon.com/codedeploy/latest/userguide/troubleshooting-deployments.html#troubleshooting-skipped-lifecycle-events",
          "comment_id": "1125099",
          "poster": "khchan123",
          "timestamp": "1705506720.0"
        },
        {
          "content": "Do you really have to have internet connectivity to use CodeDeploy? Why not use VPC endpoint in such cases? I go for CD.",
          "poster": "3a29cc4",
          "upvote_count": "1",
          "timestamp": "1704164220.0",
          "comment_id": "1111557"
        },
        {
          "comment_id": "1092418",
          "content": "Selected Answer: CD\nSome of the other options could cause a deployment to fail, but not specifically result in a \"Skipped\" status:\n\nA Networking issues may prevent the deployment from reaching instances, but this would likely cause the deployment to fail, not be skipped.\nB Lack of permissions for the IAM user would cause the deployment job itself to fail authorization.\nE Missing appspec.yml would cause validation errors prior to the deployment attempt.\n\nAnyone has diffrent views?",
          "timestamp": "1702204140.0",
          "poster": "yorkicurke",
          "comments": [
            {
              "poster": "yorkicurke",
              "comment_id": "1092419",
              "upvote_count": "1",
              "content": "oh yeah;\nwhy;\nC -> CodeDeploy needs to be able to communicate with the instances in order to deploy revisions to them. If the instances are not registered, CodeDeploy will skip deploying to them.\nD - > i think everyone know that point. i guess dont need explaintion. \nPeace :)",
              "timestamp": "1702204320.0"
            }
          ],
          "upvote_count": "2"
        },
        {
          "comment_id": "1005728",
          "upvote_count": "1",
          "timestamp": "1694520900.0",
          "content": "Selected Answer: AD\nI agree with rhinozD",
          "poster": "RVivek"
        },
        {
          "poster": "Blueee",
          "content": "Selected Answer: AD\nAD is correct",
          "comment_id": "943515",
          "upvote_count": "1",
          "timestamp": "1688548080.0"
        },
        {
          "content": "Its AD",
          "comment_id": "897266",
          "upvote_count": "1",
          "poster": "devnv",
          "timestamp": "1684042080.0"
        },
        {
          "poster": "ParagSanyashiv",
          "comment_id": "893142",
          "content": "Selected Answer: AD\nAD is correct",
          "timestamp": "1683640560.0",
          "upvote_count": "1"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:20:50.533Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "7HmcPxpIoJlv4HssgbhP",
      "question_number": 347,
      "page": 70,
      "question_text": "A company has a guideline that every Amazon EC2 instance must be launched from an AMI that the company’s security team produces. Every month, the security team sends an email message with the latest approved AMIs to all the development teams.\n\nThe development teams use AWS CloudFormation to deploy their applications. When developers launch a new service, they have to search their email for the latest AMIs that the security department sent. A DevOps engineer wants to automate the process that the security team uses to provide the AMI IDs to the development teams.\n\nWhat is the MOST scalable solution that meets these requirements?",
      "choices": {
        "A": "Direct the security team to use CloudFormation to create new versions of the AMIs and to list the AMI ARNs in an encrypted Amazon S3 object as part of the stack’s Outputs section. Instruct the developers to use a cross-stack reference to load the encrypted S3 object and obtain the most recent AMI ARNs.",
        "D": "Direct the security team to use Amazon EC2 Image Builder to create new AMIs and to create an Amazon Simple Notification Service (Amazon SNS) topic so that every development team can receive notifications. When the development teams receive a notification, instruct them to write an AWS Lambda function that will update their CloudFormation stack with the most recent AMI ARNs.",
        "C": "Direct the security team to use Amazon EC2 Image Builder to create new AMIs and to place the AMI ARNs as parameters in AWS Systems Manager Parameter Store. Instruct the developers to specify a parameter of type SSM in their CloudFormation stack to obtain the most recent AMI ARNs from Parameter Store.",
        "B": "Direct the security team to use a CloudFormation stack to create an AWS CodePipeline pipeline that builds new AMIs and places the latest AMI ARNs in an encrypted Amazon S3 object as part of the pipeline output. Instruct the developers to use a cross-stack reference within their own CloudFormation template to obtain the S3 object location and the most recent AMI ARNs."
      },
      "correct_answer": "C",
      "answer_ET": "C",
      "answers_community": [
        "C (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/108811-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-05-09 15:58:00",
      "unix_timestamp": 1683640680,
      "discussion_count": 7,
      "discussion": [
        {
          "content": "Selected Answer: C\nC is correct: <automate the process that the security team uses to provide the AMI IDs to the development teams> and <MOST scalable solution> means we need a pipeline (imange builder) to build AMI and to automate sharing\nA and B: no mention of EC2 Imange builder, which is better than codepipeline in building Ec2 image\nD: They have to do this manually",
          "timestamp": "1706933940.0",
          "poster": "thanhnv142",
          "comment_id": "1138995",
          "upvote_count": "6"
        },
        {
          "timestamp": "1731714360.0",
          "comment_id": "1312854",
          "poster": "ad3fdb1",
          "upvote_count": "1",
          "content": "A question to answer of option C - is it able to update the System Manager Parameter Store automatically? Option A seems able to do it automatically, right?"
        },
        {
          "comment_id": "1117584",
          "upvote_count": "2",
          "poster": "yuliaqwerty",
          "timestamp": "1704811980.0",
          "content": "C is the best option"
        },
        {
          "upvote_count": "2",
          "timestamp": "1698805620.0",
          "poster": "rlf",
          "content": "Answer is C.\nhttps://aws.amazon.com/ko/blogs/compute/tracking-the-latest-server-images-in-amazon-ec2-image-builder-pipelines/",
          "comment_id": "1059318"
        },
        {
          "content": "Selected Answer: C\nUse SSM Parameter Store or Secret Manager as the lookup K/V store for all the related AMIs. ANother way is also for security team to constantly update and share the images cross-account and grant them KMS keys to the encrypted AMIs. (not in question)",
          "upvote_count": "2",
          "comment_id": "946856",
          "timestamp": "1688871180.0",
          "poster": "habros"
        },
        {
          "content": "C is correct",
          "comment_id": "897267",
          "poster": "devnv",
          "timestamp": "1684042320.0",
          "upvote_count": "2"
        },
        {
          "poster": "ParagSanyashiv",
          "comment_id": "893146",
          "upvote_count": "4",
          "content": "Selected Answer: C\nC make more sense",
          "timestamp": "1683640680.0"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:20:50.533Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "B5vUzoCvJIPvSDdpctPV",
      "question_number": 348,
      "page": 70,
      "question_text": "An application runs on Amazon EC2 instances behind an Application Load Balancer (ALB). A DevOps engineer is using AWS CodeDeploy to release a new version. The deployment fails during the AllowTraffic lifecycle event, but a cause for the failure is not indicated in the deployment logs.\n\nWhat would cause this?",
      "choices": {
        "D": "The CodeDeploy agent was not installed in the EC2 instances that are part of the ALB target group.",
        "B": "The user who initiated the deployment does not have the necessary permissions to interact with the ALB.",
        "C": "The health checks specified for the ALB target group are misconfigured.",
        "A": "The appspec.yml file contains an invalid script that runs in the AllowTraffic lifecycle hook."
      },
      "correct_answer": "C",
      "answer_ET": "C",
      "answers_community": [
        "C (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/109204-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-05-14 07:36:00",
      "unix_timestamp": 1684042560,
      "discussion_count": 4,
      "discussion": [
        {
          "upvote_count": "17",
          "poster": "rhinozD",
          "timestamp": "1702655640.0",
          "content": "Selected Answer: C\nC is the answer\nrefer this: https://docs.aws.amazon.com/codedeploy/latest/userguide/troubleshooting-deployments.html#troubleshooting-deployments-allowtraffic-no-logs",
          "comment_id": "924247"
        },
        {
          "upvote_count": "5",
          "comment_id": "1138996",
          "poster": "thanhnv142",
          "timestamp": "1722651780.0",
          "content": "Selected Answer: C\nC is correct: <deployment fails during the AllowTraffic lifecycle event> means there are problems with ALB. \nA: no mention of the ALB\nB: The user who init the deployment does not need necessary permission\nD: If agent was not installed, it would fail from the start"
        },
        {
          "poster": "OrganizedChaos25",
          "comment_id": "899185",
          "timestamp": "1700145540.0",
          "upvote_count": "2",
          "content": "C is the answer"
        },
        {
          "poster": "devnv",
          "comment_id": "897271",
          "timestamp": "1699947360.0",
          "upvote_count": "1",
          "content": "C is the correct answer"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:20:50.533Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "rQgS5Agod7mwfoToFtl0",
      "question_number": 349,
      "page": 70,
      "question_text": "A company has 20 service teams. Each service team is responsible for its own microservice. Each service team uses a separate AWS account for its microservice and a VPC with the 192.168.0.0/22 CIDR block. The company manages the AWS accounts with AWS Organizations.\n\nEach service team hosts its microservice on multiple Amazon EC2 instances behind an Application Load Balancer. The microservices communicate with each other across the public internet. The company’s security team has issued a new guideline that all communication between microservices must use HTTPS over private network connections and cannot traverse the public internet.\n\nA DevOps engineer must implement a solution that fulfills these obligations and minimizes the number of changes for each service team.\n\nWhich solution will meet these requirements?",
      "choices": {
        "D": "Create a new AWS account in AWS Organizations. Create a transit gateway in this account, and use AWS Resource Access Manager to share the transit gateway with the organization. In each of the microservice VPCs, create a transit gateway attachment to the shared transit gateway. Update the route tables of each VPC to use the transit gateway. Create a Network Load Balancer (NLB) in each of the microservice VPCs. Use the NLB DNS names for communication between microservices.",
        "C": "Create a Network Load Balancer (NLB) in each of the microservice VPCs. Create VPC peering connections between each of the microservice VPCs. Update the route tables for each VPC to use the peering links. Use the NLB DNS names for communication between microservices.",
        "A": "Create a new AWS account in AWS Organizations. Create a VPC in this account, and use AWS Resource Access Manager to share the private subnets of this VPC with the organization. Instruct the service teams to launch a new Network Load Balancer (NLB) and EC2 instances that use the shared private subnets. Use the NLB DNS names for communication between microservices.",
        "B": "Create a Network Load Balancer (NLB) in each of the microservice VPCs. Use AWS PrivateLink to create VPC endpoints in each AWS account for the NLBs. Create subscriptions to each VPC endpoint in each of the other AWS accounts. Use the VPC endpoint DNS names for communication between microservices."
      },
      "correct_answer": "B",
      "answer_ET": "B",
      "answers_community": [
        "B (81%)",
        "D (19%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/108587-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-05-05 22:20:00",
      "unix_timestamp": 1683318000,
      "discussion_count": 18,
      "discussion": [
        {
          "upvote_count": "9",
          "content": "Selected Answer: B\nB is correct because all 20 services team in different separate AWS accounts are using the same CIDR block, which means they are overlapping CIDR. \n\nD state that to update the route tables of each VPC to use the transit gateway but they are all having the same CIDR block so this cannot proceed, as shared by Arnaud92 link the pre-requisite of using the transit gateway is \"No-overlapping CIDR block between VPCs.\"",
          "timestamp": "1688548680.0",
          "poster": "Blueee",
          "comment_id": "943526"
        },
        {
          "upvote_count": "1",
          "timestamp": "1731502800.0",
          "content": "Selected Answer: B\nPrivateLink = HTTPS connection",
          "poster": "Saudis",
          "comment_id": "1311256"
        },
        {
          "content": "B is the answer\nWhen VPCs have overlapping CIDR blocks, AWS PrivateLink still ensures secure and private connectivity by using Interface Endpoints (ENIs) and Network Load Balancers (NLBs) to route traffic, bypassing the need for direct IP routing between the VPCs.",
          "timestamp": "1716313380.0",
          "upvote_count": "3",
          "poster": "zijo",
          "comment_id": "1215141"
        },
        {
          "content": "B is correct: <all communication between microservices must use HTTPS over private network connections and cannot traverse the public internet> means privatelink\nA and C: no mention of privatelink\nD: Using transite gateway. But this solution need IP to route traffic and cannot be used for overlapped VPC CIDR block (every team uses 192.168.0.0/22)",
          "comment_id": "1139004",
          "timestamp": "1706935380.0",
          "poster": "thanhnv142",
          "upvote_count": "2"
        },
        {
          "comment_id": "1078634",
          "poster": "zolthar_z",
          "upvote_count": "4",
          "timestamp": "1700763360.0",
          "content": "Selected Answer: B\nAnswer is B, Transit gateway can't route overlapping networks, the solution for this is privatelink: https://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/aws-privatelink.html"
        },
        {
          "poster": "RVivek",
          "upvote_count": "2",
          "timestamp": "1694522400.0",
          "content": "Selected Answer: D\nThanks to rhinozD. Please check the side by sode comparision at the bottom of this page https://tomgregory.com/cross-account-vpc-access-in-aws",
          "comment_id": "1005750"
        },
        {
          "poster": "ixdb",
          "timestamp": "1692098100.0",
          "comment_id": "981569",
          "upvote_count": "2",
          "content": "B is right.,"
        },
        {
          "upvote_count": "3",
          "poster": "Just_Ninja",
          "content": "Selected Answer: B\nB. is the right Solution! \nDue to AWS's Transit Gateway not supporting same CIDRs (https://aws.amazon.com › transit-gateway › faqs), the most viable solution is the deployment of a Network Load Balancer (NLB) in each VPC. However, it's crucial to note that NLB operates similar to a NAT Gateway, allowing only incoming requests. After an incoming request is accepted, the NLB can then provide a response.",
          "comment_id": "949782",
          "timestamp": "1689165720.0"
        },
        {
          "timestamp": "1689097320.0",
          "upvote_count": "1",
          "poster": "SVGoogle89",
          "comment_id": "949210",
          "content": "AWS Transit Gateway doesn’t support routing between Amazon VPCs with identical CIDRs. If you attach a new Amazon VPC that has a CIDR which is identical to an already attached Amazon VPC, AWS Transit Gateway will not propagate the new Amazon VPC route into the AWS Transit Gateway route table."
        },
        {
          "poster": "habros",
          "upvote_count": "1",
          "comment_id": "946859",
          "content": "I’ll lean towards B. For D, transit gateway is really expensive and does get the job done. There is also a need for NAT gateway as by default all AWS API traffic passes through the public internet. Hence, PrivateLink endpoints are for.",
          "timestamp": "1688871900.0"
        },
        {
          "comment_id": "932907",
          "poster": "FunkyFresco",
          "upvote_count": "1",
          "content": "Selected Answer: D\nI go with option D. It makes more sense to me.",
          "timestamp": "1687638840.0"
        },
        {
          "comment_id": "927305",
          "poster": "allen_devops",
          "timestamp": "1687163700.0",
          "content": "I think the correct answer is B. Please note all service team is using the same cidr block for their vpc. It's impossible to add them in the same network mesh using vpc peering and transit gateway.",
          "upvote_count": "3"
        },
        {
          "comment_id": "915944",
          "content": "Selected Answer: D\nsee https://tomgregory.com/cross-account-vpc-access-in-aws/ , Option 3\nThe use of a central hub reduce the complexity for 20 accounts\nneed an additional account to avoid cidr block collision, in the link they put the transit gateway in one of existing account",
          "comments": [
            {
              "timestamp": "1686837660.0",
              "content": "Please read the \"Side-by-side comparison\" part at the end of the post.\nD is wrong.\nB is correct.",
              "poster": "rhinozD",
              "comment_id": "924254",
              "upvote_count": "2"
            }
          ],
          "poster": "Arnaud92",
          "timestamp": "1686031740.0",
          "upvote_count": "2"
        },
        {
          "comment_id": "915454",
          "upvote_count": "1",
          "poster": "youonebe",
          "timestamp": "1685969820.0",
          "content": "Answer is D.\nOption B is incorrect because it requires creating a Network Load Balancer in each of the microservice VPCs and using AWS PrivateLink to create VPC endpoints. This would result in a lot of configuration changes for each service team and increased complexity."
        },
        {
          "content": "B is the right answer",
          "timestamp": "1684042800.0",
          "upvote_count": "2",
          "comment_id": "897277",
          "poster": "devnv"
        },
        {
          "upvote_count": "4",
          "timestamp": "1683706380.0",
          "poster": "ParagSanyashiv",
          "content": "Selected Answer: B\nB is correct",
          "comment_id": "893742"
        },
        {
          "upvote_count": "3",
          "content": "Option D is correct to me.",
          "poster": "PhuocT",
          "comment_id": "891761",
          "timestamp": "1683514200.0"
        },
        {
          "timestamp": "1683318000.0",
          "comment_id": "890310",
          "comments": [
            {
              "comments": [
                {
                  "poster": "Arnaud92",
                  "content": "D is correct , central hub reduce complexity , need an additional account to avoid cidr block collision, in the link they put the transit gateway in one of existing account",
                  "comment_id": "915943",
                  "timestamp": "1686031680.0",
                  "upvote_count": "1"
                }
              ],
              "timestamp": "1685864040.0",
              "upvote_count": "1",
              "comment_id": "914238",
              "poster": "Arnaud92",
              "content": "see https://tomgregory.com/cross-account-vpc-access-in-aws/ , Option 3\nThe use of a central hub reduce the complexity for 20 accounts"
            }
          ],
          "upvote_count": "3",
          "content": "Option D is the correct solution to meet the requirements.",
          "poster": "kassem77"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:20:50.533Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "jIoUf6DgpB8ajoVAXKVA",
      "question_number": 350,
      "page": 70,
      "question_text": "An Amazon EC2 instance is running in a VPC and needs to download an object from a restricted Amazon S3 bucket. When the DevOps engineer tries to download the object, an AccessDenied error is received.\n\nWhat are the possible causes for this error? (Choose two.)",
      "choices": {
        "B": "There is an error in the S3 bucket policy.",
        "C": "The object has been moved to S3 Glacier.",
        "A": "The S3 bucket default encryption is enabled.",
        "D": "There is an error in the IAM role configuration.",
        "E": "S3 Versioning is enabled."
      },
      "correct_answer": "BD",
      "answer_ET": "BD",
      "answers_community": [
        "BD (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/109205-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-05-14 07:44:00",
      "unix_timestamp": 1684043040,
      "discussion_count": 8,
      "discussion": [
        {
          "timestamp": "1750302480.0",
          "poster": "xxxdolorxxx",
          "comment_id": "1578803",
          "upvote_count": "1",
          "content": "Selected Answer: BD\nB/D Get my vote."
        },
        {
          "upvote_count": "3",
          "timestamp": "1720530060.0",
          "poster": "yuliaqwerty",
          "content": "I think B and D",
          "comment_id": "1117590"
        },
        {
          "content": "ACCESS DENIED - you got it",
          "upvote_count": "2",
          "timestamp": "1719329520.0",
          "poster": "Jamshif01",
          "comment_id": "1105433"
        },
        {
          "upvote_count": "3",
          "content": "Selected Answer: BD\nIMHO it is BD",
          "poster": "RVivek",
          "comment_id": "1005755",
          "timestamp": "1710254820.0"
        },
        {
          "timestamp": "1706885520.0",
          "comment_id": "970182",
          "upvote_count": "4",
          "content": "Selected Answer: BD\nBD\nNot an error though. Misconfiguration.",
          "poster": "vherman"
        },
        {
          "comment_id": "933516",
          "timestamp": "1703510640.0",
          "upvote_count": "2",
          "content": "Selected Answer: BD\nB and D for sure.",
          "poster": "FunkyFresco"
        },
        {
          "upvote_count": "1",
          "comment_id": "899192",
          "timestamp": "1700145780.0",
          "content": "BD are the answers I got",
          "poster": "OrganizedChaos25"
        },
        {
          "poster": "devnv",
          "comment_id": "897279",
          "timestamp": "1699947840.0",
          "content": "BD are correct",
          "upvote_count": "1"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:20:50.533Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "OxQEwvkGekz39ikbXCoH",
      "question_number": 351,
      "page": 71,
      "question_text": "A company wants to use a grid system for a proprietary enterprise in-memory data store on top of AWS. This system can run in multiple server nodes in any Linux-based distribution. The system must be able to reconfigure the entire cluster every time a node is added or removed. When adding or removing nodes, an /etc/cluster/nodes.config file must be updated, listing the IP addresses of the current node members of that cluster.\n\nThe company wants to automate the task of adding new nodes to a cluster.\n\nWhat can a DevOps engineer do to meet these requirements?",
      "choices": {
        "B": "Put the file nodes.config in version control. Create an AWS CodeDeploy deployment configuration and deployment group based on an Amazon EC2 tag value for the cluster nodes. When adding a new node to the cluster, update the file with all tagged instances, and make a commit in version control. Deploy the new file and restart the services.",
        "C": "Create an Amazon S3 bucket and upload a version of the /etc/cluster/nodes.config file. Create a crontab script that will poll for that S3 file and download it frequently. Use a process manager, such as Monit or systemd, to restart the cluster services when it detects that the new file was modified. When adding a node to the cluster, edit the file’s most recent members. Upload the new file to the S3 bucket.",
        "A": "Use AWS OpsWorks Stacks to layer the server nodes of that cluster. Create a Chef recipe that populates the content of the /etc/cluster/nodes.config file and restarts the service by using the current members of the layer. Assign that recipe to the Configure lifecycle event.",
        "D": "Create a user data script that lists all members of the current security group of the cluster and automatically updates the /etc/cluster/nodes.config file whenever a new instance is added to the cluster."
      },
      "correct_answer": "A",
      "answer_ET": "A",
      "answers_community": [
        "A (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/109206-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-05-14 07:46:00",
      "unix_timestamp": 1684043160,
      "discussion_count": 8,
      "discussion": [
        {
          "upvote_count": "6",
          "comments": [
            {
              "comment_id": "1254518",
              "timestamp": "1721836740.0",
              "upvote_count": "1",
              "content": "But how the files content (get actual nodes list) is updated in that case?",
              "poster": "Exto1124"
            }
          ],
          "content": "Selected Answer: A\nI’ll use config management tool as well. In this case Opsworks (Chef/Puppet).",
          "poster": "habros",
          "timestamp": "1688872200.0",
          "comment_id": "946861"
        },
        {
          "upvote_count": "5",
          "timestamp": "1706948580.0",
          "comment_id": "1139082",
          "poster": "thanhnv142",
          "content": "Selected Answer: A\nA is correct: <wants to use a grid system> means opswork stacks\nB, C and D: no mention of opswork stack"
        },
        {
          "poster": "youonebe",
          "content": "Selected Answer: A\nAWS OpsWorks services have reached end of life and have been disabled for both new and existing customers. Will this question surface in the exam?\n\n\nhttps://aws.amazon.com/blogs/mt/migrate-your-aws-opsworks-stacks-to-aws-systems-manager/",
          "upvote_count": "3",
          "timestamp": "1734723900.0",
          "comment_id": "1329653"
        },
        {
          "poster": "hayjaykay",
          "comment_id": "1301344",
          "content": "D. \nThis approach ensures that your nodes.config file is kept up-to-date with minimal manual intervention. The script dynamically adjusts the cluster configuration by reflecting changes in the security group, making the process seamless. Efficient and automated—just the way it should be!",
          "timestamp": "1729557300.0",
          "upvote_count": "2"
        },
        {
          "comment_id": "1276538",
          "content": "D is the best",
          "poster": "Cloudxie",
          "upvote_count": "1",
          "timestamp": "1725265560.0"
        },
        {
          "upvote_count": "2",
          "comment_id": "1014399",
          "timestamp": "1695406380.0",
          "poster": "Dushank",
          "content": "Selected Answer: A\n1\nThe best solution to meet the company's requirements is to use AWS OpsWorks Stacks to layer the server nodes of the cluster. Create a Chef recipe that populates the content of the /etc/cluster/nodes.config file and restarts the service by using the current members of the layer. Assign that recipe to the Configure lifecycle event."
        },
        {
          "timestamp": "1686838620.0",
          "content": "Selected Answer: A\nA is correct.\nThis event occurs on all of the stack's instances when one of the following occurs:\nAn instance enters or leaves the online state.\nYou associate an Elastic IP address with an instance or disassociate one from an instance.\nYou attach an Elastic Load Balancing load balancer to a layer, or detach one from a layer.\nhttps://docs.aws.amazon.com/opsworks/latest/userguide/workingcookbook-events.html",
          "upvote_count": "2",
          "comment_id": "924269",
          "poster": "rhinozD"
        },
        {
          "upvote_count": "2",
          "timestamp": "1684043160.0",
          "comment_id": "897282",
          "poster": "devnv",
          "content": "A is correct"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:21:00.962Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "nM4NtbUy5MVafS168Tlx",
      "question_number": 352,
      "page": 71,
      "question_text": "A DevOps engineer is working on a data archival project that requires the migration of on-premises data to an Amazon S3 bucket. The DevOps engineer develops a script that incrementally archives on-premises data that is older than 1 month to Amazon S3. Data that is transferred to Amazon S3 is deleted from the on-premises location. The script uses the S3 PutObject operation.\n\nDuring a code review, the DevOps engineer notices that the script does not verify whether the data was successfully copied to Amazon S3. The DevOps engineer must update the script to ensure that data is not corrupted during transmission. The script must use MD5 checksums to verify data integrity before the on-premises data is deleted.\n\nWhich solutions for the script will meet these requirements? (Choose two.)",
      "choices": {
        "B": "Include the MD5 checksum within the Content-MD5 parameter. Check the operation call’s return status to find out if an error was returned.",
        "A": "Check the returned response for the VersionId. Compare the returned VersionId against the MD5 checksum.",
        "C": "Include the checksum digest within the tagging parameter as a URL query parameter.",
        "D": "Check the returned response for the ETag. Compare the returned ETag against the MD5 checksum.",
        "E": "Include the checksum digest within the Metadata parameter as a name-value pair. After upload, use the S3 HeadObject operation to retrieve metadata from the object."
      },
      "correct_answer": "BD",
      "answer_ET": "BD",
      "answers_community": [
        "BD (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/109207-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-05-14 07:49:00",
      "unix_timestamp": 1684043340,
      "discussion_count": 6,
      "discussion": [
        {
          "upvote_count": "11",
          "timestamp": "1706208960.0",
          "comment_id": "962910",
          "content": "Selected Answer: BD\nB. Explanation: When using the S3 PutObject operation, you can include the MD5 checksum of the object in the Content-MD5 parameter of the request. Amazon S3 will calculate the MD5 checksum of the object and compare it to the provided checksum. If the checksums do not match, Amazon S3 will return an error response, indicating that the data integrity check failed. This way, you can ensure that the data was successfully copied to Amazon S3 without corruption.\n\nD. Explanation: When you use the S3 PutObject operation, it returns an ETag in the response, which is the MD5 checksum of the object that was stored in Amazon S3. After performing the upload, you can check the returned ETag against the MD5 checksum you have locally calculated. If they match, it means the data was transferred successfully without corruption. If they don't match, it indicates a data integrity issue, and you can take appropriate actions.",
          "poster": "haazybanj"
        },
        {
          "content": "Selected Answer: BD\nB: the content-MD5 header is used to provide a base64encoded 128-bit MD5 digest of the object data\nwhen included in a PutObject request, S3 checks the provided MD5 against what it calculates during upload\nIf the checksums dont match, the request fails with an error (like http 4xx)\nthis will ensure data was not corrupted during upload\n\nD: for single part uploads, the ETag returned by S3 is usually the MD5 checksum of the object.\nby calculating the MD5 of the local file and comparing it to the returned ETag, the script can verify integrity",
          "timestamp": "1745137740.0",
          "upvote_count": "1",
          "comment_id": "1562165",
          "poster": "GripZA"
        },
        {
          "comment_id": "1159411",
          "content": "Selected Answer: BD\nIf the object was created by a PutObject, PostObject, or Copy operation, or via the AWS Management Console, and the object is either plain text or encrypted with server-side encryption using the Amazon S3 managed key ( SSE-S3), the object's ETag is the MD5 digest of the object data.",
          "poster": "dzn",
          "upvote_count": "3",
          "timestamp": "1724640960.0"
        },
        {
          "content": "Selected Answer: BD\nB and D are correct: <verify whether the data was successfully copied to Amazon S3> means we need to check <operation call’s return status> code. <use MD5 checksums to verify data integrity> means we need to check ETag\nA: no mention of ETag\nC and E: no mention of ETag or return status code",
          "comment_id": "1139086",
          "poster": "thanhnv142",
          "timestamp": "1722666600.0",
          "upvote_count": "3"
        },
        {
          "comment_id": "924274",
          "poster": "rhinozD",
          "upvote_count": "4",
          "content": "Selected Answer: BD\nBD\nrefer this link: https://docs.aws.amazon.com/AmazonS3/latest/userguide/checking-object-integrity.html",
          "timestamp": "1702657440.0"
        },
        {
          "upvote_count": "3",
          "timestamp": "1699948140.0",
          "comment_id": "897284",
          "content": "BD are correct",
          "poster": "devnv"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:21:00.962Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "cI1qS1z3gAMBiqmvopJR",
      "question_number": 353,
      "page": 71,
      "question_text": "A company uses AWS Key Management Service (AWS KMS) keys and manual key rotation to meet regulatory compliance requirements. The security team wants to be notified when any keys have not been rotated after 90 days.\nWhich solution will accomplish this?",
      "choices": {
        "C": "Develop an AWS Config custom rule that publishes to an Amazon Simple Notification Service (Amazon SNS) topic when keys are more than 90 days old.",
        "D": "Configure AWS Security Hub to publish to an Amazon Simple Notification Service (Amazon SNS) topic when keys are more than 90 days old.",
        "A": "Configure AWS KMS to publish to an Amazon Simple Notification Service (Amazon SNS) topic when keys are more than 90 days old.",
        "B": "Configure an Amazon EventBridge event to launch an AWS Lambda function to call the AWS Trusted Advisor API and publish to an Amazon Simple Notification Service (Amazon SNS) topic."
      },
      "correct_answer": "C",
      "answer_ET": "C",
      "answers_community": [
        "C (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/105235-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-04-05 02:32:00",
      "unix_timestamp": 1680654720,
      "discussion_count": 10,
      "discussion": [
        {
          "upvote_count": "5",
          "content": "C is correct\nA is not because KMS does not provide this function",
          "comment_id": "1133447",
          "poster": "thanhnv142",
          "timestamp": "1722086820.0"
        },
        {
          "timestamp": "1720356600.0",
          "upvote_count": "3",
          "content": "Answer C. AWS Config",
          "comment_id": "1115928",
          "poster": "yuliaqwerty"
        },
        {
          "poster": "habros",
          "timestamp": "1704381840.0",
          "content": "Selected Answer: C\nC. Config rules notifies.",
          "upvote_count": "3",
          "comment_id": "942803"
        },
        {
          "comment_id": "936928",
          "timestamp": "1703789700.0",
          "upvote_count": "3",
          "content": "Selected Answer: C\nAre these questions really came from DOP-C02?",
          "poster": "Toptip"
        },
        {
          "poster": "madperro",
          "content": "Selected Answer: C\nC makes sense. it should be a custom rule. Rule \"access-keys-rotated\" checks for access keys, not KMS keys.",
          "upvote_count": "2",
          "timestamp": "1702037100.0",
          "comment_id": "918097"
        },
        {
          "content": "C it is",
          "upvote_count": "1",
          "poster": "alce2020",
          "comment_id": "871136",
          "timestamp": "1697389320.0"
        },
        {
          "timestamp": "1696681320.0",
          "comment_id": "863848",
          "content": "Selected Answer: C\ncustom config: C",
          "poster": "ele",
          "upvote_count": "1"
        },
        {
          "timestamp": "1696577880.0",
          "upvote_count": "4",
          "poster": "asfsdfsdf",
          "comment_id": "862749",
          "content": "Selected Answer: C\nLooks like C, actually there is a managed rule for this:\nhttps://docs.aws.amazon.com/config/latest/developerguide/access-keys-rotated.html\nanyway trusted advisor cannot be used as there is no such check, also KMS does not have this action, security hub is not conducting any active checks just react to events",
          "comments": [
            {
              "upvote_count": "1",
              "comment_id": "1156606",
              "timestamp": "1724343240.0",
              "content": "IAM Access Key & KMS key are different. The managed rule is for IAM Access key",
              "poster": "zijo"
            },
            {
              "upvote_count": "3",
              "timestamp": "1705167840.0",
              "comment_id": "950809",
              "content": "access key?",
              "poster": "s50600822"
            }
          ]
        },
        {
          "comment_id": "862441",
          "upvote_count": "2",
          "content": "Selected Answer: C\nC for me. A there no such functionality, B i checked trusted advisor there is no such kms days, d is aggregator for config, guardduty. So you need config for D",
          "poster": "Dimidrol",
          "timestamp": "1696535820.0"
        },
        {
          "timestamp": "1696465920.0",
          "comment_id": "861638",
          "poster": "lqpO_Oqpl",
          "upvote_count": "1",
          "content": "Tell me Why not D..",
          "comments": [
            {
              "content": "When you enable a control in Security hub it will automatically create a Config. There are 4 KMS related controls in security hub but none of them is about the rotation age. In this case you need to create a custom Config.",
              "upvote_count": "1",
              "poster": "beanxyz",
              "timestamp": "1708412160.0",
              "comment_id": "985552"
            },
            {
              "comment_id": "917414",
              "upvote_count": "3",
              "timestamp": "1701974340.0",
              "poster": "Manny20",
              "content": "• Option D is not the correct answer because AWS Security Hub is primarily focused on aggregating and managing security findings, and it does not have a specific feature to monitor the age of AWS KMS keys."
            }
          ]
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:21:00.962Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "BRKbRC8MNnvVW3NAeC8P",
      "question_number": 354,
      "page": 71,
      "question_text": "A company deploys updates to its Amazon API Gateway API several times a week by using an AWS CodePipeline pipeline. As part of the update process, the company exports the JavaScript SDK for the API from the API Gateway console and uploads the SDK to an Amazon S3 bucket.\n\nThe company has configured an Amazon CloudFront distribution that uses the S3 bucket as an origin. Web clients then download the SDK by using the CloudFront distribution’s endpoint. A DevOps engineer needs to implement a solution to make the new SDK available automatically during new API deployments.\n\nWhich solution will meet these requirements?",
      "choices": {
        "C": "Create an Amazon EventBridge rule that reacts to UpdateStage events from aws.apigateway. Configure the rule to invoke an AWS Lambda function to download the SDK from API Gateway, upload the SDK to the S3 bucket, and call the CloudFront API to create an invalidation for the SDK path.",
        "A": "Create a CodePipeline action immediately after the deployment stage of the API. Configure the action to invoke an AWS Lambda function. Configure the Lambda function to download the SDK from API Gateway, upload the SDK to the S3 bucket, and create a CloudFront invalidation for the SDK path.",
        "D": "Create an Amazon EventBridge rule that reacts to CreateDeployment events from aws.apigateway. Configure the rule to invoke an AWS Lambda function to download the SDK from API Gateway, upload the SDK to the S3 bucket, and call the S3 API to invalidate the cache for the SDK path.",
        "B": "Create a CodePipeline action immediately after the deployment stage of the API. Configure the action to use the CodePipeline integration with API Gateway to export the SDK to Amazon S3. Create another action that uses the CodePipeline integration with Amazon S3 to invalidate the cache for the SDK path."
      },
      "correct_answer": "A",
      "answer_ET": "A",
      "answers_community": [
        "A (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/109209-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-05-14 07:53:00",
      "unix_timestamp": 1684043580,
      "discussion_count": 7,
      "discussion": [
        {
          "timestamp": "1722669300.0",
          "poster": "thanhnv142",
          "content": "Selected Answer: A\nA is correct: <by using an AWS CodePipeline pipeline> means we need CodePipeline. \nC and D: no mention of CodePipeline.\nB: < Configure the action to use the CodePipeline integration with API Gateway to export the SDK to Amazon S3>: codepipeline need to invoke other tools to do its task. There is not integration with API gateway",
          "comment_id": "1139120",
          "upvote_count": "6"
        },
        {
          "poster": "GripZA",
          "content": "Selected Answer: A\nA - AWS Lambda is a compute service that lets you run code without provisioning or managing servers. You can create Lambda functions and add them as actions in your pipelines. Don't need eventbridge",
          "timestamp": "1745139180.0",
          "comment_id": "1562169",
          "upvote_count": "1"
        },
        {
          "poster": "zanhsieh",
          "timestamp": "1714482240.0",
          "upvote_count": "4",
          "content": "Selected Answer: A\nVote A. Reasons:\nC: No. \"aws.apigateway needs API Gateway AWS integration to send events to EventBridge without using compute service, such as Lambda or Amazon EC2.\"\nhttps://aws.amazon.com/blogs/compute/capturing-client-events-using-amazon-api-gateway-and-amazon-eventbridge/\nB&D: No. S3 API doesn't contain invalidate cache call, whereas CloudFront does. Search \"invalidat\" in\nhttps://docs.aws.amazon.com/cli/latest/reference/s3api/\nhttps://docs.aws.amazon.com/cli/latest/reference/cloudfront/",
          "comment_id": "1057937"
        },
        {
          "timestamp": "1710259200.0",
          "content": "Selected Answer: A\nB & D are wrong as it suggests to invalidate the cache fro S3\nD uses event bridge rule to invoke lambada however we know Codepipeline is available and that can be used to perform the action",
          "poster": "RVivek",
          "comment_id": "1005824",
          "upvote_count": "2"
        },
        {
          "comment_id": "993262",
          "poster": "FEEREWMWKA",
          "timestamp": "1709230920.0",
          "content": "I pick C due to it stating during the deployment rather than at the end.",
          "upvote_count": "1"
        },
        {
          "poster": "ProfXsamson",
          "timestamp": "1704253320.0",
          "comments": [
            {
              "content": "??????",
              "upvote_count": "4",
              "poster": "habros",
              "comment_id": "946868",
              "timestamp": "1704779160.0"
            }
          ],
          "upvote_count": "4",
          "content": "Selected Answer: A\nLambda is king",
          "comment_id": "941360"
        },
        {
          "upvote_count": "3",
          "comment_id": "897288",
          "poster": "devnv",
          "content": "A is the right answer",
          "timestamp": "1699948380.0"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:21:00.962Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "zYuZ2C1dHcFcR48yabFn",
      "question_number": 355,
      "page": 71,
      "question_text": "A company has developed an AWS Lambda function that handles orders received through an API. The company is using AWS CodeDeploy to deploy the Lambda function as the final stage of a CI/CD pipeline.\n\nA DevOps engineer has noticed there are intermittent failures of the ordering API for a few seconds after deployment. After some investigation, the DevOps engineer believes the failures are due to database changes not having fully propagated before the Lambda function is invoked.\n\nHow should the DevOps engineer overcome this?",
      "choices": {
        "D": "Add a ValidateService hook to the AppSpec file that inspects incoming traffic and rejects the payload if dependent services, such as the database, are not yet ready.",
        "C": "Add a BeforeInstall hook to the AppSpec file that tests and waits for any necessary database changes before deploying the new version of the Lambda function.",
        "A": "Add a BeforeAllowTraffic hook to the AppSpec file that tests and waits for any necessary database changes before traffic can flow to the new version of the Lambda function.",
        "B": "Add an AfterAllowTraffic hook to the AppSpec file that forces traffic to wait for any pending database changes before allowing the new version of the Lambda function to respond."
      },
      "correct_answer": "A",
      "answer_ET": "A",
      "answers_community": [
        "A (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/109215-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-05-14 10:22:00",
      "unix_timestamp": 1684052520,
      "discussion_count": 7,
      "discussion": [
        {
          "comment_id": "1230653",
          "content": "Selected Answer: A\nIn my research, there are only TWO CodeDeploy AppSpec ifecycle event hooks for Lambda deployment:\n BeforeAllowTraffic # Use to run tasks before traffic is shifted to the deployed Lambda function version.\n AfterAllowTraffic # Use to run tasks after all traffic is shifted to the deployed Lambda function version.\n\nA: (YES) Don't redirect traffic untill ready\nB: (NO) Block traffic until ready\nC: (NO) Event hook N/A for Lambda\nD: (NO) Event hook N/A for Lambda",
          "timestamp": "1718397480.0",
          "upvote_count": "5",
          "poster": "Gomer"
        },
        {
          "content": "A is correct: <using AWS CodeDeploy to deploy> and <a CI/CD pipeline> means lifecycle event hook\nB: AfterAllowTraffic wont solve the problem, we need to hook before traffic is allowed, as in <not having fully propagated before the Lambda function is invoked>\nC: beforeInstall is used to prepare for the installation process, so it is not relevant\nD: there is no ValidateService hook",
          "poster": "thanhnv142",
          "upvote_count": "5",
          "timestamp": "1706952300.0",
          "comment_id": "1139128"
        },
        {
          "comment_id": "1205449",
          "content": "Selected Answer: A\nI think A",
          "poster": "seetpt",
          "upvote_count": "1",
          "timestamp": "1714645980.0"
        },
        {
          "poster": "jojom19980",
          "upvote_count": "3",
          "comment_id": "1153788",
          "content": "Selected Answer: A\nD can be correct if there is a wait to database to be ready so I will go with A",
          "timestamp": "1708332540.0"
        },
        {
          "timestamp": "1688874660.0",
          "upvote_count": "3",
          "comment_id": "946870",
          "content": "\"Hooks\": [\n {\n \"BeforeInstall\": \"BeforeInstallHookFunctionName\"\n },\n {\n \"AfterInstall\": \"AfterInstallHookFunctionName\"\n },\n {\n \"AfterAllowTestTraffic\": \"AfterAllowTestTrafficHookFunctionName\"\n },\n {\n \"BeforeAllowTraffic\": \"BeforeAllowTrafficHookFunctionName\"\n },\n {\n \"AfterAllowTraffic\": \"AfterAllowTrafficHookFunctionName\"\n }\n ]\n}",
          "comments": [
            {
              "comments": [
                {
                  "poster": "HugoFM",
                  "comment_id": "1082381",
                  "timestamp": "1701165360.0",
                  "upvote_count": "6",
                  "content": "Those hooks are not valid for a Lambda, see the doc.\nLambda only supports BeforeAllowTraffic and AfterAllowTraffic. Anyway the answer is A"
                }
              ],
              "comment_id": "946871",
              "upvote_count": "2",
              "poster": "habros",
              "content": "Opting for A based on this",
              "timestamp": "1688874720.0"
            }
          ],
          "poster": "habros"
        },
        {
          "upvote_count": "2",
          "content": "A is the right answer",
          "comment_id": "899204",
          "poster": "OrganizedChaos25",
          "timestamp": "1684241820.0"
        },
        {
          "timestamp": "1684052520.0",
          "content": "A is correct",
          "comment_id": "897388",
          "poster": "devnv",
          "upvote_count": "2"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:21:00.962Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "1pZSoXp4mxAsrx0AHIVB",
      "question_number": 356,
      "page": 72,
      "question_text": "A company uses a single AWS account to test applications on Amazon EC2 instances. The company has turned on AWS Config in the AWS account and has activated the restricted-ssh AWS Config managed rule.\n\nThe company needs an automated monitoring solution that will provide a customized notification in real time if any security group in the account is not compliant with the restricted-ssh rule. The customized notification must contain the name and ID of the noncompliant security group.\n\nA DevOps engineer creates an Amazon Simple Notification Service (Amazon SNS) topic in the account and subscribes the appropriate personnel to the topic.\n\nWhat should the DevOps engineer do next to meet these requirements?",
      "choices": {
        "B": "Configure AWS Config to send all evaluation results for the restricted-ssh rule to the SNS topic. Configure a filter policy on the SNS topic to send only notifications that contain the text of NON_COMPLIANT in the notification to subscribers.",
        "A": "Create an Amazon EventBridge rule that matches an AWS Config evaluation result of NON_COMPLIANT for the restricted-ssh rule. Configure an input transformer for the EventBridge rule. Configure the EventBridge rule to publish a notification to the SNS topic.",
        "D": "Create an Amazon EventBridge rule that matches all AWS Config evaluation results of NON_COMPLIANT. Configure an input transformer for the restricted-ssh rule. Configure the EventBridge rule to publish a notification to the SNS topic.",
        "C": "Create an Amazon EventBridge rule that matches an AWS Config evaluation result of NON_COMPLIANT for the restricted-ssh rule. Configure the EventBridge rule to invoke AWS Systems Manager Run Command on the SNS topic to customize a notification and to publish the notification to the SNS topic."
      },
      "correct_answer": "A",
      "answer_ET": "A",
      "answers_community": [
        "A (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/109216-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-05-14 10:28:00",
      "unix_timestamp": 1684052880,
      "discussion_count": 9,
      "discussion": [
        {
          "comment_id": "1317462",
          "poster": "steli0",
          "upvote_count": "1",
          "content": "Selected Answer: A\nD is tricky since it's not clear if the input transformer mentioned in the answer is supposed to be applied to the config rule or the EventBridge rule.",
          "timestamp": "1732536600.0"
        },
        {
          "poster": "zijo",
          "timestamp": "1717693980.0",
          "upvote_count": "3",
          "content": "AWS Config can send notifications to an SNS topic directly but here you need a customized notification which is only possible with the input transformer in Amazon EventBridge. So I think A is the better choice.",
          "comment_id": "1225656"
        },
        {
          "content": "B\nAWS Config can send notifications directly to SNS.",
          "timestamp": "1715090820.0",
          "upvote_count": "2",
          "comment_id": "1207906",
          "poster": "MalonJay"
        },
        {
          "timestamp": "1709105340.0",
          "content": "A，About strict-ssh https://docs.aws.amazon.com/zh_cn/config/latest/developerguide/restricted-ssh.html",
          "upvote_count": "1",
          "poster": "Heyang",
          "comment_id": "1161364"
        },
        {
          "poster": "thanhnv142",
          "timestamp": "1706955420.0",
          "content": "Selected Answer: A\nA is correct: <needs an automated monitoring solution that will provide a customized notification> and <creates an Amazon Simple Notification Service (Amazon SNS) topic> means they have already have SNS. we need to trigger alarm with eventbridge and send noti to SNS\nB: no mention of event bride\nC: AWS Systems Manager Run Command on the SNS topic to customize a notification: this step is unnecessary\nD: <matches all AWS Config evaluation results of NON_COMPLIAN>: we need to match NON_COMPLIANT for the restricted-ssh rule only",
          "upvote_count": "4",
          "comment_id": "1139155"
        },
        {
          "poster": "beanxyz",
          "timestamp": "1692872400.0",
          "upvote_count": "2",
          "content": "Selected Answer: A\nHere is an example\nhttps://repost.aws/knowledge-center/config-resource-non-compliant",
          "comment_id": "989074"
        },
        {
          "comments": [
            {
              "content": "Sorry A \n\nEventBridge input transformers are used to customize the data that is sent to a target of an EventBridge rule. They can be used to extract specific data from the event, to convert the data to a different format, or to filter the data.",
              "poster": "Aja1",
              "upvote_count": "3",
              "timestamp": "1692011400.0",
              "comment_id": "980716"
            },
            {
              "upvote_count": "1",
              "content": "why would you want to customize anything to SNS. I chose C, but A makes more sense. no need for sns customization",
              "comment_id": "1115436",
              "timestamp": "1704574860.0",
              "poster": "Jaguaroooo"
            }
          ],
          "comment_id": "966241",
          "upvote_count": "1",
          "content": "Option C is the most appropriate solution for creating a customized SNS notification when the restricted-ssh AWS Config rule is evaluated as NON_COMPLIANT.",
          "timestamp": "1690620120.0",
          "poster": "Aja1"
        },
        {
          "content": "Selected Answer: A\nA\nThe Amazon EventBridge rule should be set up to match AWS Config evaluation results specifically for the restricted-ssh rule.\nAn input transformer should be configured for the EventBridge rule to extract and format the required information (e.g., name and ID of the noncompliant security group) from the AWS Config evaluation result.\nThe EventBridge rule should be configured to publish a notification to the SNS topic once it detects a noncompliant result.",
          "poster": "haazybanj",
          "upvote_count": "3",
          "comment_id": "957727",
          "timestamp": "1689865860.0"
        },
        {
          "upvote_count": "2",
          "timestamp": "1684052880.0",
          "poster": "devnv",
          "comment_id": "897391",
          "content": "A is the right answer"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:21:11.408Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "GpFruqSZ0VATXXqO1QUF",
      "question_number": 357,
      "page": 72,
      "question_text": "A company requires an RPO of 2 hours and an RTO of 10 minutes for its data and application at all times. An application uses a MySQL database and Amazon EC2 web servers. The development team needs a strategy for failover and disaster recovery.\n\nWhich combination of deployment strategies will meet these requirements? (Choose two.)",
      "choices": {
        "C": "Create an Amazon Aurora multi-master cluster across multiple Regions as the data store. Use a Network Load Balancer to balance the database traffic in different Regions.",
        "A": "Create an Amazon Aurora cluster in one Availability Zone across multiple Regions as the data store. Use Aurora’s automatic recovery capabilities in the event of a disaster.",
        "B": "Create an Amazon Aurora global database in two Regions as the data store. In the event of a failure, promote the secondary Region as the primary for the application.",
        "E": "Set up the application in two Regions and use a multi-Region Auto Scaling group behind Application Load Balancers to manage the capacity based on demand. In the event of a disaster, adjust the Auto Scaling group’s desired instance count to increase baseline capacity in the failover Region.",
        "D": "Set up the application in two Regions and use Amazon Route 53 failover-based routing that points to the Application Load Balancers in both Regions. Use health checks to determine the availability in a given Region. Use Auto Scaling groups in each Region to adjust capacity based on demand."
      },
      "correct_answer": "BD",
      "answer_ET": "BD",
      "answers_community": [
        "BD (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/108863-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-05-10 14:22:00",
      "unix_timestamp": 1683721320,
      "discussion_count": 7,
      "discussion": [
        {
          "content": "Amazon Aurora clusters are designed to be region-specific, meaning that an Aurora DB cluster is limited to a single AWS region. However, you can use Amazon Aurora Global Database to span multiple AWS regions. But Amazon Aurora clusters can span across multiple AZs.\nAn AWS Auto Scaling group cannot span multiple regions. Each Auto Scaling group is limited to a single AWS region. However, within that region, an Auto Scaling group can span multiple Availability Zones to ensure high availability and fault tolerance.",
          "timestamp": "1733516040.0",
          "poster": "zijo",
          "comment_id": "1225710",
          "upvote_count": "1"
        },
        {
          "poster": "thanhnv142",
          "timestamp": "1722673980.0",
          "content": "Selected Answer: BD\nB and D are correct: <needs a strategy for failover and disaster recovery> means global table or db cluster and route53 fail-over policy\nA and C: These options mention spanning an Amazon Aurora cluster across multiple region. This is not true. A cluster can span across multiple AZs, not regions. The only Aurora solution that can span multiple regions is global table, which includes multiple clusters.\nE: No mention of Route 53 failover-based routing",
          "upvote_count": "4",
          "comment_id": "1139162"
        },
        {
          "comment_id": "1043488",
          "timestamp": "1713101760.0",
          "poster": "MaiHuong",
          "content": "between ABC, choose B. A is wrong because “Amazon Aurora cluster in one Availability Zone across multiple Regions” is nonsense. C is incorrect too because Aurora multi-master cluster can't be across multiple regions\n\nbetween DE, choose D because using Route 53 failover-based routing makes sense. E is wrong Auto Scaling group can't be multi-region",
          "upvote_count": "4"
        },
        {
          "upvote_count": "4",
          "timestamp": "1705443300.0",
          "content": "Selected Answer: BD\nNo brainer",
          "comment_id": "953610",
          "poster": "Snape"
        },
        {
          "poster": "OrganizedChaos25",
          "timestamp": "1700147280.0",
          "comment_id": "899214",
          "content": "Got BD as my answers",
          "upvote_count": "4"
        },
        {
          "comment_id": "897397",
          "content": "BD are correct",
          "upvote_count": "2",
          "timestamp": "1699957980.0",
          "poster": "devnv"
        },
        {
          "upvote_count": "3",
          "comment_id": "893891",
          "content": "Selected Answer: BD\nBD is the correct answer",
          "poster": "ParagSanyashiv",
          "timestamp": "1699626120.0"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:21:11.408Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "KauxU4OSC3Jm2BMPh0ZA",
      "question_number": 358,
      "page": 72,
      "question_text": "A business has an application that consists of five independent AWS Lambda functions.\n\nThe DevOps engineer has built a CI/CD pipeline using AWS CodePipeline and AWS CodeBuild that builds, tests, packages, and deploys each Lambda function in sequence. The pipeline uses an Amazon EventBridge rule to ensure the pipeline starts as quickly as possible after a change is made to the application source code.\n\nAfter working with the pipeline for a few months, the DevOps engineer has noticed the pipeline takes too long to complete.\n\nWhat should the DevOps engineer implement to BEST improve the speed of the pipeline?",
      "choices": {
        "C": "Modify the CodePipeline configuration to run actions for each Lambda function in parallel by specifying the same runOrder.",
        "A": "Modify the CodeBuild projects within the pipeline to use a compute type with more available network throughput.",
        "D": "Modify each CodeBuild project to run within a VPC and use dedicated instances to increase throughput.",
        "B": "Create a custom CodeBuild execution environment that includes a symmetric multiprocessing configuration to run the builds in parallel."
      },
      "correct_answer": "C",
      "answer_ET": "C",
      "answers_community": [
        "C (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/108864-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-05-10 14:30:00",
      "unix_timestamp": 1683721800,
      "discussion_count": 6,
      "discussion": [
        {
          "timestamp": "1711140540.0",
          "upvote_count": "9",
          "content": "Selected Answer: C\nParallel Execution:\nBy modifying the CodePipeline configuration to run actions for each Lambda function in parallel with the same runOrder, you allow multiple Lambda functions to be built and deployed simultaneously, which significantly improves the overall speed of the pipeline.\n\nRunOrder:\nThe runOrder parameter in CodePipeline allows you to specify the order in which actions run. If multiple actions have the same runOrder, they can run in parallel.",
          "poster": "Dushank",
          "comments": [
            {
              "poster": "davdan99",
              "timestamp": "1720451100.0",
              "comment_id": "1116824",
              "upvote_count": "1",
              "content": "Thanks for runOrder"
            }
          ],
          "comment_id": "1014413"
        },
        {
          "poster": "ParagSanyashiv",
          "content": "Selected Answer: C\nAgree with C",
          "comment_id": "893896",
          "upvote_count": "7",
          "timestamp": "1699626600.0"
        },
        {
          "poster": "thanhnv142",
          "timestamp": "1722693600.0",
          "comment_id": "1139414",
          "upvote_count": "2",
          "content": "C is correct: <the pipeline takes too long to complete> and <consists of five independent AWS Lambda functions> means we should run the lambda funcs in parallel by specifying the same runOrder\nA and D: no mention of running in parallel\nB: No mention of runOrder"
        },
        {
          "timestamp": "1703112660.0",
          "content": "Selected Answer: C\nYeah, it's definitely C.",
          "comment_id": "928796",
          "poster": "MarDog",
          "upvote_count": "4"
        },
        {
          "poster": "OrganizedChaos25",
          "upvote_count": "3",
          "timestamp": "1700147280.0",
          "comment_id": "899215",
          "content": "Answer is C"
        },
        {
          "timestamp": "1699958460.0",
          "upvote_count": "3",
          "poster": "devnv",
          "comment_id": "897403",
          "content": "C is right answer"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:21:11.408Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "AHdzmmyORBualtGpgTk5",
      "question_number": 359,
      "page": 72,
      "question_text": "A company uses AWS CloudFormation stacks to deploy updates to its application. The stacks consist of different resources. The resources include AWS Auto Scaling groups, Amazon EC2 instances, Application Load Balancers (ALBs), and other resources that are necessary to launch and maintain independent stacks. Changes to application resources outside of CloudFormation stack updates are not allowed.\n\nThe company recently attempted to update the application stack by using the AWS CLI. The stack failed to update and produced the following error message: “ERROR: both the deployment and the CloudFormation stack rollback failed. The deployment failed because the following resource(s) failed to update: [AutoScalingGroup].”\n\nThe stack remains in a status of UPDATE_ROLLBACK_FAILED.\n\nWhich solution will resolve this issue?",
      "choices": {
        "C": "Submit a request for a quota increase for the number of EC2 instances for the account. Run the aws cloudformation cancel-update-stack AWS CLI command.",
        "A": "Update the subnet mappings that are configured for the ALBs. Run the aws cloudformation update-stack-set AWS CLI command.",
        "D": "Delete the Auto Scaling group resource. Run the aws cloudformation rollback-stack AWS CLI command.",
        "B": "Update the IAM role by providing the necessary permissions to update the stack. Run the aws cloudformation continue-update-rollback AWS CLI command."
      },
      "correct_answer": "B",
      "answer_ET": "B",
      "answers_community": [
        "B (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/109217-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-05-14 10:48:00",
      "unix_timestamp": 1684054080,
      "discussion_count": 7,
      "discussion": [
        {
          "timestamp": "1704456660.0",
          "comment_id": "943564",
          "poster": "Blueee",
          "upvote_count": "11",
          "content": "Selected Answer: B\nhttps://repost.aws/knowledge-center/cloudformation-update-rollback-failed\nIf your stack is stuck in the UPDATE_ROLLBACK_FAILED state after a failed update, then the only actions that you can perform on the stack are the ContinueUpdateRollback or DeleteStack operations.\n\nSo only B has ContinueUpdateRollback"
        },
        {
          "timestamp": "1733520540.0",
          "upvote_count": "1",
          "comment_id": "1225750",
          "poster": "zijo",
          "content": "To update an AWS CloudFormation stack, you need an IAM role with permissions that allow you to perform the necessary actions on the resources defined in your CloudFormation template, as well as on the CloudFormation service itself. B is the answer"
        },
        {
          "comment_id": "1139434",
          "timestamp": "1722694080.0",
          "poster": "thanhnv142",
          "upvote_count": "3",
          "content": "B is correct: <UPDATE_ROLLBACK_FAILED> means we are left with only two options: continue-update-rollback or delete-stack. We should provide necessary permissions to update the stack as well\nA, C and D: no mention of continue-update-rollback or adding necessary permissions"
        },
        {
          "poster": "Dushank",
          "timestamp": "1711140900.0",
          "upvote_count": "2",
          "content": "Selected Answer: B\nThey should update the IAM role by providing the necessary permissions to update the stack and then run the aws cloudformation continue-update-rollback AWS CLI command",
          "comment_id": "1014415"
        },
        {
          "poster": "Blueee",
          "upvote_count": "3",
          "content": "Selected Answer: B\nB is correct",
          "timestamp": "1704456360.0",
          "comment_id": "943560"
        },
        {
          "poster": "OrganizedChaos25",
          "upvote_count": "3",
          "content": "B is the answer I got",
          "timestamp": "1700147460.0",
          "comment_id": "899217"
        },
        {
          "timestamp": "1699958880.0",
          "comment_id": "897406",
          "content": "B is correct",
          "upvote_count": "3",
          "poster": "devnv"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:21:11.408Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "9xdIUtVAoINGpTTa5Hm5",
      "question_number": 360,
      "page": 72,
      "question_text": "A company is deploying a new application that uses Amazon EC2 instances. The company needs a solution to query application logs and AWS account API activity.\n\nWhich solution will meet these requirements?",
      "choices": {
        "C": "Use the Amazon CloudWatch agent to send logs from the EC2 instances to Amazon Kinesis. Configure AWS CloudTrail to deliver the API logs to Kinesis. Use Kinesis to load the data into Amazon Redshift. Use Amazon Redshift to query both sets of logs.",
        "D": "Use the Amazon CloudWatch agent to send logs from the EC2 instances to Amazon S3. Use AWS CloudTrail to deliver the API logs to Amazon S3. Use Amazon Athena to query both sets of logs in Amazon S3.",
        "B": "Use the Amazon CloudWatch agent to send logs from the EC2 instances to Amazon CloudWatch Logs. Configure AWS CloudTrail to deliver the API logs to CloudWatch Logs. Use CloudWatch Logs Insights to query both sets of logs.",
        "A": "Use the Amazon CloudWatch agent to send logs from the EC2 instances to Amazon CloudWatch Logs. Configure AWS CloudTrail to deliver the API logs to Amazon S3. Use CloudWatch to query both sets of logs."
      },
      "correct_answer": "B",
      "answer_ET": "B",
      "answers_community": [
        "B (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/109218-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-05-14 10:51:00",
      "unix_timestamp": 1684054260,
      "discussion_count": 6,
      "discussion": [
        {
          "poster": "ogwu2000",
          "content": "B is correct\nA - wrong because CloudWatch is not a query tool.\nC - Wrong because CloudWatch agent cant send logs directly to Kinesis. Should be from CloudWatch log\nD - Wrong because CloudWatch agent cant send logs directly to S3. Should be from CloudWatch log to firehorse to S3",
          "upvote_count": "8",
          "comment_id": "953128",
          "timestamp": "1705397220.0"
        },
        {
          "upvote_count": "5",
          "comment_id": "963967",
          "content": "Selected Answer: B\nExplanation:\nOption B provides a comprehensive solution for querying application logs and AWS account API activity. The Amazon CloudWatch agent is used to send logs from the EC2 instances to Amazon CloudWatch Logs, allowing easy access to application logs. AWS CloudTrail is configured to deliver the API logs to CloudWatch Logs, enabling monitoring and analysis of AWS account activity. Finally, CloudWatch Logs Insights is utilized to query and analyze both sets of logs efficiently.",
          "timestamp": "1706293680.0",
          "poster": "haazybanj"
        },
        {
          "upvote_count": "4",
          "content": "B is correct: <query application logs and AWS account API activity> means we need cloudwatch log and cloud trail\nC and D: cloudwatch agent cannot directly send logs to S3 or Kinesis. \nA: Cloudwatch query works only on cloudwatch, not S3",
          "comment_id": "1139445",
          "poster": "thanhnv142",
          "timestamp": "1722694500.0"
        },
        {
          "content": "Selected Answer: B\nSince Cloudwatch Insights can perform query, no need to use s3/athena.",
          "timestamp": "1704255540.0",
          "poster": "ProfXsamson",
          "comment_id": "941377",
          "upvote_count": "4"
        },
        {
          "upvote_count": "1",
          "content": "B is correct",
          "comment_id": "899218",
          "poster": "OrganizedChaos25",
          "timestamp": "1700147580.0"
        },
        {
          "content": "B is the right answer",
          "comment_id": "897408",
          "poster": "devnv",
          "timestamp": "1699959060.0",
          "upvote_count": "1"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:21:11.408Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "6pv5UDq1hMJoQj5A2eH2",
      "question_number": 361,
      "page": 73,
      "question_text": "A company wants to ensure that their EC2 instances are secure. They want to be notified if any new vulnerabilities are discovered on their instances, and they also want an audit trail of all login activities on the instances.\n\nWhich solution will meet these requirements?",
      "choices": {
        "B": "Use AWS Systems Manager to detect vulnerabilities on the EC2 instances. Install the Systems Manager Agent to capture system logs and view login activity in the CloudTrail console.",
        "A": "Use AWS Systems Manager to detect vulnerabilities on the EC2 instances. Install the Amazon Kinesis Agent to capture system logs and deliver them to Amazon S3.",
        "C": "Configure Amazon CloudWatch to detect vulnerabilities on the EC2 instances. Install the AWS Config daemon to capture system logs and view them in the AWS Config console.",
        "D": "Configure Amazon Inspector to detect vulnerabilities on the EC2 instances. Install the Amazon CloudWatch Agent to capture system logs and record them via Amazon CloudWatch Logs."
      },
      "correct_answer": "D",
      "answer_ET": "D",
      "answers_community": [
        "D (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/109219-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-05-14 10:55:00",
      "unix_timestamp": 1684054500,
      "discussion_count": 7,
      "discussion": [
        {
          "content": "Selected Answer: D\nAamzon Inspector detect software vulnerabilities and unintended network exposure in near real time in AWS workloads such as Amazon EC2, AWS Lambda functions, and Amazon ECR.",
          "poster": "ProfXsamson",
          "comment_id": "941378",
          "upvote_count": "9",
          "timestamp": "1688350800.0"
        },
        {
          "timestamp": "1706977380.0",
          "comment_id": "1139454",
          "upvote_count": "5",
          "poster": "thanhnv142",
          "content": "Selected Answer: D\nD is correct: <new vulnerabilities are discovered> means AWS inspector\nA and B: AWS SSM does not support vulnerabilities scanning\nC: Amazon CloudWatch does not support vulnerabilities scanning"
        },
        {
          "timestamp": "1748016120.0",
          "content": "Selected Answer: D\nThis is probably one of the easiest questions yet on this entire site lol, never seen a more obvious answer",
          "comment_id": "1571636",
          "upvote_count": "1",
          "poster": "92a2133"
        },
        {
          "poster": "YucelFuat",
          "comment_id": "1279527",
          "content": "Selected Answer: D\nExam tip : if you see \"vulnerabilities\" in the question -> choose the answer covers \"Amazon Inspector\"",
          "timestamp": "1725619320.0",
          "upvote_count": "3"
        },
        {
          "timestamp": "1723183560.0",
          "comment_id": "1262785",
          "content": "Selected Answer: D\nD is the right answer",
          "poster": "EVAAWS",
          "upvote_count": "1"
        },
        {
          "timestamp": "1684243320.0",
          "poster": "OrganizedChaos25",
          "upvote_count": "2",
          "comment_id": "899225",
          "content": "D is correct"
        },
        {
          "upvote_count": "2",
          "content": "D is the right answer",
          "poster": "devnv",
          "comment_id": "897411",
          "timestamp": "1684054500.0"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:21:21.853Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "i5DmEFCOEoczdHD0erMn",
      "question_number": 362,
      "page": 73,
      "question_text": "A company is running an application on Amazon EC2 instances in an Auto Scaling group. Recently, an issue occurred that prevented EC2 instances from launching successfully, and it took several hours for the support team to discover the issue. The support team wants to be notified by email whenever an EC2 instance does not start successfully.\n\nWhich action will accomplish this?",
      "choices": {
        "A": "Add a health check to the Auto Scaling group to invoke an AWS Lambda function whenever an instance status is impaired.",
        "C": "Create an Amazon CloudWatch alarm that invokes an AWS Lambda function when a failed AttachInstances Auto Scaling API call is made.",
        "D": "Create a status check alarm on Amazon EC2 to send a notification to an Amazon SNS topic whenever a status check fail occurs.",
        "B": "Configure the Auto Scaling group to send a notification to an Amazon SNS topic whenever a failed instance launch occurs."
      },
      "correct_answer": "B",
      "answer_ET": "B",
      "answers_community": [
        "B (100%)"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/109220-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-05-14 10:58:00",
      "unix_timestamp": 1684054680,
      "discussion_count": 6,
      "discussion": [
        {
          "content": "Selected Answer: B\nLikely B:\nhttps://aws.amazon.com/blogs/aws/auto-scaling-notifications-recurrence-and-more-control/\n\"EC2_INSTANCE_LAUNCH_ ERROR\"",
          "poster": "MarDog",
          "comment_id": "928808",
          "upvote_count": "7",
          "timestamp": "1703113620.0"
        },
        {
          "content": "B is correct: <wants to be notified by email> means SNS. <EC2 instances in an Auto Scaling group> means this should be triggered by auto scaling group\nA, C and D: no mention of both SNS and auto scaling group",
          "upvote_count": "4",
          "poster": "thanhnv142",
          "comment_id": "1139455",
          "timestamp": "1722695160.0"
        },
        {
          "content": "Selected Answer: B\nB is correct\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-sns-notifications.html",
          "comment_id": "924932",
          "poster": "rhinozD",
          "upvote_count": "3",
          "timestamp": "1702717200.0"
        },
        {
          "timestamp": "1700148240.0",
          "content": "I got B as my answer",
          "comment_id": "899226",
          "upvote_count": "1",
          "poster": "OrganizedChaos25"
        },
        {
          "timestamp": "1700004660.0",
          "poster": "2pk",
          "content": "Selected Answer: B\ni think B is correct , but Option A is incorrect because, this would only be triggered if an instance was already running and experiencing issues. It would not provide notification when an instance fails to launch.",
          "upvote_count": "3",
          "comment_id": "897891"
        },
        {
          "timestamp": "1699959480.0",
          "upvote_count": "1",
          "comment_id": "897415",
          "content": "B is correct",
          "poster": "devnv"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:21:21.853Z",
      "extraction_method": "api_direct_v1"
    },
    {
      "question_id": "qMgtxc3IEAq0odErUP9v",
      "question_number": 363,
      "page": 73,
      "question_text": "A company is using AWS Organizations to centrally manage its AWS accounts. The company has turned on AWS Config in each member account by using AWS CloudFormation StackSets. The company has configured trusted access in Organizations for AWS Config and has configured a member account as a delegated administrator account for AWS Config.\n\nA DevOps engineer needs to implement a new security policy. The policy must require all current and future AWS member accounts to use a common baseline of AWS Config rules that contain remediation actions that are managed from a central account. Non-administrator users who can access member accounts must not be able to modify this common baseline of AWS Config rules that are deployed into each member account.\n\nWhich solution will meet these requirements?",
      "choices": {
        "B": "Create an AWS Config conformance pack that contains the AWS Config rules and remediation actions. Deploy the pack from the Organizations management account by using CloudFormation StackSets.",
        "D": "Create an AWS Config conformance pack that contains the AWS Config rules and remediation actions. Deploy the pack from the delegated administrator account by using AWS Config.",
        "A": "Create a CloudFormation template that contains the AWS Config rules and remediation actions. Deploy the template from the Organizations management account by using CloudFormation StackSets.",
        "C": "Create a CloudFormation template that contains the AWS Config rules and remediation actions. Deploy the template from the delegated administrator account by using AWS Config."
      },
      "correct_answer": "D",
      "answer_ET": "D",
      "answers_community": [
        "D (88%)",
        "13%"
      ],
      "topic": "1",
      "exam_id": 24,
      "is_multiple_choice": true,
      "url": "https://www.examtopics.com/discussions/amazon/view/108806-exam-aws-certified-devops-engineer-professional-dop-c02/",
      "question_images": [],
      "answer_images": [],
      "timestamp": "2023-05-09 14:25:00",
      "unix_timestamp": 1683635100,
      "discussion_count": 12,
      "discussion": [
        {
          "content": "Selected Answer: D\nOption D. Create an AWS Config conformance pack that contains the AWS Config rules and remediation actions. Deploy the pack from the delegated administrator account by using AWS Config.\n\nConformance packs are a collection of AWS Config rules and remediation actions that can be easily deployed as a single entity in an account and a region, and across an organization in AWS Organizations. These packs are created and managed from a central account, and help to establish a secure and compliant posture for your accounts. Non-administrator users can view the AWS Config rules within a conformance pack but they cannot modify them. AWS Config conformance packs are therefore a good fit for achieving the desired control and security policy.\n\nThe other options, while potentially viable for deploying Config rules, do not inherently protect the baseline AWS Config rules from being modified by non-administrator users in the member accounts.",
          "poster": "Certified101",
          "upvote_count": "10",
          "timestamp": "1704965880.0",
          "comment_id": "948790"
        },
        {
          "content": "Selected Answer: D\nD is correct: <a common baseline of AWS Config rule> means conformance pack. <a member account as a delegated administrator account for AWS Config> means delegated admin\nA and C: no mentionf of conformance pack\nB: should deploy this using AWS config and in the delegated account, not the management account",
          "upvote_count": "5",
          "timestamp": "1722695880.0",
          "comment_id": "1139465",
          "poster": "thanhnv142"
        },
        {
          "timestamp": "1743289080.0",
          "comment_id": "1411884",
          "content": "Selected Answer: B\nption B is the most appropriate solution because it leverages AWS Config conformance packs, which are purpose-built for applying and managing AWS Config rules across multiple accounts. Deploying the conformance pack from the central management account using CloudFormation StackSets ensures that the security policy is applied uniformly across all accounts in the organization, and it restricts non-administrator users from modifying the baseline.",
          "upvote_count": "1",
          "poster": "Srikantha"
        },
        {
          "content": "The question says\n'The policy must require all current and future AWS member accounts to use a common baseline of AWS Config rules'\nDoes D account for that?",
          "comment_id": "1207914",
          "timestamp": "1730997540.0",
          "poster": "MalonJay",
          "upvote_count": "1"
        },
        {
          "upvote_count": "5",
          "comment_id": "943931",
          "content": "Selected Answer: D\nNot sure why some people are saying B.\nA= CFN cannot protect the config.\nB= Yes technically, where is the actual CONFIG management plane? Its in the delegated admin account, which is not the management account = delegated admin config account will have no idea of management account config.\nC= CFN cannot protect config.\nD= Yes. Delegated CONFIG account can config on orgz level & protect the rules. Only logical option.",
          "poster": "lunt",
          "timestamp": "1704478740.0"
        },
        {
          "comment_id": "927885",
          "poster": "allen_devops",
          "upvote_count": "1",
          "content": "D is correct. Deploying via Cloudformation StackSet cannot make sure that the aws config itself is not modified by the member accounts. Deploy aws organizational rule will achieve both permission restriction and auto deployment\n\nhttps://docs.aws.amazon.com/config/latest/developerguide/config-rule-multi-account-deployment.html",
          "timestamp": "1703023740.0"
        },
        {
          "content": "Selected Answer: D\nD is correct\nhttps://aws.amazon.com/blogs/mt/deploying-conformance-packs-across-an-organization-with-automatic-remediation/",
          "poster": "rhinozD",
          "upvote_count": "3",
          "timestamp": "1702717920.0",
          "comment_id": "924940"
        },
        {
          "poster": "Nickexams",
          "upvote_count": "1",
          "comment_id": "909415",
          "timestamp": "1701270120.0",
          "content": "option B is the most appropriate solution for centrally managing and enforcing the common baseline of AWS Config rules across all member accounts while ensuring that non-administrator users cannot modify the rules."
        },
        {
          "poster": "stream3652",
          "upvote_count": "2",
          "timestamp": "1701163320.0",
          "comment_id": "908402",
          "content": "Can't you use D?"
        },
        {
          "comments": [
            {
              "content": "the question tells you there's a \"delegated account\". so your answer should be looking for that account in your answer choices as well.",
              "comment_id": "1115445",
              "timestamp": "1720293600.0",
              "poster": "Jaguaroooo",
              "upvote_count": "2"
            },
            {
              "poster": "rhinozD",
              "comments": [
                {
                  "content": "It's B\nhttps://catalog.us-east-1.prod.workshops.aws/workshops/7bb9fd8f-d049-4163-98e3-5c0cbb211f0c/en-US/enable-custom-conformance-pack-using-stacksets",
                  "upvote_count": "1",
                  "poster": "emupsx1",
                  "timestamp": "1703063280.0",
                  "comment_id": "928224"
                }
              ],
              "upvote_count": "1",
              "content": "No, you just need the manager account to deploy the comformance pack to all organization.",
              "timestamp": "1702717980.0",
              "comment_id": "924941"
            }
          ],
          "comment_id": "897895",
          "poster": "2pk",
          "timestamp": "1700005080.0",
          "content": "Selected Answer: B\ni think its B, because AWS Config conformance packs are a way to package AWS Config rules and remediation actions into a single, shareable entity. With AWS Organizations, you can use CloudFormation StackSets to deploy conformance packs across all member accounts in your organization. This allows you to centrally manage the deployment of AWS Config rules and remediation actions across multiple AWS accounts. By deploying the conformance pack from the Organizations management account, you can ensure that non-administrator users cannot modify the baseline rules deployed to each member account.",
          "upvote_count": "3"
        },
        {
          "timestamp": "1699959780.0",
          "upvote_count": "3",
          "content": "D is the right answer",
          "poster": "devnv",
          "comment_id": "897419"
        },
        {
          "upvote_count": "5",
          "poster": "Jeanphi72",
          "timestamp": "1699539900.0",
          "comment_id": "893086",
          "content": "Selected Answer: D\nhttps://docs.aws.amazon.com/config/latest/developerguide/conformance-packs.html"
        }
      ],
      "answer_description": "",
      "extracted_at": "2025-12-24T14:21:21.853Z",
      "extraction_method": "api_direct_v1"
    }
  ],
  "statistics": {
    "questions_per_page_avg": 4.97,
    "topics": [
      "1"
    ],
    "question_types": {
      "multiple_choice": 363,
      "other": 0
    },
    "has_images": {
      "question_images": 11,
      "answer_images": 0
    },
    "discussion_stats": {
      "with_discussion": 363,
      "total_comments": 3468
    }
  }
}